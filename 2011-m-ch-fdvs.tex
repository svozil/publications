\chapter{Finite-dimensional vector spaces}
\label{ch:lvs}

\newthought{Vector Spaces} are prevalent in physics;
\marginnote{{\it ``I would have written a shorter letter, but I did not have the time.''}
(Literally: {\it ``I made this [letter] very long, because I did not have the leisure to make it shorter.''})
Blaise Pascal, {\it Provincial Letters: Letter XVI (English Translation)}
}
they are essential for an understanding
of mechanics, relativity theory, quantum mechanics, and statistical physics.


\section{Basic definitions}

In what follows excerpts from Halmos' beautiful treatment
``Finite-Dimensional Vector Spaces'' will be reviewed \cite{halmos-vs}.
Of course, there exist zillions of other very nice presentations, among them
Greub's ``Linear algebra,'' and
Strang's ``Introduction to Linear Algebra,''
among many others, even freely downloadable  ones
\cite{Greub75,Strang:2009:ILA,Homes-rorres,lipschutz:schaul-la,Hefferon}
%Homes and Chris Rorres'  ``Elementary Linear Algebra: Applications Version'' \cite{Homes-rorres},
%Mathews' ``Elementary Linear Algebra'' \cite{Mathews-LA} (an eprint),
%or Schaum's Outlines \cite{lipschutz:schaul-la}, to name just a very few among the many
competing for your attention.


The more physically oriented notation in Mermin's book on
quantum information theory \cite{mermin-04,mermin-07} is adopted.
Vectors are typed in bold face.
The overline sign stands for complex conjugation; that is,
if
${a}= \Re a +i\Im a $ is a complex number, then
$\overline{a}= \Re a -i\Im a$.

Unless stated differently, only
finite-dimensional vector spaces are considered.

\subsection{Fields of real and complex numbers}

In physics, scalars are either real or complex numbers and their associated fields.
Thus we shall restrict our attention to these cases.

A {\em field}  $\langle  {\Bbb F} , +, \cdot , -, ^{-1}, 0, 1\rangle$
\index{field}
is a set together with two operations,
usually called {\em addition} and {\em multiplication}, and denoted by ``$+$'' and ``$\cdot$''
(often  ``$a\cdot b$'' is identified with the expression ``$ab$'' without the center dot)
respectively, such that the following axioms hold:
\begin{itemize}
\item[(i)]
closure of ${\Bbb F}$ under addition and multiplication
for all $a, b \in {\Bbb F}$, both $a + b$ and $a   b$ are in ${\Bbb F}$;
\item[(ii)]
associativity of addition and multiplication:
for all $a$, $b$, and $c$ in ${\Bbb F}$,
the following equalities hold: $a + (b + c) = (a + b) + c$,
and
$a (b c) = (a  b) c$;
\item[(iii)]
commutativity of addition and multiplication:
for all a and b in F, the following equalities hold: $a + b = b + a$ and $a b = b  a$;
\item[(iv)]
additive and multiplicative identity:
There exists an element of ${\Bbb F}$,
called the additive identity element and denoted by $0$, such that for all $a$ in ${\Bbb F}$,
$a + 0 = a$.
Likewise, there is an element, called the multiplicative identity element and denoted by $1$,
such that for all $a$ in ${\Bbb F}$, $1 \cdot a  = a$.
(To exclude the trivial ring, the additive identity and the multiplicative
identity are required to be distinct.)
\item[(v)]
Additive and multiplicative inverses:
for every $a$ in ${\Bbb F}$, there exists an element $-a$ in ${\Bbb F}$, such that $a + (-a) = 0$.
Similarly, for any $a$ in ${\Bbb F}$ other than $0$, there exists an element $a^{-1}$ in ${\Bbb F}$,
such that $a \cdot a^{-1} = 1$.
(The elements $+ (-a)$ and  $a^{-1}$
are also denoted $-a$ and $\frac{1}{b}$, respectively.)
Stated differently: subtraction and division operations exist.
\item[(vi)]
Distributivity of multiplication over addition
For all $a$, $b$ and $c$ in ${\Bbb F}$, the following equality holds:
$a (b + c) = (a  b) + (a  c)$.
\end{itemize}

\subsection{Vectors and vector space}
\marginnote{For proofs and additional information see \S 2 in   \cite{halmos-vs}}


A {\em linear vector space}      $\langle  \frak V , +, \cdot , -,  0, 1\rangle$
\index{linear vector space}
is a set $\frak V$ of elements called {\em vectors}
satisfying certain axioms; among them,
with respect to addition of vectors:
\begin{itemize}
\item[(i)]
commutativity,

\item[(ii)]associativity,

\item[(iii)]
the uniqueness of the origin or null vector $0$,
as well as
\item[(iv)]
the uniqueness of  the negative vector;
\item[ ]
with respect to multiplication of vectors with scalars associativity:
\item[(v)]
the existence of a unit factor $1$; and
\item[(vi)]
distributivity with respect to scalar and vector additions, that is
\begin{equation}
\begin{array}{l}
(\alpha +\beta ){\bf x}
=   \alpha {\bf x}+\beta  {\bf x}, \\
\alpha ({\bf x} +{\bf y})
=   \alpha {\bf x}+\alpha {\bf y}$, with ${\bf x}, {\bf y},
\in \frak V$ and scalars $\alpha ,\beta,
\end{array}
\end{equation}
respectively.
\end{itemize}

{
\color{blue}
\bexample
Examples of vector spaces are:
\begin{itemize}
\item[(i)]
The set ${\Bbb C}$ of complex numbers: ${\Bbb C}$  can be interpreted as a complex vector space by  interpreting as vector addition and scalar multiplication
as the usual addition and multiplication of complex numbers, and with $0$ as the null vector;
\item[(ii)]
The set ${\Bbb C}^n$, $n \in {\Bbb N}$ of $n$-tuples of complex numbers:
Let
${\bf x}=
(x_1,\ldots , x_n)$
and
${\bf y}=
(y_1,\ldots , y_n)$.
 ${\Bbb C}^n$  can be interpreted as a complex vector space by  interpreting as vector addition
 and scalar multiplication
as the ordinary addition  $ {\bf x} + {\bf y} =  (x_1+y_1,\ldots , x_n+y_n)$
and the multiplication $\alpha {\bf x}=
(\alpha  x_1,\ldots ,\alpha  x_n)$ by a complex number $\alpha$, respectively;
the null tuple $0 =
(0,\ldots ,0)$ is the neutral element of vector addition;
\item[(iii)]
The set ${\frak P}$
 of all polynomials with complex coefficients in a variable $t$:
${\frak P}$  can be interpreted as a complex vector space by  interpreting as vector addition and scalar multiplication
as the ordinary addition of polynomials and the multiplication of a polynomial by a complex number, respectively;
the null polynomial is the neutral element of vector addition.  \eexample
\end{itemize}
}


\section{Linear independence}

A set ${\frak S}=\{
{\bf x}_1,
{\bf x}_2,
\ldots ,
{\bf x}_k\} \subset {\frak V}$
of vectors ${\bf x}_i$ in a linear vector space
is {\em linear independent}
\index{linear independence}
if no vector  can be written as a linear combination of other vectors in this set ${\frak S}$;
that is,
${\bf x}_i=\sum_{1\le j \le k, \; j\neq i}\alpha_j {\bf x}_j$.

Equivalently, linear independence of the vectors in $\frak B$ means that
 no vector in  $\frak S$ can be written as a linear combinations of others in  $\frak S$.
That is, let  $\{
{\bf x}_1, \ldots , {\bf x}_k\}$;
if $\sum_{i=1}^n \alpha_i {\bf x}_i = 0$
implies $\alpha_i =0$ for each $i$, then the set
${\frak S}=\{
{\bf x}_1,
{\bf x}_2,
\ldots ,
{\bf x}_k\}  $ is linearly independent.

\section{Subspace}
\label{2011-m-subspace}
\marginnote{For proofs and additional information see \S 10 in  \cite{halmos-vs}}
A nonempty subset ${\frak M}$ of a vector space is a {\em subspace}
\index{subspace}
or, used synonymuously,
a {\em linear manifold}
\index{linear manifold}
if, along with every pair of vectors ${\bf x}$   and  ${\bf y}$
contained in  ${\frak M}$,
every linear combination
$\alpha {\bf x} + \beta {\bf y}$ is also contained in  ${\frak M}$.

If
${\frak U}$
and
${\frak V}$
are two subspaces of a vector space,
then
${\frak U}+{\frak V}$
is the subspace spanned by
${\frak U}$
and
${\frak V}$;
that is,
the set of all vectors
${\bf z}={\bf x}+{\bf y}$, with
${\bf x}\in {\frak U}$  and
${\bf y}\in {\frak V}$.

${\frak M}$ is the {\em linear span}
\index{linear span}
\index{span}
\begin{equation}
{\frak M}
= \textrm{span}({\frak U},{\frak V})
= \textrm{span}({\bf x},{\bf y}) =
\{\alpha {\bf x} +\beta {\bf y}\mid \alpha ,\beta \in {\Bbb F}, {\bf x} \in {\frak U},
{\bf x} \in {\frak V}\}.
\end{equation}


A generalization to more than two vectors and more than two subspaces is straightforward.


For every vector space ${\frak V}$, the set
containing the null vector $\{0\}$,
and the vector space ${\frak V}$ itself are subspaces of ${\frak V}$.

\subsection{Scalar or inner product
[\S61]}
\index{scalar product}
\index{inner product}
\label{2011-m-scalarproduct}

A {\em scalar} or {\em inner} product presents some form of measure of ``distance'' or ``apartness''
of two vectors in a linear vector space.
It should not be confused with the bilinear functionals (introduced on page \pageref{2011-m-dvs}) that connect a vector space with its dual vector space,
although for real Euclidean vector spaces these may coincide,
and although the scalar product is also bilinear in its arguments.
It should also not be confused with the tensor product introduced on page \pageref{2011-m-tensorp}.

An inner product space is a vector space $\frak V$,
together with an inner product; that is, with a map
 $\langle \cdot \mid \cdot \rangle :  \frak V  \times  \frak V  \longrightarrow {\Bbb R}$
or ${\Bbb C}$, in general ${\Bbb F}$,
that satisfies the following three axioms for all vectors  and all scalars:

\begin{itemize}
\item[(i)]
Conjugate symmetry:
$
\langle {\bf x}\mid {\bf y}\rangle
=
\overline{\langle {\bf y}\mid {\bf x}\rangle }$.
\marginnote{For real, Euclidean vector spaces, this function is symmetric; that is
$
\langle {\bf x}\mid {\bf y}\rangle
=
{\langle {\bf y}\mid {\bf x}\rangle}$.
}
\item[(ii)]
Linearity in the first (and second) argument:
$$
\langle \alpha {\bf x} +\beta {\bf y}\mid {\bf z}\rangle
=
\alpha {\langle {\bf x}\mid {\bf z}\rangle}
+\beta {\langle {\bf v}\mid {\bf z}\rangle}
.
$$

\item[(ii)]
Positive-definiteness:
$
\langle {\bf x}\mid {\bf x}\rangle
\ge
0$;  with equality only for ${\bf x} = 0$.
\end{itemize}

The {\em norm} of a vector ${\bf x}$
\index{norm}
is defined by
\begin{equation}
\|
{\bf x}
\|
=\sqrt{\langle {\bf x}\mid {\bf x}\rangle }
\end{equation}

{
\color{blue}
\bexample
One example is the
{\em dot product}
\index{dot product}
\begin{equation}
\langle  {\bf x} \vert {\bf y} \rangle
=
\sum_{i=1}^n \overline{x_i}y_i
\end{equation}
of two vectors ${\bf x}=
(x_1,\ldots , x_n)$
and
${\bf y}=
(y_1,\ldots , y_n)$ in ${\Bbb C}^n$,
which, for real Euclidean space,  reduces to the well-known dot product
$\langle  {\bf x} \vert {\bf y} \rangle
=
{x_1}y_1 + \cdots + {x_n}y_n  = \| {\bf x}\| \| {\bf y}\| \cos \angle ({\bf x},{\bf y})$.


It is mentioned without proof that the most general form of an inner product in ${\Bbb C}^n$
is
$\langle  {\bf x} \vert {\bf y} \rangle
=  {\bf y} \textsf{\textbf{A}} {\bf x}^\dagger$,
where the symbol ``$\dagger$'' stands for the conjugate transpose, or Hermitian conjugate,
and $ \textsf{\textbf{A}} $ is a positive definite Hermitian matrix (all of its eigenvalues are positive).
\eexample
}

Two nonzero vectors $  {\bf x} , {\bf y} \in {\frak V}$,  $  {\bf x},   {\bf y}\neq 0$
are {\em orthogonal}, denoted by ``${\bf x} \perp {\bf y}$''
\index{othogonality}
if their scalarproduct vanishes; that is, if
\begin{equation}
\langle  {\bf x} \vert {\bf y} \rangle   = 0.
\end{equation}


Let ${\frak E}$ be any set of vectors in an inner product space ${\frak V}$.
The symbols
\begin{equation}
{\frak E}^\perp  =\{ {\bf x}\mid  \langle  {\bf x} \vert {\bf y} \rangle=0,  {\bf x} \in {\frak V},
\forall {\bf y} \in {\frak E}
\end{equation}
 denote the set of all vectors in  ${\frak V}$ that are
orthogonal to every vector in  ${\frak E}$.

Note that, regardless of whether or not ${\frak E}$ is a subspace
\marginnote{See page \pageref {2011-m-subspace} for a definition of subspace.}
(${\frak E}$ may be just vectors of an incomplete basis),
${\frak E}^\perp$ is a subspace.
Furthermore,
${\frak E}$  is contained in $({\frak E}^\perp)^\perp= {\frak E}^{\perp\perp}$.
In case ${\frak E}$ is a subspace, we call ${\frak E}^\perp$
the {\em orthogonal complement}
\index{orthogonal complement}
of ${\frak E}$.

The following {\em projection theorem}
\index{projection theorem}
is mentioned without proof.
If ${\frak M}$ is any subspace of a finite-dimensional inner product space ${\frak V}$,
then ${\frak V}$ is the direct sum of ${\frak M}$ and ${\frak M}^\perp$;
that is, ${\frak M}^{\perp \perp}={\frak M}$.

{
\color{blue}
\bexample
For the sake of an example, suppose ${\frak V}={\Bbb R}^2$,
and take ${\frak E}$ to be the set of all vectors spanned by the vector $(1,0)$;
then ${\frak E}^\perp$ is the set of all vectors spanned by $(0,1)$.
\eexample
}


\subsection{Hilbert space}


A (quantum mechanical) {\em Hilbert space} is a linear
\index{Hilbert space}
vector space ${\frak V}$ over the field ${\Bbb C}$ of complex numbers
equipped with vector addition, scalar multiplication, and some scalar product.
Furthermore, {\em closure} is an additional requirement,
but nobody has made operational sense of that so far:
If ${\bf x}_n\in {\frak V}$, $n=1,2,\ldots$, and if $\lim_{n,m\rightarrow
\infty} ({\bf x}_n-{\bf x}_m,{\bf x}_n-{\bf x}_m)=0$,
then there exists an ${\bf x}\in {\frak V}$ with
$\lim_{n\rightarrow \infty} ({\bf x}_n-{\bf x},{\bf x}_n-{\bf x})=0$.





Infinite dimensional vector spaces and continuous spectra are nontrivial
extensions of the finite
dimensional Hilbert space treatment. As a heuristic rule, which is not
always correct, it might be
stated that the sums become integrals, and the Kronecker delta function
$\delta_{ij}$ defined by
\index{Kronecker delta function}
\begin{equation}
\delta_{ij} =\cases{0, &for $i\neq j$;\cr
                           1, &for $i = j$\cr}.
\end{equation}
becomes the Dirac delta function $\delta (x-y)$, which is a
generalized function in the continuous variables $x,y$.
In the Dirac bra-ket notation, unity is given by
${\bf 1}=\int_{-\infty}^{+\infty} \vert x\rangle \langle  x\vert \, dx$.
For a careful treatment, see, for instance,
the books by
Reed and Simon \cite{reed-sim1,reed-sim2}.


\section{Basis}
\marginnote{For proofs and additional information see \S 7 in    \cite{halmos-vs}}
A (linear) {\em basis}
\index{basis}
 (or a {\em coordinate system})
\index{coordinate system}
is a set    $\frak B$
of linearly independent vectors
so that every vector
in $\frak V$ is a linear combination of the vectors in the basis; hence
$\frak B$ spans $\frak V$.

A vector space is finite dimensional if its basis is finite; that is,   its basis
contains a finite number of elements.


\section{Dimension
[\S8]}
The {\em dimension}
\index{dimension}
of $\frak V$ is the number of elements in $\frak B$ ;
all bases $\frak B$ contain the same number of elements.


What basis should one choose?
Note that a vector is some directed entity with a particular length,
oriented in some (vector) ``space.''
It is ``laid out there'' in front of our eyes, as it is: some directed quantity.
{\it A priori}, this space, in its most primitive form,
is not equipped with a basis, or synonymuously, frame of reference, or reference frame.
Insofar it is not yet coordinatized.
In order to formalize the notion of a vector, we have to code this vector.
As for numbers (e.g., by different bases, or by prime decomposition),
there exist many ``competing'' ways to code a vector.

Some of these ways are rather straightforward, such as, in particular, the {\em Cartesian basis},
or, used synonymuosly, the  {\em standard basis}.
Other bases are less suggestive at first; alas it may be ``economical'' or pragmatical to use them;
mostly to cope with, and adapt to, the {\em symmetry} of a physical configuration:
if the physical situation at hand is, for instance, rotationally invariant,
we might want to use rotationally invariant bases --
such as, for instance polar coordinares in two dimensions, or spherical coordinates in three dimensions --
to represent a vector, or, more generally, to code any given physical entity
(e.g., tensors, operators) by such bases.

{\color{Purple}
In quantum physics, the dimension of a quantized system is associated with
the {\em number of mutually exclusive measurement outcomes}.
For a spin state measurement of an electron
along a particular direction,
as well as for a measurement of the linear polarization
of a photon in a particular direction,
the dimension is two, since both measurements
may yield two distingt outcomes
$
\mid \uparrow \rangle    =\mid + \rangle$ {\it versus} $
\mid \downarrow \rangle   =\mid - \rangle
$,
and
$
\mid H \rangle $ {\it versus} $
\mid V \rangle
$, respectively.
}

\section{Coordinates [\S46]}
The coordinates of a vector with respect to some basis
represent the coding of that vector in that particular basis.
It is important to realize that, as bases change, so do coordinates.
Indeed, the changes in coordinates have to ``compensate'' for the bases change,
because the same coordinates in a different basis would render an altogether different
vector.
Figure \ref{2011-m-bases} presents some geometrical demonstration of
these thoughts, for your contemplation.

\begin{figure}[ht]
\caption{Coordinazation of vectors:
(a) some primitive vector;
(b)  some primitive vectors, laid out in some space, denoted by dotted lines
(c) vector coordinates $x_2$ and $x_2$ of the vector  ${\bf x} =  (x_1,x_2) = x_1{\bf e}_1 +  x_2{\bf e}_2$ in a standard basis;
(d) vector coordinates $x_2'$ and $x_2'$ of the vector  ${\bf x} = (x_1',x_2') =  x_1'{\bf e}_1' +  x_2'{\bf e}_2'$  in some nonorthogonal basis.
\label{2011-m-bases}}
\begin{center}
\begin{tabular}{cc}

%TeXCAD (http://texcad.sf.net/) Picture. File: [p1.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{4.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(60,60)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
\end{picture}
&
%TeXCAD (http://texcad.sf.net/) Picture. File: [p2.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(79.375,76)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
%\dottedbox(1,1)(78.375,75)
\multiput(.93,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\multiput(.93,.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(.93,75.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(79.305,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
%\end
\end{picture}
\\
(a)&(b)\\
$\;$\\
%TeXCAD (http://texcad.sf.net/) Picture. File: [p3.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(79.375,76)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
%\dottedbox(1,1)(78.375,75)
\multiput(.93,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\multiput(.93,.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(.93,75.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(79.305,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
%\end
\thinlines
{\color{blue}
\put(20,20){\vector(0,1){50}}
\put(20,20){\vector(1,0){50}}
}
%\dashline{1}(60,20)(60,60)
\put(59.93,19.93){\line(0,1){.9756}}
\put(59.93,21.881){\line(0,1){.9756}}
\put(59.93,23.832){\line(0,1){.9756}}
\put(59.93,25.783){\line(0,1){.9756}}
\put(59.93,27.735){\line(0,1){.9756}}
\put(59.93,29.686){\line(0,1){.9756}}
\put(59.93,31.637){\line(0,1){.9756}}
\put(59.93,33.588){\line(0,1){.9756}}
\put(59.93,35.539){\line(0,1){.9756}}
\put(59.93,37.491){\line(0,1){.9756}}
\put(59.93,39.442){\line(0,1){.9756}}
\put(59.93,41.393){\line(0,1){.9756}}
\put(59.93,43.344){\line(0,1){.9756}}
\put(59.93,45.296){\line(0,1){.9756}}
\put(59.93,47.247){\line(0,1){.9756}}
\put(59.93,49.198){\line(0,1){.9756}}
\put(59.93,51.149){\line(0,1){.9756}}
\put(59.93,53.1){\line(0,1){.9756}}
\put(59.93,55.052){\line(0,1){.9756}}
\put(59.93,57.003){\line(0,1){.9756}}
\put(59.93,58.954){\line(0,1){.9756}}
%\end
%\dashline{1}(60,60)(20,60)
\put(59.93,59.93){\line(-1,0){.9756}}
\put(57.978,59.93){\line(-1,0){.9756}}
\put(56.027,59.93){\line(-1,0){.9756}}
\put(54.076,59.93){\line(-1,0){.9756}}
\put(52.125,59.93){\line(-1,0){.9756}}
\put(50.174,59.93){\line(-1,0){.9756}}
\put(48.222,59.93){\line(-1,0){.9756}}
\put(46.271,59.93){\line(-1,0){.9756}}
\put(44.32,59.93){\line(-1,0){.9756}}
\put(42.369,59.93){\line(-1,0){.9756}}
\put(40.418,59.93){\line(-1,0){.9756}}
\put(38.466,59.93){\line(-1,0){.9756}}
\put(36.515,59.93){\line(-1,0){.9756}}
\put(34.564,59.93){\line(-1,0){.9756}}
\put(32.613,59.93){\line(-1,0){.9756}}
\put(30.661,59.93){\line(-1,0){.9756}}
\put(28.71,59.93){\line(-1,0){.9756}}
\put(26.759,59.93){\line(-1,0){.9756}}
\put(24.808,59.93){\line(-1,0){.9756}}
\put(22.857,59.93){\line(-1,0){.9756}}
\put(20.905,59.93){\line(-1,0){.9756}}
%\end
\put(68.375,12.125){\makebox(0,0)[cc]{${\bf e}_{ 1}$}}
\put(11.375,68.125){\makebox(0,0)[cc]{${\bf e}_{ 2}$}}
\put(38.125,16.625){\makebox(0,0)[cc]{$x_1$}}
\put(13.375,43.375){\makebox(0,0)[cc]{$x_2$}}
\put(15,15){\makebox(0,0)[cc]{$0$}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
\end{picture}
&
%TeXCAD (http://texcad.sf.net/) Picture. File: [p4.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(79.375,76)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
%\dottedbox(1,1)(78.375,75)
\multiput(.93,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\multiput(.93,.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(.93,75.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(79.305,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
%\end
\thinlines
%\vector(20,20)(74.375,37.5)
{\color{blue}
\put(74.375,37.5){\vector(3,1){.07}}\multiput(20,20)(.1047687861,.0337186898){519}{\line(1,0){.1047687861}}
%\end
%\vector(20,20)(42.75,71.5)
\put(42.75,71.5){\vector(1,2){.07}}\multiput(20,20)(.0337037037,.0762962963){675}{\line(0,1){.0762962963}}
%\end
}
%\dashline{1}(60,60)(44.25,28)
\multiput(59.93,59.93)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(59.078,58.2)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(58.227,56.47)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(57.376,54.741)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(56.524,53.011)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(55.673,51.281)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(54.822,49.551)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(53.97,47.822)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(53.119,46.092)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(52.268,44.362)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(51.416,42.632)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(50.565,40.903)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(49.713,39.173)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(48.862,37.443)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(48.011,35.713)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(47.159,33.984)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(46.308,32.254)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(45.457,30.524)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(44.605,28.795)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
%\end
%\dashline{1}(60,60)(32.625,48.75)
\multiput(59.93,59.93)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(58.164,59.204)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(56.397,58.478)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(54.631,57.752)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(52.865,57.026)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(51.099,56.301)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(49.333,55.575)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(47.567,54.849)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(45.801,54.123)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(44.035,53.397)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(42.268,52.672)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(40.502,51.946)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(38.736,51.22)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(36.97,50.494)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(35.204,49.768)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(33.438,49.043)(-.080279,-.032991){11}{\line(-1,0){.080279}}
%\end
\put(36.5,19.875){\makebox(0,0)[cc]{${x_1}'$}}
\put(20.5,39.625){\makebox(0,0)[cc]{${x_2}'$}}
\put(72.375,29.125){\makebox(0,0)[cc]{${\bf e}_{ 1}'$}}
\put(33.72,70){\makebox(0,0)[cc]{${\bf e}_{ 2}'$}}
\put(15,15){\makebox(0,0)[cc]{$0$}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
\end{picture}
\\
(c)&(d)\\
\end{tabular}
\end{center}
\end{figure}

\marginnote{Elementary high school tutorials often condition students into believing that the components of the vector
``is'' the vector, rather then emphasizing that these components {\em represent}
the vector with respect to some (mostly implicitly assumed) basis.
A similar problem occurs in many introductions to quantum theory,
where the span
(i.e., the onedimensional linear subspace spanned by that vector)
\index{span}
$\{
{\bf y}
\mid
{\bf y} = \alpha {\bf x}, \alpha \in {\Bbb C}
\}$, or, equivalently,  for orthogonal prjections,
the {\em projector} (i.e., the projection operator; see also page \pageref{2011-m-projec})
\index{projector}
$\textsf{\textbf{E}}_{\bf x} = {\bf x}^T \otimes {\bf x}$
corresponding to a unit (of length $1$) vector ${\bf x}$
often is identified with that vector.
In many instances, this is a great help and,
if administered properly, is consistent and fine (fapp).}

The standard (Cartesian) basis in $n$-dimensional complex space ${\Bbb C}^n$
is the set of vectors $x_i, i=1, \ldots , n$, represented by $n$-tuples,
defined by the condition that the $i$'th coordinate of the $j$'th basis vector
${\bf e}_j$ is given by $\delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta function
\index{Kronecker delta function}
\begin{equation}
\delta_{ij} =\cases{0, &for $i\neq j$;\cr
                           1, &for $i = j$\cr}.
\end{equation}
Thus,
\begin{equation}
\begin{array}{lcl}
{\bf e}_1&=&(1,0,\ldots,0),\\
{\bf e}_2&=&(0,1,\ldots,0),\\
&\vdots&\\
{\bf e}_n&=&(0,0,\ldots,1).
\end{array}
\end{equation}


In terms of these standard base vectors, every vector ${\bf x}$
can be written as a linear combination
\begin{equation}
{\bf x} = \sum_{i=1}^n x_i{\bf e}_i = (x_1,x_2, \ldots , x_n),
\end{equation}
or, in ``dot product notation,''
that is,
``column times row''
and
``row times column;'' the dot is usually omitted (the superscript ``$T$'' stands for transposition),
\begin{equation}
{\bf x} = (x_1,x_2, \ldots , x_n)^T
\cdot
 ({\bf e}_1,{\bf e}_2, \ldots , {\bf e}_n)
=
\left(
\begin{array}{c}
x_1\\x_2\\ \vdots \\ x_n
\end{array}
\right)
({\bf e}_1,{\bf e}_2, \ldots , {\bf e}_n)
,
\end{equation}
(the superscript ``$T$'' stands for transposition)
of the product of the coordinates $x_i$  with respect to that standard basis.
Here the equality sign ``$=$'' really means ``coded with respect to that standard basis.''

In what follows, we shall often identify the column vector
$$
\left(
\begin{array}{c}
x_1\\x_2\\ \vdots \\ x_n
\end{array}
\right)
$$
containing the coordinates of the vector ${\bf x}$
with the vector ${\bf x}$, but we always need to keep in mind that
the tuples of coordinates are defined only with respect to a particular basis
$\{ {\bf e}_1,{\bf e}_2, \ldots , {\bf e}_n \}$; otherwise these numbers lack any meaning whatsoever.

Indeed, with respect to some arbitrary  basis ${\frak B}=\{
{\bf f}_1, \ldots , {\bf f}_n\}$ of some $n$-dimensional vector space ${\frak V}$
with the base vectors ${\bf f}_i$, $1\le i\le n$, every vector ${\bf x}$ in  ${\frak V}$
can be written as a unique linear combination
\begin{equation}
{\bf x} = \sum_{i=1}^n x_i{\bf f}_i = (x_1,x_2, \ldots , x_n)
\end{equation}
of the product of the coordinates $x_i$ with respect to the basis  ${\frak B}$.

{\color{OliveGreen}
\bproof
The uniqueness of the coordinates is proven indirectly by {\em reductio ad absurdum:}
Suppose there is another decomposition
${\bf x} = \sum_{i=1}^n y_i{\bf f}_i = (y_1,y_2, \ldots , y_n) $;
then by subtraction, $0 = \sum_{i=1}^n (x_i-y_i) {\bf f}_i = (0,0, \ldots , 0)$.
Since the basis vectors ${\bf f}_i$ are linearly independent,
this can only be valid if all coefficients in the summation  vanish;
thus $x_i-y_i=0$ for all $1\le i\le n$; hence finally  $x_i=y_i$ for all $1\le i\le n$.
This is in contradiction with our assumption that the coordinates $x_i$ and $y_i$
(or at least some of them) are different.
Hence the only consistent alternative is the assumption that, with respect to a given basis, the coordinates are uniquely determined.
\eproof
}

A  set    $\frak B = \{
{\bf a}_1, \ldots , {\bf a}_n\}$
of  vectors
is {\em orthonormal}
\index{orthonormal}
if, whenever  for both
${\bf a}_i$ and
${\bf a}_j$  which
are in
${\frak B}$
it follows that
\begin{equation}
\langle {\bf a}_i \mid {\bf a}_j \rangle =\delta_{ij}.
\end{equation}
Any such set is called {\em complete}
\index{completeness}
if it is not contained in any larger orthonormal set.
Any complete set is a basis.


\section{Finding orthogonal bases from nonorthogonal ones}

A {\em Gram-Schmidt process} is a systematic method for orthonormalising a set of vectors
\index{Gram-Schmidt process}
\index{scalar product}
\index{inner product}
in a space equipped with a {\em scalar product,}
or by a synonym preferred in matematics, {\em inner product.}
\marginnote{The scalar or inner product
$\langle {\bf x}\vert {\bf y} \rangle$ of two vectors
${\bf x}$ and ${\bf y}$ is defined on page \pageref{2011-m-scalarproduct}.
In Euclidean  space such as ${\Bbb R}^n$,
one often identifies the ``dot product''
${\bf x}\cdot {\bf y} =x_1y_1+ \cdots +x_ny_n$
of two vectors ${\bf x}$ and $ {\bf y}$ with their scalar or inner product.}
The Gram-Schmidt process takes a finite, linearly independent set
of base vectors
and generates an orthonormal basis that spans the same (sub)space as the original set.

The general method is to start out with the original basis,
say,
$\{
{\bf x}_1,
{\bf x}_2,
{\bf x}_3,
\ldots ,
{\bf x}_n
\}$,
and generate a new orthogonal basis
$\{
{\bf y}_1,
{\bf y}_2,
{\bf y}_3,
\ldots ,
{\bf y}_n
\}$
by
\begin{equation}
\begin{array}{lcl}
{\bf y}_1&=&{\bf x}_1,\\
{\bf y}_2&=&{\bf x}_2 - p_{{\bf y}_1}({\bf x}_2),\\
{\bf y}_3&=&{\bf x}_3 - p_{{\bf y}_1}({\bf x}_3)- p_{{\bf y}_2}({\bf x}_3),\\
&\vdots&\\
{\bf y}_n&=&{\bf x}_n -\sum_{i=1}^{n-1} p_{{\bf y}_i}({\bf x}_n),
\end{array}
\end{equation}
where
\begin{equation}
p_{{\bf y}}({\bf x}) =
\frac{\langle {\bf x}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
{\bf y}
,\textrm{ and }
p_{{\bf y}}^\perp ({\bf x}) = {\bf x} -
\frac{\langle {\bf x}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
{\bf y}
\end{equation}
are the orthogonal projections of ${\bf x}$ onto ${\bf y}$ and ${\bf y}^\perp$, respectively
(the latter is mentioned for the sake of completeness and is not required here).
\label{2011-m-gsp}
Note that these orthogonal projections are idempotent
%(i.e., $p^2=p(p)=p$ and $(p^\perp)^2=p^\perp (p^\perp)=p^\perp$)
and mutually orthogonal; that is,
\begin{equation}
\begin{array}{l}
p_{{\bf y}}^2({\bf x})  = p_{{\bf y}}(p_{{\bf y}}({\bf x}) ) =
\frac{\langle {\bf x}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
\frac{\langle {\bf y}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
{\bf y} =p_{{\bf y}}({\bf x}),  \\
%
(p_{{\bf y}}^\perp)^2({\bf x})  = p_{{\bf y}}^\perp(p_{{\bf y}}^\perp({\bf x}) ) =
{\bf x}- \frac{\langle {\bf x}\vert {\bf y} \rangle }{\langle {\bf y}\vert {\bf y} \rangle }{\bf y}
-\left(
\frac{\langle {\bf x}\vert {\bf y} \rangle }{\langle {\bf y}\vert {\bf y} \rangle }
-
\frac{\langle {\bf x}\vert {\bf y} \rangle \langle {\bf y}\vert {\bf y} \rangle}{\langle {\bf y}\vert {\bf y} \rangle^2 }
\right)
{\bf y},
=p_{{\bf y}}^\perp({\bf x}),  \\
p_{{\bf y}}(p_{{\bf y}}^\perp({\bf x}) ) =  p_{{\bf y}}^\perp(p_{{\bf y}}({\bf x}) ) =
\frac{\langle {\bf x}\vert {\bf y} \rangle }{\langle {\bf y}\vert {\bf y} \rangle }{\bf y}
-
\frac{\langle {\bf x}\vert {\bf y} \rangle \langle {\bf y}\vert {\bf y} \rangle}{\langle {\bf y}\vert {\bf y} \rangle^2 }
{\bf y}
=0;
\end{array}
\end{equation}
see also page \pageref{2011-m-projec}.

Subsequently, in order to obtain an orthonormal basis,
one can divide every basis vector by its length.

{\color{OliveGreen}
\bproof
The idea of the proof is as follows (see also Greub \cite{Greub75}, section 7.9).
In order to generate an orthogonal basis from a nonorthogonal one,
the first vector of the old basis is identified with the first vector of the new basis;
that is ${\bf y}_1={\bf x}_1$.
Then, the second vector of the new basis is obtained by
taking the second vector of the old basis and
subtracting its projection on the first vector of the new basis.
More precisely, take the Ansatz
\begin{equation}
{\bf y}_2={\bf x}_2 + \lambda  {\bf y}_1,
\end{equation}
thereby determining the arbitrary scalar $\lambda$ such that
${\bf y}_1$
and
${\bf y}_2$
are orthogonal; that is,
$\langle {\bf y}_1\vert {\bf y}_2 \rangle =0$.
This yields
\begin{equation}
\langle {\bf x}_2\vert {\bf y}_1  \rangle
+ \lambda
\langle {\bf y}_1\vert {\bf y}_1 \rangle =0,
\end{equation}
and thus, since ${\bf y}_1 \neq 0$,
\begin{equation}
\lambda =
-
\frac{\langle {\bf x}_2\vert {\bf y}_1  \rangle}
{\langle {\bf y}_1\vert {\bf y}_1 \rangle} .
\end{equation}
To obtain the third vector ${\bf y}_3$ of the new basis,
take the Ansatz
\begin{equation}
{\bf y}_3={\bf x}_3 + \mu  {\bf y}_1  + \nu  {\bf y}_2,
\end{equation}
and require that it is orthogonal to the two previous orthogonal basis vectors
${\bf y}_1$
and
${\bf y}_2$;
that is
$\langle {\bf y}_1\vert {\bf y}_3 \rangle =\langle {\bf y}_2\vert {\bf y}_3 \rangle =0$.
As a result,
\begin{equation}
\mu = -  \frac{\langle {\bf x}_3\vert {\bf y}_1  \rangle}
{\langle {\bf y}_1\vert {\bf y}_1 \rangle},\quad
\nu =- \frac{\langle {\bf x}_3\vert {\bf y}_2  \rangle}
{\langle {\bf y}_2\vert {\bf y}_2 \rangle}.
\end{equation}
A generalization of this construction for all the other new base vectors
${\bf y}_3, \ldots ,  {\bf y}_n$ is straightforward.
\eproof
}

{\color{blue}
\bexample
Consider, as an example, the standard Euclidean scalar product denoted by ``$\cdot$''
and the basis
$\{(0,1),(1,1)\}$.
Then two orthogonal bases are obtained obtained by taking
\begin{itemize}
\item[(i)]
either the basis vector
$(0,1)$ and
$$
(1,1) -
\frac{(1,1)\cdot (0,1)}{(0,1)\cdot (0,1)} (0,1) = (1,0),
$$
\item[(ii)]
or the basis vector
$(1,1)$ and
$$
(0,1) -
\frac{(0,1)\cdot (1,1)}{(1,1)\cdot (1,1)} (1,1) = \frac{1}{2}(-1,1). \textrm{\eexample}
$$
\end{itemize}
}

\section{Mutually unbiased bases}
\index{mutually unbiased bases}

Two  orthonormal bases
$\{
{\bf x}_1,
\ldots ,
{\bf x}_n
\}$
and
$\{
{\bf y}_1,
\ldots ,
{\bf y}_n
\}$
are said to be {\em mutually unbiased}
if
their scalar or inner products are
\begin{equation}
\vert \langle {\bf x}_i\vert {\bf y}_j  \rangle \vert^2
=
\frac{1}{n}
\end{equation}
for all $1\le i,j\le n$.
Note without proof -- that is, you do not have to be concerned
that you need to understand  this from what has been said so far --
that the elements of two or more mutually unbiased bases are mutually ``maximally apart.''

{\color{Purple}
In physics, one seeks maximal sets of orthogonal bases whose elements in different bases
are maximally apart \cite{WooFie,durt} .
Such maximal sets are used in quantum information theory
to assure maximal performance of certain protocols
used in quantum cryptography, or for the production of
quantum random sequences by beam splitters.
They are essential for the practical exploitations of quantum complementary properties
and resources.
}

{\color{blue}
\bexample
Consider, for example, the real plane ${\Bbb R}^2$.
There the two bases
\begin{equation}
\begin{array}{l}
\{(0,1),(1,0)\} \; {\rm and }\\
\{\frac{1}{\sqrt{2}}(1,1),\frac{1}{\sqrt{2}}(-1,1)\}
\end{array}
\end{equation}
are mutually unbiased.
\eexample
}

{\color{OliveGreen}
\bproof
For a proof, just form the four inner products.
\eproof
}




\section{Direct sum}
\index{direct sum}
\marginnote{For proofs and additional information see \S 18 in   \cite{halmos-vs}}

Let
${\frak U}$
and
${\frak V}$
be vector spaces (over the same field, say ${\Bbb C}$).
Their {\em direct sum}
${\frak W}={\frak U}\oplus{\frak V}$
consist of all ordered pairs
$({\bf x},{\bf y})$, with
${\bf x}\in {\frak U}$ in
${\bf y}\in {\frak V}$,
and with the linear operations defined by
\begin{equation}
(
\alpha {\bf x}_1 +\beta {\bf x}_2
,
\alpha {\bf y}_1 +\beta {\bf y}_2
)
=
\alpha  ({\bf x}_1,{\bf y}_1)
+
\beta   ({\bf x}_2,{\bf y}_2).
\end{equation}

We state without proof that,
if
${\frak U}$
and
${\frak V}$
are subspaces of a vector space
${\frak W}$,
then the following three conditions are equivalent:
\begin{itemize}
\item[(i)]
${\frak W}={\frak U}\oplus{\frak V}$;
\item[(ii)]
${\frak U}\bigcap{\frak V}={\frak 0}$
and
${\frak U}+{\frak V}={\frak W}$
(i.e., ${\frak U}$
and
${\frak V}$
are complements of each other);
\item[(iii)]
every vector ${\bf z}\in {\frak W}$ can be written as
${\bf z}={\bf x}+{\bf y}$, with
${\bf x}\in {\frak U}$  and
${\bf y}\in {\frak V}$, in one and only one way.
\end{itemize}


\section{Dual space}
\label{2011-m-dvs}
\marginnote{For proofs and additional information see \S 13--15 in   \cite{halmos-vs}}

Every vector space ${\frak V}$
has a corresponding {\em dual vector space}
\index{dual vector space}
\index{dual space}
(or just {\em dual space})
consisting of all linear functionals on ${\frak V}$.

A {\em linear functional}
\index{linear functional}
on a vector space ${\frak V}$ is a scalar-valued linear function ${\bf y}$
defined for every vector   ${\bf x} \in {\frak V}$, with the linear property that
\begin{equation}
{\bf y} (\alpha_1 {\bf x}_1 +\alpha_2 {\bf x}_2)
=
\alpha_1 {\bf y} ({\bf x}_1) +\alpha_2 {\bf y} ({\bf x}_2) .
\end{equation}

{\color{blue}
\bexample
For example,
let ${\bf x} = (x_1,\ldots , x_n)$, and
take
${\bf y} ({\bf x}) =x_1$.

For another example,
let again ${\bf x} = (x_1,\ldots , x_n)$, and
let $\alpha_1,\ldots , \alpha_n \in {\Bbb C}$ be scalars; and
take
${\bf y} ({\bf x}) =\alpha_1 x_1 + \cdots +\alpha_n x_n$.
\eexample
}


If we adopt a bracket notation ``$[\cdot , \cdot ]$''
for the functional
\begin{equation}
{\bf y} ({\bf x})
=
[{\bf x},{\bf y}],
\end{equation}
then this ``bracket'' functional is
{\em bilinear} in its two arguments; that is,
\begin{equation}
[ \alpha_1 {\bf x}_1 +\alpha_2 {\bf x}_2, {\bf y}]
=
\alpha_1 [{\bf x}_1 ,{\bf y}]  +\alpha_2  [{\bf x}_2,{\bf y}],
\end{equation}
and
\begin{equation}
[
{\bf x}, \alpha_1 {\bf y}_1 +\alpha_2 {\bf y}_2
]
=
\alpha_1
[{\bf x},{\bf y}_1 ]
+
\alpha_2
[{\bf x},{\bf y}_2].
\end{equation}
\marginnote{The square bracket can be identified with the scalar (dot) product
$[ {\bf x},{\bf y} ] = \langle {\bf x}\mid {\bf y}\rangle$
only for Euclidean
vector spaces ${\Bbb R}^n$, since for complex spaces this would no longer be positive definite.
That is, for Euclidean
vector spaces ${\Bbb R}^n$ the inner or scalar product is bilinear.
}


If ${\frak V}$ is an $n$-dimensional vector space, and if ${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is a basis of  ${\frak V}$, and if
$\{\alpha_1, \ldots ,\alpha_n\}$  is any set of $n$ scalars, then there is
a unique linear functional ${\bf y}$  on  ${\frak V}$ such that
$ [ {\bf f}_i, {\bf y}] = \alpha_i $ for all $0\le 1 \le n$.

A constructive proof of this theorem can be given as follows:
Since every ${\bf x}\in {\frak V}$
can be written as a linear combination
$ {\bf x} = x_1 {\bf f}_1 +\cdots + x_n {\bf f}_n$
of the base vectors in ${\frak B}$ in a unique way;
and since ${\bf y}$ is a (bi)linear functional, we obtain
\begin{equation}
[{\bf x},{\bf y}]
=
x_1 [{\bf f}_1 ,{\bf y}] +\cdots + x_n [{\bf f}_n ,{\bf y}] ,
\end{equation}
and uniqueness follows.
With $[{\bf f}_i ,{\bf y}] = \alpha_i$  for all  $0\le 1 \le n$,
the value of
$[{\bf x},{\bf y}]$ is determined by
$[{\bf x},{\bf y}] = x_i\alpha_i + \cdots + x_n\alpha_n$.

\subsection{Dual basis}
\label{2011-m-Dualbasis}

We now can define a {\em dual basis}, or, used synonymuously a {\em reciprocal basis}.
\index{dual basis}
\index{reciprocal basis}
If ${\frak V}$ is an $n$-dimensional vector space, and if
${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is a basis of  ${\frak V}$,
then there is a unique {\em dual basis}
${\frak B}^*
=\{{\bf f}_1^*,\ldots , {\bf f}_n^*\}$ in the dual vector space ${\frak V}^*$
with the property that
\begin{equation}
[f_i^* , f_j]=\delta_{ij},
\label{2011-m-Dualbasis-e1}
\end{equation}
where  $\delta_{ij}$
is the Kronecker delta function.
More generally, if $g$ is the {\em metric tensor},
\index{metric tensor}
the dual basis is defined by
\begin{equation}
g( {\bf f}_i^*,{\bf f}_j)=\delta_{ij}.
\label{2011-m-Dualbasis-e2}
\end{equation}
or, in a different notation in which ${\bf f}_j^* = {\bf f}^j$,
\begin{equation}
g( {\bf f}^j,{\bf f}_i)=\delta_{i}^j.
\label{2011-m-Dualbasis-e3}
\end{equation}
In terms of the inner product, the representation
of the metric $g$ as outlined and characterized on page \pageref{2011-m-metrict} with respect to a particular basis
${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is $g_{ij}=g({\bf f}_i,{\bf f}_j)=\langle {\bf f}_i\mid {\bf f}_j\rangle$.
Note that the coordinates $g_{ij}$ of
the metric $g$ need not necessarily be positive definite.
For example,  special relativity uses the ``pseudo-Euclidean'' metric
 $g={\rm diag}(+1,+1,+1,-1)$ (or just $g={\rm diag}(+,+,+,-)$), where ``${\rm diag}$''
stands for the {\em diagonal matrix}
\index{diagonal matrix}
with the arguments in the diagonal.
\marginnote{The metric tensor $g_{ij}$ represents a {\em bilinear functional}
$g({\bf x},{\bf y}) =x^iy^j g_{ij}$ that is {\em symmetric}; that is,
$g({\bf x},{\bf y}) = g({\bf x},{\bf y})$
and {\em nondegenerate}; that is, for any nonzero vector ${\bf x}\in {\frak V}$,   ${\bf x}\neq 0$,
there is some  vector  ${\bf y}\in {\frak V}$, so that  $g({\bf x},{\bf y}) \neq 0$.
$g$ also satisfies the triangle
inequality
$\vert\vert {\bf x} -{\bf z} \vert\vert  \le \vert\vert {\bf x} - {\bf y} \vert\vert  + \vert\vert  {\bf y} - {\bf z} \vert\vert $.
}


The dual space  ${\frak V}^*$  is $n$-dimensional.

In a real Euclidean vector space ${\Bbb R}^n$
with the dot product as scalar product,
the dual basis of an orthogonal basis  is also orthogonal.
Moreover, for an orthonormal basis, the bases vectors are uniquely identifiable by
${\bf e_i} \longrightarrow {\bf e_i}^* = {\bf e_i}^T$.
This is {\em not} true for nonorthogonal bases.

{\color{OliveGreen}
\bproof
%For a proof ask your audience or a wizard \frownie

In a proof by {\it reductio ad absurdum.}
Suppose there exist a vector ${\bf e_i}^*$ in the dual basis  ${\frak B}^*$
which is not in  the ``original'' ortogonal basis ${\frak B}$;
that is, $[{\bf e_i}^*,{\bf e_j}]=\delta_{ij}$ for all ${\bf e_j} \in {\frak B}$.
But since ${\frak B}$ is supposed to span the corresponding vector space ${\frak V}$,
${\bf e_i}^*$ has to be contained in ${\frak B}$.
Moreover, since for  a real Euclidean vector space ${\Bbb R}^n$
with the dot product as scalar product, the two products
$[\cdot , \cdot ]=\langle \cdot \mid \cdot \rangle$
coincide, ${\bf e_i}^*$ has to be collinear
-- for normalized basis vectors even identical -- to exactly one element of ${\frak B}$.

For nonorthogonal bases, take the counterexample explicitly mentioned at page \pageref{2011-m-cenobdb}.
\eproof
}

How can one determine the dual basis from a given,
not necessarily orthogonal, basis?
The tuples of {\em row vectors} of the basis ${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
can be arranged into a matrix
\begin{equation}
\textsf{\textbf{B}}
=
\left(
\begin{array}{cccccccccc}
{\bf f}_{1}\\
{\bf f}_{2}\\
\vdots  \\
{\bf f}_{n}
\end{array}
\right)  =
\left(
\begin{array}{cccccccccc}
{\bf f}_{1,1}&\cdots & {\bf f}_{1,n}\\
{\bf f}_{2,1}&\cdots & {\bf f}_{2,n}\\
\vdots&\vdots & \vdots \\
{\bf f}_{n,1}&\cdots & {\bf f}_{n,n}
\end{array}
\right).
\end{equation}
Then take the
{\em inverse matrix}
$\textsf{\textbf{B}}^{-1}$,
and interprete the
{\em columns vectors} of $\textsf{\textbf{B}}^{-1}$
as the tuples of the dual basis  ${\frak B}^*$.

For orthogonal but not orthonormal bases, the term {\em reciprocal} basis
can be easily explained from the fact that the norm (or length) of each vector in the {\em reciprocal basis}
is just the {\em inverse} of the length of the original vector.

{\color{OliveGreen}
\bproof
For a proof, consider $\textsf{\textbf{B}}\cdot \textsf{\textbf{B}}^{-1} ={\Bbb I}_n$.
\eproof
}


{\color{blue}
\bexample
\begin{itemize}
\item[(i)]
For example,
if $$
{\frak B}=\{{\bf e}_1, {\bf e}_2,\ldots ,{\bf e}_n\}
=\{
(1,0,\ldots,0),
(0,1,\ldots,0),
\ldots,
(0,0,\ldots,1)\}$$
is the standard basis in $n$-dimensional vector space containing unit vectors of norm (or length) one,
then  (the superscript ``$T$'' indicates transposition)
$$
\begin{array}{rcl}
{\frak B}^*
&=&\{{\bf e}_1^* ,{\bf e}_2^*, \ldots ,{\bf e}_n^* \}\\
&\quad =&\left\{
(1,0,\ldots,0)^T,
(0,1,\ldots,0)^T,
\ldots,
(0,0,\ldots,1)^T\right\}  \\
&\quad =&  \left\{
\left(
\begin{array}{c}
1\\
0\\
\vdots \\
0
\end{array}
\right),
\left(
\begin{array}{c}
0\\
1\\
\vdots \\
0
\end{array}
\right),
\ldots ,
\left(
\begin{array}{c}
0\\
0\\
\vdots \\
1
\end{array}
\right)
\right\}
\end{array}
$$
has elements with identical components,
but those tuples are the transposed tuples.

\item[(ii)]
If $$
{\frak X}=\{\alpha_1 {\bf e}_1, \alpha_2 {\bf e}_2,\ldots ,\alpha_n {\bf e}_n\}
=\{
(\alpha_1 ,0,\ldots,0),
(0,\alpha_2 ,\ldots,0),
\ldots,
(0,0,\ldots,\alpha_n )\},$$  $\alpha_1,\alpha_2,\ldots ,\alpha_n \in {\Bbb R}$,
is a ``dilated'' basis in $n$-dimensional vector space containing unit vectors of norm (or length) $\alpha_i$,
then
$$
\begin{array}{rcl}
{\frak X}^*
&=&\{\frac{1}{\alpha_1 }{\bf e}_1^* ,\frac{1}{\alpha_2 }{\bf e}_2^*, \ldots ,\frac{1}{\alpha_n }{\bf e}_n^* \}\\
&\quad =&\left\{
(\frac{1}{\alpha_1 },0,\ldots,0)^T,
(0,\frac{1}{\alpha_2 },\ldots,0)^T,
\ldots,
(0,0,\ldots,\frac{1}{\alpha_n })^T\right\}  \\
&\quad =&  \left\{
\frac{1}{\alpha_1 }\left(
\begin{array}{c}
1\\
0\\
\vdots \\
0
\end{array}
\right),
\frac{1}{\alpha_2 }
\left(
\begin{array}{c}
0\\
1\\
\vdots \\
0
\end{array}
\right),
\ldots ,
\frac{1}{\alpha_n }
\left(
\begin{array}{c}
0\\
0\\
\vdots \\
1
\end{array}
\right)
\right\}
\end{array}
$$
has elements with identical components of inverse length $\frac{1}{\alpha_i }$,
and again those tuples are the transposed tuples.

\item[(iii)]
Consider the nonorthogonal basis
${\frak B} =
\{(1, 2), (3, 4)\}$.
The associated row matrix is
$$
\textsf{\textbf{B}}
=
\left(
\begin{array}{rrrr}
1&2\\
3&4
\end{array}
\right).
$$
The inverse matrix is
$$
\textsf{\textbf{B}}^{-1}
=
\left(
\begin{array}{rrrr}
-2&1\\
\frac{3}{2}&-\frac{1}{2}
\end{array}
\right);
$$
and the associated dual basis is obtained from the columns of $\textsf{\textbf{B}}^{-1} $ by
\label{2011-m-cenobdb}
${\frak B}^* =
\left\{\left(
\begin{array}{r}
-2\\ \frac{3}{2}
\end{array}\right),
\left(
\begin{array}{r}
1\\
-\frac{1}{2}
\end{array}
\right)
\right\} =
\left\{\frac{1}{2} \left(
\begin{array}{r}
-4\\ 3
\end{array}\right),
\frac{1}{2}
\left(
\begin{array}{r}
2\\
-1
\end{array}
\right)
\right\}
$.
\eexample
\end{itemize}
}


\subsection{Dual coordinates}

With respect to a given basis,
the components of a vector are often written as tuples of ordered
(``$x_i$ is written before $x_{i+1}$'' -- not ``$x_i < x_{i+1}$'')
scalars  as {\em column vectors}
\begin{equation}
{\bf x}= \left(
\begin{array}{c}
x_1\\x_2\\
\vdots \\ x_n
\end{array}
\right),
\end{equation}
whereas the components of vectors in dual spaces are often written in terms of
 tuples of ordered
scalars  as {\em row vectors}
\begin{equation}
{\bf x}^*= (x_1^*,x_2^*,\ldots , x_n^*)
.
\end{equation}
The coordinates  $(x_1,x_2,\ldots , x_n)^T=\left(
\begin{array}{c}
x_1\\x_2\\
\vdots \\ x_n
\end{array}
\right)$
are called
{\em covariant},
\index{covariant coordinates}
whereas the coordinates  $(x_1^*,x_2^*,\ldots , x_n^*)$
are called
{\em contravariant},
\index{contravariant coordinates}.
Alternatively, one can denote
covariant coordinates by subscripts,
and contravariant coordinates by superscripts; that is
(see also
Havlicek \cite{havlicek-laftm}, Section 11.4),
\begin{equation}
x_i =  \left(
\begin{array}{c}
x_1\\x_2\\
\vdots \\ x_n
\end{array}
\right)
\textrm{ and }
 x^i =
(x_1^*,x_2^*,\ldots , x_n^* ).
\end{equation}
Note again that the covariant and contravariant components
$x_i$ and $x^i$ are not absolute, but always defined {\em with respect to}
a particular (dual) basis.

The {\em Einstein summation convention}
\index{Einstein summation convention}
requires that, when an index variable appears twice in a single term it implies that one has to
sum over all of the possible index values.
This saves us from drawing the sum sign ``$\sum_i$ for the index $i$;
for instance $x_iy_i =\sum_{i}x_iy_i$.

In the particular context of covariant and contravariant components
--
made necessary by nonorthogonal bases whose associated dual bases are {\em not} identical
--
the summation always is between some superscript and some subscript;
e.g., $x_iy^i$.

Note again that for orthonormal basis,
$x^i=x_i$.


\subsection{Representation of a functional by inner product}
\label{2011-m-corr-bil-ip}
\marginnote{For proofs and additional information see \S 67 in   \cite{halmos-vs}}
The following representation theorem is about the connection between any functional
in a vector space and its inner product; it is stated without proof:
To any linear functional ${\bf z}$
on a finite-dimensional inner product space ${\frak V}$
there corresponds a unique vector   ${\bf y}\in {\frak V}$,
such that
\begin{equation}
{\bf z} ({\bf x}) =[{\bf x}, {\bf z}]= \langle {\bf x}\mid {\bf y}\rangle
\end{equation}
for all ${\bf x}\in {\frak V}$.

Note that in  real vector space ${\Bbb R}^n$ and with the dot product,
 ${\bf y}={\bf z}$.


\section{Tensor product}
\label{2011-m-tensorp}
\marginnote{For proofs and additional information see \S 24 in   \cite{halmos-vs}}


\subsection{Definition}

For the moment, suffice it to say that
the {\em tensor product}
\index{tensor product}
 ${\frak V} \otimes {\frak U}$
of two linear vector spaces  ${\frak V}$ and  ${\frak U}$
should be such that,
to every
${\bf x} \in  {\frak V}$
and avery
${\bf y} \in  {\frak U}$
there corresponds a tensor product ${\bf z} = {\bf x} \otimes {\bf y}
\in {\frak V} \otimes {\frak U}$
which is bilinear in both factors.

If ${\frak A} =\{{\bf f}_1,\ldots , {\bf f}_n\}$ and
${\frak B} =\{{\bf g}_1,\ldots , {\bf g}_m\}$
are bases of  $n$- and $m$-
dimensional vector spaces ${\frak V}$ and  ${\frak U}$, respectively,
then the set
${\frak Z}$ of vectors ${\bf z}_{ij}= {\bf f}_i \otimes {\bf g}_j$
with $i=1,\ldots n$ and $j=1,\ldots m$
 is a basis of the { tensor product}
 ${\frak V} \otimes {\frak U}$.

A generalization to more than one factors is straightforward.

\subsection{Representation}

The tensor product ${\bf z} = {\bf x} \otimes {\bf y}$     has three equivalent  representations:
\begin{itemize}
\item[(i)]
as the scalar coordinates $x_iy_j$ with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
\item[(ii)]
as the quasi-matrix $z_{ij}  =x_iy_j$, whose components $z_{ij}$ are  defined with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
\item[(iii)]
as a quasi-vector or ``flattened matrix'' defined by the Kronecker product
${\bf z} = ({ x}_1  {\bf y}, { x}_2  {\bf y}, \ldots , { x}_n  {\bf y})=
({ x}_1  { y}_1, { x}_1  { y}_2, \ldots , { x}_n  { y}_n)
$. Again, the scalar coordinates $x_iy_j$ are defined
with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded.
\index{Kronecker product}
\end{itemize}
In all three cases, the pairs $x_iy_j$  are properly represented by distinct mathematical entities.


\section{Linear transformation}
\marginnote{For proofs and additional information see \S 32-34 in   \cite{halmos-vs}}

\subsection{Definition}
A {\em linear transformation} (or, used synonymuosly, {\em linear operator}
\index{linear transformation}
\index{linear operator}
$\textsf{\textbf{A}} $ on a vector space ${\frak V}$ is a correspondence that assigns every vector
${\bf x}\in {\frak V}$ a vector $\textsf{\textbf{A}} {\bf x}\in {\frak V}$,
in a linear way that
\begin{equation}
\textsf{\textbf{A}}  (\alpha {\bf x}+ \beta {\bf y}) = \alpha \textsf{\textbf{A}}{\bf x}
+  \beta \textsf{\textbf{A}} {\bf y},
\end{equation}
identically for all vectors ${\bf x},{\bf y}\in {\frak V}$ and all scalars $\alpha , \beta$.


\subsection{Operations}
The {\em sum}
\index{sum of transformations}
$\textsf{\textbf{S}} =\textsf{\textbf{A}} +\textsf{\textbf{B}} $
of two linear transformations $\textsf{\textbf{A}}$ and $\textsf{\textbf{B}} $
is defined by
$\textsf{\textbf{S}} {\bf x}=\textsf{\textbf{A}}{\bf x} +\textsf{\textbf{B}} {\bf x}$
for every ${\bf x}\in {\frak V}$.

The {\em product}
\index{product of transformations}
$\textsf{\textbf{P}} =\textsf{\textbf{A}} \textsf{\textbf{B}} $
of two linear transformations $\textsf{\textbf{A}}$ and $\textsf{\textbf{B}} $
is defined by
$\textsf{\textbf{P}} {\bf x}=\textsf{\textbf{A}}(\textsf{\textbf{B}} {\bf x})$
for every ${\bf x}\in {\frak V}$.

The notation
$\textsf{\textbf{A}}^n\textsf{\textbf{A}}^m=\textsf{\textbf{A}}^{n+m}$
and $(\textsf{\textbf{A}}^n)^m= \textsf{\textbf{A}}^{nm}$,
with $\textsf{\textbf{A}}^1=\textsf{\textbf{A}}$ and
$\textsf{\textbf{A}}^0 =\textsf{\textbf{1}}$ turns out to be useful.

With the exception of commutativity, all formal algebraic properties
of numerical addition and multiplication,
are valid for transformations; that is
$
\textsf{\textbf{A}}\textsf{\textbf{0}}=
\textsf{\textbf{0}}\textsf{\textbf{A}} =\textsf{\textbf{0}}
$,
$
\textsf{\textbf{A}}\textsf{\textbf{1}}=
\textsf{\textbf{1}}\textsf{\textbf{A}} =\textsf{\textbf{A}}
$,
$
\textsf{\textbf{A}} (\textsf{\textbf{B}}+\textsf{\textbf{C}})=
\textsf{\textbf{A}} \textsf{\textbf{B}}
+
\textsf{\textbf{A}} \textsf{\textbf{C}}
$,
$
(\textsf{\textbf{A}}+ \textsf{\textbf{B}})\textsf{\textbf{C}}=
\textsf{\textbf{A}} \textsf{\textbf{C}}
+
\textsf{\textbf{B}} \textsf{\textbf{C}}
$,  and
$
\textsf{\textbf{A}} (\textsf{\textbf{B}}\textsf{\textbf{C}})=
(\textsf{\textbf{A}} \textsf{\textbf{B}})
 \textsf{\textbf{C}}
$.
In {\em matrix notation},  $\textsf{\textbf{1}} ={\Bbb{1}}$, and the entries of $\textsf{\textbf{0}}$
are $0$ everywhere.

The {\em inverse operator}
\index{inverse operator}
$\textsf{\textbf{A}}^{-1}$
of $\textsf{\textbf{A}}$
is defined by
$\textsf{\textbf{A}}\textsf{\textbf{A}}^{-1}=\textsf{\textbf{A}}^{-1}\textsf{\textbf{A}}=
\textsf{\textbf{I}}$.


The {\em commutator}
\index{commutator}
of two matrices $\textsf{\textbf{A}}$  and $\textsf{\textbf{B}}$ is defined by
\begin{equation}
[\textsf{\textbf{A}}, \textsf{\textbf{B}} ]
=
\textsf{\textbf{A}} \textsf{\textbf{B}}
-
 \textsf{\textbf{B}}      \textsf{\textbf{A}}.
\end{equation}
\marginnote{The commutator should not be confused with the bilinear funtional
introduced for dual spaces.}

The {\em polynomial}
\index{polynomial}
can be directly adopted from ordinary arithmetic; that is,
any finite polynomial $p$ of degree $n$
of an operator (transformation) $\textsf{\textbf{A}}$ can be written as
\begin{equation}
p(\textsf{\textbf{A}})= \alpha_0   \textsf{\textbf{1}}
+ \alpha_1   \textsf{\textbf{A}}^1
+ \alpha_2   \textsf{\textbf{A}}^2+
\cdots
+
\alpha_n   \textsf{\textbf{A}}^n
=\sum_{i=0}^n \alpha_i \textsf{\textbf{A}}^i
.
\end{equation}

The Baker-Hausdorff formula
 \begin{equation}
 e^{i\textsf{\textbf{A}}}\textsf{\textbf{B}}e^{-i\textsf{\textbf{A}}}=
B+i[\textsf{\textbf{A}},\textsf{\textbf{B}}]+
{i^2\over 2!}[\textsf{\textbf{A}},[\textsf{\textbf{A}},\textsf{\textbf{B}}]]+\cdots
 \end{equation}
for two arbitrary noncommutative linear operators $\textsf{\textbf{A}}$ and
$\textsf{\textbf{B}}$ is mentioned without proof
(cf.  Messiah, {\sl Quantum Mechanics, Vol. 1} \cite{messiah-61}).

If $[\textsf{\textbf{A}},\textsf{\textbf{B}}]$ commutes with $\textsf{\textbf{A}}$ and
$\textsf{\textbf{B}}$, then
 \begin{equation}
 e^\textsf{\textbf{A}}e^\textsf{\textbf{B}}=
e^{\textsf{\textbf{A}}+\textsf{\textbf{B}}+{1\over 2}[\textsf{\textbf{A}},\textsf{\textbf{B}}]}.
 \end{equation}



\subsection{Linear transformations as matrices}

\index{matrix}

Due to linearity, there is a close connection between a matrix
defined by an $n$-by-$n$ square array
\begin{equation}
A = \langle i\vert A\vert j\rangle = a_{ij} =\left(
\begin{array}{ccccccc}
\alpha_{11}&
\alpha_{12}&
\cdots    &
\alpha_{1n}\\
\alpha_{21}&
\alpha_{22}&
\cdots    &
\alpha_{2n}\\
\vdots&
\vdots&
\cdots    &
\vdots\\
\alpha_{n1}&
\alpha_{n2}&
\cdots   &
\alpha_{nn}
\end{array}
\right)
\end{equation}
containing $n^2$ entries,
also called matrix coefficients or matrix coordinates, $\alpha_{ij}$ and a linear transformation
$\textsf{\textbf{A}}$, encoded with respect to a particular basis ${\frak B}$.
This can be well understood in terms of transformations of the basis elements,
as every vector is a unique linear combination of these basis elements;
more explicitly, see the {\it Ansatz} ${\bf y}_i=  \textsf{\textbf{A}}{\bf x}_i $ in Eq. (\ref{2011-m-btbe}) below.


Let ${\frak V}$ be an $n$-dimensional vector space;
let
${\frak B}=\{{\bf f}_1,{\bf f}_2,\ldots ,{\bf f}_n\}$ be any basis of ${\frak V}$,
and let  $\textsf{\textbf{A}}$ be a linear transformation on ${\frak V}$.
Because every vector is a linear combination of the basis vectors
${\bf f}_i$,
it is possible to define some matrix coefficients or coordinates
$\alpha_{ij}$ such that
\begin{equation}
\textsf{\textbf{A}} {\bf f}_j = \sum_i \alpha_{ij}{\bf f}_i
\end{equation}
for all $j=1,\ldots ,n$.
Again, note that this definition of a {\em transformation matrix}
is tied up with a basis.
\index{transformation matrix}

{\color{blue}
\bexample
In terms of this matrix notation, it is quite easy to present an example
for which the commutator
$
[\textsf{\textbf{A}}, \textsf{\textbf{B}} ]
$
does not vanish; that is
$\textsf{\textbf{A}}$  and $\textsf{\textbf{B}}$
do not commute.

Take, for the sake of an example, the
{\em Pauli spin matrices}
\index{Pauli spin matrices}
which are proportional to the angular momentum operators along the $x,y,z$-axis
\cite{schiff-55}:
\begin{equation}
\begin{array}{l}
\sigma_1=\sigma_x=
\left(
\begin{array}{rrrr}
0&1\\
1&0
\end{array}
\right),   \\
\sigma_2=\sigma_y=
\left(
\begin{array}{rrrrrr}
0&-i\\
i&0
\end{array}
\right),   \\
\sigma_1=\sigma_z=
\left(
\begin{array}{rrrrrrr}
1&0\\
0&-1
\end{array}
\right).
\end{array}
\end{equation}
Together with unity, i.e., ${\Bbb I}_2=\textrm{diag}(1,1)$,
they form a complete basis of all $(4\times 4)$ matrices.
Now take, for instance, the commutator
\begin{equation}
\begin{array}{l}
[\sigma_1,\sigma_3]= \sigma_1\sigma_3-\sigma_3\sigma_1\\
\qquad
=
\left(
\begin{array}{rrrrrrrr}
0&1\\
1&0
\end{array}
\right)
\left(
\begin{array}{rrrrrrrrrr}
1&0\\
0&-1
\end{array}
\right)
-
\left(
\begin{array}{rrrrrrrrrrrr}
1&0\\
0&-1
\end{array}
\right)
\left(
\begin{array}{rrrrrrrrrrrrrrrr}
0&1\\
1&0
\end{array}
\right)\\
\qquad
=  2
\left(
\begin{array}{rrrrrrrrrrrrr}
0&-1\\
1&0
\end{array}
\right)\neq
\left(
\begin{array}{cccccccccc}
0&0\\
0&0
\end{array}
\right)
.    \textrm{\eexample}
\end{array}
\end{equation}
%\eexample
}

\section{Projector or Projection}
\label{2011-m-projec}
\marginnote{For proofs and additional information see \S 41 in    \cite{halmos-vs}}
\index{projector}
\index{projection}

\subsection{Definition}
If ${\frak V}$ is the direct sum of some subspaces
${\frak M}$
and
${\frak N}$
so that verey ${\bf z} \in {\frak V}$ can be uniquely written in the form
$
{\bf z}
=
{\bf x}
+
{\bf y}
$, with
${\bf x} \in {\frak M}$
and with
${\bf y} \in {\frak N}$,
then
the {\em projector}, or, uses synonymuosly,
{\em projection}
on ${\frak M}$
along ${\frak N}$ is the transformation $\textsf{\textbf{E}}$
defined by $\textsf{\textbf{E}}{\bf z}={\bf x}$.
Conversely,
 $\textsf{\textbf{F}}{\bf z}={\bf y}$  is the projector
on ${\frak N}$
along ${\frak M}$.

A linear transformation
$\textsf{\textbf{E}}$ is a projector if and only if
it is idempotent; that is,
$\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}$.

{\color{OliveGreen}
\bproof
For a proof note that, if $\textsf{\textbf{E}}$  is the projector
on ${\frak M}$
along ${\frak N}$,
and if
$
{\bf z}
=
{\bf x}
+
{\bf y}
$, with
${\bf x} \in {\frak M}$
and with
${\bf y} \in {\frak N}$,
the the decomposition of ${\bf x}$ yields
${\bf x}+0$, so that
$\textsf{\textbf{E}}^2{\bf z}=\textsf{\textbf{E}}\textsf{\textbf{E}}{\bf z}=\textsf{\textbf{E}}{\bf x}
={\bf x}=\textsf{\textbf{E}}{\bf z}$.
The converse --
idempotence
``$\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}$''
implies that $\textsf{\textbf{E}}$ is a projector -- is more difficult to prove.
For this proof we refer to the literature; e.g., Halmos \cite{halmos-vs}.
\eproof
}

We also mention without proof that a linear transformation
$\textsf{\textbf{E}}$ is a projector if and only if
$\textsf{\textbf{1}}-\textsf{\textbf{E}}$ is a projector.
Note that $(\textsf{\textbf{1}}-\textsf{\textbf{E}})^2
=\textsf{\textbf{1}}-\textsf{\textbf{E}}-\textsf{\textbf{E}}+ \textsf{\textbf{E}}^2
=\textsf{\textbf{1}}-\textsf{\textbf{E}}$;
furthermore,
$
\textsf{\textbf{E}}(\textsf{\textbf{1}}-\textsf{\textbf{E}})=
(\textsf{\textbf{1}}-\textsf{\textbf{E}})\textsf{\textbf{E}}=
\textsf{\textbf{E}}- \textsf{\textbf{E}}^2=0
$.


Furthermore, if $\textsf{\textbf{E}}$  is the projector
on ${\frak M}$
along ${\frak N}$,
then
 $\textsf{\textbf{1}}-\textsf{\textbf{E}}$ is the projector
on ${\frak N}$
along ${\frak M}$.




\subsection{Construction of projectors from unit vectors}

How can we construct projectors from unit vectors, or systems of orthogonal projectors from some vector in some orthonormal basis
with the standard dot product?

Let ${\bf x}$ be the coordinates of a unit vector;
that is $\|{\bf x} \| =1$.
Then   the dyadic or tensor product  (also in Dirac's bra and ket notation)
\begin{equation}
\begin{array}{l}
\textsf{\textbf{E}}_{\bf x} = {\bf x} \otimes {\bf x}^T = \vert{\bf x}\rangle \langle {\bf x}\vert
\\
\qquad
=
\left(
\begin{array}{l}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
(x_1,x_2,\ldots ,x_n)\\
\qquad
=
\left(
\begin{array}{cccccccccccccccc}
x_1(x_1,x_2,\ldots ,x_n)\\
x_2(x_1,x_2,\ldots ,x_n)\\
\vdots  \\
x_n(x_1,x_2,\ldots ,x_n)
\end{array}
\right) \\
\qquad
=
\left(
\begin{array}{cccccccccccccccc}
x_1x_1&x_1x_2& \cdots&x_1x_n\\
x_2x_1&x_2x_2& \cdots&x_2x_n\\
\vdots & \vdots & \vdots &\vdots \\
x_nx_1&x_nx_2& \cdots&x_nx_n
\end{array}
\right)
\end{array}
\end{equation}
is the projector
associated with ${\bf x}$.

If the vector ${\bf x}$ is not normalized,
then the associated projector is
\begin{equation}
\textsf{\textbf{E}}_{\bf x} = \frac{{\bf x} \otimes {\bf x}^T}{\langle {\bf x}\mid {\bf x}\rangle}
= \frac{\vert{\bf x}\rangle \langle {\bf x}\vert}{\langle {\bf x}\mid {\bf x}\rangle}
\end{equation}
This construction is related to
$p_{\bf x}$ on page \pageref{2011-m-gsp}
by $p_{\bf x}({\bf y})=\textsf{\textbf{E}}_{\bf x}{\bf y}$.

{\color{OliveGreen}
\bproof
For a proof, let $\textsf{\textbf{E}}_{\bf x} = {\bf x}\otimes {\bf x}^T $,
then
$$
\begin{array}{l}
\textsf{\textbf{E}}_{\bf x}\textsf{\textbf{E}}_{\bf x}
= ({\bf x}\otimes {\bf x}^T ) \cdot ({\bf x}\otimes {\bf x}^T )
\\
\qquad
=
\left(
\left(
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
(x_1,x_2,\ldots ,x_n) \right) \left(
\left(
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
(x_1,x_2,\ldots ,x_n)
 \right)
\\
\qquad
=
\left(
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
\left(
(x_1,x_2,\ldots ,x_n)
\left(
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
 \right)
(x_1,x_2,\ldots ,x_n)
\\
\qquad
=
\left(
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
\cdot 1 \cdot
(x_1,x_2,\ldots ,x_n)
=
\left(
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}
\right)
(x_1,x_2,\ldots ,x_n)
=\textsf{\textbf{E}}_{\bf x}. \textrm{\eproof }
\end{array}
$$
}

{
\color{blue}
\bexample
Fo two examples, let
${\bf x}=(1,0)^T$
and
${\bf y}=(1,-1)^T$;
then
$$
\textsf{\textbf{E}}_{\bf x}
=
\left(
\begin{array}{l}
1\\
0
\end{array}
\right)
(1,0)
=
\left(
\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrr}
1(1,0)\\
0(1,0)
\end{array}
\right)
=
\left(
\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrr}
1&0\\
0&0
\end{array}
\right),
$$
and
$$
\textsf{\textbf{E}}_{\bf y}
= \frac{1}{2}
\left(
\begin{array}{r}
1\\
-1
\end{array}
\right)
(1,-1)
= \frac{1}{2}
\left(
\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrr}
1(1,-1)\\
-1(1,-1)
\end{array}
\right)
= \frac{1}{2}
\left(
\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrr}
1&-1\\
-1&1
\end{array}
\right).\textrm{\eexample}
$$
%\eexample
}


%\subsection{Combination of projectors}



\section{Change of basis}
\index{change of basis}
\marginnote{For proofs and additional information see \S 46 in    \cite{halmos-vs}}
\index{basis change}

Let ${\frak V}$ be an $n$-dimensional vector space land let
${\frak X}
=
\{
{\bf x}_1,
\ldots ,
{\bf x}_n
\}$
and
${\frak Y}
=  \{
{\bf y}_1,
\ldots ,
{\bf y}_n
\}$ be two bases  in ${\frak V}$.

Take an arbitrary vector ${\bf x}\in {\frak V}$.
In terms of the two bases
${\frak X}$ and
${\frak Y}$,
${\bf x}$ can be written as
\begin{equation}
{\bf x}=
\sum_{i=1}^n x^i{\bf x}_i
=
\sum_{i=1}^n  y^i{\bf y}_i,
\label{2011-m-btbexy}
\end{equation}
where $x^i$ and $y^i$ stand for the coordinates of the vector  ${\bf x}$
with respect to the bases ${\frak X}$ and
${\frak Y}$,
respectively.


What is the relation between the coordinates $x^i$ and $y^i$?
Suppose that, as an {\it Ansatz},
we define a linear transformation between the corresponding vectors of the bases
 ${\frak X}$ and
${\frak Y}$ by
\begin{equation}
{\bf y}_i=  \textsf{\textbf{A}}{\bf x}_i
,
\label{2011-m-btbe}
\end{equation}
for all $i=1, \ldots , n$.
Note that somewhat ``hidden'' in Eq. (\ref{2011-m-btbe})
there is a matrix multiplication which can be made explicit:
let $a_{ij}$ be the matrix components corresponding to $\textsf{\textbf{A}}$
in the basis ${\frak X}$;
then
\begin{equation}
{\bf y}_j=  \textsf{\textbf{A}}{\bf x}_j  =   \sum_{i=1}^n a_{ij}{\bf x}_i.
\end{equation}
Here, the subscript ``$j$'' refers to the $j$th coordinate of ${\bf y}$,
whereas
in Eq. (\ref{2011-m-btbe})
the subscript ``$j$'' refers to the $j$th vectors  ${\bf x}_j$ and ${\bf y}_i$ in the two bases  ${\frak X}$ and
${\frak Y}$, respectively.

Then, since
$${\bf x} =
 \sum_{j=1}^n y^j {\bf y}_j=
 \sum_{j=1}^n  y^j \textsf{\textbf{A}}{\bf x}_j=
 \sum_{j=1}^n  y^j  \sum_{i=1}^n a_{ij}{\bf x}_i=
  \sum_{i=1}^n \left(\sum_{j=1}^n  a_{ij}y^j \right)   {\bf x}_i;
$$
and hence by comparison of the coefficients in Eq. (\ref{2011-m-btbexy}),
\begin{equation}
x^i= \sum_{j=1}^n  a_{ij}y^j.
\end{equation}


%In matrix notation,
%\begin{equation}
% \textsf{\textbf{A}}\left(\sum_{i=1}^n x^i{\bf x}_i\right)  =   \sum_{i=1}^n x^i{\bf y}_i
%.
%\end{equation}


\section{Rank}

\index{rank}

The (column or row) {\em rank}, $\rho (\textsf{\textbf{A}})$
of a linear transformation $\textsf{\textbf{A}}$
in an $n$-dimensional vector space ${\frak V}$
is the maximum number of linearly independent (column or, without proof, equivalently,
row) vectors of the associated $n$-by-$n$ square matrix $a_{ij}$.



\section{Determinant}
\index{determinant}

\subsection{Definition}

Suppose $A=a_{ij}$ is the  $n$-by-$n$ square matrix representation of
a linear transformation $\textsf{\textbf{A}}$
in an $n$-dimensional vector space ${\frak V}$.
We shall define its {\em determinant} recursively.

First,
a {\em minor}
\index{minor}
$M_{ij}$ of an  $n$-by-$n$ square matrix  $A$ is
defined to be the determinant of the
$(n-1)\times (n-1)$ submatrix
that remains after the entire $i$th row and $j$th column have been deleted from $A$.

A {\em cofactor}
\index{cofactor}
$A_{ij}$
of an $n$-by-$n$ square matrix  $A$
is defined in terms of its associated minor by
\begin{equation}
A_{ij}=(-1)^{i+j}M_{ij}.
\end{equation}

The {\em determinant} of a square matrix $A$, denoted by
$\textrm{det} A$ or $\vert A\vert$, is a scalar regursively defined by
\begin{equation}
\textrm{det}A
=\sum_{j=1}^n a_{ij}A_{ij}
=\sum_{i=1}^n a_{ij}A_{ij}
\end{equation}
for any $i$ (row expansion) or $j$ (column expansion), with $i,j=1,\ldots ,n$.
For $1\times 1$ matrices, $\textrm{det}A =a_{11}$.

\subsection{Properties}

The following properties of determinants are mentioned without proof:

\begin{itemize}
\item[(i)]
If $A$ and $B$ are square matrices of the same order, then
$\textrm{det}AB = \textrm{det}A  \textrm{det}B$.

\item[(ii)]
If either two rows or two columns are exchanged, then the determinant is multiplied
by a factor ``$-1$.''


\item[(iii)]
$\textrm{det}(A^T) = \textrm{det}A .$

\item[(iv)]
The determinant $\textrm{det}A $ of a matrix $A$ is non-zero if and only if $A$ is invertible.
In particular, if $A$ is not invertible, $\textrm{det}A =0$.
If $A$ has an inverse matrix $A^{-1}$, then $\textrm{det}(A^{-1}) = (\textrm{det}A)^{-1} $.


\item[(v)]
Multiplication of any row or column with a factor $\alpha$  results in a determinant
which is $\alpha$ times the original determinant.
\end{itemize}

\section{Trace}
\index{trace}
\index{Spur}

\subsection{Definition}
\marginnote{The German word for trace is {\em Spur}.}
The {\em trace} of an $n$-by-$n$ square matrix $A=a_{ij}$, denoted by
$\textrm{Tr} A$,  is a scalar
defined to be the sum of the elements on the main diagonal
 (the diagonal from the upper left to the lower right) of A; that is  (also in Dirac's bra and ket notation),
\begin{equation}
\textrm{Tr}A
= a_{11} +a_{22}+ \cdots +a_{nn}
=\sum_{i=1}^n a_{ii}=\sum_{i=1}^n \langle i \vert A\vert i \rangle.
\end{equation}

\subsection{Properties}

The following properties of traces are mentioned without proof:

\begin{itemize}
\item[(i)]
$\textrm{Tr}(A+B)=\textrm{Tr}A+\textrm{Tr}B$;
\item[(ii)]
$\textrm{Tr}(\alpha A)= \alpha \textrm{Tr}A$, with $\alpha \in {\Bbb C}$;
\item[(iii)]
$\textrm{Tr}(AB) = \textrm{Tr}(BA)$;
\item[(iv)]
$\textrm{Tr}A = \textrm{Tr}A^T$;
\item[(v)]
$\textrm{Tr}(A\otimes B)= \textrm{Tr}A \textrm{Tr}B$;
\item[(vi)]
the trace is the sum of the eigenvalues of a normal operator;
\item[(vii)]
$ \textrm{det}(e^A)=e^{\textrm{Tr}A} $;
\item[(viii)]
 the trace is the derivative of the determinant at the identity;
\item[(xi)]
the complex conjugate of the trace of an operator is equal to the trace of its adjoint; that is
$\overline{(  \textrm{Tr} A)}=\textrm{Tr} (A^\dagger)$;
\item[(xi)]
the trace is invariant under rotations of the basis and under cyclic permutations.
\end{itemize}


A {\em trace class} operator is a compact operator for which a trace is finite and independent of the choice of basis.
\index{trace class}

\section{Adjoint}
\index{adjoints}

\subsection{Definition}

Let ${\frak V}$ be a vector space and let ${\bf y}$
be any element of its dual space ${\frak V}^*$.
For any linear transformation $\textsf{\textbf{A}}$, consider
the bilinear functional
\marginnote{Here $[\cdot ,\cdot ]$ is the bilinear functional, not the commutator.}
${\bf y}' ({\bf x}) =[{\bf x} ,{\bf y}'] =[\textsf{\textbf{A}}{\bf x},{\bf y}]$
Let the {\em adjoint} transformation $\textsf{\textbf{A}}^\dagger$ be defined by
\begin{equation}
[{\bf x},\textsf{\textbf{A}}^\dagger{\bf y}]=
[\textsf{\textbf{A}}{\bf x},{\bf y}].
\end{equation}


\subsection{Properties}
We mention without proof that the adjoint operator is a linear operator.
Furthermore,
$\textsf{\textbf{0}}^\dagger = \textsf{\textbf{0}}$,
$\textsf{\textbf{1}}^\dagger = \textsf{\textbf{1}}$,
$(\textsf{\textbf{A}}+\textsf{\textbf{B}})^\dagger = \textsf{\textbf{A}}^\dagger+\textsf{\textbf{B}}^\dagger$,
$(\alpha \textsf{\textbf{A}})^\dagger = \alpha \textsf{\textbf{A}}^\dagger$,
$( \textsf{\textbf{A}}\textsf{\textbf{B}})^\dagger =   \textsf{\textbf{B}}^\dagger
 \textsf{\textbf{A}}^\dagger$,
and
$( \textsf{\textbf{A}}^{-1})^\dagger
=
( \textsf{\textbf{A}}^\dagger )^{-1}
$;
as well as  (in finite dimensional spaces)
\begin{equation}
\textsf{\textbf{A}}^{\dagger \dagger}=
\textsf{\textbf{A}}.
\end{equation}

\subsection{Matrix notation}

In matrix notation and in complex vector space with the dot product,
note that there is a correspondence with the inner product
(cf. page \pageref{2011-m-corr-bil-ip})
so that, for all ${\bf z}\in {\frak V}$ and for all ${\bf x}\in {\frak V}$,
 there exist a unique ${\bf y}\in {\frak V}$ with
\begin{equation}
\begin{array}{l}
[\textsf{\textbf{A}}{\bf x}, {\bf z}]\\
\qquad =\langle \textsf{\textbf{A}} {\bf x}\mid {\bf y}\rangle  \\
\qquad =\overline{ \langle{\bf y}\mid \textsf{\textbf{A}} {\bf x}\rangle } \\
\qquad = y_i\overline{ A}_{ij} x_j  \\
\qquad = y_i\overline{ A}_{ji}^T x_j  \\
\qquad \qquad  = x\overline{ A}^T y\\
=
[{\bf x}, \textsf{\textbf{A}}^\dagger {\bf z}]\\
\qquad =
\langle {\bf x}\mid \textsf{\textbf{A}}^\dagger {\bf y}\rangle        \\
\qquad = x_i A_{ij}^\dagger y_j   \\
\qquad \qquad   =   x A ^\dagger y,
\end{array}
\end{equation}
and hence
\begin{equation}
A ^\dagger =(\overline{ A})^T =\overline{ A^T}, \textrm{ or } A^\dagger_{ij}=\overline{A}_{ji} .
\end{equation}
In words: in matrix notation, the adjoint transformation is just the
transpose of the complex conjugate of the original matrix.

\section{Self-adjoint transformation}
\index{self-adjoint transformation}



The following definition yields some analogy to real numbers as compared to complex numbers
(``a complex number $z$ is real if $\overline{z}=z$''),
expressed in terms of operators on a complex vector space.
An operator    $\textsf{\textbf{A}}$   on a linear vector space   ${\frak V}$
is called {\em self-adjoint}, if
\begin{equation}
\textsf{\textbf{A}}^{\dagger}=
\textsf{\textbf{A}}.
\end{equation}
In terms of matrices, a matrix $A$ corresponding to an operator $\textsf{\textbf{A}}$ in
some fixed basis is self-adjoint
if
\begin{equation}
A^{\dagger}= (\overline{A_{ij}})^T=  \overline{A_{ij}} =A_{ij}=A.
\end{equation}

More generally, if the matrix corresponding to $\textsf{\textbf{A}}$  in some basis ${\frak B}$
is $A_{ij}$,
then the matrix corresponding to $\textsf{\textbf{A}}^*$ with respect to the dual basis
${\frak B}^*$
is
$\overline{(A_{ij}})^T$.

In real inner product spaces,
the usual word for a self-adjoint  transformation
is {\em symmetric} transformation.
In complex inner product spaces,
the usual word for self-adjoint
is {\em Hermitian} transformation.
\index{symmetric transformation}
\index{Hermitian transformation}

{\color{blue}
\bexample
For the sake of an example, consider again the
{\em Pauli spin matrices}
\index{Pauli spin matrices}
\begin{equation}
\begin{array}{l}
\sigma_1=\sigma_x=
\left(
\begin{array}{rrrr}
0&1\\
1&0
\end{array}
\right),   \\
\sigma_2=\sigma_y=
\left(
\begin{array}{rrrrrr}
0&-i\\
i&0
\end{array}
\right),   \\
\sigma_1=\sigma_z=
\left(
\begin{array}{rrrrrrr}
1&0\\
0&-1
\end{array}
\right).
\end{array}
\end{equation}
which, together with unity, i.e., ${\Bbb I}_2=\textrm{diag}(1,1)$,  are all self-adjoint.

The following operators are not self-adjoint:
\begin{equation}
\left(
\begin{array}{rrrrrrrr}
0&1\\
0&0
\end{array}
\right) ,
\left(
\begin{array}{rrrrrrrrrr}
1&1\\
0&0
\end{array}
\right)
,
\left(
\begin{array}{rrrrrrrrrrrr}
1&0\\
i&0
\end{array}
\right),
\left(
\begin{array}{rrrrrrrrrrrrrrrr}
0&i\\
i&0
\end{array}
\right)   .{\textrm{\eexample}}
\end{equation}
%
}

\section{Positive transformation}
\index{positive transformation}

A linear transformation  $\textsf{\textbf{A}}$ on an inner product space ${\frak V}$ is {\em positive},
that is in symbols $\textsf{\textbf{A}}\ge 0$, if it is self-adjoint,
and if $\langle \textsf{\textbf{A}}{\bf x}\mid {\bf x}\rangle  \ge 0$ for all ${\bf x}\in {\frak V}$.
If  $\langle \textsf{\textbf{A}}{\bf x}\mid {\bf x}\rangle = 0$ implies
${\bf x}=0$, $\textsf{\textbf{A}}$ is called {\em strictly positive}.

\section{Unitary transformations and isometries}
\index{unitary transformation}
\marginnote{For proofs and additional information see \S 73 in    \cite{halmos-vs}}
\index{isometry}

\subsection {Definition}
Note that a complex number $z$ has absolute value one if $\overline{z}=1/z$, or $z\overline{z}=1$.
In analogy to this ``modulus one'' behavior,
consider {\em unitary transformations}, or, used synonymuously, {\em isometries}
$\textsf{\textbf{U}}$ for which
\begin{equation}
\textsf{\textbf{U}}^*= \textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^{-1},
\textrm{ or } \textsf{\textbf{U}}\textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^\dagger \textsf{\textbf{U}}=\textsf{\textbf{I}}.
\end{equation}
Alternatively, we mention without proof that the following conditions are equivalent:
\begin{itemize}
\item[(i)]
$\langle \textsf{\textbf{U}}{\bf x}\mid \textsf{\textbf{U}}{\bf y} \rangle
=
\langle {\bf x}\mid {\bf y} \rangle$ for all ${\bf x} ,{\bf y} \in {\frak V}$;
\item[(ii)]
$\| \textsf{\textbf{U}}{\bf x}\|
=
\|{\bf x}\|$ for all ${\bf x} ,{\bf y} \in {\frak V}$;
\end{itemize}

\subsection {Characterization of change of orthonormal basis}

Let ${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
be an orthonormal basis of an $n$-dimensional inner product space ${\frak V}$.
If
$\textsf{\textbf{U}}$ is an isometry, then
 $\textsf{\textbf{U}}{\frak B}=\{\textsf{\textbf{U}}{\bf f}_1, \textsf{\textbf{U}} {\bf f}_2,
\ldots ,\textsf{\textbf{U}} {\bf f}_n\}$
is also an orthonormal basis of  ${\frak V}$.
(The converse is also true.)

\subsection {Characterization in terms of orthonormal basis}



A complex matrix $\textsf{\textbf{U}}$ is unitary if and only if its row (or column) vectors form
an orthonormal basis.

This can be readily verified \cite{Schwinger.60} by writing $\textsf{\textbf{U}}$
in terms of two orthonormal bases
${\frak B}=\{{\bf e}_1,  {\bf e}_2, \ldots , {\bf e}_n\}$
${\frak B}'=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$ as
\begin{equation}
\textsf{\textbf{U}}_{ef}= \sum_{i=1}^n  {\bf e}_i^\dagger {\bf f}_i
=  \sum_{i=1}^n  \vert {\bf e}_i\rangle \langle {\bf f}_i \vert
.
\end{equation}
Together with $\textsf{\textbf{U}}_{fe}= \sum_{i=1}^n  {\bf f}_i^\dagger {\bf e}_i=  \sum_{i=1}^n  \vert {\bf f}_i\rangle \langle {\bf e}_i \vert $
we form
\begin{equation}
\begin{array}{l}
{\bf e}_k \textsf{\textbf{U}}_{ef}\\
\quad = {\bf e}_k\sum_{i=1}^n  {\bf e}_i^\dagger {\bf f}_i \\
\quad
= \sum_{i=1}^n  ({\bf e}_k{\bf e}_i^\dagger) {\bf f}_i \\
\quad
= \sum_{i=1}^n  \delta_{ki} {\bf f}_i \\
\quad  = {\bf f}_k
.
\end{array}
\end{equation}
In a similar wey we find that
\begin{equation}
\begin{array}{l}
\textsf{\textbf{U}}_{ef} {\bf f}_k^\dagger = {\bf e}_k^\dagger,\\
{\bf f}_k\textsf{\textbf{U}}_{fe}   = {\bf f}_k,\\
\textsf{\textbf{U}}_{fe} {\bf e}_k^\dagger = {\bf f}_k^\dagger.
\end{array}
\end{equation}
Moreover,
\begin{equation}
\begin{array}{l}
\textsf{\textbf{U}}_{ef}\textsf{\textbf{U}}_{fe}\\
\quad
=
 \sum_{i=1}^n  \sum_{j=1}^n
\vert {\bf e}_i\rangle \langle {\bf f}_i \vert
\vert {\bf f}_j\rangle \langle {\bf e}_j \vert \\
\quad
=
 \sum_{i=1}^n  \sum_{j=1}^n
\vert {\bf e}_i\rangle \delta_{ij} \langle {\bf e}_j \vert \\
\quad
=
 \sum_{i=1}^n
\vert {\bf e}_i\rangle   \langle {\bf e}_i \vert \\
\quad
=
{\Bbb I}
.
\end{array}
\end{equation}
In a similar way we obtain
$\textsf{\textbf{U}}_{fe}\textsf{\textbf{U}}_{ef}=
{\Bbb I}$.
Since
\begin{equation}
\textsf{\textbf{U}}_{ef}^\dagger = \sum_{i=1}^n ( {\bf e}_i^\dagger)^\dagger {\bf f}_i^\dagger
= \textsf{\textbf{U}}_{fe},
\end{equation}
we obtain that $\textsf{\textbf{U}}_{ef}^\dagger = (\textsf{\textbf{U}}_{ef})^{-1}$
and $\textsf{\textbf{U}}_{fe}^\dagger = (\textsf{\textbf{U}}_{fe})^{-1}$.

Note also that the {\em composition} holds; that is, $\textsf{\textbf{U}}_{ef} \textsf{\textbf{U}}_{fg}=  \textsf{\textbf{U}}_{eg}$.



If we
identify one of the bases  ${\frak B}$ and ${\frak B}'$ by the Cartesian standard basis,
it becomes clear that, for instance,
every unitary operator  $\textsf{\textbf{U}}$  can be written in terms of an orthonormal basis
${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
by ``stacking'' the vectors of that orthonormal basis ``on top of each other;''
that is
\marginnote{For proofs and additional information see
 \S 5.11.3, Theorem 5.1.5 and subsequent Corollary in   \cite{Joglekar-I}}
\begin{equation}
\textsf{\textbf{U}}= \left(
\begin{array}{c}
{\bf f}_1\\
{\bf f}_2\\
\vdots\\
{\bf f}_n
\end{array}
\right).
\end{equation}
Thereby the vectors of the orthonormal basis  ${\frak B}$ serve as the
rows of $\textsf{\textbf{U}}$.

Also, every unitary operator  $\textsf{\textbf{U}}$  can be written in terms of an orthonormal basis
${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
by ``pasting'' the (transposed) vectors of that orthonormal basis ``one after another;''
that is
\begin{equation}
\textsf{\textbf{U}}= \left(
{\bf f}_1^T,
{\bf f}_2^T,
\cdots,
{\bf f}_n^T
\right).
\end{equation}
Thereby the (transposed) vectors of the orthonormal basis  ${\frak B}$ serve as the
columns of $\textsf{\textbf{U}}$.

Of course, any permutation of vectors in ${\frak B}$ would also yield unitary matrices.







\section{Orthogonal projectors}
\index{orthogonal projector}
\marginnote{For proofs and additional information see \S 75 \& 76 in    \cite{halmos-vs}}

A linear transformation $\textsf{\textbf{E}}$ is an orthogonal projector
if and only if $\textsf{\textbf{E}} = \textsf{\textbf{E}}^2=\textsf{\textbf{E}}^*$.

If $\textsf{\textbf{E}}_1,\textsf{\textbf{E}}_2, \ldots , \textsf{\textbf{E}}_n$ are orthogonal
projectors,
then a necessary and sufficient condition that
$\textsf{\textbf{E}} =\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2+\cdots +\textsf{\textbf{E}}_n$
be an orthogonal projector is that  $\textsf{\textbf{E}}_i \textsf{\textbf{E}}_j =0$
whenever $i\neq j$; that is, that all $E_i$ are pairwise orthogonal.



\section{Proper value or eigenvalue}
\index{proper value}
\marginnote{For proofs and additional information see \S 54 in    \cite{halmos-vs}}
\index{proper vector}
\index{eigenvalue}
\index{eigenvector}
\index{eigensystem}

\subsection{Definition}

A scalar $\lambda$ is a {\em proper value} or {\em eigenvalue},
and a non-zero vector ${\bf x}$ is a {\em proper vector} or {\em eigenvector}
of a linear transformation $\textsf{\textbf{A}}$
if
\begin{equation}
\textsf{\textbf{A}}{\bf x}=   \lambda {\bf x}.
\end{equation}
In an $n$-dimensional
vector space $\frak V$
The set of the set of eigenvalues and the set of the associated eigenvectors
$\{\{\lambda_1,\ldots ,\lambda_k\},\{{\bf x}_1,\ldots ,{\bf x}_n\}\}$
of a linear transformation $\textsf{\textbf{A}}$ form an {\em eigensystem} of $\textsf{\textbf{A}}$.

\subsection{Determination}
\index{characteristic equation}
\index{sekular determinant}
\index{sekular equation}


% http://vergil.chemistry.gatech.edu/notes/linear_algebra/node5.html

Since the eigenvalues and eigenvectors are those scalars $\lambda$  vectors ${\bf x}$ for which $\textsf{\textbf{A}}{\bf x}=   \lambda {\bf x}$,
this equation can be rewritten with a zero vector on the right side of the equation; that is ($\textsf{\textbf{I}}=\textrm{diag}(1,\ldots ,1)$ stands for the identity matrix),
\begin{equation}
(\textsf{\textbf{A}} - \lambda \textsf{\textbf{I}}){\bf x}= {\bf 0}.
\label{2011-m-eve}
\end{equation}
Suppose that $\textsf{\textbf{A}} - \lambda \textsf{\textbf{I}}$ is invertible. Then we could formally write
${\bf x} = (\textsf{\textbf{A}} - \lambda \textsf{\textbf{I}})^{-1}{\bf 0}$; hence ${\bf x}$ must be the zero vector.

We are not interested in this trivial solution of Eq. (\ref{2011-m-eve}).
Therefore, suppose that, contrary to the previous assumption,
$\textsf{\textbf{A}} - \lambda \textsf{\textbf{I}}$ is {\em not} invertible.
We have mentioned earlier (without proof) that this implies that its determinant vanishes; that is,
\begin{equation}
\textrm{det} (\textsf{\textbf{A}} - \lambda \textsf{\textbf{I}}) = \vert \textsf{\textbf{A}} - \lambda \textsf{\textbf{I}}\vert =0.
\end{equation}
This is called the {\em sekular determinant};
and the corresponding equation after expansion of the determinant is called the
{\em sekular equation}
or {\em characteristic equation}.
Once the eigenvalues, that is, the roots (i.e., the solutions) of this equation are determined,
the eigenvectors can be obtained one-by-one by inserting these eigenvalues one-by-one into Eq. (\ref{2011-m-eve}).


{\color{blue}
\bexample
For the sake of an example, consider  the
{matrix}
\begin{equation}
A=
\left(
\begin{array}{rrrr}
1&0&1\\
0&1&0\\
1&0&1
\end{array}
\right).
\end{equation}

The secular determinant yields
$$
\left|
\begin{array}{ccccc}
1-\lambda &0&1\\
0&1-\lambda &0\\
1&0&1-\lambda
\end{array}
\right| = 0,
$$
which yields the characteristic equation
$
(1-\lambda )^3 -(1-\lambda ) =(1-\lambda )[(1-\lambda )^2 - 1]=(1-\lambda )[\lambda ^2 - 2\lambda ]= - \lambda (1-\lambda )(2-\lambda ) =0$,
and therefore three  eigenvalues
$\lambda_1=0$,
$\lambda_2=1$, and
$\lambda_3=2$ which are the roots of $\lambda (1-\lambda )(2-\lambda ) =0$.

Let us now determine the eigenvectors of $A$, based on the eigenvalues.
Insertion  $\lambda_1=0$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\left(
\begin{array}{rrrr}
1&0&1\\
0&1&0\\
1&0&1
\end{array}
\right)  -
\left(
\begin{array}{rrrr}
0&0&0\\
0&0&0\\
0&0&0
\end{array}
\right)
\right]
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
1&0&1\\
0&1&0\\
1&0&1
\end{array}
\right)
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0\\
0\\
0
\end{array}
\right)
;
\end{equation}
therefore $x_1+x_3=0$ and $x_2=0$.
We are free to choose any (nonzero) $x_1=-x_3$,
but if we are interested in normalized eigenvectors, we obtain
${\bf x}_1 =(1/\sqrt{2})(1,0,-1)^T$.

Insertion  $\lambda_2=1$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\left(
\begin{array}{rrrr}
1&0&1\\
0&1&0\\
1&0&1
\end{array}
\right)  -
\left(
\begin{array}{rrrr}
1&0&0\\
0&1&0\\
0&0&1
\end{array}
\right)
\right]
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0&0&1\\
0&0&0\\
1&0&0
\end{array}
\right)
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0\\
0\\
0
\end{array}
\right)
;
\end{equation}
therefore $x_1=x_3=0$ and $x_2$ is arbitrary.
We are again free to choose any (nonzero) $x_2$,
but if we are interested in normalized eigenvectors, we obtain
${\bf x}_2 = (0,1,0)^T$.


Insertion  $\lambda_3=2$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\left(
\begin{array}{rrrr}
1&0&1\\
0&1&0\\
1&0&1
\end{array}
\right)  -
\left(
\begin{array}{rrrr}
2&0&0\\
0&2&0\\
0&0&2
\end{array}
\right)
\right]
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
-1&0&1\\
0&-1&0\\
1&0&-1
\end{array}
\right)
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0\\
0\\
0
\end{array}
\right)
;
\end{equation}
therefore $-x_1+x_3=0$ and $x_2=0$.
We are free to choose any (nonzero) $x_1=x_3$,
but if we are once more interested in normalized eigenvectors, we obtain
${\bf x}_1 =(1/\sqrt{2})(1,0,1)^T$.

Note that the eigenvectors are mutually orthogonal.
We can construct the corresponding orthogonal projectors by the dyadic product
of the eigenvectors; that is,
\begin{equation}
\begin{array}{l}
\textsf{\textbf{E}}_1 =
{\bf x}_1 \otimes {\bf x}_1^T =
\frac{1}{2} (1,0,-1)^T(1,0,-1) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1(1,0,-1)\\
0(1,0,-1)\\
-1(1,0,-1)
\end{array}
\right) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1&0&-1\\
0&0&0\\
-1&0&1
\end{array}
\right)
\\
\textsf{\textbf{E}}_{2} =
{\bf x}_{2} \otimes {\bf x}_{2}^T =
 (0,1,0)^T(0,1,0) =
\left(
\begin{array}{rrrr}
0(0,1,0)\\
1(0,1,0)\\
0(0,1,0)
\end{array}
\right) =
\left(
\begin{array}{rrrr}
0&0&0\\
0&1&0\\
0&0&0
\end{array}
\right)
\\
\textsf{\textbf{E}}_{3} =
{\bf x}_{3} \otimes {\bf x}_{3}^T =
\frac{1}{2} (1,0,1)^T(1,0,1) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1(1,0,1)\\
0(1,0,1)\\
1(1,0,1)
\end{array}
\right) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1&0&1\\
0&0&0\\
1&0&1
\end{array}
\right)
\end{array}
\end{equation}
Note also that $A$ can be written as the sum of the products of the
eigenvalues with the associated projectors; that is (here, $\textsf{\textbf{E}}$
stands for the corresponding matrix),
$A= 0  \textsf{\textbf{E}}_1 + 1  \textsf{\textbf{E}}_{2} +2\textsf{\textbf{E}}_{3} $.
{\textrm{\eexample}}
}

If the eigenvalues obtained are not distinct und thus some eigenvalues are {\em degenerate},
\index{degenerate eigenvalues}
the associated eigenvectors traditionally -- that is, by convention and not necessity -- are chosen to be
{\em mutually orthogonal.}
A more formal motivation will come from the spectral theorem below.


{\color{blue}
\bexample
For the sake of an example, consider  the
{matrix}
\begin{equation}
B=
\left(
\begin{array}{rrrr}
1&0&1\\
0&2&0\\
1&0&1
\end{array}
\right).
\end{equation}

The secular determinant yields
$$
\left|
\begin{array}{ccccc}
1-\lambda &0&1\\
0&2-\lambda &0\\
1&0&1-\lambda
\end{array}
\right| = 0,
$$
which yields the characteristic equation
$
(2-\lambda )(1-\lambda )^2 +[-(2-\lambda )]=
(2-\lambda )[(1-\lambda )^2 -1]=
-\lambda (2-\lambda )^2 =0$,
and therefore just two  eigenvalues
$\lambda_1=0$,  and
$\lambda_2=2$ which are the roots of $\lambda (2-\lambda )^2 =0$.

Let us now determine the eigenvectors of $B$, based on the eigenvalues.
Insertion  $\lambda_1=0$ into Eq. (\ref{2011-m-eve})  yields
\begin{equation}
\left[
\left(
\begin{array}{rrrr}
1&0&1\\
0&2&0\\
1&0&1
\end{array}
\right)  -
\left(
\begin{array}{rrrr}
0&0&0\\
0&0&0\\
0&0&0
\end{array}
\right)
\right]
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
1&0&1\\
0&2&0\\
1&0&1
\end{array}
\right)
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0\\
0\\
0
\end{array}
\right)
;
\end{equation}
therefore $x_1+x_3=0$ and $x_2=0$.
Again we are free to choose any (nonzero) $x_1=-x_3$,
but if we are interested in normalized eigenvectors, we obtain
${\bf x}_1 =(1/\sqrt{2})(1,0,-1)^T$.

Insertion  $\lambda_2=2$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\left(
\begin{array}{rrrr}
1&0&1\\
0&2&0\\
1&0&1
\end{array}
\right)  -
\left(
\begin{array}{rrrr}
2&0&0\\
0&2&0\\
0&0&2
\end{array}
\right)
\right]
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
-1&0&1\\
0&0&0\\
1&0&-1
\end{array}
\right)
\left(
\begin{array}{rrrr}
x_1\\
x_2\\
x_3
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0\\
0\\
0
\end{array}
\right)
;
\end{equation}
therefore $x_1=x_3$; $x_2$ is arbitrary.
We are again free to choose any values of $x_1$, $x_3$ and $x_2$ as long
 $x_1=x_3$ as well as $x_2$ are satisfied.
Take, for the sake of choice, the orthogonal
normalized eigenvectors
${\bf x}_{2,1} = (0,1,0)^T$ and
${\bf x}_{2,2} = (1/\sqrt{2})(1,0,1)^T$,
which are also orthogonal to ${\bf x}_1 =(1/\sqrt{2})(1,0,-1)^T$.

Note again that we can find the corresponding orthogonal projectors by the dyadic product
of the eigenvectors; that is,  by
\begin{equation}
\begin{array}{l}
\textsf{\textbf{E}}_1 =
{\bf x}_1 \otimes {\bf x}_1^T =
\frac{1}{2} (1,0,-1)^T(1,0,-1) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1(1,0,-1)\\
0(1,0,-1)\\
-1(1,0,-1)
\end{array}
\right) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1&0&-1\\
0&0&0\\
-1&0&1
\end{array}
\right)
\\
\textsf{\textbf{E}}_{2,1} =
{\bf x}_{2,1} \otimes {\bf x}_{2,1}^T =
(0,1,0)^T(0,1,0) =
\left(
\begin{array}{rrrr}
0(0,1,0)\\
1(0,1,0)\\
0(0,1,0)
\end{array}
\right) =
\left(
\begin{array}{rrrr}
0&0&0\\
0&1&0\\
0&0&0
\end{array}
\right)
\\
\textsf{\textbf{E}}_{2,2} =
{\bf x}_{2,2} \otimes {\bf x}_{2,2}^T =
\frac{1}{2} (1,0,1)^T(1,0,1) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1(1,0,1)\\
0(1,0,1)\\
1(1,0,1)
\end{array}
\right) =
\frac{1}{2}
\left(
\begin{array}{rrrr}
1&0&1\\
0&0&0\\
1&0&1
\end{array}
\right)
\end{array}
\end{equation}
Note also that $B$ can be written as the sum of the products of the
eigenvalues with the associated projectors; that is (here, $\textsf{\textbf{E}}$
stands for the corresponding matrix),
$B= 0  \textsf{\textbf{E}}_1 + 2 (\textsf{\textbf{E}}_{1,2} + \textsf{\textbf{E}}_{1,2} )$.
This leads us to the much more general spectral theorem.
{\textrm{\eexample}}
}


The following theorems are enumerated without proofs.

If $\textsf{\textbf{A}}$
is a self-adjoint transformation on an inner product space, then every proper value (eigenvalue)  of $\textsf{\textbf{A}}$
is real.
If $\textsf{\textbf{A}}$ is positive, or stricly positive,
then every proper value of  $\textsf{\textbf{A}}$ is positive, or stricly positive, respectively

Due to their idempotence $\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}$,
projectors have eigenvalues $0$ or $1$.

Every eigenvalue of an isometry has absolute value one.

If  $\textsf{\textbf{A}}$  is either a self-adjoint transformation or an isometry,
then proper vectors of $ \textsf{\textbf{A}}$
belonging to distinct proper values are orthogonal.


\section{Normal transformation}
\index{normal transformation}

A transformation $\textsf{\textbf{A}}$ is called {\em normal}
if it commutes with its adjoint; that is, $[\textsf{\textbf{A}},\textsf{\textbf{A}}^*]= \textsf{\textbf{A}}\textsf{\textbf{A}}^* -
\textsf{\textbf{A}}^* \textsf{\textbf{A}} =0$.

It follows from their definition that Hermitean and unitary transformations are normal.


We mention without proof that
a normal transformation on a finite-dimensional unitary space is
(i) Hermitian,
(ii) positive,
(iii) strictly positive,
(iv) unitary,
(v) invertible,
(vi) idempotent
if and only if all its proper values are
(i') real,
(ii) positive,
(iii) strictly positive,
(iv) of absolute value one,
(v) different from zero,
(vi) equal to zero or one.

\section{Spectrum}
\index{spectrum}
\marginnote{For proofs and additional information see \S 78 in   \cite{halmos-vs}}

\subsection{Spectral theorem}

Let $\frak V$ be an $n$-dimensional linear vector space.
The {\em spectral theorem} states
\index{spectral theorem}
that to every self-adjoint (more general, normal) transformation $ \textsf{\textbf{A}}$
on an $n$-dimensional inner product space there correspond real numbers, the {\em spectrum}
$
\lambda_1,
\lambda_2, \ldots ,
\lambda_k
$
of all the eigenvalues of   $ \textsf{\textbf{A}}$,
and their associated  orthogonal projectors
$
\textsf{\textbf{E}}_1,
\textsf{\textbf{E}}_2, \ldots ,
\textsf{\textbf{E}}_k
$
where $0<k\le n$ is a strictly positive integer so that
\begin{itemize}
\item[(i)]
the $\lambda_i$ are pairwise distinct,
\item[(ii)]
the $\textsf{\textbf{E}}_i$ are pairwise orthogonal and different from $\textsf{\textbf{0}}$,
\item[(iii)]
$\sum_{i=1}^k \textsf{\textbf{E}}_i=\textsf{\textbf{I}}_n$, and
\item[(iv)]
$
\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i
$
is the {\em spectral form} of $\textsf{\textbf{A}}$.
\index{spectral form}
\end{itemize}

\subsection{Composition of the spectral form}

If the spectrum of a  Hermitian (or, more general, normal) operator $\textsf{\textbf{A}}$ is nondegenerate, that is, $k=n$, then the
$i$th projector
can be written as the dyadic or tensor product $
\textsf{\textbf{E}}_i={\bf x}_i \otimes {\bf x}_i^T$
of the $i$th normalized eigenvector ${\bf x}_i $ of $\textsf{\textbf{A}}$.
In this case, the set of all normalized eigenvectors $\{{\bf x}_1, \ldots ,{\bf x}_n\}$ is n othonormal basis of the vector space $\frak V$.
If the spectrum of $\textsf{\textbf{A}}$ is degenerate, then the projector can be chosen to be the orthogonal sum of projectors
corresponding to orthogonal eigenvectors, associated with the same  eigenvalues.

Furthermore, for a  Hermitian (or, more general, normal) operator $\textsf{\textbf{A}}$,
if $1\le i \le k$,
then there exist polynomials with real coefficients, such as,  for instance,
\begin{equation}
p_i  (t)
=
\prod_{
\begin{array}{c}
1\le j\le k\\
j\neq i
\end{array}
}\frac{t-\lambda_j}{\lambda_i -\lambda_j}
\label{2011-m-epsf}
\end{equation}
so that
$p_i(\lambda_j) =\delta_{ij}$;
moreover, for every such polynomial,
$p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$.

{\color{OliveGreen}\bproof

For a proof, it is not too difficult
to show that
$p_i  (\lambda_i)=1$, since in this case in the product of fractions all numerators are equal to denominators,
and
$p_i  (\lambda_j)=0$ for $j\neq i $, since some numerator in the product of fractions vanishes.

Now, substituting for $t$ the spectral form $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
of $\textsf{\textbf{A}}$, as well as
decomposing unity in terms of the projectors $\textsf{\textbf{E}}_i$ in the spectral form of
$\textsf{\textbf{A}}$; that is, $\textsf{\textbf{I}}_n=\sum_{i=1}^k \textsf{\textbf{E}}_i$,
yields
\begin{equation}
\begin{array}{l}
p_i  (\textsf{\textbf{A}})
=
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\textsf{\textbf{A}} - \lambda_j\textsf{\textbf{I}}_n}{\lambda_i -\lambda_j}  \\
=
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\sum_{l=1}^k \lambda_l\textsf{\textbf{E}}_l - \lambda_j\sum_{l=1}^k \textsf{\textbf{E}}_l}{\lambda_i -\lambda_j}  \\
=
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\sum_{l=1}^k \textsf{\textbf{E}}_l(\lambda_l- \lambda_j)}{\lambda_i -\lambda_j}  \\
= \sum_{l=1}^k \textsf{\textbf{E}}_l
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\lambda_l- \lambda_j}{\lambda_i -\lambda_j}  \\
= \sum_{l=1}^k \textsf{\textbf{E}}_l
\delta_{li} = \textsf{\textbf{E}}_i.  \textrm{\eproof}
\end{array}
\end{equation}
}

With the help of the polynomial $p_i(t)$ defined in Eq. (\ref{2011-m-epsf}),
which requires knowledge of the eigenvalues,
the spectral form of a Hermitian (or, more general, normal) operator  $\textsf{\textbf{A}}$ can thus be rewritten as
\begin{equation}
\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i p_i(\textsf{\textbf{A}})=  \sum_{i=1}^k \lambda_i \prod_{
1\le j\le k,\;
j\neq i
}\frac{\textsf{\textbf{A}} - \lambda_j\textsf{\textbf{I}}_n}{\lambda_i -\lambda_j}.
\end{equation}

If $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
is the spectral form of a self-adjoint transformation  $\textsf{\textbf{A}}$
on a finite-dimensional inner product space,
then a necessary and sufficient condition (``if and only if $=$ iff'')
that a linear transformation
 $\textsf{\textbf{B}}$ commutes with
 $\textsf{\textbf{A}}$
is that it commuts with each
$\textsf{\textbf{E}}_i$, $1\le i\le r$.





\section{Functions of normal transformations}
\index{Functions of normal transformation}

Suppose $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i  \textsf{\textbf{E}}_i $ is a normal transformation
with its spectral form.
If $f$ is an arbitrary complex-valued function defined at least at the eigenvalues of $\textsf{\textbf{A}}$,
then a linear transformation  $f(\textsf{\textbf{A}})$ can be defined by
\begin{equation}
f(\textsf{\textbf{A}})=\sum_{i=1}^k f(\lambda_i)  \textsf{\textbf{E}}_i.
\end{equation}

{
\color{blue}
\bexample
For the definition of the ``square root''
for every positve operator $\textsf{\textbf{A}})$, consider
\begin{equation}
\sqrt{\textsf{\textbf{A}}}=\sum_{i=1}^k \sqrt{\lambda_i}  \textsf{\textbf{E}}_i.
\end{equation}
Clearly, $(\sqrt{\textsf{\textbf{A}}})^2=\sqrt{\textsf{\textbf{A}}}\sqrt{\textsf{\textbf{A}}}= {\textsf{\textbf{A}}}$.
\eexample
}

\section{Decomposition of operators}
\marginnote{For proofs and additional information see \S 83 in   \cite{halmos-vs}}

\subsection{Standard decomposition}

In analogy to the decomposition of every imaginary number $z= \Re z +i \Im z$ with $\Re z,\Im z\in {\Bbb R}$,
every arbitrary transformation $\textsf{\textbf{A}}$ on a finite-dimensional vector space can be decomposed into two Hermitian operators
$\textsf{\textbf{B}}$
$\textsf{\textbf{C}}$
such that
\begin{equation}
\begin{array}{l}
\textsf{\textbf{A}}=\textsf{\textbf{B}} + i \textsf{\textbf{C}}; \textrm{ with }  \\
\textsf{\textbf{B}}=\frac{1}{2}(\textsf{\textbf{A}} +   \textsf{\textbf{A}}^\dagger ), \\
\textsf{\textbf{C}}=\frac{1}{2i}(\textsf{\textbf{A}} -   \textsf{\textbf{A}}^\dagger ).
\end{array}
\end{equation}


\subsection{Polar representation}

The analogue of the polar representation of every imaginary number $z= R e^{i\varphi}$ with $R,\varphi \in {\Bbb R}$, $R>0$,
$0\le \varphi < 2\pi$,
every arbitrary transformation $\textsf{\textbf{A}}$ on a finite-dimensional inner product space can be decomposed into
a unique positive transform
$\textsf{\textbf{P}}$ and an isometry
$\textsf{\textbf{U}}$, such that $\textsf{\textbf{A}}= \textsf{\textbf{U}} \textsf{\textbf{P}}$.
If $\textsf{\textbf{A}}$ is invertible, then $\textsf{\textbf{U}}$  is uniquely determined by
$\textsf{\textbf{A}}$.
A necessary and sufficient condition that $\textsf{\textbf{A}}$ is normal is that
$\textsf{\textbf{U}} \textsf{\textbf{P}}=\textsf{\textbf{P}} \textsf{\textbf{U}} $.

\subsection{Decomposition of isometries}

Any unitary or orthogonal transformation   in finite-dimensional inner product space
can be composed from a succession of two-parameter unitary transformations in
two-dimensional subspaces,
and a multiplication of a single diagonal matrix with elements of modulus one
in an algorithmic, constructive and tractable manner.
The method is similar to Gaussian elimination and facilitates the parameterization of elements
of the unitary group  in arbitrary dimensions (e.g., Ref. \cite{murnaghan}, Chapter 2).

{\color{Purple}
It has been suggested to implement
these group theoretic results by realizing interferometric analogues
of any discrete unitary and hermitean operators
in a unified and experimentally feasible way by ``generalized beam splitters''   \cite{rzbb,reck-94}.}


\subsection{Singular value decomposition}

The {\em singular value decomposition}
\index{singular value decomposition}
(SVD)
of an ($m\times n$)  matrix $\textsf{\textbf{A}}$ is a factorization of the form
\begin{equation}
\textsf{\textbf{A}} = \textsf{\textbf{U}} \Sigma \textsf{\textbf{V}} ,
\end{equation}
where
$\textsf{\textbf{U}}$ is a unitary ($m\times m$)  matrix (i.e. an isometry),
$\textsf{\textbf{V}}$ is a unitary ($n\times n$)  matrix,
and
$\Sigma$ is a unique ($m\times n$)   diagonal matrix with nonnegative real numbers on the diagonal;
that is,
\begin{equation}
\Sigma =
\left(
\begin{array}{ccc|ccc}
\sigma_1&&&&\vdots& \\
  &\ddots &&\cdots &0&\cdots \\
&&\sigma_r&&\vdots& \\
\hline
&\vdots&&&\vdots& \\
\cdots  &0&\cdots &\cdots &0&\cdots \\
&\vdots&&&\vdots& \\
&\vdots&&&\vdots& \\
\end{array}
\right).
\end{equation}
The entries $\sigma_1\ge \sigma_2 \cdots \ge \sigma_r$>0 of $\Sigma$ are called {\em singular values}
of $\textsf{\textbf{A}}$.  No proof is presented here.
\index{singular values}

\subsection{Schmidt decomposition of the tensor product of two vectors}
\index{Schmidt decomposition}
\label{2011-m-Schmidtdecomposition}

Let  ${\frak U}$  and   ${\frak V}$ be
two linear vector spaces
of dimension $n\ge m$ and $m$, respectively.
Then, for any vector
${\bf z} \in {\frak U}\otimes {\frak V}$
in the tensor product space,
there exist
orthonormal basis sets of vectors
$\{ {\bf u}_1, \ldots ,{\bf u}_n \}  \subset  {\frak U}$
and
$\{ {\bf v}_1, \ldots ,{\bf v}_m \}  \subset  {\frak V}$
such that
${\bf z}=\sum_{i=1}^m
\sigma_i  {\bf u}_i \otimes  {\bf v}_i$,
where the $\sigma_i$s are nonnegative scalars and the set of scalars is uniquely determined by
${\bf z}$.

Equivalently \cite{nielsen-book}, suppose that
$\vert {\bf z}\rangle $
 is some tensor product  contained in   the set of all tensor products of vectors
$ {\frak U}\otimes {\frak V}$ of     two linear vector spaces
 ${\frak U}$  and   ${\frak V}$.
Then there exist orthonoral vectors
$ \vert {\bf u}_i  \rangle \in  {\frak U}$
and
$ \vert {\bf v}_j  \rangle \in  {\frak V}$
so that
\begin{equation}
  \vert {\bf z}\rangle = \sum_i \sigma_i   \vert {\bf u}_i  \rangle  \vert {\bf v}_i  \rangle ,
\label{2011-e-sd}
\end{equation}
where the  $\sigma_i$s are nonnegative scalars; if $  \vert {\bf z}\rangle$
is normalized, then the  $\sigma_i$s are  satisfying
$\sum_i \sigma_i^2=1$;
they are called the
{\em Schmidt coefficients}.
\index{Schmidt coefficients}

For a proof by reduction to the singular value decomposition,
let
$\vert i\rangle$
and
$\vert j\rangle$
be any two fixed orthonormal bases of $ {\frak U}$ and $ {\frak V}$, respectively.
Then,
$\vert {\bf z}\rangle $
can be expanded as
$\vert {\bf z}\rangle  = \sum_{ij}a_{ij} \vert i\rangle \vert j\rangle$,
where the $a_{ij}$s can be interpreted as the components of a matrix
$\textsf{\textbf{A}}$.
$\textsf{\textbf{A}}$ can then be subjected to a
singular value decomposition
$\textsf{\textbf{A}} = \textsf{\textbf{U}} \Sigma \textsf{\textbf{V}}$,
or, written in index form (note that $\Sigma=\textrm{diag}(\sigma_1, \ldots, \sigma_n)$ is a diagonal matrix),
$a_{ij}= \sum_l u_{il}\sigma_l v_{lj}$;
and hence  $\vert {\bf z}\rangle  = \sum_{ijl} u_{il}\sigma_l v_{lj}\vert i\rangle \vert j\rangle$.
Finally, by identifying
$\vert {\bf u}_l  \rangle = \sum_i u_{il} \vert i\rangle$
as well as
$\vert {\bf v}_l  \rangle = \sum_l v_{lj} \vert j\rangle$
one obtains the Schmidt decompsition (\ref{2011-e-sd}).
Since $u_{il}$ and $v_{ lj}$ represent unitary martices,
and because
 $\vert i\rangle$ as well as
 $\vert j\rangle$
are orthonormal,
the newly formed vectors
$\vert {\bf u}_l \rangle$
as well as
$\vert {\bf v}_l  \rangle$
form orthonormal bases as well.
The sum of squares of the $\sigma_i$s is one if  $\vert {\bf z}\rangle $ is a unit vector,
because  (note that $\sigma_i$s are real-valued)
 $\langle {\bf z}\vert {\bf z}\rangle =1
=   \sum_{lm} \sigma_l \sigma_m   \langle {\bf u}_l  \vert  {\bf u}_m  \rangle   \langle  {\bf v}_l  \vert  {\bf v}_m  \rangle
=   \sum_{lm} \sigma_l \sigma_m  \delta_{lm}
=   \sum_{l} \sigma_l^2
$.






\section{Commutativity}
\marginnote{For proofs and additional information see \S 84 in    \cite{halmos-vs}}

A set $\textsf{\textbf{M}}
=
\{
\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\}
$
of  self-adjoint transformations on a finite-dimensional inner product space
are mutually commuting if and only if there exists
a self-adjoint transformation  $\textsf{\textbf{R}}$ and
a set of real-valued functions
$F
=
\{
f_1,
f_2,
\ldots ,
f_k
\}
$ of a real variable so that
$
\textsf{\textbf{A}}_1=f_1 (\textsf{\textbf{R}})
$,
$
\textsf{\textbf{A}}_2=f_2 (\textsf{\textbf{R}})
$,
$\ldots $,
$\textsf{\textbf{A}}_k=f_k (\textsf{\textbf{R}})$.
If such a {\em maximal operator} $\textsf{\textbf{R}}$ exists, then
it can be written as a function of all transformations in the set $\textsf{\textbf{M}}$; that is,
$\textsf{\textbf{R}}=G(\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k)$,
where $G$ is a suitable real-valued function of $n$ variables
(cf. Ref. \cite{v-neumann-31}, Satz 8).
\index{maximal operator}

The  maximal operator $\textsf{\textbf{R}}$ can be interpreted as
containing all the information of a collection of commuting operators at once;
stated pointedly, rather than consider all the  operators in $\textsf{\textbf{M}}$
separately,
the  maximal operator  $\textsf{\textbf{R}}$  represents  $\textsf{\textbf{M}}$;
in a sense, the operators  $\textsf{\textbf{A}}_i \in \textsf{\textbf{M}}$
are all just incomplete {\em aspects}  of,
or   individual functional views on, the  maximal operator $\textsf{\textbf{R}}$.

\newcommand{\comment}[1]{}
\comment{


a = {{0, 1, 0}, {1, 0, 0}, {0, 0, 0}};
b = {{2, 3, 0}, {3, 2, 0}, {0, 0, 0}};
c = {{5, 7, 0}, {7, 5, 0}, {0, 0, 11}};

Commutator[x_,y_]=x.y-y.x;

MatrixForm[Commutator[a, b]]
MatrixForm[Commutator[a, c]]
MatrixForm[Commutator[b, c]]


Eigensystem[a]
Eigensystem[b]
Eigensystem[c]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};

Solve[a.{{x11, x12,x13},
{x21, x22,x23},
{x31, x32,x33}}-{{x11, x12,x13},
{x21, x22,x23},
{x31, x32,x33}}.a ==  {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}},{x11, x12,x13,x21, x22,x23,x31, x32,x33}]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};
b = {{0, 0, 2}, {0, 0, 0}, {0, 0, 0}};

Solve[{a.{{x11, x12, x13}, {x21, x22, x23}, {x31, x32, x33}} - {{x11,
       x12, x13}, {x21, x22, x23}, {x31, x32, x33}}.a == {{0, 0,
     0}, {0, 0, 0}, {0, 0, 0}},
  b.{{x11, x12, x13}, {x21, x22, x23}, {x31, x32, x33}} - {{x11, x12,
       x13}, {x21, x22, x23}, {x31, x32, x33}}.b == {{0, 0, 0}, {0, 0,
      0}, {0, 0, 0}}}, {x11, x12, x13, x21, x22, x23, x31, x32, x33}]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};
b = {{0, 0, 2}, {0, 0, 0}, {0, 0, 0}};
c = {{3, 5, 7}, {0, 3, 0}, {0, 0, 3}};

Commutator[x_,y_]=x.y-y.x;

MatrixForm[Commutator[a, b]]
MatrixForm[Commutator[a, c]]
MatrixForm[Commutator[b, c]]
}


{\color{blue}
\bexample
Let us demonstrate the machinery developed so far by an example.
Consider the normal matrices
$$
\textsf{\textbf{A}} = \left( \begin{array}{ccc}0& 1& 0\\ 1& 0& 0\\ 0& 0& 0\end{array}\right),\;
\textsf{\textbf{B}} = \left( \begin{array}{ccc}2& 3& 0\\ 3& 2& 0\\ 0& 0& 0\end{array}\right),\;
\textsf{\textbf{C}} = \left( \begin{array}{ccc}5& 7& 0\\ 7& 5& 0\\ 0& 0& 11\end{array}\right),
$$
which are mutually commutative; that is,
$
[\textsf{\textbf{A}}, \textsf{\textbf{B}}]=
\textsf{\textbf{A}} \textsf{\textbf{B}}-\textsf{\textbf{B}}\textsf{\textbf{A}}=
[\textsf{\textbf{A}}, \textsf{\textbf{C}}]=
\textsf{\textbf{A}} \textsf{\textbf{C}}-\textsf{\textbf{B}}\textsf{\textbf{C}}=
[\textsf{\textbf{B}}, \textsf{\textbf{C}}]=
\textsf{\textbf{B}} \textsf{\textbf{C}}-\textsf{\textbf{C}}\textsf{\textbf{B}}=0$.

The eigensystems -- that is, the set of the set of eigenvalues and the set of the associated eigenvectors -- of $\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$
and
$\textsf{\textbf{C}}$
are
$$
\begin{array}{l}
\{\{1,-1,  0\}, \{(1, 1, 0)^T, (-1, 1, 0)^T, (0, 0, 1)^T\}\} ,\\
\{\{5, -1, 0\},  \{(1, 1, 0)^T, (-1, 1, 0)^T, (0, 0, 1)^T\}\},\\
\{\{12, -2, 11\},  \{(1, 1, 0)^T, (-1, 1, 0)^T, (0, 0, 1)^T\}\}.
\end{array}
$$
They share a common orthonormal set of eigenvalues
$$
\left\{
\frac{1}{\sqrt{2}}\left(
\begin{array}{rrr}
1\\ 1\\ 0
\end{array}\right),
\frac{1}{\sqrt{2}}\left(
\begin{array}{rrr}
-1\\ 1\\ 0
\end{array}\right),
\left(
\begin{array}{rrr}
0\\ 0\\ 1\end{array}\right)
\right\}
$$
which form an orthonormal basis of ${\Bbb R}^3$ or ${\Bbb C}^3$.
The associated projectors are obtained by the dyadic or tensor products of these vectors; that is,
$$
\begin{array}{l}
\textsf{\textbf{E}}_1= \frac{1}{2}
\left(
\begin{array}{rrr}
1& 1& 0\\
1& 1& 0\\
0& 0& 0
\end{array}
\right),\\
\textsf{\textbf{E}}_2= \frac{1}{2}
\left(
\begin{array}{rrr}
1& -1& 0\\
-1& 1& 0\\
0& 0& 0
\end{array}
\right),\\
\textsf{\textbf{E}}_3=
\left(
\begin{array}{rrr}
0& 0& 0\\
0& 0& 0\\
0& 0& 1
\end{array}
\right).
\end{array}
$$
Thus the spectral decompositions of
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$  and
$\textsf{\textbf{C}}$ are
\begin{equation}
\begin{array}{l}
\textsf{\textbf{A}}= \textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0  \textsf{\textbf{E}}_3,\\
\textsf{\textbf{B}}= 5\textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0 \textsf{\textbf{E}}_3,\\
\textsf{\textbf{C}}= 12\textsf{\textbf{E}}_1  -2 \textsf{\textbf{E}}_2  + 11\textsf{\textbf{E}}_3,
\end{array}
\label{2011-m-empc}
\end{equation}
respectively.

One way to define the  maximal operator  $\textsf{\textbf{R}}$ for this problem
would be
$$
\textsf{\textbf{R}} = \alpha \textsf{\textbf{E}}_1  + \beta \textsf{\textbf{E}}_2  + \gamma  \textsf{\textbf{E}}_3,
$$
with
$\alpha ,  \beta ,   \gamma \in {\Bbb R}-0$ and
$\alpha  \neq \beta  \neq   \gamma \neq \alpha  $.
The functional coordinates
$f_i(\alpha )$, $f_i(\beta)$, and $f_i(\gamma)$,
$i\in \{\textsf{\textbf{A}},\textsf{\textbf{B}},\textsf{\textbf{C}}\}$,  of the three functions
$ f_\textsf{\textbf{A}}(\textsf{\textbf{R}})$,
$ f_\textsf{\textbf{B}}(\textsf{\textbf{R}})$, and
$ f_\textsf{\textbf{C}}(\textsf{\textbf{R}})$
chosen to match the projector coefficients obtained in Eq. (\ref{2011-m-empc});
that is,
$$
\begin{array}{l}
\textsf{\textbf{A}}= f_\textsf{\textbf{A}}(\textsf{\textbf{R}})=  \textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0  \textsf{\textbf{E}}_3,\\
\textsf{\textbf{B}}=  f_\textsf{\textbf{B}}(\textsf{\textbf{R}})= 5\textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0 \textsf{\textbf{E}}_3,\\
\textsf{\textbf{C}}=  f_\textsf{\textbf{C}}(\textsf{\textbf{R}})= 12\textsf{\textbf{E}}_1  -2 \textsf{\textbf{E}}_2  + 11\textsf{\textbf{E}}_3.
\end{array}
$$
\eexample
}

It is no coincidence that the projectors in the spectral forms of
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$  and
$\textsf{\textbf{C}}$ are identical.
Indeed it can be shown that mutually commuting normal operators always share the same eigenvectors; and thus also the same projectors.

Let the set $\textsf{\textbf{M}}
=
\{
\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\}
$
be mutually commuting  normal (or Hermitian, or self-adjoint) transformations on an $n$-dimensional inner product space.
Then there exists an orthonormal basis
${\frak B}= \{
{\bf f}_1,
\ldots ,
{\bf f}_n\}$
such that every ${\bf f}_j \in {\frak B}$  is an eigenvalue  of each of the $\textsf{\textbf{A}}_i \in  \textsf{\textbf{M}}$.
Equivalently, there exist $n$ orthogonal projectors  (let the vectors ${\bf f}_j$ be represented by the coordinates which are column vectors)
$\textsf{\textbf{E}}_j= {\bf f}_j\otimes {\bf f}_j^T$
such that every $\textsf{\textbf{E}}_j$, $1\le j\le n$ occurs in the spectral form of each of the $\textsf{\textbf{A}}_i \in  \textsf{\textbf{M}}$.



\section{Measures on closed subspaces}

In what follows we shall assume that all {\em (probability) measures}
or {\em states} $w$
\index{probability measures}
\index{measures}
\index{states}
behave quasi-classically on sets of mutually commuting self-adjoint operators,
and in particular on orthogonal projectors.

Suppose
 $\textsf{\textbf{E}}
=
\{
\textsf{\textbf{E}}_1,
\textsf{\textbf{E}}_2,
\ldots ,
\textsf{\textbf{E}}_n
\}
$
is a set of mutually commuting orthogonal projectors
on a finite-dimensional inner product space   ${\frak V}$.
Then, the probability measure $w$ should be {\em additive}; that is,
\begin{equation}
w(\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2\cdots +\textsf{\textbf{E}}_n)=
w(\textsf{\textbf{E}}_1)+
w(\textsf{\textbf{E}}_2)+
\cdots +
w(\textsf{\textbf{E}}_n).
\end{equation}

Stated differently, we shall assume that,
for any two orthogonal projectors $ \textsf{\textbf{E}},\textsf{\textbf{F}}$
\index{orthogonal projector}
so that
 $ \textsf{\textbf{E}}\textsf{\textbf{F}}= \textsf{\textbf{F}}\textsf{\textbf{E}}=0$,
their sum
 $  \textsf{\textbf{G}} =\textsf{\textbf{E}}+\textsf{\textbf{F}}$
has expectation value
\begin{equation}
\langle \textsf{\textbf{G}}\rangle =
\langle \textsf{\textbf{E}} \rangle +
\langle \textsf{\textbf{F}} \rangle.
\end{equation}

We shall consider only vector spaces of dimension three or greater, since only in these cases two  orthonormal bases can be interlinked by a common vector -- in two dimensions,
distinct orthonormal bases contain distinct basis vectors.

\subsection{Gleason's theorem}

For a Hilbert space of dimension three or greater,
the only possible form of the  expectation value
of an self-adjoint operator  $\textsf{\textbf{A}}$
has the form
\cite{Gleason,r:dvur-93,pitowsky:218,rich-bridge,peres}
\begin{equation}
\langle
\textsf{\textbf{A}}
\rangle
=
\textrm{Tr}({  \rho} \textsf{\textbf{A}}),
\end{equation}
the trace of the operator product of the  density matrix (which is a positive operator of the trace class)
${  \rho}$
for the system with the matrix representation of $\textsf{\textbf{A}}$.

In particular, if $\textsf{\textbf{A}}$ is a projector $\textsf{\textbf{E}}$ corresponding to an elementary yes-no proposition
{\it ``the system has property Q,''} then $\langle \textsf{\textbf{E}}\rangle = \textrm{Tr}({  \rho}  \textsf{\textbf{E}})$ corresponds
to the probability of that property $Q$ if the system is in state ${  \rho}$.




\subsection{Kochen-Specker theorem}
\index{Kochen-Specker theorem}
\label{2011-m-KST}

For a Hilbert space of dimension three or greater,
there does not exist any
two-valued probability measures interpretable as consistent, overall truth assignment
\cite{specker-60,kochen1}.
As a result of the nonexistence of two-valued states, the classical strategy
to construct probabilities by a convex combination of all two-valued states fails entirely.

In  {\em Greechie diagram} \cite{greechie:71},
points  represent basis vectors.
\index{Greechie diagram}
If they belong to the same basis, they are connected by  smooth curves.

\begin{center}
%TeXCAD Picture [1.pic]. Options:
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\quality{8.000}
%\graddiff{0.010}
%\snapasp{1}
%\zoom{5.6569}
\unitlength .6mm % = 1.423pt
%\allinethickness{2pt}
 \thicklines %\linethickness{0.8pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(134.09,125.99)(0,0)

%\emline(86.39,101.96)(111.39,58.46)
\multiput(86.39,101.96)(.119617225,-.208133971){209}{{\color{green}\line(0,-1){.208133971}}}
%\end
%\emline(86.39,14.96)(111.39,58.46)
\multiput(86.39,14.96)(.119617225,.208133971){209}{{\color{red}\line(0,1){.208133971}}}
%\end
%\emline(36.47,101.96)(11.47,58.46)
\multiput(36.47,101.96)(-.119617225,-.208133971){209}{{\color{yellow}\line(0,-1){.208133971}}}
%\end
%\emline(36.47,14.96)(11.47,58.46)
\multiput(36.47,14.96)(-.119617225,.208133971){209}{{\color{magenta}\line(0,1){.208133971}}}
%\end
\color{blue}\put(86.39,15.21){\color{blue}\line(-1,0){50}}
\put(86.39,101.71){\color{violet}\line(-1,0){50}}
%
\put(36.34,15.16){\color{magenta}\circle{6}}
\put(36.34,15.16){\color{blue}\circle{4}}
\put(52.99,15.16){\color{blue}\circle{4}}
\put(52.99,15.16){\color{cyan}\circle{6}}
\put(69.68,15.16){\color{blue}\circle{4}}
\put(69.68,15.16){\color{orange}\circle{6}}
\put(86.28,15.16){\color{blue}\circle{4}}
\put(86.28,15.16){\color{red}\circle{6}}
%
\put(93.53,27.71){\color{red}\circle{4}}
\put(93.53,27.71){\color{orange}\circle{6}}
\put(102.37,43.44){\color{red}\circle{4}}
\put(102.37,43.44){\color{olive}\circle{6}}
\put(111.21,58.45){\color{red}\circle{4}}
\color{green}\put(111.21,58.45){\circle{6}}
%
\put(102.37,73.47){\color{green}\circle{4}}
\put(102.37,73.47){\color{olive}\circle{6}}
\put(93.53,89.21){\color{green}\circle{4}}
\put(93.53,89.21){\color{cyan}\circle{6}}
\put(86.28,101.76){\color{green}\circle{4}}
\put(86.28,101.76){\color{violet}\circle{6}}
%
\put(69.68,101.76){\color{violet}\circle{4}}
\put(69.68,101.76){\color{cyan}\circle{6}}
\put(52.99,101.76){\color{violet}\circle{4}}
\put(52.99,101.76){\color{orange}\circle{6}}
\put(36.34,101.76){\color{violet}\circle{4}}
\put(36.34,101.76){\color{yellow}\circle{6}}
%
\put(29.24,89.21){\color{yellow}\circle{4}}
\put(29.24,89.21){\color{orange}\circle{6}}
\put(20.4,73.47){\color{yellow}\circle{4}}
\put(20.4,73.47){\color{olive}\circle{6}}
\put(11.56,58.45){\color{yellow}\circle{4}}
\put(11.56,58.45){\color{magenta}\circle{6}}

\put(20.4,43.44){\color{magenta}\circle{4}}
\put(20.4,43.44){\color{olive}\circle{6}}
\put(29.24,27.71){\color{magenta}\circle{4}}
\put(29.24,27.71){\color{cyan}\circle{6}}

\color{cyan}
\qbezier(29.2,27.73)(23.55,-5.86)(52.99,15.24)
\qbezier(29.2,27.88)(36.93,75)(69.63,101.91)
\qbezier(52.69,15.24)(87.47,40.96)(93.72,89.27)
\qbezier(93.72,89.27)(98.4,125.99)(69.49,102.06)
\color{orange}
\qbezier(93.57,27.73)(99.22,-5.86)(69.78,15.24)
\qbezier(93.57,27.88)(85.84,75)(53.13,101.91)
\qbezier(70.08,15.24)(35.3,40.96)(29.05,89.27)
\qbezier(29.05,89.27)(24.37,125.99)(53.28,102.06)
\color{olive}
\qbezier(20.15,73.72)(-11.67,58.52)(20.15,43.31)
\qbezier(20.33,73.72)(61.34,93.16)(102.36,73.72)
\qbezier(102.36,73.72)(134.09,58.52)(102.53,43.31)
\qbezier(102.53,43.31)(60.99,23.43)(20.15,43.49)
{\color{black}
\put(30.41,114.02){\makebox(0,0)[cc]{$M$}}
\put(30.41,2.65){\makebox(0,0)[cc]{$A$}}
\put(52.68,114.38){\makebox(0,0)[cc]{$L$}}
\put(52.68,2.3){\makebox(0,0)[cc]{$B$}}
\put(91.93,114.2){\makebox(0,0)[cc]{$J$}}
\put(91.93,2.48){\makebox(0,0)[cc]{$D$}}
\put(69.65,114.38){\makebox(0,0)[cc]{$K$}}
\put(73.65,2.3){\makebox(0,0)[cc]{$C$}}
\put(103.24,94.22){\makebox(0,0)[cc]{$I$}}
\put(17.45,94.22){\makebox(0,0)[cc]{$ N$}}
\put(106.24,22.45){\makebox(0,0)[cc]{$E$}}
\put(17.45,22.45){\makebox(0,0)[cc]{$ R$}}
\put(115.13,77.96){\makebox(0,0)[cc]{$H$}}
\put(8.55,77.96){\makebox(0,0)[cc]{$ O$}}
\put(115.13,38.72){\makebox(0,0)[cc]{$F$}}
\put(10.55,38.72){\makebox(0,0)[cc]{$ Q$}}
\put(120.92,57.98){\makebox(0,0)[l]{$ G$}}
\put(1.77,57.98){\makebox(0,0)[rc]{$  P$}}
}
\put(61.341,9.192){\color{blue}\makebox(0,0)[cc]{$a$}}
\put(102.883,35.355){\color{red}\makebox(0,0)[cc]{$b$}}
\put(102.53,84.322){\color{green}\makebox(0,0)[cc]{$c$}}
\put(60.457,108.01){\color{violet}\makebox(0,0)[cc]{$d$}}
\put(18.031,84.145){\color{yellow}\makebox(0,0)[cc]{$e$}}
\put(18.561,33.057){\color{magenta}\makebox(0,0)[cc]{$f$}}
\put(61.341,39.774){\color{olive}\makebox(0,0)[cc]{$g$}}
\put(72.124,67.882){\color{orange}\makebox(0,0)[cc]{$h$}}
\put(48.79,67.705){\color{cyan}\makebox(0,0)[cc]{$i$}}
\end{picture}
\end{center}
The most compact way of deriving the Kochen-Specker theorem in four dimensions has been given by Cabello \cite{cabello-96,cabello-99}.
For the sake of demonstration, consider a Greechie (orthogonality) diagram of a finite subset of the continuum of blocks or contexts embeddable in
four-dimensional real Hilbert space without a two-valued probability measure
The proof of the Kochen-Specker theorem  uses  nine tightly interconnected contexts
$\color{blue}a=\{A,B,C,D\}$,
$\color{red}b=\{D,E,F,G\}$,
$\color{green}c=\{G,H,I,J\}$,
$\color{violet}d=\{J,K,L,M\}$,
$\color{yellow}e=\{M,N,O,P\}$,
$\color{magenta}f=\{P,Q,R,A\}$,
$\color{orange}g=\{B,I,K,R\}$,
$\color{olive}h=\{C,E,L,N\}$,
$\color{cyan}i=\{F,H,O,Q\}$
consisting of the 18 projectors associated with the one dimensional subspaces spanned by  the vectors from the origin $(0,0,0,0)$ to
$ A=(0,0,1,-1)    $,
$ B=(1,-1,0,0)    $,
$ C=(1,1,-1,-1)   $,
$ D=(1,1,1,1)     $,
$  E=(1,-1,1,-1)  $,
$  F=(1,0,-1,0)   $,
$  G=(0,1,0,-1)   $,
$  H=(1,0,1,0)    $,
$  I=(1,1,-1,1)   $,
$ J=(-1,1,1,1)    $,
$ K=(1,1,1,-1)    $,
$ L=(1,0,0,1)     $,
$ M=(0,1,-1,0)    $,
$  N=(0,1,1,0)    $,
$  O=(0,0,0,1)    $,
$  P=(1,0,0,0)    $,
$  Q=(0,1,0,0)    $,
$  R=(0,0,1,1)    $, respectively.
%
Greechie diagrams represent atoms by points, and  contexts by maximal smooth, unbroken curves.

{\color{OliveGreen}\bproof
In a proof by contradiction,note that, on the one hand, every observable proposition occurs in exactly {\em two} contexts.
Thus, in an enumeration of the four observable propositions of each of the nine contexts,
there appears to be an {\em even} number of true propositions,
provided that the value of an observable does not depend on the context (i.e. the assignment is {\em noncontextual}).
Yet, on the other hand, as there is an {\em odd} number (actually nine) of contexts,
there should be an {\em odd} number (actually nine) of true propositions.
\bproof
}


{\color{Purple}

\section{Hilbert space quantum mechanics and quantum logic}

\subsection{Quantum mechanics}
\index{quantum mechanics}

The following is a very brief introduction to quantum mechanics.
Introductions to quantum mechanics can be found in
Refs. \cite{feynman-III,ba-89,messiah-61,peres,wheeler-Zurek:83}.

All quantum
mechanical entities are represented by objects
of Hilbert spaces \cite{v-neumann-49,birkhoff-36}.
The following identifications between physical and theoretical objects
are made (a {\it caveat:} this is an incomplete list).

In what follows, unless stated differently, only
{\em finite} dimensional Hilbert spaces are considered.
 Then, the vectors
corresponding to states can be written as usual vectors in complex
Hilbert space.
Furthermore, bounded
self-adjoint operators are  equivalent to bounded Hermitean operators.
They can be represented by matrices, and the self-adjoint
conjugation
is just transposition and complex conjugation of the matrix elements.
Let ${\frak B}=\{{\bf b}_1,{\bf b}_2,\ldots , {\bf b}_n\}$ be an orthonormal basis in $n$-dimensional Hilbert space ${\frak H}$.
That is,  orthonormal base vectors in ${\frak B}$
satisfy
$\langle {\bf b}_i, {\bf b}_j\rangle =\delta_{ij}$,
where $\delta_{ij}$ is the Kronecker delta function.

\begin{itemize}
\item[(I)]
 A quantum {\em   state} is represented by
\index{state}
\index{quantum state}
a  positive Hermitian operator  ${    \rho}$
of trace class one in  the Hilbert space ${\frak H} $;
that is
\begin{itemize}
\item[(i)]
 ${    \rho}^\dagger ={    \rho}=\sum_{i=1}^n p_i  \vert {\bf b}_i \rangle \langle {\bf b}_i  \vert $,
    with  $p_i\ge 0$ for all $i=1,\ldots , n$, ${\bf b}_i \in {\frak B}$, and $\sum_{i=1}^n p_i =1$, so that
\item[(ii)]
$\langle {    \rho}{\bf x}\mid {\bf x}\rangle =\langle {\bf x}\mid {    \rho}{\bf x}\rangle  \ge 0$,
\item[(iii)]
$\textrm{Tr}( {    \rho})=\sum_{i=1}^n \langle {\bf b}_i \mid {    \rho} \mid  {\bf b}_i \rangle =1 $.
\end{itemize}

A {\em pure  state} is represented by a  (unit) vector ${\bf x}$, also denoted by
$\mid  {\bf x}\rangle$,  of  the Hilbert space ${\frak H} $ spanning a one-dimensional subspace (manifold)
 ${\frak M}_{\bf x}$ of  the Hilbert space ${\frak H} $.
\index{pure  state}
Equivalently, it is represented by the one-dimensional subspace (manifold) ${\frak M}_{\bf x}$ of  the Hilbert space ${\frak H} $ spannned by the vector ${\bf x}$.
Equivalently, it is represented by
the projector  $ \textsf{\textbf{E}}_{\bf x}=\mid {\bf x}\rangle \langle {\bf x} \mid$
onto the unit  vector ${\bf x}$ of  the Hilbert space ${\frak H} $.

Therefore, if two vectors ${\bf x},{\bf y}\in {\frak H}$ represent pure
states, their vector sum
${\bf z}={\bf x}+{\bf y}\in{\frak H}$ represents a pure state as well.
This state ${\bf z}$ is called the {\em coherent superposition} of state ${\bf x}$
\index{coherent superposition}
and
${\bf y}$. Coherent state superpositions between classically mutually exclusive (i.e. orthogonal) states, say
$\mid  {\bf 0}\rangle$
and
$\mid  {\bf 1}\rangle$,
 will become most important in quantum
information theory.


Any pure state ${\bf x}$ can be written as a linear
combination of
the set of orthonormal base vectors $\{{\bf b}_1,{\bf b}_2,\cdots {\bf b}_n\}$,
that is,
${\bf x} =\sum_{i=1}^n   \beta_i {\bf b}_i$, where $n$ is the dimension of ${\frak H}$ and
$\beta_i=\langle {\bf b}_i \mid {\bf x}\rangle \in {\Bbb C}$.

In the Dirac bra-ket notation, unity is given by
${\bf 1}=\sum_{i=1}^n \vert {\bf b}_i\rangle \langle {\bf b}_i\vert $,
or just ${\bf 1}=\sum_{i=1}^n \vert i\rangle \langle i\vert $.

\item[(II)]
{\em Observables}  are represented by self-adjoint or, synonymuously, Hermitian,
operators or transformations   $ \textsf{\textbf{A}}= \textsf{\textbf{A}}^\dagger$
on the Hilbert space ${\frak H}$ such that $\langle  \textsf{\textbf{A}} {\bf x}\mid
{\bf y}\rangle=\langle  {\bf x}\mid
\textsf{\textbf{A}} {\bf y}\rangle$ for all
${\bf x},{\bf y}\in {\frak H}$. (Observables and their corresponding operators are
identified.)

The trace of an operator  $ \textsf{\textbf{A}}$ is given by
$\textrm{Tr}  \textsf{\textbf{A}} =\sum_{i=1}^n \langle {\bf b}_i\vert   \textsf{\textbf{A}} \vert{\bf b}_i \rangle$.


Furthermore,
any Hermitian operator has a spectral representation   as a spectral sum
$  \textsf{\textbf{A}} =\sum_{i=1}^n \alpha_i    \textsf{\textbf{E}}_i$,
where the $  \textsf{\textbf{E}} _i$'s  are orthogonal projection operators onto the
orthonormal eigenvectors ${\bf a}_i$ of $   \textsf{\textbf{A}}$ (nondegenerate
case).

Observables are said to be {\em compatible} if they can be defined
simultaneously with arbitrary accuracy; i.e., if they are
``independent.'' A criterion for compatibility is the {\em commutator.}
Two observables ${ \textsf{\textbf{A}}},{ \textsf{\textbf{B}}}$ are compatible, if their {\em
commutator} vanishes; that is,
if $\left[
{ \textsf{\textbf{A}}},
{ \textsf{\textbf{B}}}
\right] =
{ \textsf{\textbf{A}}}
{ \textsf{\textbf{B}}}  -
{ \textsf{\textbf{B}}}
{ \textsf{\textbf{A}}}   =0$.


It has recently been demonstrated that
(by an analog embodiment using
particle beams) every Hermitian operator in a finite dimensional Hilbert
space can be experimentally realized \cite{rzbb}.

\item[(III)]
The result of any single measurement of the observable $A$
on a state ${\bf x}\in {\frak H}$
can only be one of the real eigenvalues of the corresponding
Hermitian operator $ \textsf{\textbf{A}}$.
If ${\bf x}$ is in a coherent superposition of eigenstates of $ \textsf{\textbf{A}}$, the
particular outcome of any such single measurement is believed to be indeterministic \cite{born-26-1,born-26-2,zeil-05_nature_ofQuantum};
that is,
it cannot be predicted with certainty. As a
result of the measurement,
the system is in the state which corresponds to the eigenvector ${\bf a}_i$ of
$\textsf{\textbf{A}}$ with the associated real-valued eigenvalue
$\alpha_i$; that is, $\textsf{\textbf{A}} {\bf x}=\alpha_n {\bf a}_n$ (no Einstein sum convention here).

This ``transition'' ${\bf x}\rightarrow {\bf a}_n$ has given rise to speculations
concerning the
``collapse
of the wave function (state).''  But, subject to technology and in principle,  it may be
possible to reconstruct coherence; that is, to ``reverse the collapse of
the wave function (state)'' if the process of measurement is
reversible. After this reconstruction, no information about the
measurement must be left, not even in principle.
How did Schr\"odinger, the creator of wave mechanics, perceive the
$\psi$-function? In his
1935 paper
``Die Gegenw\"artige
Situation in der Quantenmechanik'' (``The present situation in quantum
mechanics''
\cite{schrodinger}), on page 53, Schr\"odinger states,
{\em ``the $\psi$-function as expectation-catalog:}
$\ldots$
In it [[the $\psi$-function]] is embodied the momentarily-attained sum
of theoretically based future expectation, somewhat as laid down in a
{\em catalog.}
$\ldots$
For each measurement one is required to ascribe to the $\psi$-function
($=$the prediction catalog) a characteristic, quite sudden change,
which {\em depends on the measurement result obtained,} and so {\em
cannot be foreseen;} from which alone it is already quite clear
that this second kind of change of the $\psi$-function has nothing
whatever in common with its orderly development {\em between} two
measurements. The abrupt change [[of the $\psi$-function ($=$the
prediction catalog)]] by measurement $\ldots$ is the most interesting
point of the entire theory. It is precisely {\em the} point that demands
the break with naive realism. For {\em this} reason one cannot put the
$\psi$-function directly in place of the model or of the physical thing.
And indeed not because one might never dare impute abrupt unforeseen
changes to a physical thing or to a model, but because in the realism
point of view observation is a natural process like any other and cannot
{\em per se} bring about an interruption of the orderly flow of natural
events.''
\marginnote{German original: {\em ``Die $\psi$-Funktion als Katalog der Erwartung:}
$\ldots$
Sie [[die $\psi$-Funktion]] ist jetzt das Instrument zur Voraussage der
Wahrscheinlichkeit von Ma\ss zahlen. In ihr ist die jeweils erreichte
Summe theoretisch begr\"undeter Zukunftserwartung verk\"orpert,
gleichsam wie in einem {\em Katalog} niedergelegt.
$\ldots$
Bei jeder Messung ist man gen\"otigt, der $\psi$-Funktion ($=$dem
Voraussagenkatalog) eine eigenartige, etwas pl\"otzliche Ver\"anderung
zuzuschreiben, die von der {\em gefundenen Ma\ss zahl} abh\"angt und
sich {\em nicht vorhersehen l\"a\ss t;} woraus allein schon deutlich
ist, da\ss~ diese zweite Art von Ver\"anderung der $\psi$-Funktion mit
ihrem regelm\"a\ss igen Abrollen {\em zwischen} zwei Messungen nicht das
mindeste zu tun hat. Die abrupte Ver\"anderung durch die Messung
$\ldots$ ist der interessanteste Punkt der ganzen Theorie. Es ist genau
{\em der} Punkt, der den Bruch mit dem naiven Realismus verlangt. Aus
{\em diesem} Grund kann man die $\psi$-Funktion {\em nicht} direkt an
die Stelle des Modells oder des Realdings setzen. Und zwar nicht etwa
weil man einem Realding oder einem Modell nicht abrupte unvorhergesehene
\"Anderungen zumuten d\"urfte, sondern weil vom realistischen Standpunkt
die Beobachtung ein Naturvorgang ist wie jeder andere und nicht per se
eine Unterbrechung des regelm\"a\ss igen Naturlaufs hervorrufen darf.
}



The late Schr\"odinger was much more polemic about these issues; compare for instance his remarks in
his {D}ublin Seminars (1949-1955), published in Ref.
\cite{schroedinger-interpretation}, pages 19-20:
{ ``The idea that  [the alternate measurement outcomes] be not alternatives but {\em all} really happening simultaneously
seems lunatic to [the quantum theorist], just {\em impossible.}
He thinks that if the laws of nature took {\em this} form for,
let me say,
a quarter of an hour, we should find our surroundings rapidly turning into a quagmire, a sort of a featureless jelly or plasma,
all contours becoming blurred, we ourselves probably becoming jelly fish.
It is strange that he should believe this.
For I understand he grants that unobserved nature does behave this way -- namely according to the wave equation.
$\ldots$ according to the quantum theorist, nature is prevented from rapid
jellification only by our perceiving or observing it.''}


\item[(IV)]
The probability $P_{{\bf x}}({\bf y})$ to find a system represented by state ${    \rho}_{\bf x}$
in some pure state ${\bf y}$  is given by  the
{\em Born rule}
\index{Born rule}
which is derivable from Gleason's theorem:
\index{Gleason's theorem}
$P_{{\bf x}}({\bf y})=\textrm{Tr} ({    \rho}_{{\bf x}}\textsf{\textbf{E}}_{\bf y})  $.
Recall that   the density ${    \rho}_{{\bf x}}$ is a positive Hermitian operator of trace class one.

For pure states with ${    \rho}_{{\bf x}}^2={    \rho}_{{\bf x}}$, ${    \rho}_{{\bf x}}$ is a onedimensional projector
${    \rho}_{{\bf x}} =  \textsf{\textbf{E}}_{{\bf x}} =\vert {{\bf x}}\rangle \langle {{\bf x}} \vert$
onto the unit vector ${\bf x}$; thus expansion of the trace
and $ \textsf{\textbf{E}}_{{\bf y}} =\vert {{\bf y}}\rangle \langle {{\bf y}} \vert$ yields
$P_{{\bf x}}({\bf y})=
\sum_{i=1}^n \langle i \mid   {{\bf x}}\rangle \langle {{\bf x}} \vert  {\bf y}\rangle \langle {\bf y} \mid  i  \rangle =
\sum_{i=1}^n \langle {\bf y} \mid  i  \rangle \langle i \mid   {{\bf x}}\rangle \langle {{\bf x}} \vert  {\bf y}\rangle =
\sum_{i=1}^n \langle {\bf y} \mid {\bf 1} \mid   {{\bf x}}\rangle \langle {{\bf x}} \vert  {\bf y}\rangle =
\vert \langle {\bf y} \mid {\bf x} \rangle \vert^2 $.


\item[(V)]
The {\em average value} or {\em expectation value} of an observable
$\textsf{\textbf{A}}$ in a quantum  state
${\bf x}$
is given by
$\langle A\rangle_{\bf y} =
\textrm{Tr} ({    \rho}_{{\bf x}} \textsf{\textbf{A}})$.

The {\em average value} or {\em expectation value} of an observable
$  \textsf{\textbf{A}} =\sum_{i=1}^n \alpha_i    \textsf{\textbf{E}}_i$ in a pure  state
${\bf x}$
is given by
$\langle A\rangle_{\bf x} =
\sum_{j=1}^n \sum_{i=1}^n \alpha_i
\langle j \mid   {{\bf x}}\rangle \langle {{\bf x}} \vert  {\bf a}_i\rangle \langle {\bf a}_i \mid  j  \rangle =
\sum_{j=1}^n \sum_{i=1}^n \alpha_i
 \langle {\bf a}_i \mid  j  \rangle \langle j \mid   {{\bf x}}\rangle \langle {{\bf x}} \vert  {\bf a}_i\rangle =
\sum_{j=1}^n \sum_{i=1}^n \alpha_i
 \langle {\bf a}_i \mid {\bf 1} \mid    {{\bf x}}\rangle \langle {{\bf x}} \vert  {\bf a}_i\rangle =
\sum_{i=1}^n \alpha_i
\vert \langle {\bf x}\mid {\bf a}_i\rangle \vert^2$.

\item[(VI)]
The dynamical law or equation of motion can be written in the form
$x (t) =\textsf{\textbf{U}}x (t_0) $,
where $\textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^{-1}$ (``$\dagger $ stands for transposition and
complex conjugation) is a
linear {\em unitary transformation} or {\em isometry}.
\index{isometry}
\index{unitary transformation}

The {\em Schr\"odinger equation}
$
i\hbar {\partial \over \partial t}  \psi (t)    =
H \psi (t) $
 is obtained by identifying $\textsf{\textbf{U}}$ with
$\textsf{\textbf{U}}=e^{-i\textsf{\textbf{H}}t/\hbar }$,
where $\textsf{\textbf{H}}$ is a self-adjoint  Hamiltonian (``energy'') operator,
by differentiating the equation of motion
with respect to the time variable $t$.

For stationary $ \psi_n
(t)=
e^{-(i/\hbar )E_nt}  \psi_n $, the Schr\"odinger equation
can be brought into its time-independent form
$\textsf{\textbf{H}}\, \psi_n
=
E_n\, \psi_n $.
Here,
$i\hbar {\partial \over \partial t} \psi_n (t)
=
E_n \, \psi_n (t) $  has been used;
$E_n$
and $\psi_n $
stand for the $n$'th eigenvalue and eigenstate of
$\textsf{\textbf{H}}$, respectively.

Usually, a physical problem is defined by the Hamiltonian ${\textsf{\textbf{H}}}$.
The problem of finding the physically relevant states reduces to finding
a complete set of eigenvalues and eigenstates of ${\textsf{\textbf{H}}}$.
Most elegant solutions utilize the symmetries of the problem; that is, the symmetry of
${\textsf{\textbf{H}}}$. There exist two ``canonical'' examples, the $1/r$-potential
and
the harmonic oscillator potential, which can be solved wonderfully by
these methods (and they are presented over and over again in standard
courses of quantum mechanics), but not many more. (See, for instance,
\cite{davydov} for a detailed treatment of various Hamiltonians ${\textsf{\textbf{H}}}$.)
\end{itemize}





\subsection{Quantum logic}
\index{quantum logic}

The dimensionality of the Hilbert space for a given quantum system
depends on the number of possible mutually exclusive outcomes.
In the spin--${1\over 2}$ case, for example, there are two outcomes
``up'' and ``down,'' associated with spin state measurements along arbitrary directions.
Thus, the dimensionality of Hilbert space needs to be two.

Then the following identifications can be made.
Table
 \ref{tcompa} lists the identifications of relations of operations of
classical Boolean set-theoretic and quantum Hillbert lattice types.
\begin{table}
\begin{center}
{\footnotesize
 \begin{tabular}{|ccccc|} \hline\hline
 generic lattice  &  order relation   & ``meet''
&
``join''  & ``complement''\\
\hline
propositional&implication&disjunction&conjunction&negation\\
calculus&$\rightarrow$&``and'' $\wedge$&``or'' $\vee$&``not'' $\neg$\\
\hline
``classical'' lattice  &  subset $\subset $  & intersection $\cap$ &
union
$\cup$ & complement\\
of subsets&&&&\\
of a set&&&&\\
\hline
Hilbert & subspace& intersection of & closure of     & orthogonal \\
lattice & relation& subspaces $\cap$&  linear& subspace   \\
        & $\subset$ &                 & span $\oplus$  &  $\perp$   \\
\hline
lattice of& $\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2=\textsf{\textbf{E}}_1$& $\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2$& $\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2-\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2$& orthogonal\\
commuting&&&&projection\\
\{noncommuting\}&&\{$\displaystyle\lim_{n\rightarrow \infty}(\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2)^n$\}&&\\
projection&&&&\\
operators&&&&\\
 \hline\hline
 \end{tabular}
}
 \caption{Comparison of the identifications of lattice relations and
 operations for the lattices of subsets of a set, for
 experimental propositional calculi, for  Hilbert lattices, and for
lattices of commuting projection operators.
 \label{tcompa}}
 \end{center}
\end{table}


\begin{itemize}

\item[(i)]
Any closed linear
subspace ${\mathfrak M}_{{\bf p}}$ spanned by a vector  ${{\bf p}}$ in a Hilbert space ${\frak H}$ -- or, equivalently, any
projection operator $\textsf{\textbf{E}}_{{\bf p}} =\vert {{\bf p}} \rangle \langle {{\bf p}}\vert$
 on  a Hilbert space ${\frak H}$ corresponds to an elementary
proposition ${{\bf p}}$. The elementary {``true''}-{``false''} proposition can in
English be spelled out explicitly as
\begin{quote}
``The physical system has a property corresponding to the associated closed linear subspace.''
\end{quote}
It is coded into the two eigenvalues $0$ and $1$ of  the projector $\textsf{\textbf{E}}_{{\bf p}}$
(recall that $\textsf{\textbf{E}}_{{\bf p}}\textsf{\textbf{E}}_{{\bf p}}=\textsf{\textbf{E}}_{{\bf p}}$).

\item[(ii)]
The logical {``and''} operation is identified with the set
theoretical intersection of two propositions ``$\cap$''; i.e., with the
intersection of two subspaces.
It is denoted by the symbol ``$\wedge$''.
So, for two
propositions ${{\bf p}}$ and ${{\bf q}}$ and their associated closed linear
subspaces
${\mathfrak M}_{{\bf p}}$ and
${\mathfrak M}_{{\bf q}}$,
$$
{\mathfrak M}_{{{\bf p}}\wedge q} = \{x \mid x\in
{\mathfrak M}_{{\bf p}}, \;
x\in {\mathfrak M}_{{\bf q}}\} .$$


\item[(iii)]
The logical {``or''} operation is identified with the closure of the
linear span ``$\oplus$'' of the subspaces corresponding to the two
propositions.
 It is denoted by the symbol ``$\vee$''.
So, for two
propositions ${{\bf p}}$ and $q$ and their associated closed linear
subspaces
${\mathfrak M}_{{\bf p}}$ and
${\mathfrak M}_{{\bf q}}$,
$$
{\mathfrak M}_{{{\bf p}}\vee q} =
{\mathfrak M}_{{{\bf p}}} \oplus
{\mathfrak M}_{{{\bf q}}} =
 \{{{\bf x}} \mid {{\bf x}}=\alpha {{\bf y}}+\beta {{\bf z}},\; \alpha,\beta \in {\mathbb C},\; {{\bf y}}\in
{\mathfrak M}_{{\bf p}}, \;
{{\bf z}}\in {\mathfrak M}_{{\bf q}}\} .$$



The symbol $\oplus$ will used to indicate the closed linear subspace
spanned by two vectors. That is,
$${{\bf u}}\oplus {{\bf v}}=\{ {{\bf w}}\mid {{\bf w}}=\alpha {{\bf u}}+ \beta {{\bf v}},\; \alpha,\beta \in {\mathbb C}
,\; {{\bf u}},{{\bf v}} \in {\mathfrak H}\}.$$


Notice that
a vector of Hilbert space may be an element of
$
{\mathfrak M}_{{{\bf p}}} \oplus
{\mathfrak M}_{{{\bf q}}}
$
without being an element of either
$
{\mathfrak M}_{{{\bf p}}} $ or
${\mathfrak M}_{{{\bf q}}}
$, since
$
{\mathfrak M}_{{{\bf p}}} \oplus
{\mathfrak M}_{{{\bf q}}}
$
includes all the vectors in
$
{\mathfrak M}_{{{\bf p}}} \cup
{\mathfrak M}_{{{\bf q}}}
$, as well as all of their linear combinations (superpositions) and
their limit vectors.


\item[(iv)]
The logical {``not''}-operation, or ``negation'' or ``complement,''
is
identified with operation of taking the orthogonal subspace ``$\perp$''.
It is denoted by the symbol ``~$'$~''.
In particular, for a
proposition ${{\bf p}}$ and its associated closed linear
subspace
${\mathfrak M}_{{\bf p}}$, the negation $p'$ is associated with
$$
{\mathfrak M}_{{{\bf p}}'} =
 \{{{\bf x}} \mid \langle {{\bf x}}\mid {{\bf y}}\rangle =0,\; {{\bf y}}\in
{\mathfrak M}_{{\bf p}}
\} ,$$
where $\langle {{\bf x}}\mid {{\bf y}}\rangle$ denotes the scalar product of ${{\bf x}}$ and ${{\bf y}}$.

\item[(v)]
The logical {``implication''} relation is identified with the set
theoretical subset relation ``$\subset$''.
It is denoted by the symbol ``$\rightarrow$''.
So, for two
propositions ${{\bf p}}$ and ${{\bf q}}$ and their associated closed linear
subspaces
${\mathfrak M}_{{\bf p}}$ and
${\mathfrak M}_{{\bf q}}$,
$$
{p\rightarrow q} \Longleftrightarrow
{\mathfrak M}_{{{\bf p}}} \subset
{\mathfrak M}_{{{\bf q}}}.$$

\item[(vi)]
A trivial statement which is always {``true''} is denoted by $1$.
It is represented by the entire Hilbert space $\mathfrak H$.
So, $${\mathfrak M}_1={\mathfrak H}.$$

\item[(vii)]
An absurd statement which is always {``false''} is denoted by $0$.
It is represented by the zero vector $0$.
So, $${\mathfrak M}_0= 0.$$
\end{itemize}



\subsection{Diagrammatical representation, blocks, complementarity}

Propositional structures are often represented by
Hasse and Greechie diagrams.
\index{Hasse diagram}
\index{Greechie diagram}
A {\em Hasse diagram} is a convenient representation of the
logical implication,
as well as of the {``and''} and {``or''}
operations
among propositions.
 Points
``~$\bullet$~'' represent propositions. Propositions
which are implied by other ones are drawn higher than the other ones.
Two propositions are connected by a line if one implies the other.
Atoms are propositions which ``cover'' the least element $0$; i.e.,
they lie ``just above'' $0$ in a Hasse diagram of the partial order.



A much more compact representation of the propositional calculus can be
given in terms of
its {\em Greechie diagram} \cite{greechie:71}.
In this representation, the emphasis is on Boolean subalgebras.
Points ``~$\circ$~'' represent the atoms.
\index{Greechie diagram}
If they belong to the same Boolean subalgebra, they are connected by edges or smooth curves.
The collection of all atoms and elements belonging to the same Boolean subalgebra is called {\em block};
\index{block}
i.e., every block represents a Boolean subalgebra within a nonboolean structure.
The blocks can be joined or pasted together as follows.
\begin{itemize}
\item[(i)]
The tautologies of all blocks are identified.
\item[(ii)]
The absurdities of all blocks are identified.
\item[(iii)]
Identical elements in different blocks are identified.
\item[(iii)]
The logical and algebraic structures of all blocks remain intact.
\end{itemize}
This construction is often referred to as {\em pasting} construction.
If the blocks are only pasted together at the tautology and
the absurdity, one calls the resulting logic a {\em horizontal
sum}.

Every single block represents some ``maximal collection of co-measurable observables''
which will be identified with some quantum {\em context}.
\index{context}
Hilbert lattices can be thought of as the pasting of a continuity of such blocks or contexts.

Note that whereas all propositions within a given block or context are co-measurable;
propositions belonging to different blocks are not.
This latter feature is an expression of  complementarity.
Thus from a strictly operational point of view,
it makes no sense to speak of the ``real physical existence'' of different contexts,
as knowledge of a single context makes impossible the measurement of all the other ones.

Einstein-Podolski-Rosen (EPR) type arguments \cite{epr} utilizing a configuration
sketched in Fig.~\ref{2009-gtq-f3}
claim to be able to infer two different contexts counterfactually.
One context is measured on one side of the setup, the other context on the other side of it.
By the uniqueness property \cite{svozil-2006-uniquenessprinciple} of certain two-particle states,
knowledge of a property of one particle entails the certainty
that, if this property were measured on the other particle as well, the outcome of the measurement would be
a unique function of the outcome of the measurement performed.
This makes possible the measurement of one context, as well as the simultaneous counterfactual inference of another, mutual exclusive, context.
Because, one could argue, although one has actually measured on one side a different, incompatible context compared to the context measured on the other side,
if on both sides the same  context {\em would be measured}, the outcomes on both sides {\em would be uniquely correlated}.
Hence measurement of one context per side is sufficient, for the outcome could be counterfactually inferred on the other side.

As problematic as counterfactual physical reasoning may appear from an operational
point of view even for a two particle state, the simultaneous ``counterfactual inference'' of three or more blocks or contexts fails
because of the missing uniqueness property
of quantum states.
}

{\color{blue}
\bexample

\subsection{Realizations of two-dimensional beam splitters}
\label{2004-analog-appendixA}

In what follows, lossless devices will be considered.
The  matrix
\begin{equation}
\textsf{\textbf{T}}(\omega ,\phi )=
\left(
\begin{array}{cc}
\sin \omega &\cos  \omega \\
e^{-i \phi }\cos  \omega & -e^{-i \phi }\sin \omega
\end{array}
\right)
\label{2004-analog-eurm1}
\end{equation}
introduced in Eq.~(\ref{2004-analog-eurm1})
has physical realizations in terms of  beam splitters
and  Mach-Zehnder interferometers equipped with an appropriate number of phase shifters.
Two such realizations are depicted in Fig.~\ref{f:qid}.
\begin{figure}
\begin{center}
\unitlength=0.60mm
\linethickness{0.4pt}
\begin{picture}(120.00,200.00)
\put(20.00,120.00){\framebox(80.00,80.00)[cc]{}}
\put(57.67,160.00){\line(1,0){5.00}}
\put(64.33,160.00){\line(1,0){5.00}}
\put(50.67,160.00){\line(1,0){5.00}}
\put(78.67,170.00){\framebox(8.00,4.33)[cc]{}}
\put(82.67,178.00){\makebox(0,0)[cc]{$P_3,\varphi$}}
\put(73.33,160.00){\makebox(0,0)[lc]{$S(T)$}}
\put(8.33,183.67){\makebox(0,0)[cc]{${\bf 0}$}}
\put(110.67,183.67){\makebox(0,0)[cc]{${\bf 0}'$}}
\put(110.67,143.67){\makebox(0,0)[cc]{${\bf 1}'$}}
\put(8.00,143.67){\makebox(0,0)[cc]{${\bf 1}$}}
\put(24.33,195.67){\makebox(0,0)[lc]{$\textsf{\textbf{T}}^{bs}(\omega ,\alpha ,\beta ,\varphi )$}}
\put(0.00,179.67){\vector(1,0){20.00}}
\put(0.00,140.00){\vector(1,0){20.00}}
\put(100.00,180.00){\vector(1,0){20.00}}
\put(100.00,140.00){\vector(1,0){20.00}}
\put(20.00,14.67){\framebox(80.00,80.00)[cc]{}}
\put(20.00,34.67){\line(1,1){40.00}}
\put(60.00,74.67){\line(1,-1){40.00}}
\put(20.00,74.67){\line(1,-1){40.00}}
\put(60.00,34.67){\line(1,1){40.00}}
\put(55.00,74.67){\line(1,0){10.00}}
\put(55.00,34.67){\line(1,0){10.00}}
\put(37.67,54.67){\line(1,0){5.00}}
\put(44.33,54.67){\line(1,0){5.00}}
\put(30.67,54.67){\line(1,0){5.00}}
\put(77.67,54.67){\line(1,0){5.00}}
\put(84.33,54.67){\line(1,0){5.00}}
\put(70.67,54.67){\line(1,0){5.00}}
\put(88.67,64.67){\framebox(8.00,4.33)[cc]{}}
\put(93.67,73.67){\makebox(0,0)[rc]{$P_4,\varphi$}}
\put(60.00,80.67){\makebox(0,0)[cc]{$M$}}
\put(59.67,29.67){\makebox(0,0)[cc]{$M$}}
\put(28.67,57.67){\makebox(0,0)[rc]{$S_1$}}
\put(88.33,57.67){\makebox(0,0)[lc]{$S_2$}}
\put(8.33,78.34){\makebox(0,0)[cc]{${\bf 0}$}}
\put(110.67,78.34){\makebox(0,0)[cc]{${\bf 0}'$}}
\put(110.67,38.34){\makebox(0,0)[cc]{${\bf 1}'$}}
\put(8.00,38.34){\makebox(0,0)[cc]{${\bf 1}$}}
\put(49.00,39.67){\makebox(0,0)[cc]{$c$}}
\put(71.33,68.67){\makebox(0,0)[cc]{$b$}}
\put(24.33,90.34){\makebox(0,0)[lc]{$\textsf{\textbf{T}}^{MZ}(\alpha ,\beta ,\omega,\varphi )$}}
\put(0.00,74.34){\vector(1,0){20.00}}
\put(0.00,34.67){\vector(1,0){20.00}}
\put(100.00,74.67){\vector(1,0){20.00}}
\put(100.00,34.67){\vector(1,0){20.00}}
\put(48.67,64.67){\framebox(8.00,4.33)[cc]{}}
\put(56.67,60.67){\makebox(0,0)[lc]{$P_3,\omega$}}
\put(10.00,110.00){\makebox(0,0)[cc]{a)}}
\put(10.00,4.67){\makebox(0,0)[cc]{b)}}
\put(20.00,140.00){\line(2,1){80.00}}
\put(20.00,180.00){\line(2,-1){80.00}}
\put(32.67,170.00){\framebox(8.00,4.33)[cc]{}}
\put(36.67,182.00){\makebox(0,0)[cc]{$P_1,\alpha +\beta $}}
\put(24.67,64.67){\framebox(8.00,4.33)[cc]{}}
\put(24.67,73.67){\makebox(0,0)[lc]{$P_1,\alpha +\beta$}}
\put(24.67,41.67){\framebox(8.00,4.33)[cc]{}}
\put(31.34,35.67){\makebox(0,0)[cc]{$P_2,\beta$}}
\put(32.67,147.00){\framebox(8.00,4.33)[cc]{}}
\put(36.67,155.00){\makebox(0,0)[cc]{$P_2,\beta$}}
\end{picture}
\end{center}
\caption{A universal quantum interference device operating on a qubit can be realized by a
4-port interferometer with two input ports ${\bf 0} ,{\bf 1} $
and two
output ports
${\bf 0} ',{\bf 1} '$;
a) realization
by a single beam
splitter $S(T)$
with variable transmission $T$
and three phase shifters $P_1,P_2,P_3$;
b) realization by two 50:50 beam
splitters $S_1$ and $S_2$ and four phase
shifters
$P_1,P_2,P_3,P_4$.
 \label{f:qid}}
\end{figure}
The
elementary quantum interference device $\textsf{\textbf{T}}^{bs}$  in
Fig.~\ref{f:qid}a)
is a unit consisting of two phase shifters $P_1$ and $P_2$ in the input ports, followed by a
beam splitter $S$, which is followed by a phase shifter  $P_3$ in one of the output
ports.
The device can
be quantum mechanically described by \cite{green-horn-zei}
\begin{equation}
\begin{array}{rlcl}
P_1:&\vert {\bf 0}\rangle  &\rightarrow& \vert {\bf 0}\rangle e^{i(\alpha +\beta)}
 , \\
P_2:&\vert {\bf 1}\rangle  &\rightarrow& \vert {\bf 1}\rangle
e^{i \beta}
, \\
S:&\vert {\bf 0} \rangle
&\rightarrow& \sqrt{T}\,\vert {\bf 1}'\rangle  +i\sqrt{R}\,\vert {\bf 0}'\rangle
, \\
S:&\vert {\bf 1}\rangle  &\rightarrow& \sqrt{T}\,\vert {\bf 0}'\rangle  +i\sqrt{R}\,\vert
{\bf 1}'\rangle
, \\
P_3:&\vert {\bf 0}'\rangle  &\rightarrow& \vert {\bf 0}'\rangle e^{i
\varphi
} ,
\end{array}
\end{equation}
where
every reflection by a beam splitter $S$ contributes a phase $\pi /2$
and thus a factor of $e^{i\pi /2}=i$ to the state evolution.
Transmitted beams remain unchanged; i.e., there are no phase changes.
Global phase shifts from mirror reflections are omitted.
With
$\sqrt{T(\omega )}=\cos \omega$
and
$\sqrt{R(\omega )}=\sin \omega$,
the corresponding unitary evolution matrix
is given by
\begin{equation}
\textsf{\textbf{T}}^{bs} (\omega ,\alpha ,\beta ,\varphi )=
\left(
\matrix{
 i \,e^{i \,\left( \alpha + \beta + \varphi \right) }\,\sin \omega &
   e^{i \,\left( \beta + \varphi \right) }\,\cos \omega
\cr
   e^{i \,\left( \alpha + \beta \right) }\, \cos \omega&
i \,e^{i \,\beta}\,\sin \omega  \cr}
\right)
.
\label{e:quid1}
\end{equation}
Alternatively, the action of a lossless beam splitter may be
described by the matrix
\footnote{
The standard labelling of the input and output ports are interchanged,
therefore sine and cosine are exchanged in the transition matrix.}
$$
\left(
\begin{array}{cc}
i \, \sqrt{R(\omega )}& \sqrt{T(\omega )}
\\
\sqrt{T(\omega )}&  i\, \sqrt{R(\omega )}
 \end{array}
\right)
=
\left(
\begin{array}{cc}
i \, \sin \omega  & \cos \omega
\\
\cos \omega&  i\, \sin \omega
 \end{array}
\right)
.
$$
A phase shifter in two-dimensional Hilbert space is represented by
either
${\rm  diag}\left(
e^{i\varphi },1
\right)
$
or
${\rm  diag}
\left(
1,e^{i\varphi }
\right)
$.
 The action of the entire device consisting of such elements is
calculated by multiplying the matrices in reverse order in which the
quanta pass these elements \cite{yurke-86,teich:90}; i.e.,
\begin{equation}
\textsf{\textbf{T}}^{bs} (\omega ,\alpha ,\beta ,\varphi )=
\left(
\begin{array}{cc}
e^{i\varphi}& 0\\
0& 1
\end{array}
\right)
\left(
\begin{array}{cc}
i \, \sin \omega  & \cos \omega
\\
\cos \omega&  i\, \sin \omega
\end{array}
\right)
\left(
\begin{array}{cc}
e^{i(\alpha + \beta)}& 0\\
0& 1
\end{array}
\right)
\left(
\begin{array}{cc}
1&0\\
0& e^{i\beta }
\end{array}
\right).
\end{equation}

The
elementary quantum interference device $\textsf{\textbf{T}}^{MZ}$ depicted in
Fig.~\ref{f:qid}b)
is a Mach-Zehnder interferometer with {\em two}
input and output ports and three phase shifters.
The process can
be quantum mechanically described by
\begin{equation}
\begin{array}{rlcl}
P_1:&\vert {\bf 0}\rangle  &\rightarrow& \vert {\bf 0}\rangle e^{i
(\alpha +\beta )} , \\
P_2:&\vert {\bf 1}\rangle  &\rightarrow& \vert {\bf 1}\rangle e^{i
\beta} , \\
S_1:&\vert {\bf 1}\rangle  &\rightarrow& (\vert b\rangle  +i\,\vert
c\rangle )/\sqrt{2} , \\
S_1:&\vert {\bf 0}\rangle  &\rightarrow& (\vert c\rangle  +i\,\vert
b\rangle )/\sqrt{2}, \\
P_3:&\vert b\rangle  &\rightarrow& \vert b\rangle e^{i \omega },\\
S_2:&\vert b\rangle  &\rightarrow& (\vert {\bf 1}'\rangle  + i\, \vert
{\bf 0}'\rangle )/\sqrt{2} ,\\
S_2:&\vert c\rangle  &\rightarrow& (\vert {\bf 0}'\rangle  + i\, \vert
{\bf 1}'\rangle )/\sqrt{2} ,\\
P_4:&\vert {\bf 0}'\rangle  &\rightarrow& \vert {\bf 0}'\rangle e^{i
\varphi
}.
\end{array}
\end{equation}
The corresponding unitary evolution matrix
is given by
\begin{equation}
\textsf{\textbf{T}}^{MZ} (\alpha ,\beta ,\omega ,\varphi )=
i \, e^{i(\beta +{\omega \over 2})}\;\left(
\begin{array}{cc}
-e^{i(\alpha +  \varphi )}\sin {\omega \over 2}
&
e^{i  \varphi }\cos {\omega \over 2} \\
e^{i  \alpha }\cos {\omega \over 2}
&
\sin {{\omega }\over 2}
 \end{array}
\right)
.
\label{e:quid2}
\end{equation}
Alternatively, $\textsf{\textbf{T}}^{MZ}$ can be computed by matrix multiplication; i.e.,
\begin{equation}
\begin{array}{l}
\textsf{\textbf{T}}^{MZ} (\alpha ,\beta ,\omega ,\varphi )=
i \, e^{i(\beta +{\omega \over 2})}\;
\left(
\begin{array}{cc}
e^{i\varphi }& 0\\
0& 1
 \end{array}
\right)
{1\over \sqrt{2}}\left(
\begin{array}{cc}
i& 1\\
1& i
 \end{array}
\right)
\left(
\begin{array}{cc}
e^{i\omega}& 0\\
0&1
 \end{array}
\right)   \cdot \\  \qquad
\qquad
\qquad
\qquad  \cdot
{1\over \sqrt{2}}\left(
\begin{array}{cc}
i& 1\\
1& i
 \end{array}
\right)
\left(
\begin{array}{cc}
e^{i(\alpha+\beta )}& 0\\
0&1
 \end{array}
\right)
\left(
\begin{array}{cc}
1& 0\\
0& e^{i\beta}
 \end{array}
\right)
 .
 \end{array}
\label{e:quid2mm}
\end{equation}



Both elementary quantum interference devices
$\textsf{\textbf{T}}^{bs}$
and
$\textsf{\textbf{T}}^{MZ}$
are  universal in the
sense that
 every unitary quantum
evolution operator in two-dimensional Hilbert space can be brought into a
one-to-one correspondence with
$\textsf{\textbf{T}}^{bs}$
and
$\textsf{\textbf{T}}^{MZ}$.
As the emphasis is on the realization of the elementary beam splitter
$\textsf{\textbf{T}}$ in Eq.~(\ref{2004-analog-eurm1}),
which spans a subset of the set of all two-dimensional unitary transformations,
the comparison of the parameters in
$\textsf{\textbf{T}}(\omega ,\phi )=
\textsf{\textbf{T}}^{bs}(\omega ',\beta ',\alpha ',\varphi ')=
\textsf{\textbf{T}}^{MZ}(\omega '',\beta '',\alpha '',\varphi '')$
yields
$\omega =\omega' =\omega''/2$,
$\beta'=\pi /2 -\phi$,
$\varphi'=\phi-\pi /2$,
$\alpha'=-\pi /2$,
$\beta''=\pi /2 - \omega -\phi$,
$\varphi''=\phi - \pi $,
$\alpha''=\pi $,
and thus
\begin{equation}
\textsf{\textbf{T}} (\omega ,\phi ) =
\textsf{\textbf{T}}^{bs} (\omega ,- {\pi \over 2 },{\pi \over 2} -\phi ,\phi-{\pi \over  2} ) =
\textsf{\textbf{T}}^{MZ} (2\omega ,\pi  ,{\pi \over 2} - \omega -\phi ,\phi - \pi  )
.
\end{equation}




Let us examine the realization of a few primitive logical ``gates''
corresponding to (unitary) unary operations on qubits.
The ``identity'' element ${\Bbb I}_2$ is defined by
$\vert  {\bf 0}  \rangle  \rightarrow  \vert  {\bf 0}  \rangle $,
$\vert  {\bf 1}  \rangle  \rightarrow  \vert  {\bf 1}  \rangle $ and can be realized by
\begin{equation}
{\Bbb I}_2 =
\textsf{\textbf{T}}({\pi \over 2},\pi)=
\textsf{\textbf{T}}^{bs}({\pi \over 2},-{\pi \over 2},-{\pi \over 2},{\pi \over 2})=
\textsf{\textbf{T}}^{MZ}(\pi ,\pi ,-\pi ,0)
={\rm diag}
\left( 1,1
\right)
\quad .
\end{equation}

The ``${\tt not}$'' gate is defined by
$\vert  {\bf 0}  \rangle  \rightarrow  \vert  {\bf 1}  \rangle $,
$\vert  {\bf 1}  \rangle  \rightarrow  \vert  {\bf 0}  \rangle $ and can be realized by
\begin{equation}
{\tt not} =
\textsf{\textbf{T}}(0,0)=
\textsf{\textbf{T}}^{bs}(0,-{\pi \over 2},{\pi \over 2},-{\pi \over 2})=
\textsf{\textbf{T}}^{MZ}(0,\pi ,{\pi \over 2} ,\pi )
=
\left(
\begin{array}{cc}
0&1
\\
1&0
 \end{array}
\right)
\quad .
\end{equation}


The next gate, a modified ``$\sqrt{{\Bbb I}_2}$,'' is a truly quantum
mechanical, since it converts a classical bit
into
a coherent superposition; i.e., $\vert  {\bf 0}  \rangle $ and $\vert  {\bf 1}  \rangle $.
$\sqrt{{\Bbb I}_2}$ is defined by
$\vert  {\bf 0}  \rangle  \rightarrow  (1/\sqrt{2})(\vert  {\bf 0}  \rangle  + \vert  {\bf 1}  \rangle )$,
$\vert  {\bf 1}  \rangle  \rightarrow  (1/\sqrt{2})(\vert  {\bf 0}  \rangle  - \vert  {\bf 1}  \rangle )$ and can
be realized by
\begin{equation}
\sqrt{{\Bbb I}_2} =
\textsf{\textbf{T}}({\pi \over 4},0)=
\textsf{\textbf{T}}^{bs}({\pi \over 4},-{\pi \over 2},{\pi \over 2},-{\pi \over 2})=
\textsf{\textbf{T}}^{MZ}({\pi \over 2},\pi ,{\pi \over 4} ,-\pi )
=
{1 \over \sqrt{2}}
\left(
\begin{array}{cc}
1&1
\\
1&-1
 \end{array}
\right)
\quad .
\end{equation}
Note that $\sqrt{{\Bbb I}_2}\cdot \sqrt{{\Bbb I}_2} = {\Bbb I}_2$.
However, the reduced parameterization of $\textsf{\textbf{T}}(\omega,\phi)$
is insufficient to represent $\sqrt{{\tt not}}$, such as
\begin{equation}
\sqrt{{\tt not}} =
\textsf{\textbf{T}}^{bs}({\pi \over 4},-\pi ,
{3\pi \over 4},
-\pi )=
{1 \over 2}
\left(
\begin{array}{cc}
1+i&1-i
\\
1-i&1+i
 \end{array}
\right)
,
\end{equation}
with
$
\sqrt{{\tt not}}
\sqrt{{\tt not}} = {\tt not}$.


\subsection{Two particle correlations}

In what follows, spin state measurements along certain directions or angles in spherical coordinates will be considered.
Let us, for the sake of clarity, first specify and make precise what we mean by ``direction of measurement.''
Following, e.g., Ref.~\cite{RAMACHANDRAN:61}, page 1, Fig. 1, and Fig.~\ref{f-2009-gtq-f1}, when not specified otherwise,
we consider a particle travelling along the positive $z$-axis; i.e., along $0Z$, which is taken to be horizontal.
The $x$-axis   along $0X$ is also taken to be horizontal.
The remaining  $y$-axis is taken vertically along $0Y$.
The three axes together form a right-handed system of coordinates.
%
%
%
\begin{figure}
\begin{center}
%TeXCAD Picture [1.pic]. Options:
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\on}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{6.7272}
\unitlength .4mm % = 1.707pt
%\allinethickness{0.6pt}
\thicklines %\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(120,102)(0,0)
%\emline(0,8)(41,30)
\multiput(0,8)(.1045918367,.056122449){392}{\line(1,0){.1045918367}}
%\end
\put(41,29.5){\line(0,1){72.5}}
%\emline(41,102)(0,80)
\multiput(41,102)(-.1045918367,-.056122449){392}{\line(-1,0){.1045918367}}
%\end
\put(0,80.5){\line(0,-1){72.5}}
{\color{blue}
\put(20,51){\vector(1,0){100}}
\put(20,51){\vector(0,1){34}}
\put(20,51){\vector(3,2){20}}
}
{
%\bezvec{618}[middle](45,51)(50,62)(35,61)
\put(45,59){\color{red}\vector(-2,3){.117}}\color{red}\bezier{618}(45,51)(50,62)(35,61)
%\end
%\bezvec{487}[middle](35,61)(28.5,72)(20,71)
\put(28,69){\color{red}\vector(-4,3){.117}}\color{red}\bezier{487}(35,61)(28.5,72)(20,71)
%\end
}
{\color{blue}
\put(20,45){\makebox(0,0)[cc]{$0$}}
\put(106,43){\makebox(0,0)[cc]{$Z$}}
\put(23,84){\makebox(0,0)[cc]{$Y$}}
\put(37,68){\makebox(0,0)[cc]{$X$}}
}
{\color{red}
\put(49,63){\makebox(0,0)[cc]{$\theta$}}
\put(29,75){\makebox(0,0)[cc]{$\varphi$}}
}
\end{picture}
\end{center}
\caption{\label{f-2009-gtq-f1}Coordinate system for measurements of particles travelling along $0Z$}
\end{figure}

The Cartesian $(x  , y , z )$--coordinates can be translated into spherical coordinates
$(r, \theta ,\varphi )$ via
$x = r\sin \theta \cos \varphi$,
$y = r\sin \theta \sin \varphi$,
$z = r\cos \theta $,
whereby  $\theta$ is the polar angle in the $x$--$z$-plane measured
from the $z$-axis, with $0 \le \theta \le \pi$,
and $\varphi $ is  the azimuthal angle in the $x$--$y$-plane, measured
from the $x$-axis with $0 \le \varphi < 2 \pi$. We shall only consider directions taken from the origin $0$,
characterized by the angles
$\theta$ and $\varphi$, assuming a unit radius $r=1$.
\label{2011-m-spericalcoo}
\index{spherical coordinates}



Consider two particles or quanta. On each one of the two quanta, certain measurements
(such as the spin state or polarization) of
(dichotomic) observables
$O({ a})$ and
$O({ b})$
along the directions $a$ and $b$, respectively, are performed.
The individual outcomes are
encoded or labeled by the symbols ``$-$'' and  ``$+$,'' or values ``-1'' and ``+1'' are recorded along
the directions ${ a}$ for the first particle, and  ${ b}$ for the second particle, respectively.
(Suppose that the measurement direction ${a}$ at ``Alice's location''
is unknown to an observer ``Bob'' measuring ${ b}$ and {\it vice versa}.)
A two-particle correlation function $E(a,b )$
is defined by averaging over the product of the outcomes $O({ a})_i, O({ b} )_i\in \{-1,1\}$
in the $i$th experiment for a total of $N$ experiments; i.e.,
\begin{equation}
E(a,b )={1\over N}\sum_{i=1}^N O({ a})_i O({ b})_i.
\end{equation}


Quantum mechanically, we shall follow a standard procedure for obtaining the probabilities upon which the expectation functions are based.
We shall start from the angular momentum operators, as for instance defined in Schiff's
{\em ``Quantum Mechanics''} \cite{schiff-55}, Chap. VI, Sec.24
in arbitrary directions, given by the spherical angular momentum co-ordinates $\theta$ and $\varphi$, as defined above.
Then, the projection operators corresponding to the eigenstates associated with the different eigenvalues are derived
from the dyadic (tensor) product of the normalized eigenvectors.
In Hilbert space based \cite{v-neumann-49} quantum logic \cite{birkhoff-36}, every projector corresponds to
a proposition that the system is in a state corresponding to that observable.
The quantum probabilities associated with these eigenstates are derived from the Born rule, assuming singlet states for the physical reasons discussed above.
These probabilities contribute to the correlation and expectation functions.

%\subsection
{Two-state particles:}

% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical
% ~~~~~~~~~~~~~~~   2 x 2 classical

%\subsubsection
{Classical case:}


For the two-outcome (e.g., spin one-half case of photon polarization) case,
it is quite easy to demonstrate that the {\em classical} expectation function
in the plane perpendicular to the direction connecting the two particles is a {\em linear} function of the azimuthal measurement angle.
Assume uniform  distribution of (opposite but otherwise) identical ``angular momenta'' shared by the two particles and lying on the circumference
of the unit circle in the plane spanned by $0X$ and $0Y$, as depicted in Figs.~\ref{f-2009-gtq-f1} and~\ref{f-2009-gtq-f2}.
%
\begin{marginfigure}
\begin{center}
\begin{tabular}{c}
%
%TeXCAD Picture [2.pic]. Options:
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\quality{8.000}
%\graddiff{0.010}
%\snapasp{1}
%\zoom{5.7082}
\unitlength .7mm % = 1.138pt
%\allinethickness{1pt}
\thicklines \linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
%\begin{picture}(220.345,235.75)(0,0)
\begin{picture}(220.345,70)(0,0)
{\color{blue}
\put(30.25,29.75){\bigcircle{61.0}}
%
\put(30.00,68.5){\makebox(0,0)[cc]{$a$}}
\put(30.25,30.25){\line(0,1){30.5}}
\put(-.091,29.825){\line(1,0){61}}
%\dottedline(1.75,235.75)(2,235.25)
%\multiput(1.574,235.574)(.125,-.25){3}{{\rule{.4pt}{.4pt}}}
%\end
\put(18.89,42.78){\makebox(0,0)[cc]{$+$}}
\put(29.44,12.22){\makebox(0,0)[cc]{$-$}}
}
\end{picture}
\\
%
%TeXCAD Picture [2.pic]. Options:
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\quality{8.000}
%\graddiff{0.010}
%\snapasp{1}
%\zoom{5.7082}
\unitlength .7mm % = 1.138pt
%\allinethickness{1pt}
\thicklines %\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
%\begin{picture}(220.345,235.75)(0,0)
\begin{picture}(220.345,70)(80,-5)
{\color{red}
\put(109.92,29.75){\bigcircle{61.0}}
%
%\emline(110,30)(128.33,54)
\multiput(110,30)(.084082569,.110091743){218}{\line(0,1){.110091743}}
%\end
%\emline(85.59,48.466)(134.056,11.196)
\multiput(85.59,48.466)(.1096526165,-.0843225288){442}{\line(1,0){.1096526165}}
%\end
\put(133.61,62.94){\makebox(0,0)[cc]{$b$}}
\put(110.56,46.67){\makebox(0,0)[cc]{$-$}}
\put(99.44,17.22){\makebox(0,0)[cc]{$+$}}
}
\end{picture}
\\
%
%TeXCAD Picture [2.pic]. Options:
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\quality{8.000}
%\graddiff{0.010}
%\snapasp{1}
%\zoom{5.7082}
\unitlength .7mm % = 1.138pt
%\allinethickness{1pt}
\thicklines %\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
%\begin{picture}(220.345,235.75)(0,0)
\begin{picture}(220.345,70)(160,0)
{\color{black}
%\put(189.58,29.75){\circle{61.53}}
\put(189.58,29.75){\bigcircle{61.0}}
\put(193.00,40.00){\makebox(0,0)[cc]{$\theta$}}
\put(199.00,26.00){\makebox(0,0)[lc]{$\theta$}}
\put(178.00,34.00){\makebox(0,0)[rc]{$\theta$}}
\bezier{44}(189.44,45)(195,46.11)(198.89,42.22)
\bezier{106}(172.209,29.957)(171.946,35.651)(175.538,40.293)
\bezier{94}(204.794,29.782)(204.444,24.526)(201.29,20.672)
}
%
\put(165.08,38){\makebox(0,0)[cc]{$+$}}
\put(182.83,13.5){\makebox(0,0)[cc]{{\color{blue}$-$}$\cdot${\color{red}$+$}$=-$}}
\put(210.5,35.5){\makebox(0,0)[cc]{{\color{blue}$+$}$\cdot${\color{red}$-$}$=-$}}
\put(211.58,21.25){\makebox(0,0)[cc]{$+$}}
{\color{blue}
\put(189.58,30.25){\line(0,1){30.5}}
\put(159.33,30){\line(1,0){61}}
\put(189.58,68.5){\makebox(0,0)[cc]{$a$}}
%\emline(189.44,30)(207.78,54)
\multiput(189.44,30)(.08412844,.110091743){218}{\color{red}\line(0,1){.110091743}}
%\end
%\emline(165.125,48.642)(213.591,11.371)
\multiput(165.125,48.642)(.1096526165,-.0843225288){442}{\color{red}\line(1,0){.1096526165}}
%\end
}
{\color{red}
\put(213.28,62.94){\makebox(0,0)[cc]{$b$}}
}
\end{picture}
\end{tabular}
\end{center}
\caption{Planar geometric demonstration of the classical two two-state particles correlation.}
\label{f-2009-gtq-f2}
\end{marginfigure}

By considering the length  $A_+(a,b)$ and $A_-(a,b)$ of the positive and negative contributions to expectation function,
one obtains for
$0\le \theta=\vert a-b\vert \le \pi$,
\begin{equation}
\begin{array}{l}
 E_{\textrm{cl},2,2}(\theta ) =E_{\textrm{cl},2,2}(a,b) = \frac{1}{2\pi} \left[A_+(a,b)-A_-(a,b)\right]\\
  \quad =  \frac{1}{2\pi} \left[2A_+(a,b) -2\pi \right]=
{2\over \pi}\vert a-b\vert - 1 = {2\theta \over \pi} - 1,
\label{2009-gtq-eclass}
\end{array}
\end{equation}
where the subscripts stand for the number of mutually exclusive measurement outcomes per particle, and
for the number of particles, respectively.
Note that $A_+(a,b)+A_-(a,b)=2\pi$.


% ~~~~~~~~~~~~~~~   2 x 2
% ~~~~~~~~~~~~~~~   2 x 2
% ~~~~~~~~~~~~~~~   2 x 2
% ~~~~~~~~~~~~~~~   2 x 2
% ~~~~~~~~~~~~~~~   2 x 2
% ~~~~~~~~~~~~~~~   2 x 2


%\subsubsection
{Quantum case:}

The two spin one-half particle case is one of the standard quantum mechanical exercises, although
it is seldomly computed explicitly.
For the sake of completeness and with the prospect to generalize the results to more particles of higher spin,
this case will be enumerated explicitly.
In what follows, we shall use the following notation:
Let
$
\vert +\rangle
$
denote the pure state corresponding to
$ {   {\bf e}}_1 =(0,1)
$,
and
$
\vert -\rangle $ denote the orthogonal pure state
corresponding to
${   {\bf e}}_2 =(1,0)
$.
The superscript
``$T$,''
``$\ast$'' and
``$\dagger$'' stand for transposition, complex and Hermitian conjugation, respectively.

In finite-dimensional Hilbert space, the matrix representation of projectors $E_{\bf a}$
from normalized vectors ${\bf a}=(a_1,a_2,\ldots ,a_n)^T$ with respect to some basis of $n$-dimensional Hilbert space
is obtained by taking the dyadic product; i.e., by
\begin{equation}
\textsf{\textbf{E}}_{\bf a}= \left[{\bf a},{\bf a}^\dagger\right]=\left[{\bf a},({\bf a}^\ast)^T\right]=
{\bf a}\otimes {\bf a}^\dagger =
\left(
\begin{array}{cccccccccc}
a_1{\bf a}^\dagger \\
a_2{\bf a}^\dagger \\
\ldots  \\
a_n{\bf a}^\dagger
\end{array}
\right)
=
\left(
\begin{array}{cccccccccc}
a_1a_1^\ast & a_1a_2^\ast & \ldots & a_1a_n^\ast \\
a_2a_1^\ast & a_2a_2^\ast & \ldots & a_2a_n^\ast \\
\ldots & \ldots & \ldots & \ldots \\
a_na_1^\ast & a_na_2^\ast & \ldots & a_na_n^\ast
\end{array}
\right)
.
\end{equation}
The tensor or Kronecker product of two vectors ${\bf a}$ and ${\bf b} =(b_1,b_2,\ldots ,b_m)^T$ can be represented by
\begin{equation}
{\bf a} \otimes {\bf b} = (a_1{\bf b},a_2{\bf b},\ldots ,a_n{\bf b})^T = (a_1b_1,a_1b_2,\ldots ,a_nb_m)^T
\end{equation}
The tensor or Kronecker product of some operators
\begin{equation}
\textsf{\textbf{A}}=
\left(
\begin{array}{cccccccccc}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\ldots & \ldots & \ldots & \ldots \\
a_{n1} & a_{n2} & \ldots & a_{nn}
\end{array}
\right)
\textrm{ and  }\textsf{\textbf{B}}=
\left(
\begin{array}{cccccccccc}
b_{11} & b_{12} & \ldots & b_{1m} \\
b_{21} & b_{22} & \ldots & b_{2m} \\
\ldots & \ldots & \ldots & \ldots \\
b_{m1} & b_{m2} & \ldots & b_{mm}
\end{array}
\right)
\end{equation}
is represented by an $nm\times nm$-matrix
\begin{equation}
\textsf{\textbf{A}}\otimes \textsf{\textbf{B}}
=
\left(
\begin{array}{cccccccccc}
a_{11} B& a_{12} B& \ldots & a_{1n}B \\
a_{21} B& a_{22} B& \ldots & a_{2n}B \\
\ldots & \ldots & \ldots & \ldots \\
a_{n1} B& a_{n2} B& \ldots & a_{nn}B
\end{array}
\right)
=
\left(
\begin{array}{cccccccccc}
a_{11} b_{11}& a_{11} b_{12} & \ldots & a_{1n}b_{1m} \\
a_{11} b_{21}& a_{11} b_{22}& \ldots & a_{2n} b_{2m}\\
\ldots & \ldots & \ldots & \ldots \\
a_{nn} b_{m1}& a_{nn} b_{m2}& \ldots & a_{nn} b_{mm}
\end{array}
\right)
.
\end{equation}

%\subsubsection*
{Observables:}

Let us start with the spin one-half angular momentum observables of {\em a single} particle along an arbitrary direction
in spherical co-ordinates $\theta$ and $\varphi$
in units of $\hbar$~\cite{schiff-55}; i.e.,
\begin{equation}
\textsf{\textbf{M}}_x=
\frac{1}{2}
\left(
\begin{array}{cccccccccc}
0&1\\
1&0
\end{array}
\right),
\qquad
\textsf{\textbf{M}}_y=
\frac{1}{2}
\left(
\begin{array}{cccccccccc}
0&-i\\
i&0
\end{array}
\right),
\qquad
\textsf{\textbf{M}}_z=
\frac{1}{2}
\left(
\begin{array}{cccccccccc}
1&0\\
0&-1
\end{array}
\right).
\end{equation}
The angular momentum operator in arbitrary direction $\theta$, $\varphi$ is given by its spectral decomposition
\begin{equation}
\begin{array}{rcl}
\textsf{\textbf{S}}_\frac{1}{2} (\theta ,\varphi) &=&
x\textsf{\textbf{M}}_x
+
y\textsf{\textbf{M}}_y
+
z\textsf{\textbf{M}}_z
=
 \textsf{\textbf{M}}_x  \sin \theta \cos \varphi
+
\textsf{\textbf{M}}_y   \sin \theta \sin \varphi
+
\textsf{\textbf{M}}_z   \cos \theta
\\
&=&   \frac{1}{2}\sigma (\theta ,\varphi)=
{1\over 2}
\left(\begin{array}{rcl}
\cos \theta &  e^{-i \varphi }\sin \theta \\
e^{i \varphi }\sin \theta & - \cos \theta
\end{array}
\right)\\
&=&
-
\frac{1}{2}
\left(
\begin{array}{cc}
 \sin ^2 \frac{\theta }{2} & -\frac{1}{2} e^{-i \varphi } \sin \theta  \\
 -\frac{1}{2} e^{i \varphi } \sin \theta  & \cos ^2\frac{\theta  }{2}
\end{array}
\right)
+
\frac{1}{2}
 \left(
\begin{array}{cc}
 \cos ^2 \frac{\theta }{2} & \frac{1}{2} e^{-i \varphi } \sin \theta  \\
 \frac{1}{2} e^{i \varphi } \sin \theta  & \sin ^2 \frac{\theta }{2}
\end{array}
\right)\\
&=&
-
\frac{1}{2}
\left\{
\frac{1}{2}
\left[
{\Bbb I}_2 - \sigma (\theta ,\varphi)
\right]
\right\}
+
\frac{1}{2}
\left\{
\frac{1}{2}
\left[
{\Bbb I}_2 + \sigma (\theta ,\varphi)
\right]
\right\}
.
\end{array}
\label{e-2009-gtq-s2}
\end{equation}

The  orthonormal eigenstates (eigenvectors)  associated with the eigenvalues $-\frac{1}{2}$ and $+\frac{1}{2}$ of
$\textsf{\textbf{S}}_\frac{1}{2}(\theta , \varphi )$ in Eq.~(\ref{e-2009-gtq-s2})
are
\begin{equation}
\label{e-2009-gtq-s2ev}
\begin{array}{cccc}
\vert -\rangle_{\theta ,\varphi} \equiv {\bf x}_{-\frac{1}{2}}(\theta ,\varphi)&=e^{i\delta_{+}}& \left(-
e^{-\frac{i\varphi}{2}} \sin{\theta \over 2} ,e^{\frac{i\varphi}{2}}  \cos{\theta \over 2}
\right),\\
\vert +\rangle_{\theta ,\varphi} \equiv {\bf x}_{+\frac{1}{2}}(\theta ,\varphi)&=e^{i\delta_{-}}& \left(
e^{-\frac{i\varphi}{2}} \cos{\theta \over 2}, e^{\frac{i\varphi}{2}}\sin{\theta \over 2}
\right) ,
\end{array}
\end{equation}
respectively. $\delta_{+}$ and $\delta_{-}$ are arbitrary phases.
These orthogonal unit vectors correspond to the two orthogonal projectors
\begin{equation}
\label{e-2009-gtq-s2evproj}
 \textsf{\textbf{F}}_\mp (\theta ,\varphi ) =
\frac{1}{2}
\left[
{\Bbb I}_2 \mp \sigma (\theta ,\varphi)
\right]
\end{equation}
for the spin down and up states along $\theta $ and $\varphi$, respectively.
By setting all the phases and angles to zero, one obtains the original
orthonormalized basis $\{\vert -\rangle,\vert +\rangle\}$.

In what follows, we shall consider two-partite correlation operators based on the spin observables discussed above.

\begin{enumerate}

\item{Two-partite angular momentum observable}

If we are only interested in spin state measurements with the associated outcomes of spin states in units of $\hbar$,
Eq.~(\ref{2004-gtq-e2F2}) can be rewritten to include all possible cases at once; i.e.,
\begin{equation}
 \textsf{\textbf{S}}_{\frac{1}{2} \frac{1}{2} } ({\hat \theta},{\hat \varphi} ) =
 \textsf{\textbf{S}}_{\frac{1}{2} }( \theta_1,\varphi_1 )
\otimes
 \textsf{\textbf{S}}_{\frac{1}{2} }( \theta_2,\varphi_2 ).
\label{2004-gtq-e2F2nat}
\end{equation}

\item{General two-partite observables}


The two-particle projectors
$F_{\pm \pm }$ or, by another notation, $F_{\pm_1 \pm_2 }$ to indicate the outcome on the first or the second particle,
corresponding to a two~spin-${1\over 2}$~particle joint measurement
aligned (``$+$'') or antialigned  (``$-$'') along arbitrary directions are
\begin{equation}
  \textsf{\textbf{F}}_{\pm_1 \pm_2 } ({\hat \theta},{\hat \varphi} ) =
{\frac{1}{2}}\left[{\mathbb I}_2 \pm_1 {  \sigma}( \theta_1,\varphi_1 )\right]
\otimes
{\frac{1}{2}}\left[{\mathbb I}_2 \pm_2 { \sigma}( \theta_2,\varphi_2 )\right];
\label{2004-gtq-e2F2}
\end{equation}
where ``$\pm_i$,'' $i=1,2$ refers to the outcome on the $i$'th particle,
and the notation ${\hat \theta},{\hat \varphi}$ is used to indicate all angular parameters.

To demonstrate its physical interpretation, let us consider as a concrete example
a spin state measurement on two quanta as depicted in Fig.~\ref{2009-gtq-f3}:
$F_{- +  } ({\hat \theta},{\hat \varphi} )$ stands for the proposition
\begin{quote}
{\em `The spin state of the first particle measured along $\theta_1,\varphi_1$ is ``$-$''
      and
      the spin state of the second particle measured along $\theta_2,\varphi_2$ is ``$+$''~.'
}
\end{quote}

\begin{figure}
\begin{center}
%TeXCAD Picture [1.pic]. Options:
%\grade{\off}
%\emlines{\off}
%\epic{\on}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\quality{2.000}
%\graddiff{0.010}
%\snapasp{1}
%\zoom{9.5137}
\unitlength 0.8mm % = 2.845pt
%\allinethickness{1pt}
\thicklines %\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(120,25.01)(0,0)
\put(56,9.086){\line(4,3){8}}
\put(64,9.086){\line(-4,3){8}}
\put(5,5.01){\oval(10,10)[l]}
\put(5,.01){\line(0,1){10}}
\put(2.5,5.01){\makebox(0,0)[cc]{$-$}}
\put(5,20.01){\oval(10,10)[l]}
\put(5,15.01){\line(0,1){10}}
\put(2.5,20.01){\makebox(0,0)[cc]{$+$}}
\put(10,5.01){\framebox(10,15)[cc]{$\theta_1,\varphi_1$}}
\put(115,5.01){\oval(10,10)[r]}
\put(115,.01){\line(0,1){10}}
\put(117.5,5.01){\makebox(0,0)[cc]{$-$}}
\put(115,20.01){\oval(10,10)[r]}
\put(115,15.01){\line(0,1){10}}
\put(117.5,20.01){\makebox(0,0)[cc]{$+$}}
\put(100,5.01){\framebox(10,15)[cc]{$\theta_2,\varphi_2$}}
\put(60.019,11.983){\circle{9.727}}
%\vector[middle]{\line}
\put(65.379,12.088){\line(1,0){33.846}}\put(82.302,12.088){\vector(1,0){.07}}
%\end
%\vector[middle]{\line}
\put(54.658,12.088){\line(-1,0){33.846}}\put(37.735,12.088){\vector(-1,0){.07}}
%\end
\end{picture}
\end{center}
\caption{Simultaneous spin state measurement of
the two-partite state represented in Eq.~(\ref{2009-gtq-s1s21}).
Boxes indicate spin state analyzers such as Stern-Gerlach apparatus
oriented along the directions $\theta_1,\varphi_1 $ and
$\theta_2,\varphi_2 $;
their two output ports are occupied with detectors  associated
with the outcomes
``$+$''
and
``$-$'',
respectively.
\label{2009-gtq-f3}}
\end{figure}




More generally, we will consider two different numbers $\lambda_+$ and $\lambda_-$,
and the generalized single-particle operator
\begin{equation}
 \textsf{\textbf{R}}_{\frac{1}{2}} (\theta ,\varphi) =
\lambda_-
\left\{
\frac{1}{2}
\left[
{\Bbb I}_2 - \sigma (\theta ,\varphi)
\right]
\right\}
+
\lambda_+
\left\{
\frac{1}{2}
\left[
{\Bbb I}_2 + \sigma (\theta ,\varphi)
\right]
\right\}
,
\label{e-2009-gtq-s2g}
\end{equation}
as well as the resulting two-particle operator
\begin{equation}
\begin{array}{l}
\textsf{\textbf{R}}_{\frac{1}{2} \frac{1}{2}} ({\hat \theta},{\hat \varphi} ) =
\textsf{\textbf{R}}_{\frac{1}{2}}( \theta_1,\varphi_1 )
\otimes
\textsf{\textbf{R}}_{\frac{1}{2}} ( \theta_2,\varphi_2 )\\
\quad =
\lambda_- \lambda_- F_{--} +
\lambda_- \lambda_+ F_{-+} +
\lambda_+ \lambda_- F_{+-} +
\lambda_+ \lambda_+ F_{++}
.
\end{array}
\label{2004-gtq-e2F2g}
\end{equation}


\end{enumerate}

%\subsubsection*
{Singlet state:}




Singlet states $\vert \Psi_{d,n,i} \rangle$ could be labeled by three numbers $d$, $n$ and $i$,
denoting
the number $d$ of outcomes associated with the dimension of Hilbert space per particle,
the number $n$ of participating particles,
and the state count $i$ in an enumeration of all possible singlet states of $n$ particles of spin $j=(d-1)/2$, respectively
\cite{schimpf-svozil}.
For $n=2$, there is only one singlet state,
and $i=1$ is always one.
For historic reasons, this singlet state is also called {\em Bell state}  and denoted by $\vert {\Psi^-} \rangle $.
\index{Bell state}
\index{singlet state}

Consider the {\em singlet} ``Bell'' state of two spin-${1\over 2}$
particles
\begin{equation}
\vert {\Psi^-} \rangle
=
 {1\over \sqrt{2}}
\bigl(
\vert +- \rangle -
\vert -+ \rangle
\bigr)
.
\label{2009-gtq-s1s21}
\end{equation}

With the identifications
$
\vert +\rangle
\equiv {   {\bf e}}_1 =(1,0)
$
and
$
\vert -\rangle \equiv {   {\bf e}}_2 =(0,1)
$ as before,
the Bell state has a vector representation as
\begin{equation}
\begin{array}{lll}
\vert  {\Psi^-}\rangle
 \equiv
{1\over \sqrt{2}}\left({   {\bf e}}_1\otimes {   {\bf e}}_2-{   {\bf e}}_2\otimes {   {\bf e}}_1 \right) \\
\quad = {1\over \sqrt{2}}\left[ (1,0)\otimes (0,1) - (0,1) \otimes (1,0)\right]
=\left( 0,\frac{1}{\sqrt{2}},- \frac{1}{\sqrt{2}} ,  0 \right).
\end{array}
\label{2005-hp-ep12s1v}
\end{equation}
The density operator $\rho_{{\Psi^-}}$
is just the projector of the dyadic product of this vector, corresponding to the one-dimensional
linear subspace spanned by  $\vert  {\Psi^-}\rangle $; i.e.,
\begin{equation}
%\begin{array}{lll}
\rho_{{\Psi^-}} = \vert  {\Psi^-}\rangle \langle  {\Psi^-} \vert
=
\left[ \vert  {\Psi^-}\rangle ,\vert  {\Psi^-}\rangle^\dagger \right]
=
\frac{1}{2}
 \left(
\begin{array}{rrrr}
0&0&0&0\\
0&1&-1&0\\
0&-1&1&0\\
0&0&0&0
\end{array}
\right)
.
%\end{array}
\end{equation}



Singlet states are form invariant with respect to arbitrary unitary
transformations in the single-particle Hilbert spaces and thus
also rotationally invariant in configuration space,
in particular under the rotations
$
\vert + \rangle =
e^{ i{\frac{\varphi}{2}} }
\left(
\cos \frac{\theta}{2} \vert +'  \rangle
-
\sin \frac{\theta}{2} \vert -'   \rangle
\right)
$
and
$
\vert - \rangle =
e^{ -i{\frac{\varphi}{2}} }
\left(
\sin \frac{\theta}{2} \vert +'   \rangle
+
\cos \frac{\theta}{2} \vert -'  \rangle
\right)
$
in the spherical coordinates $\theta , \varphi$ defined earlier
[e.\,g., Ref.~\cite{krenn1}, Eq.~(2), or Ref.~\cite{ba-89}, Eq.~(7--49)].

The Bell singlet state is unique in the sense that the outcome of a spin state measurement
along a particular direction on one particle ``fixes'' also the opposite outcome of a spin state measurement
along {\em the same} direction on its ``partner'' particle: (assuming lossless devices)
whenever a ``plus'' or a ``minus'' is recorded on one side,
a ``minus'' or a ``plus'' is recorded on the other side, and {\it vice versa.}




%\subsubsection*
{Results:}

We now turn to the calculation of quantum predictions.
The joint probability to register the spins of the two particles
in state $\rho_{{\Psi^-}}$
aligned or antialigned along the directions defined by
($\theta_1$, $\varphi_1 $) and
($\theta_2$, $\varphi_2 $)
can be evaluated by a straightforward calculation of
\begin{equation}
\begin{array}{l}
P_{{ {\Psi^-}}\,\pm_1 \pm_2 } ({\hat \theta},{\hat \varphi} )=
{\rm Tr}\left[\rho_{ {\Psi^-}} \cdot \textsf{\textbf{F}}_{\pm_1 \pm_2 } \left({\hat \theta},{\hat \varphi} \right)\right] \\
\qquad
=\frac{1}{4} \left\{ 1-(\pm_1 1)( \pm_2 1) \left[\cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2 \cos (\varphi_1-\varphi_2) \right]\right\}
.
\end{array}
\end{equation}
Again, ``$\pm_i$,'' $i=1,2$ refers to the outcome on the $i$'th particle.

Since $P_= + P_{\neq} = 1$ and $E= P_= - P_{\neq}$, the joint probabilities to find the two particles
in an even or in an odd number of
spin-``$-\frac{1}{2}$''-states when measured along
($\theta_1$, $\varphi_1 $) and
($\theta_2$, $\varphi_2 $)
are in terms of the expectation function given by
\begin{equation}
\begin{array}{l}
P_= = P_{++}+P_{--} =
{1\over2}\left(1 + E  \right)   \\
\qquad =\frac{1}{2} \left\{ 1- \left[\cos \theta_1 \cos \theta_2 - \sin \theta_1 \sin \theta_2 \cos (\varphi_1-\varphi_2) \right]\right\}
,
\\
P_{\neq} = P_{+-}+P_{-+} =
{1\over2}\left(1 - E \right)  \\
\qquad =\frac{1}{2} \left\{ 1+ \left[\cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2 \cos (\varphi_1-\varphi_2) \right]\right\}
.
\end{array}
\end{equation}
Finally, the quantum mechanical expectation function is obtained by  the difference $P_= -P_{\neq }$; i.e.,
\begin{equation}
E_{{ {\Psi^-}}\,-1,+1  }(\theta_1,\theta_2,\varphi_1 , \varphi_2)=
-\left[\cos \theta_1 \cos \theta_2 + \cos (\varphi_1 - \varphi_2) \sin \theta_1 \sin \theta_2\right]
.
\label{2009-gtq-gme22}
\end{equation}
By setting either the azimuthal angle differences equal to zero,
or by assuming measurements in the plane perpendicular to the direction of particle propagation,
i.e., with $\theta_1=\theta_2 =\frac{\pi}{2}$,
one obtains
\begin{equation}
\label{2009-gtq-edosgc}
\begin{array}{rcl}
E_{{ {\Psi^-}}\,-1,+1  }(\theta_1,\theta_2)&=& -\cos (\theta_1 - \theta_2),\\
E_{{ {\Psi^-}}\,-1,+1  }(\frac{\pi}{2},\frac{\pi}{2},\varphi_1 , \varphi_2) &=& - \cos (\varphi_1 - \varphi_2).
\end{array}
\end{equation}


The general computation of the quantum expectation function for operator~(\ref{2004-gtq-e2F2g})
yields
\begin{equation}
\begin{array}{l}
E_{{ {\Psi^-}}\,\lambda_1 \lambda_2 } ({\hat \theta},{\hat \varphi} )=
{\rm Tr}\left[\rho_{ {\Psi^-}} \cdot R_{\frac{1}{2}\frac{1}{2} } \left({\hat \theta},{\hat \varphi} \right)\right] =\\
\quad  =
\frac{1}{4} \left\{( \lambda_- + \lambda_+ )^2-( \lambda_- - \lambda_+ )^2 \left[\cos
    \theta_1  \cos  \theta_2 +\cos ( \varphi_1 - \varphi_2 ) \sin
    \theta_1  \sin  \theta_2 \right]\right\}.
\end{array}
\end{equation}
The standard two-particle quantum mechanical expectations~(\ref{2009-gtq-gme22}) based on the dichotomic outcomes
``$-1$''
and
``$+1$''
are obtained by setting
$  \lambda_+ = -  \lambda_- =1$.

A more ``natural'' choice of $\lambda_\pm$ would be in terms of the spin state observables~(\ref{2004-gtq-e2F2nat}) in units of $\hbar$;
i.e., $  \lambda_+ = -  \lambda_- =\frac{1}{2}$.
The expectation function of  these observables can be directly calculated {\it via} $S_{\frac{1}{2}}$; i.e.,
\begin{equation}
\begin{array}{l}
E_{{ {\Psi^-}}\,-\frac{1}{2},+ \frac{1}{2} } ({\hat \theta},{\hat \varphi} )=
{\rm Tr}\left\{ \rho_{ {\Psi^-}} \cdot \left[ S_{\frac{1}{2}}(\theta_1,\varphi_1) \otimes S_{\frac{1}{2}}(\theta_2,\varphi_2)\right]\right\} \\
\quad =
\frac{1}{4} \left[\cos
    \theta_1  \cos  \theta_2 +\cos ( \varphi_1 - \varphi_2 ) \sin \theta_1  \sin  \theta_2 \right]
= \frac{1}{4}E_{{ {\Psi^-}}\,-1 ,+1 } ({\hat \theta},{\hat \varphi} )
.
\label{2009-gtq-sso2}
\end{array}
\end{equation}


\eexample
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







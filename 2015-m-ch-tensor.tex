\chapter{Tensors}
\label{ch:t}


What follows is a ``corollary,'' or rather an expansion and extension,
of what has been presented in the previous chapter; in particular, with regards to dual vector spaces
(page
\pageref{2011-m-dvs}),
and the tensor product
(page
\pageref{2011-m-tensorp}).

\section{Notation}

Let us consider the vector space ${\Bbb R}^n$ of dimension $n$;
a basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n\}$ consisting of
$n$ basis vectors ${\bf e}_i$,
and $k$ arbitrary vectors
${\bf x}_1,{\bf x}_2,\ldots ,{\bf x}_k\in {\Bbb R}^n$;
the vector ${\bf x}_i$ having the vector components
$X^i_1,X^i_2,\ldots ,X^i_k\in {\Bbb R}$.


Please note again that,
just like any tensor (field), the product
${\bf z} = {\bf x} \times {\bf y}$   of two vectors ${\bf x}$ and ${\bf y}$   has three equivalent  representations:
\begin{itemize}
\item[(i)]
as the scalar coordinates $X^iY^j$ with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
this form is often used in the theory of (general) relativity;
\item[(ii)]
as the quasi-matrix $z^{ij}  =X^iY^j$, whose components $z^{ij}$ are
defined with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
this form is often used in classical (as compared to quantum) mechanics and electrodynamics;
\item[(iii)]
as a quasi-vector or ``flattened matrix'' defined by the Kronecker product
${\bf z} = (X^1  {\bf y}, X^2  {\bf y}, \ldots , X^n  {\bf y})=
(X^1  { Y}^1, \ldots ,X^1  { Y}^n, \ldots  ,X^n  { Y}^1, \ldots , X^n  { Y}^n)
$. Again, the scalar coordinates $X^iY^j$ are defined
with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded.
\index{Kronecker product}
This latter form is often used in (few-partite) quantum mechanics.
\end{itemize}
In all three cases, the pairs $X^iY^j$  are properly represented by distinct mathematical entities.


{\em Tensor fields} define tensors in every point of ${\Bbb R}^n$ separately.
In general, with respect to a particular basis, the components of a tensor field
depend on the coordinates.


We adopt Einstein's summation convention to sum over equal indices
(a pair with a superscript and a subscript).
Sometimes, sums are written out explicitly.


In what follows, the notations
``$x\cdot y$'',
``$(x,y)$'' and
``$\langle x\mid y\rangle $'' will be used synonymously for the {\em
scalar product}
or
{\em inner product}.
\index{scalar product}
\index{inner product}
Note, however, that the ``dot notation $x\cdot y$''
may be a little bit misleading; for example, in the case of the ``pseudo-Euclidean'' metric
represented by the matrix
 ${\rm diag}(+,+,+,\cdots ,+,-)$, it is no more the standard Euclidean dot product
${\rm diag}(+,+,+,\cdots ,+,+)$.

For a more systematic treatment, see for instance Klingbeil's
or Dirschmid's introductions
\cite{Klingbeil,Dirschmid}.


\section{Multilinear form}

A {\em multilinear form}
$\alpha :{ \frak V }^k \mapsto {\Bbb R}$  or  ${\Bbb C}$
is a map from (multiple) arguments ${\bf x}_i$ which are elements of some vector space  $\frak V$
into some scalars in ${\Bbb R}$ or ${\Bbb C}$,  satisfying
\begin{equation}
\begin{split}
\alpha ( {\bf x}_1,{\bf x}_2,\ldots , A {\bf y}+ B {\bf z}, \ldots ,{\bf x}_k)
=
A\alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf y}, \ldots ,{\bf x}_k)   \\
   \qquad +
B\alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf z}, \ldots ,{\bf x}_k)
\end{split}
\end{equation}
for every one of its (multi-)arguments.

In what follows we shall concentrate on {\em real-valued} multilinear forms which map
$k$ vectors in
${\Bbb R}^n$
into
${\Bbb R}$.


\section{Covariance and contra-variance}
\index{covariance}
\index{contravariance}

As already mentioned in section~\ref{2012-m-ch-fdlvs-changeofbasis}
on page~\pageref{2012-m-ch-fdlvs-changeofbasis},
[see, in particular, Eq.~(\ref{2011-m-btbexy})],
in order to code and represent the same vector, any change of scale in the
basis vectors, which can be identified with the coordinate or reference axes,
has to be {\em compensated} by an {\em inverse} change of scale in the coordinates or components.
Thus, the vector coordinates (or, used synonymuously, the components) must {\em contra-vary}
with respect to variances of scales of the basis vectors (or, used synonymuously, the reference axes).
Thus, we will say that, whenever the basis vectors vary covariantly,
their respective coordinates (components) vary contravariantly.
%https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors


Let  ${\bf x}_i =\sum_{j_i=1}^n X^{j_i}_i {\bf e}_{j_i} =X^{j_i}_i {\bf e}_{j_i}$
be some vector in (i.e., some element of) an $n$--dimensional vector space ${\frak V}$ labelled by an index $i$.
A tensor of rank $k$
\index{rank of tensor}
\index{tensor rank}
\index{tensor type}
\begin{equation}
\alpha:{ \frak V }^k \mapsto {\Bbb R}
\end{equation}
is a multilinear form
\begin{equation}
\alpha ( {\bf x}_1,{\bf x}_2,\ldots ,{\bf x}_k)=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_1}_1 X^{i_2}_2\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k}).
\end{equation}
The
\begin{equation}
A_{{i_1}{i_2}\cdots {i_k}}
\stackrel{{\tiny \textrm{ def }}}{=}
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k})
\end{equation}
 are the
{\em components} or
{\em coordinates}
of the tensor $\alpha $ with respect to the basis
${\mathfrak B}$.

Note that a tensor of type (or rank) $k$ in $n$-dimensional vector space has $n^k$  coordinates.

{\color{OliveGreen}
\bproof
To prove that tensors are multilinear forms, insert
\begin{equation}
\begin{split}
 \alpha ( {\bf x}_1,{\bf x}_2,\ldots , A{\bf x}^1_j+B{\bf x}_j^2,\ldots ,{\bf x}_k)
\\
=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  [A(X^1)^{i_j}_j+B(X^2)^{i_j}_j]\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
= A
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  (X^1)^{i_j}_j\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
\quad +
B
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  (X^2)^{i_j}_j\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
=
A \alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf x}^1_j,\ldots ,{\bf x}_k)+
B \alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf x}_j^2,\ldots ,{\bf x}_k)\nonumber
\end{split}
\end{equation}
\eproof
}

\subsection{Change of Basis}
Let
${\mathfrak B}$
and
${\mathfrak B'}$
be two arbitrary bases of
${\Bbb R}^n$.
Then every vector ${\bf e}'_i$ of
${\mathfrak B'}$
can be represented as linear combination of basis vectors from
${\mathfrak B}$ [see also Eqs.~(\ref{2011-m-btbe}) and (\ref{2011-m-btbe-r}]; that is,
\begin{equation}
{\bf e}'_i=\sum_{j=1}^n {a_i}^j {\bf e}_j, \qquad i=1,\ldots , n  .
\label{2001-mu-tensors}
\end{equation}
%Formally, one may treat  ${\bf e}'_i$ and ${\bf e}_i$ as scalar variables $e'_i$ and $e_j$, respectively; such that ${a_i}^j ={\partial { e}'_i \over \partial { e}_j}$.


Consider an arbitrary vector ${\bf x} \in {\Bbb R}^n$
with components $X^i$ with respect to the basis
${\mathfrak B}$
and   ${X'}^i$  with respect to the basis
${\mathfrak B'}$:
\begin{equation}
{\bf x}
=\sum_{i=1}^n X^i {\bf e}_i
=\sum_{i=1}^n {X'}^i {\bf e}'_i
.
\end{equation}
Insertion into (\ref{2001-mu-tensors}) yields
\begin{equation}
\begin{split}
{\bf x}
=\sum_{i=1}^n X^i {\bf e}_i
=\sum_{i=1}^n {X'}^i {\bf e}'_i
=\sum_{i=1}^n {X'}^i \sum_{j=1}^n {a_i}^j {\bf e}_j\\
=
\sum_{i=1}^n\left[\sum_{j=1}^n {a_i}^j{X'}^i \right] {\bf e}_j
=
\sum_{j=1}^n\left[ \sum_{i=1}^n {a_i}^j{X'}^i \right] {\bf e}_j
=
\sum_{i=1}^n\left[ \sum_{j=1}^n {a_j}^i{X'}^j \right] {\bf e}_i
.
\end{split}
\end{equation}
A comparison of coefficient
(and a renaming of the
indices $i \leftrightarrow j$)
yields the transformation laws of vector components
[see also Eq.~(\ref{2012-m-ch-e-tl1})]
\begin{equation}
X^j   = \sum_{i=1}^n {a_i}^j{X'}^i.
\label{2012-m-ch-di-choic}
\end{equation}
The matrix $a=\{{a_i}^j\}$ is called the {\em transformation matrix}.

If the basis transformations involve nonlinear coordinate changes -- such as from the
Cartesian to the polar or spherical coordinates discussed later -- we have to employ differentials
\begin{equation}
dX^j   = \sum_{i=1}^n {a_i}^j \,d{X'}^i  ,
\label{2012-m-ch-di-choic11}
\end{equation}
as well as
\begin{equation}
{a_i}^j ={\partial X^j \over \partial X'^i}   .
\label{2001-mu-tensor-tl11}
\end{equation}

By assuming that the coordinate transformations are linear, ${a_i}^j$ can be expressed as in terms of the coordinates $X^j$
\begin{equation}
{a_i}^j =\frac{  X^j }{  X'^i}  .
\label{2001-mu-tensor-tl1}
\end{equation}

A similar argument using
\begin{equation}
{\bf e}_i=\sum_{j=1}^n {{a'}_i}^j {\bf e}'_j, \qquad i=1,\ldots , n
\label{2012-m-ch-tlcbv}
\end{equation}
yields the transformation laws for the contravariant components (coordinates)
\begin{equation}
{X'}^j   = \sum_{i=1}^n {{a'}_i}^j{X}^i
\label{2015-m-ch-tensor-tlcc}
\end{equation}
%Again, formally, we may treat  ${\bf e}'_i$ and ${\bf e}_i$  as scalar variables $e'_i$ and $e_j$, respectively; such that ${{a'}_i}^j ={\partial {e}_i \over \partial { e}'_j}$.
with respect to the covariant basis vectors, which will soon turn out to me the inverse transformations.
Thereby,
\begin{equation}
{\bf e}_i=\sum_{j=1}^n {{a'}_i}^j {\bf e}'_j
=\sum_{j=1}^n {{a'}_i}^j \sum_{k=1}^n {a_j}^k {\bf e}_k
=\sum_{j=1}^n \sum_{k=1}^n [{{a'}_i}^j {a_j}^k] {\bf e}_k,
\end{equation}
which, due to the linear independence of the basis vectors ${\bf e}_i$ of ${\mathfrak B}$,
is only satisfied if
\begin{equation}
{{a'}_i}^j {a_j}^k =\delta_i^k
\qquad
{\rm or}
\qquad
\textsf{\textbf{A}}'\textsf{\textbf{A}}={\Bbb I}.
\end{equation}
That is, $\textsf{\textbf{A}}'$ is the {\em inverse matrix}
$\textsf{\textbf{A}}^{-1}$  of $\textsf{\textbf{A}}$. In index notation,
\begin{equation}
{(a^{-1})_i}^j = {{a'}_i}^j =\frac{ {X'}^j }{   X^i}
\label{2001-mu-tensor-tl2}
\end{equation}
for linear   coordinate transformations and
\begin{equation}
d{X'}^j   =
\sum_{i=1}^n {{a'}_i}^j\, d{X}^i,
=
\sum_{i=1}^n {{({a^{-1}})}_i}^j\, d{X}^i,
\end{equation}
as well as
\begin{equation}
{{(a^{-1})}_i}^j =
{{a'}_i}^j =
{\partial {X'}^j \over \partial X^i}   =J_{ij},
\label{2001-mu-tensor-tl2nl}
\end{equation}
where $J_{ij}$ stands for
the {\em Jacobian matrix}
\index{Jacobian matrix}
\begin{equation}
J\equiv
J_{ij}={\partial {X'}^i\over \partial {X}^j }
\equiv
\begin{pmatrix}
{\partial {X'}^1\over \partial {X}^1}&\cdots&{\partial {X'}^1\over \partial {X}^n}\\
\vdots&\ddots &\vdots\\
{\partial {X'}^n\over \partial {X}^1}&\cdots&{\partial {X'}^n\over \partial {X}^n}
\end{pmatrix} .
\label{2013-m-t-jm}
\end{equation}


\subsection{Transformation of tensor components}

Because of multilinearity  and by insertion into
(\ref{2001-mu-tensors}),
\begin{eqnarray}
&&\alpha ( {\bf e}'_{j_1},{\bf e}'_{j_2},\ldots ,{\bf e}'_{j_k})=
\alpha \left(
\sum_{i_1=1}^n {a_{j_1}}^{i_1} {\bf e}_{i_1},
\sum_{i_2=1}^n {a_{j_2}}^{i_2} {\bf e}_{i_2},
\ldots ,
\sum_{i_k=1}^n {a_{j_k}}^{i_k} {\bf e}_{i_k}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots {a_{j_k}}^{i_k} \alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k})
\label{2012-m-ch-tensor-etotc1}
\end{eqnarray}
or
\begin{equation}
A'_{{j_1}{j_2}\cdots {j_k}}=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots {a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_k}.
\label{2011-m-tvtcov}
\end{equation}


\section{Contravariant tensors}

Very similar considerations that apply for the inverse scaling of contravariant vector coordinates with respect to covariant base vectors
apply for the inverse scaling of dual base vectors  with respect to covariant base vectors:
in order to compensate those scale changes, dual basis vectors vary contravariantly,
and their respective dual coordinates vary covariantly.
\index{covariance}

\subsection{Definition of contravariant basis}

Consider again a covariant basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n\}$ consisting of
$n$ basis vectors ${\bf e}_i$.
Just as on page \pageref{2011-m-Dualbasis} earlier, we shall define a {\em contravariant} basis
${\mathfrak B^\ast}=\{{\bf e}^1,{\bf e}^2,\ldots ,{\bf e}^n\}$ consisting of
$n$ basis vectors ${\bf e}^i$
by the requirement that the scalar product obeys
\begin{equation}
\delta_i^j =  {\bf e}^i\cdot{\bf e}_j\equiv ({\bf e}^i,{\bf e}_j)\equiv \langle {\bf e}^i\mid {\bf e}_j\rangle
 =\left\{
 \begin{array}{l}
1 \mbox{ if } i=j \\
0 \mbox{ if } i\neq j  \\
\end{array}
 \right. .
\label{2001-mu-tensors0}
\end{equation}



To distinguish elements of the two bases, the covariant vectors are denoted by {\em subscripts},
whereas the contravariant vectors are denoted by {\em superscripts}.
The last terms $ {\bf e}^i\cdot{\bf e}_j\equiv ({\bf e}^i,{\bf e}_j)\equiv \langle {\bf e}^i\mid {\bf e}_j\rangle  $
recall different notations of the scalar product.

Again, note that, for orthonormal bases and Euclidean scalar (dot) products (the coordinates of) the dual basis vectors of an orthonormal basis can be coded identically
as  (the coordinates of) the original basis vectors; that is,
in this case,
(the coordinates of) the dual basis vectors are just rearranged as the transposed form of the original basis vectors.


The entire tensor formalism developed so far can be transferred and applied to define {\em contravariant} tensors
as multinear forms
\begin{equation}
\beta:{ \frak V^\ast }^k \mapsto {\Bbb R}
\end{equation}
by
\begin{equation}
\beta ( {\bf x}^1,{\bf x}^2,\ldots ,{\bf x}^k)=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X_{i_1}^1 X_{i_2}^2 \ldots X_{i_k}^k
\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k}).
\end{equation}
The
\begin{equation}
B^{{i_1}{i_2}\cdots {i_k}}=\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k})
\label{2011-m-tvtcontrav}
\end{equation}
 are the
{\em components} of the contravariant tensor $\beta $ with respect to the basis
${\mathfrak B}^\ast$.


More generally,
suppose ${\frak V}$ is an $n$-dimensional vector space, and
${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is a basis of  ${\frak V}$;
if $g_{ij}$ is the {\em metric tensor},
\index{metric tensor}
the dual basis is defined by
\begin{equation}
g(f_i^*, f_j)=g( f^i,f_j)={\delta^i}_{j},
\end{equation}
where again  ${\delta^i}_{j}$    is Kronecker delta function, which,
due to the symmetry of the metric, in particular,  $g( f^i,f_j)=g( f^j,f_i)={\delta^j}_{i}$,
is defined by
\index{Kronecker delta function}
\begin{equation}
{\delta^i}_{j} = {\delta^j}_{i} =\begin{cases}
0  &\text{ for }i\neq j , \\
1  &\text{ for }i = j;
\end{cases}
\end{equation}
regardless of the order of indices.

\subsection{Connection between the transformation of covariant and contravariant entities}

Because of linearity we can make the formal {\it Ansatz}
\begin{equation}
{{\bf e}'}^j=\sum_i{b_i}^j{\bf e}^i,
\end{equation}
where $\left[{b_i}^j\right] = \textsf{\textbf{B}}$ is
the transformation matrix associated with the contravariant basis.
How is $b$ related to $a$,
the transformation matrix associated with the covariant basis?

By exploiting (\ref{2001-mu-tensors0}) one can find the connection between
the transformation of covariant and contravariant basis elements and thus
tensor components; that is,
\begin{equation}
\delta_i^j= {{\bf e}'}_i\cdot {{\bf e}'}^j=({a_i}^k{\bf e}_k)\cdot ({b_l}^j{\bf e}^l)={a_i}^k {b_l}^j {\bf e}_k\cdot {\bf e}^l={a_i}^k{b_l}^j \delta_k^l
={a_i}^k {b_k}^j,
\end{equation}
and thus, if $\textsf{\textbf{B}}$ represents the transformation (matrix) whose components are ${b_i}^j$,
\begin{equation}
\textsf{\textbf{B}}=\textsf{\textbf{A}}^{-1} =\textsf{\textbf{A}}',\textrm{ and } {{\bf e}'}^j=\sum_i{(a^{-1})_i}^j{\bf e}^i=\sum_i{ a'_i}^j{\bf e}^i.
\label{2012-m-ch-tensor-tocontrav}
\end{equation}
The argument concerning transformations of covariant tensors and components
can be carried through to the contravariant case.
Hence, the contravariant components transform as
\begin{eqnarray}
&&\beta ( {{\bf e}'}^{j_1},{{\bf e}'}^{j_2},\ldots ,{{\bf e}'}^{j_k})=
\beta \left(
\sum_{i_1=1}^n {a'_{i_1}}^{j_1} {\bf e}^{i_1},
\sum_{i_2=1}^n {a'_{i_2}}^{j_2} {\bf e}^{i_2},
\ldots ,
\sum_{i_k=1}^n {a'_{i_k}}^{j_k} {\bf e}^{i_k}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a'_{i_1}}^{j_1}{a'_{i_2}}^{j_2}\cdots {a'_{i_k}}^{j_k} \beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k})
 \label{2012-m-ch-tensor-etotccon1}
\end{eqnarray}
or
\begin{equation}
B'^{{j_1}{j_2}\cdots {j_k}}=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a'_{i_1}}^{j_1}{a'_{i_2}}^{j_2}\cdots {a'_{i_k}}^{j_k} B^{i_1 i_2\ldots i_k}.
 \label{2012-m-ch-tensor-etotccon2}
\end{equation}

For the same, compensatory, reasons yielding to the transformation of the contravariant coordinates (or, used synonymously, components)
with respect to covariant bases
--
reflected in Eqs.~(\ref{2001-mu-tensors}), (\ref{2015-m-ch-tensor-tlcc}), and (\ref{2001-mu-tensor-tl2nl})
--
the coordinates (or, used synonymously, components) with respect to the dual, contravariant, basis vectors, transform covariantly.
\index{contravariance}

We may therefore say that
``basis vectors ${\bf e}_i$, as well as dual components (coordinates) $X_i$ vary covariantly.''
Likewise,
``vector components (coordinates) $X^i$, as well as dual basis vectors ${\bf e}^\ast_i= {\bf e}^i$ vary contra-variantly.''

\section{Orthonormal bases}
For orthonormal bases of $n$-dimensional Hilbert space,
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j
\textrm { if and only if }
{\bf e}_i= {\bf e}^i  \textrm { for all } 1\le i,j \le n.
\end{equation}
Therefore, the vector space and its dual vector space are ``identical''
in the sense that the coordinate tuples representing their bases are identical
(though relatively transposed).
That is, besides transposition, the two bases are identical
\begin{equation}
{\mathfrak B}\equiv {\mathfrak B}^\ast
\end{equation}
and  formally any distinction between covariant and contravariant vectors becomes
irrelevant. Conceptually, such a distinction persists, though.
In this sense, we might ``forget about the difference between
covariant and contravariant orders.''



\section{Invariant tensors and physical motivation}

\section{Metric tensor}

Metric tensors are defined in metric vector spaces.
A metric vector space (sometimes also refered to
as ``vector space with metric'' or ``geometry'')
is a vector space with some inner or scalar product.
This includes (pseudo-) Euclidean spaces with indefinite metric.
(I.e., the distance needs not be positive or zero.)




\subsection{Definition metric}
\index{metric tensor}
\index{metric}
\label{2011-m-metrict}

A {\em metric} $g$ is a functional ${\Bbb R}^n\times{\Bbb R}^n\mapsto {\Bbb R}$
with the following properties:
\begin{itemize}
\item
$g$ is symmetric; that is, $g(x,y)=g(y,x)$;
\item
$g$ is bilinear; that is,
$g(
\alpha x + \beta y, z)
= \alpha g( x,z) + \beta g(y, z)
$ (due to symmetry $g$ is also bilinear in the second argument);
\item
$g$ is nondegenerate; that is,
for every $x\in {\frak V}$, $x\neq 0$, there exists a
$y\in {\frak V}$ such that $g(x,y)\neq 0$.
\end{itemize}



\subsection{Construction of a metric from a scalar product by metric tensor}

In particular cases, the {\em metric} tensor may be defined {\it via}  the scalar product
\begin{equation}
g_{ij}={\bf e}_i\cdot {\bf e}_j\equiv ({\bf e}_i, {\bf e}_j)\equiv \langle {\bf e}_i \mid {\bf e}_j\rangle .
\end{equation}
and
\begin{equation}
g^{ij}={\bf e}^i\cdot {\bf e}^j\equiv ({\bf e}^i, {\bf e}^j)\equiv \langle {\bf e}^i \mid {\bf e}^j\rangle .
\end{equation}
By definition of the (dual) basis in Eq. (\ref{2011-m-Dualbasis-e3}) on page \pageref{2011-m-Dualbasis-e3},
\begin{equation}
{g^i}_{j}
= g({\bf e}^i , {\bf e}_j)
={\delta^i}_j ,
\label{2014-m-ch-tensor-deltag}
\end{equation}
which is a reflection of the covariant and contravariant metric tensors being inverse,
since the basis and the associated dual basis is inverse (and {\it vice versa}).
Note that it is possible to change a covariant tensor into a contravariant one and {\em vice versa}
by the application of a metric tensor.
This can be seen as follows.
Because of linearity, any contravariant basis vector ${\bf e}^i$
can be written as a linear sum of covariant (transposed, but we do not mark transposition here) basis vectors:
\begin{equation}
{\bf e}^i=A^{ij}{\bf e}_j.
\end{equation}
Then,
\begin{equation}
g^{ik} ={\bf e}^i\cdot {\bf e}^k =(A^{ij}{\bf e}_j)\cdot {\bf e}^k=A^{ij}({\bf e}_j\cdot {\bf e}^k)=A^{ij}\delta_j^k=A^{ik}
\end{equation}
and thus
\begin{equation}
{\bf e}^i=g^{ij}{\bf e}_j
\end{equation}
and
\begin{equation}
{\bf e}_i=g_{ij}{\bf e}^j.
\end{equation}


For orthonormal bases, the metric tensor can be
represented as a Kronecker delta function, and thus  remains form invariant.
Moreover, its covariant and contravariant components are identical; that is,
$g_{ij}=\delta_{ij}=\delta^i_j=\delta_i^j=\delta^{ij}=g^{ij}$.



\subsection{What can the metric tensor do for us?}

Most often it is used to raise or lower the indices; that is,
to change from contravariant to covariant and conversely from covariant
to contravariant.
{
\color{blue}
\bexample
For example,
\begin{equation}
{\bf x} =
X^i {\bf e}_i = X^i g_{ij} {\bf e}^j   = X_j {\bf e}^j,
\end{equation}
and hence $X_j = X^i g_{ij}$.
\eexample
}


In the previous section, the metric tensor has been derived from the scalar product.
The converse can be true as well.
(Note, however, that the metric need not be positive.)
In Euclidean space with the dot (scalar, inner) product
the metric tensor represents the scalar product between vectors: let
${\bf x}=X^i{\bf e}_i \in {\Bbb R}^n$ and ${\bf y}=Y^j{\bf e}_j \in {\Bbb R}^n$ be two vectors.
Then ("$T$" stands for the transpose),
\begin{equation}
{\bf x}\cdot {\bf y}\equiv ({\bf x},{\bf y})\equiv \langle {\bf x}\mid {\bf y}\rangle
= X^i {\bf e}_i\cdot Y^j {\bf e}_j
= X^iY^j {\bf e}_i\cdot  {\bf e}_j
=X^iY^j g_{ij}= X^T g Y.
\end{equation}

It also characterizes the length of a vector: in the above
equation, set ${\bf y}={\bf x}$. Then,
\begin{equation}
{\bf x}\cdot {\bf x}\equiv ({\bf x},{\bf x})\equiv \langle {\bf x}\mid {\bf x}\rangle
=X^iX^j g_{ij}\equiv X^T g X,
\end{equation}
and thus
\begin{equation}
\vert\vert  x\vert\vert  =\sqrt{X^iX^j g_{ij}}= \sqrt{X^T g X}.
\end{equation}


The square of an infinitesimal vector $ds =\vert\vert d{\bf x} \vert\vert $ is
\begin{equation}
(d s)^2  = g_{ij}d{\bf x}^id{\bf x}^j= d{\bf x}^T g d{\bf x}.
\end{equation}




\subsection{Transformation of the metric tensor}

Insertion into the definitions and coordinate transformations
(\ref{2012-m-ch-tlcbv}) as well as      (\ref{2001-mu-tensor-tl2})
yields
\begin{equation}
\begin{split}
g_{ij}={\bf e}_i\cdot {\bf e}_j
={a'_i}^l{\bf e'}_l\cdot {a'_j}^m{\bf e'}_m
={a'_i}^l {a'_j}^m {\bf e'}_l\cdot {\bf e'}_m \\
= {a'_i}^l {a'_j}^m {g'}_{lm}
= {\partial {X'}^l\over \partial X^i}{\partial {X'}^m\over \partial X^j} {g'}_{lm}
.
\label{2011-m-emtdc}
\end{split}
\end{equation}

Conversely,  (\ref{2001-mu-tensors}) as well as    (\ref{2001-mu-tensor-tl1})
yields
\begin{equation}
\begin{split}
g'_{ij}={\bf e}'_i\cdot {\bf e}'_j
={a_i}^l{\bf e}_l\cdot {a_j}^m{\bf e}_m
={a_i}^l {a_j}^m {\bf e}_l\cdot {\bf e}_m  \\
= {a_i}^l {a_j}^m {g}_{lm}
= {\partial {X}^l\over \partial {X'}^i}{\partial {X}^m\over \partial {X'}^j} {g}_{lm}
.
\end{split}
\end{equation}


If the geometry (i.e., the basis) is locally orthonormal, ${g}_{lm}=\delta_{lm}$,
then
$g'_{ij}={\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}$.

Just to check consistency with Eq.~(\ref{2014-m-ch-tensor-deltag}) we can compute,
for suitable differentiable coordinates $X$ and $X'$,
\begin{equation}
\begin{split}
{g'_i}^j={\bf e}'_i\cdot {{\bf e}'}^j
={a_i}^l{\bf e}_l\cdot {(a^{-1})^j}_m{\bf e}^m
={a_i}^l {(a^{-1})^j}_m {\bf e}_l\cdot {\bf e}^m  \\
= {a_i}^l {(a^{-1})^j}_m \delta_l^m
= {a_i}^l {(a^{-1})^j}_l  \\
= {\partial {X}^l\over \partial {X'}^i}{\partial {X'}_l\over \partial {X}_j}
= {\partial {X}^l\over \partial {X}^j}{\partial {X'}_l\over \partial {X'}_i}
= \delta^{lj}\delta_{li}
= \delta^l_i
.
\end{split}
\end{equation}

In terms of the
{\em  Jacobian matrix} defined in Eq.~(\ref{2013-m-t-jm})
\index{Jacobian matrix}
the metric tensor in Eq. (\ref{2011-m-emtdc})
can be rewritten as
\begin{equation}
g = J^T g' J
\equiv g_{ij}= J_{li}J_{mj}g'_{lm}
.
\label{2011-m-emtdcJ}
\end{equation}
The metric tensor and the Jacobian (determinant)
are thus related by
\begin{equation}
\textrm{det }g = (\textrm{det }J^T) (\textrm{det } g')(\textrm{det } J)
.
\label{2011-m-emtdcJd}
\end{equation}
If the manifold is embedded into an Euclidean space,
then $g'_{lm}=\delta_{lm}$
and  $g = J^T  J $.

\subsection{Examples}

In what follows a few metrics are enumerated and briefly commented.
For a more systematic treatment, see, for instance, Snapper and Troyer's {\em Metric Affine geometry} \cite{snapper-troyer}.

Note also that due to the properties of the metric tensor, its coordinate representation has to be a {\em symmetric matrix}
with {\em nonzero diagonals}.
For the symmetry $g( {\bf x}  ,{\bf y} )=g({\bf y} ,{\bf x} )$ implies that $g_{ij}x_iy^j= g_{ij}y^ix_j=  g_{ij}x_jy^i= g_{ji}x_iy^j$ for all coordinate tuples
$x^i$ and $y^j$. And for any zero diagonal entry (say, in the $k$'th position of the diagonal we can choose a nonzero vector  ${\bf z}$
whose coordinates are all zero except the $k$'th coordinate. Then $g ({\bf z},{\bf x})=0$ for all ${\bf x}$ in the vector space.


\subsection*{$n$-dimensional Euclidean space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{n\; {\rm times}})
\end{equation}

One application in physics is quantum mechanics,
where $n$ stands for the dimension of a complex Hilbert space.
Some definitions can be easily adopted to accommodate the complex numbers.
E.g., axiom 5 of the scalar product becomes
$(x,y)=\overline{(x,y)}$, where ``$\overline{(x,y)}$'' stands for complex conjugation of $(x,y)$.
Axiom 4 of the scalar product becomes
$(x,\alpha y)=\overline{\alpha} (x,y)$.

\subsection*{Lorentz plane}


\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (1,-1)
\end{equation}

\subsection*{Minkowski space of dimension $n$}

In this case the metric tensor is called the
{\em Minkowski metric}
\index{Minkowski metric}
and is often denoted by  ``$\eta$'':
\begin{equation}
\eta \equiv \{\eta_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{n-1\; {\rm times}},-1)
\label{2012-m-ch-tensor-minspn}
\end{equation}


One application in physics is the theory of special relativity,
where $D=4$.
Alexandrov's theorem states that the mere requirement of the preservation of
zero distance (i.e., lightcones), combined with bijectivity (one-to-oneness) of the transformation law
yields the Lorentz transformations
\cite{alex1,alex2,alex3,alex-col,borchers-heger,benz,lester,svozil-2001-convention}.



\subsection*{Negative Euclidean space of dimension $n$}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{-1,-1,\ldots ,-1}_{n\; {\rm times}})
\end{equation}

\subsection*{Artinian four-space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (+1,+1,-1 ,-1)
\end{equation}



\subsection*{General relativity}

In general relativity, the metric tensor $g$ is linked to the energy-mass distribution.
There, it appears as the primary concept when compared to the scalar product.
In the case of zero gravity, $g$ is just the  Minkowski metric (often denoted by  ``$\eta$'')
${\rm diag} (1,1,1,-1) $ corresponding to ``flat'' space-time.

The best known non-flat metric is the Schwarzschild metric
\begin{equation}
g
\equiv
\begin{pmatrix}
(1-2m/r)^{-1}&0&0&0\\
0&r^2&0&0\\
0&0&r^2\sin^2 \theta &0\\
0&0&0&- \left( 1-{2m/r}\right)
\end{pmatrix}
\end{equation}
with respect to the spherical space-time coordinates $r,\theta ,\phi ,t$.

{
\color{blue}
\bexample

\subsection*{Computation of the metric tensor of the circle of radius $r$}
Consider the transformation from the standard orthonormal
threedimensional ``Cartesian'' coordinates
$X_1=x$,
$X_2=y$,
into polar coordinates
\index{spherical coordinates}
$X_1'=r$,
$X_2'=\varphi$.
In terms of  $r$ and $\varphi$, the Cartesian coordinates can be written as
\begin{equation}
\begin{split}
 X_1=r \cos \varphi \equiv X_1' \cos X_2'  , \\
 X_2=r \sin \varphi \equiv X_1'\sin X_2'  .
\end{split}
\end{equation}
Furthermore,  since the basis we start with is the Cartesian orthonormal basis,
$g_{ij}=\delta_{ij}$; therefore,
\begin{equation}
g'_{ij}= {\partial {X}^l\over \partial {X'}^i}{\partial {X}_k\over \partial {X'}^j} g_{lk}
= {\partial {X}^l\over \partial {X'}^i}{\partial {X}_k\over \partial {X'}^j} \delta_{lk}
= {\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}.
\end{equation}
More explicity, we obtain for the coordinates of the transformed metric tensor $g'$
\begin{equation}
\begin{split}
g'_{11}
= {\partial {X}^l\over \partial {X'}^1}{\partial {X}_l\over \partial {X'}^1} \\
=
{\partial (r \cos \varphi) \over \partial {r}}{\partial (r \cos \varphi) \over \partial {r}}
+
{\partial (r \sin \varphi) \over \partial {r}}{\partial (r \sin \varphi) \over \partial {r}}       \\
=
( \cos \varphi )^2
+
(\sin \varphi )^2 =1,\\
g'_{12}
= {\partial {X}^l\over \partial {X'}^1}{\partial {X}_l\over \partial {X'}^2} \\
=
{\partial (r \cos \varphi) \over \partial {r}}{\partial (r \cos \varphi) \over \partial {\varphi }}
+
{\partial (r \sin \varphi) \over \partial {r}}{\partial (r \sin \varphi) \over \partial {\varphi }}  \\
=
(\cos \varphi ) (- r \sin \varphi )
+
(\sin \varphi )( r \cos \varphi )  =0,\\
g'_{21}
= {\partial {X}^l\over \partial {X'}^2}{\partial {X}_l\over \partial {X'}^1} \\
=
{\partial (r \cos \varphi) \over \partial {\varphi }}{\partial (r \cos \varphi) \over \partial {r}}
+
{\partial (r \sin \varphi) \over \partial {\varphi }}{\partial (r \sin \varphi) \over \partial {r}}   \\
=
(- r \sin \varphi ) ( \cos \varphi )
+
( r\cos  \varphi )( \sin \varphi )  =0,\\
g'_{22}
= {\partial {X}^l\over \partial {X'}^2}{\partial {X}_l\over \partial {X'}^2} \\
=
{\partial (r \cos \varphi) \over \partial {\varphi}}{\partial (r \cos \varphi) \over \partial {\varphi }}
+
{\partial (r \sin \varphi) \over \partial {\varphi}}{\partial (r \sin \varphi) \over \partial {\varphi }}      \\
=
(- r \sin \varphi )^2
+
( r \cos \varphi )^2  =r^2;
\end{split}
\end{equation}
that is, in matrix notation,
\begin{equation}
g'
=
\begin{pmatrix}
1&0\\
0&r^2
\end{pmatrix}
,
\end{equation}
and thus
\begin{equation}
(d s')^2  = g_{ij}'d{\bf x'}^i d{\bf x'}^j=   (dr)^2 + r^2 (d\varphi )^2.
\label{2014-gppolarc}
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Computation of the metric tensor of the ball}
Consider the transformation from the standard orthonormal
threedimensional ``Cartesian'' coordinates
$X_1=x$,
$X_2=y$,
$X_3=z$,
into spherical coordinates
%$$$(for a definition of spherical coordinates, see also page \pageref{2011-m-spericalcoo})
\index{spherical coordinates}
$X_1'=r$,
$X_2'=\theta$,
$X_3'=\varphi$.
In terms of  $r,\theta , \varphi$, the Cartesian coordinates can be written as
\begin{equation}
\begin{split}
 X_1=r \sin \theta \cos \varphi \equiv X_1' \sin X_2' \cos X_3'  , \\
 X_2=r \sin \theta \sin \varphi \equiv X_1'\sin X_2' \sin X_3'  ,    \\
 X_3=r \cos \theta  \equiv X_1'\cos X_2'  .
\end{split}
\end{equation}
Furthermore,  since the basis we start with is the Cartesian orthonormal basis,
$g_{ij}=\delta_{ij}$; hence finally
\begin{equation}
g'_{ij}= {\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}
\equiv {\rm diag}(1,r^2,r^2\sin^2 \theta ),
\end{equation}
and
\begin{equation}
(ds')^2 =(dr)^2+r^2(d\theta )^2+r^2\sin^2 \theta (d\varphi )^2.
\end{equation}

The expression $(ds)^2 =(dr)^2+r^2(d\varphi )^2$
for polar coordinates in two dimensions (i.e., $n=2$) of Eq.~(\ref{2014-gppolarc})  is recovereded by setting $\theta = \pi/2 $ and $d\theta =0$.

\subsection*{Computation of the metric tensor of the Moebius strip}
The parameter representation of the Moebius strip is
\begin{equation}
\Phi (u,v) =\left(
\begin{array}{c}
(1+v\cos \frac{u}{2})\sin u \\
(1+v\cos \frac{u}{2})\cos u \\
v\sin \frac{u}{2}
\end{array}
\right),
\end{equation}
where
$u\in [0,2\pi ]$ represents the position of the point on the circle,  and where $2a>0$ is the ``width'' of the Moebius strip,
and where $v\in [-a,a]$.


\begin{equation}
\begin{split}
\Phi _{v}=\frac{\partial \Phi }{\partial v}=\allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}
 \\
\Phi _{u}=\frac{\partial \Phi }{\partial u}=\allowbreak
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
\end{split}
\end{equation}


\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial u}
=  \allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}^{T}
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
\\
=
-\frac{1}{2}\left( \cos \frac{u}{2}\sin ^{2}u\right) v\sin \frac{u}{2}-%
\frac{1}{2}\left( \cos \frac{u}{2}\cos ^{2}u\right) v\sin \frac{u}{2}
\\
+%
\frac{1}{2}\sin \frac{u}{2} v\cos \frac{u}{2}=\allowbreak 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial v}
=\allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}^{T}
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}
 \\
=
\cos ^{2}\frac{u}{2}\sin ^{2}u+\cos ^{2}\frac{u}{2}\cos ^{2}u+\sin ^{2}%
\frac{u}{2}=\allowbreak 1
\end{split}
\end{equation}


\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial u})^{T}\frac{\partial \Phi }{\partial u}
=
\\
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos
u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin
u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}^{T} \cdot
\\
\cdot
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos
u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin
u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
 \\
=
\frac{1}{4}v^{2}\sin ^{2}\frac{u}{2}\sin ^{2}u+\cos
^{2}u+2 v \cos ^{2}u \cos \frac{u}{2}+
v^{2}\cos ^{2}u  \cos ^{2}\frac{u}{2}
\\
+\frac{1}{4}v^{2}\sin ^{2}\frac{u}{2}\cos^{2}u
+\sin ^{2}u+2  v\sin ^{2}u\cos \frac{u}{2}+ v^{2} \sin^{2}u\cos ^{2}\frac{u}{2}
 \\
+\frac{1}{4}v^{2}\cos ^{2}\frac{1}{2}%
u =\allowbreak \frac{1}{4}v^{2}+v^{2}\cos ^{2}\frac{u}{2}+1+2v\cos \frac{%
1}{2}u
 \\
=\left(1+v\cos \frac{u}{2}\right)^{2}+\frac{1}{4}v^{2}
\end{split}
\end{equation}


Thus the metric tensor is given by
\begin{equation}
\begin{split}
g'_{ij}
= {\partial {X}^s\over \partial {X'}^i}{\partial {X}^t\over \partial {X'}^j}g_{st}
= {\partial {X}^s\over \partial {X'}^i}{\partial {X}^t\over \partial {X'}^j}\delta_{st}\\
\quad \equiv
\left(
\begin{array}{cc}
\Phi _{u}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{u} \\
\Phi _{v}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{v}
\end{array}
\right) ={\rm diag}\left(
\left(1+v\cos \frac{u}{2}\right)^{2}+\frac{1}{4}v^{2} , 1\right).
\end{split}
\end{equation}

\eexample
}





\section{General tensor}

A (general) Tensor $T$ can be defined as a multilinear form  on the
$r$-fold product of a vector space ${\frak V}$, times the
$s$-fold product of the dual vector space ${\frak V}^\ast$;
that is,
\begin{equation}
T: \left( {\frak V} \right)^r \times \left( {\frak V}^\ast \right)^s
=
\underbrace{{\frak V}\times \cdots \times {\frak V}}_{r\textrm{ \scriptsize copies}}
\times
\underbrace{{\frak V}^\ast \times \cdots \times {\frak V}^\ast}_{s\textrm{ \scriptsize copies}}
\mapsto {\Bbb F}
,
 \label{2012-m-ch-tensor-gdt}
\end{equation}
where, most commonly, the scalar field
${\Bbb F}$
will be identified with the set ${\Bbb R}$ of reals,
or with the set ${\Bbb C}$ of complex numbers.
Thereby,
$r$ is called the
{\em covariant order}, and
\index{covariance}
$s$ is called the
{\em contravariant order}
\index{contravariance}
of $T$.
A tensor of covariant order $r$ and contravariant order $s$
is then pronounced a tensor of
{\em type} (or {\em rank})
\index{tensor rank}
\index{tensor type}
$(r,s)$.
By convention, covariant indices are denoted by {\em subscripts},
whereas the contravariant indices  are denoted by {\em superscripts}.

With the standard, ``inherited'' addition and scalar multiplication,
the set ${\frak T}_r^s$ of all tensors of type $(r,s)$
forms a linear vector space.


Note that a tensor of type $(1,0)$ is called  a
{\em covariant vector}
\index{covariance},
or just a
{\em vector}.
\index{vector}
A tensor of type $(0,1)$ is called a
{\em contravariant vector}.
\index{contravariance}

Tensors can change their type by the invocation of the {\em metric tensor}.
That is, a covariant tensor (index) $i$ can be made into a contravariant tensor (index) $j$
by summing over the index $i$ in a product involving the tensor and $g^{ij}$.
Likewise,  a contravariant tensor (index) $i$ can be made into a covariant tensor (index) $j$
by summing over the index $i$ in a product involving the tensor and $g_{ij}$.


Under basis or other linear transformations,
covariant tensors with index $i$ transform by summing over this index with (the transformation matrix) ${a_i}^j$.
Contravariant tensors with index $i$ transform by summing over this index with the inverse (transformation matrix)  ${(a^{-1})_i}^j$.

\section{Decomposition of tensors}

Although a tensor of type (or rank) $n$ transforms like the tensor product of $n$ tensors of type 1,
not all type-$n$ tensors can be decomposed into a single
tensor product of $n$ tensors of type (or rank) 1.

Nevertheless,
by a generalized Schmidt decomposition (cf. page \pageref{2011-m-Schmidtdecomposition}),
any type-$2$ tensor  can be decomposed into
the sum of
tensor products of two tensors of type 1.

\section{Form invariance of tensors}

A tensor (field) is
form invariant  with respect to some basis change
\index{form invariance}
if its representation in the new basis has the same form as in the old basis.
For instance, if the ``12122--component'' $T_{12122} (x)$ of the tensor $T$
with respect to the old basis and old coordinates $x$   equals some function $f(x)$ (say, $f(x)=x^2$),
then, a necessary condition for $T$ to be form invariant is that, in terms of the new basis,
that component  $T'_{12122} (x')$  equals the same function $f(x')$ as before, but in the new coordinates $x'$.
A sufficient condition for form invariance of $T$ is that {\em all}
coordinates or components of $T$ are form invariant in that way.


Although form invariance is a gratifying feature for the reasons explained shortly,
a tensor (field) needs not necessarily
be form invariant with respect to all or even any (symmetry) transformation(s).



A physical motivation for the use of form invariant tensors can be given as follows.
What makes some tuples (or matrix, or tensor components in general)  of
numbers or scalar functions a tensor? It is the
interpretation of the scalars as tensor components {\em with respect to
a particular basis}. In another basis, if we were talking about the same
tensor, the tensor components; that is, the numbers or scalar functions,
would be different.
Pointedly stated, the tensor coordinates represent some
encoding of a multilinear function with respect to a particular basis.

Formally, the tensor coordinates are numbers; that is, scalars,
which are grouped together in vector touples or matrices or whatever form we consider useful.
As the tensor coordinates are scalars, they can be treated as scalars.
For instance, due to commutativity and associativity, one can exchange
their order. (Notice, though, that this is generally not the case for
differential operators such as $\partial_i=\partial / \partial {\bf x}^i$.)

A {\em form invariant} tensor with respect to  certain transformations
is a tensor which retains
the same functional form if the transformations are performed; that is,
if the basis changes accordingly.
That is, in this case,
the functional form of mapping numbers or coordinates or other entities remains unchanged, regardless of the coordinate change.
Functions remain the same but with the new parameter components as
arguement. For instance; $4\mapsto 4$ and $f(X_1,X_2,X_3)\mapsto
f(X'_1,X'_2,X'_3)$.

Furthermore, if a tensor is invariant with respect to one transformation, it need not
be invariant with respect to another transformation, or with respect to
changes of the scalar product; that is, the metric.

Nevertheless, totally symmetric (antisymmetric) tensors remain totally
symmetric (antisymmetric) in all cases:
\begin{equation}
A_{i_1i_2 \ldots i_si_t\ldots i_k}
=
\pm A_{i_1i_2 \ldots i_ti_s\ldots i_k}
\end{equation}
implies
\begin{equation}
\begin{split}
A'_{j_1i_2 \ldots j_s j_t\ldots j_k}
=
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}
\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}
\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_s i_t\ldots  i_k}
 \\
=
\pm {a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
  \\
=
\pm {a_{j_1}}^{i_1}{a_{j_2}}^{i_2}
\cdots
{a_{j_t}}^{i_t}{a_{j_s}}^{i_s}
\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
  \\
=
\pm A'_{j_1i_2 \ldots j_t j_s\ldots j_k}    .
\end{split}
\end{equation}


In physics, it would be nice if the natural laws could be written into a
form which does not depend on the particular reference frame or  basis
used.
Form invariance thus is a gratifying physical feature, reflecting the
{\em symmetry} against changes of coordinates and bases.

After all, physicists want the formalization of their fundamental laws not to artificially depend on,
say, spacial directions, or on some particular basis, if there is no physical reason why this should be so.
Therefore, physicists tend to be crazy to write down everything in a
form invariant manner.

One strategy to accomplish  form invariance  is to start out with form invariant
tensors and compose -- by tensor products and index reduction -- everything from them. This method guarantees form
invarince.

{
\color{blue}
\bexample

The ``simplest'' form invariant tensor under all transformations is the constant tensor of rank $0$.

Another constant form invariant tensor  under all transformations is represented by the Kronecker symbol $\delta^i_j$,
because
\begin{equation}
{(\delta ')}^i_j= {(a^{-1})_{i'}}^i {a_j}^{j'}\delta^{i'}_{j'}={(a^{-1})_{i'}}^i {a_j}^{i'}= {a_j}^{i'}{(a^{-1})_{i'}}^i=\delta^i_j
.
\end{equation}

A simple form invariant tensor field is a vector ${\bf x}$,
because if $T({\bf x})= x^i t_i= x^i {\bf e}_i={\bf x}$, then
the ``inner transformation''
${\bf x} \mapsto  {\bf x}'$
and the ``outer transformation''
$T \mapsto  T'= \textsf{\textbf{A}}T$
just compensate each other; that is, in coordinate representation, Eqs.(\ref{2012-m-ch-di-choic}) and (\ref{2011-m-tvtcov}) yield
\begin{equation}
T'({\bf x}')= {x'}^i t'_i= {(a^{-1})_l}^i x^l   {a_i}^j t_j = {(a^{-1})_l}^i  {a_i}^j {\bf e}_j  x^l
= \delta_l^j {\bf e}_j  x^l = {\bf x} = T({\bf x}).
\end{equation}



For the sake of another demonstration of form invariance, consider the following two factorizable tensor fields:
while
\begin{equation}
{S}(x)=
\begin{pmatrix}
  {  x}_2  \\
- {  x}_1
\end{pmatrix}
\otimes
\begin{pmatrix}
   {  x}_2  \\
 - {  x}_1
\end{pmatrix}^T
=
\left(    {  x}_2 ,- {  x}_1  \right)^T
\otimes
\left(    {  x}_2 ,- {  x}_1  \right)
\equiv
\begin{pmatrix}
   {  x}_2^2          & -{ x}_1{x}_2  \\
 - {  x}_1{  x}_2     & { x}_1^2
\end{pmatrix}
\label{2012-m-ch-tensor-etotccon1factorized}
\end{equation}
is a form invariant tensor field with respect to the basis $\{(0,1),(1,0)\}$
and orthogonal transformations (rotations around the origin)
\begin{equation}
\begin{pmatrix}
  \cos \varphi & \sin \varphi  \\
 -\sin \varphi & \cos \varphi
\end{pmatrix}
,
\end{equation}
\begin{equation}
{ T}(x)=
\begin{pmatrix}
{  x}_2  \\
{  x}_1
\end{pmatrix}
\otimes
\begin{pmatrix}
{  x}_2  \\
{  x}_1 \end{pmatrix}^T
=
\left(    {  x}_2 ,  {  x}_1  \right)^T
\otimes
\left(    {  x}_2 ,  {  x}_1  \right)
\equiv
\begin{pmatrix}
{  x}_2^2 & { x}_1{  x}_2  \\
{  x}_1{  x}_2          & { x}_1^2
\end{pmatrix}
\end{equation}
is not.

This can be proven by considering the single factors from which $S$ and $T$ are composed.
Eqs. (\ref{2012-m-ch-tensor-etotc1})-(\ref{2011-m-tvtcov})
and
(\ref{2012-m-ch-tensor-etotccon1})-(\ref{2012-m-ch-tensor-etotccon2})
show that the form
invariance of the factors implies the form invariance of the tensor products.

For instance, in our example, the factors $\left(    {  x}_2 ,- {  x}_1  \right)^T$
of $S$ are invariant, as they transform as
$$
\begin{pmatrix} \cos \varphi & \sin \varphi  \\
                         -\sin \varphi & \cos \varphi
\end{pmatrix}
\begin{pmatrix}
{  x}_2  \\
 - {  x}_1
\end{pmatrix}
=
\begin{pmatrix}
 {  x}_2 \cos \varphi  - x_1 \sin \varphi  \\
            - x_2 \sin \varphi         - {  x}_1 \cos \varphi
\end{pmatrix}
=
\begin{pmatrix}
{  x}_2'  \\
 - {  x}_1'
\end{pmatrix},
$$
where the transformation of the coordinates
$$
\begin{pmatrix}
{  x}_1'  \\
  {  x}_2'
\end{pmatrix}
=
\begin{pmatrix}
 \cos \varphi & \sin \varphi  \\
   -\sin \varphi & \cos \varphi
\end{pmatrix}
\begin{pmatrix}
 {  x}_1  \\
 {  x}_2
\end{pmatrix}
=
\begin{pmatrix}
{  x}_1 \cos \varphi  + x_2 \sin \varphi  \\
- x_1 \sin \varphi         + {  x}_2 \cos \varphi
\end{pmatrix}
$$
has been used.


Note that  the notation identifying tensors of type (or rank) two with matrices,
creates an ``artifact'' insofar as the transformation of the ``second index'' must then be represented by
the exchanged multiplication order, together with the transposed transformation matrix;
that is,
$$
a_{ik}a_{jl}A_{kl}
=  a_{ik}A_{kl}a_{jl}
=  a_{ik}A_{kl}\left(a^T \right)_{lj}
\equiv a\cdot A\cdot a^T .
$$
Thus for a
transformation  of
the transposed touple  $\left(    {  x}_2 ,- {  x}_1  \right)$
we must consider the {\em transposed} transformation matrix arranged {\em after} the factor; that is,
$$
\left(   {  x}_2 , - {  x}_1 \right)
\begin{pmatrix}  \cos \varphi & -\sin \varphi  \\
  \sin \varphi & \cos \varphi
\end{pmatrix}
=
\left(
{  x}_2 \cos \varphi  - x_1 \sin \varphi ,
 - x_2 \sin \varphi         - {  x}_1 \cos \varphi
\right)
=
\left(
{  x}_2'  ,
 - {  x}_1'
\right).
$$



In contrast, a similar calculation shows that the factors
$\left(    {  x}_2 ,  {  x}_1  \right)^T$
of $T$ do not transform invariantly.
However, noninvariance with respect to certain transformations does not imply that
$T$ is not a valid, ``respectable'' tensor field; it is just not form invariant under rotations.
\eexample
}

Nevertheless, note again that, while the tensor product of form invariant tensors is again a form invariant tensor,  not every form
invariant tensor might be decomposed into products of form invariant tensors.

{
\color{blue}
\bexample
Let
$\vert + \rangle  \equiv   (0,1)$
and
$\vert - \rangle  \equiv   (1,0)$.
For a nondecomposable tensor, consider the sum of two-partite tensor products (associated with two ``entangled'' particles)
\index{entanglement}
Bell state (cf. Eq. (\ref{2014-m-ch-fdvs-bellbasis}) on page \pageref{2014-m-ch-fdvs-bellbasis}) in the standard basis     \index{Bell state}
\begin{equation}
\begin{split}
\vert \Psi^-\rangle = \frac{1}{\sqrt{2}}\left(\vert +-\rangle   - \vert -+\rangle  \right)   \\
\qquad \equiv  \left( 0,\frac{1}{\sqrt{2}},- \frac{1}{\sqrt{2}} ,  0 \right)     \\
\qquad \equiv  \frac{1}{2}
\begin{pmatrix}
0&0&0&0\\
0&1&-1&0\\
0&-1&1&0\\
0&0&0&0
\end{pmatrix}
.
\end{split}
\label{2011-m-bellstatenondec}
\end{equation}
\marginnote{$\vert \Psi^-\rangle $,   together with the other three Bell states
$\vert \Psi^+\rangle = \frac{1}{\sqrt{2}}\left(\vert +-\rangle   + \vert -+\rangle  \right) $,
$\vert \Phi^+\rangle = \frac{1}{\sqrt{2}}\left(\vert --\rangle   + \vert ++\rangle  \right) $,
and
$\vert \Phi^-\rangle = \frac{1}{\sqrt{2}}\left(\vert --\rangle   - \vert ++\rangle  \right) $,
forms an orthonormal basis of ${\Bbb C}^4$.
}

Why is $\vert \Psi^-\rangle$ not decomposable?
In order to be able to answer this question
(see alse Section \ref{2012-m-c-fdvs-entanglement} on page \pageref{2012-m-c-fdvs-entanglement}), consider
the most general two-partite state
\begin{equation}
\vert \psi \rangle
=
\psi_{--}\vert -- \rangle
+
\psi_{-+}\vert -+ \rangle
+
\psi_{+-}\vert +- \rangle
+
\psi_{++}\vert ++ \rangle
,
\end{equation}
with $\psi_{ij}\in {\Bbb C}$,
and compare it to the most general state obtainable through products of single-partite states
$\vert \phi_1\rangle  = \alpha_-  \vert - \rangle    + \alpha_+  \vert + \rangle$,
and
$\vert \phi_2\rangle  = \beta_-  \vert - \rangle    + \beta_+  \vert + \rangle$
with $\alpha_{i}, \beta_i \in {\Bbb C}$;
that is,
\begin{equation}
\begin{split}
\vert \phi \rangle  =\vert \phi_1\rangle    \vert \phi_2\rangle   \\
\qquad =
(\alpha_-  \vert - \rangle    + \alpha_+  \vert + \rangle )
(\beta_-  \vert - \rangle    + \beta_+  \vert + \rangle )   \\
\qquad  =\alpha_- \beta_- \vert -- \rangle    + \alpha_-\beta_+  \vert -+ \rangle +
\alpha_+ \beta_- \vert +- \rangle    + \alpha_+\beta_+  \vert ++ \rangle. \\
\end{split}
\end{equation}
Since the two-partite basis states
\begin{equation}
\begin{split}
\vert -- \rangle  \equiv (1,0,0,0)
,\\
\vert -+ \rangle    \equiv (0,1,0,0)
,\\
\vert +- \rangle     \equiv (0,0,1,0)
,\\
\vert ++ \rangle      \equiv (0,0,0,1)
\end{split}
\end{equation}
are linear independent (indeed, orthonormal),
a comparison of $\vert \psi \rangle  $ with  $\vert \phi \rangle$ yields
\begin{equation}
\begin{split}
\psi_{--}=  \alpha_- \beta_-
,\\
\psi_{-+}=   \alpha_-\beta_+
,\\
\psi_{+-}=  \alpha_+ \beta_-
,\\
\psi_{++}= \alpha_+\beta_+
.
\end{split}
\end{equation}
Hence,
$\psi_{--}/ \psi_{-+} =   \beta_- / \beta_+ =   \psi_{+-} / \psi_{++}$,
and thus a necessary and sufficient condition for a two-partite quantum state to be decomposable
into a product of single-particle quantum states is that its amplitudes obey
 \begin{equation}
\psi_{--}\psi_{++}  =  \psi_{-+}   \psi_{+-} .
\end{equation}
This is not satisfied for the Bell state $\vert \Psi^-\rangle$ in Eq. (\ref{2011-m-bellstatenondec}),
because in this case $\psi_{--}=\psi_{++} =0$
and  $ \psi_{-+} = - \psi_{+-} =1/\sqrt{2}$.
Such nondecomposability is in physics referred to as {\em entanglement}
\cite{CambridgeJournals:1737068,CambridgeJournals:2027212,schrodinger}.
\index{entanglement}

Note also that $\vert \Psi^-\rangle$ is a {\em singlet state},
as it is form invariant under the following generalized rotations in two-dimensional complex Hilbert subspace; that is,
(if you do not believe this please check yourself)
\begin{equation}
\begin{split}
\vert + \rangle =
e^{ i{\frac{\varphi}{2}} }
\left(
\cos \frac{\theta}{2} \vert +'  \rangle
-
\sin \frac{\theta}{2} \vert -'   \rangle
\right),
\\
 \vert - \rangle =
e^{ -i{\frac{\varphi}{2}} }
\left(
\sin \frac{\theta}{2} \vert +'   \rangle
+
\cos \frac{\theta}{2} \vert -'  \rangle
\right)
\end{split}
\end{equation}
in the spherical coordinates $\theta , \varphi$,
but it cannot be composed or written as a product of a {\em single} (let alone form invariant) two-partite tensor product.

\eexample
}


%There exists totally symmetric (antisymmetric) tensors which are form
%invariant under all basis and metric changes.
%The symmetric tensor is  associated with the Kronecker delta
%\begin{equation}
%\delta_i^j =
%\delta^i_j =
%\left\{
% \begin{array}{l}
%1 \mbox{ if } i=j \\
%0 \mbox{ if } i\neq j  \\
%\end{array}
% \right. .
%\end{equation}
%This can be easily seen by evaluating
%${\delta'}_{j_1}^{j_2} = {a_{j_1}}^{i_1}a^{j_2}_{i_2}\delta_{i_1}^{i_2} =
%a_{j_1}^{i}a^{j_2}_i={\delta}_{j_1}^{j_2}$
%
%The antisymmetric tensor is  associated with
%\begin{equation}
%\epsilon_{12\cdots n} = 1, \qquad
%\epsilon_{i_1i_2\cdots i_si_t \cdots i_k} =  -\epsilon_{i_1i_2\cdots
%i_ti_s \cdots i_k}.
%\end{equation}
%

In order to prove form invariance of a constant tensor,
one has to transform the tensor according to the standard transformation laws
(\ref{2011-m-tvtcov}) and (\ref{2011-m-tvtcontrav}), and compare the result with the input;
that is, with the untransformed, original, tensor.
This is sometimes referred to as the ``outer transformation.''


In order to prove form invariance of a tensor field,
one has to additionally transform the spatial coordinates on which the field depends;
that is, the arguments of that field; and then compare.
This is sometimes referred to as the ``inner transformation.''
This will become clearer with the following example.

{
\color{blue}
\bexample


Consider again the tensor field defined
earlier in Eq.
(\ref{2012-m-ch-tensor-etotccon1factorized}),
but let us not choose the ``elegant''
ways of proving form invariance by factoring; rather we explicitly
consider the transformation of all the  components
$$S_{ij}(x_1,x_2)
=
\begin{pmatrix}
 -x_1x_2 & - x_2^2  \\
 x_1^2 & x_1x_2
\end{pmatrix}
$$
with respect to the standard basis  $\{(1,0), (0,1)\}$.

Is $S$ form invariant with respect to rotations around the origin?
That is, $S$ should be form invariant with repect to transformations
$x_i' = a_{ij} x_j$
with
$$
a_{ij}=\begin{pmatrix}
 \cos \varphi & \sin \varphi  \\
  -\sin \varphi & \cos \varphi
\end{pmatrix}.
$$


Consider the ``outer'' transformation first.
As has been pointed out earlier,
the term on the right hand side in $
S_{ij}'= a_{ik}a_{jl}S_{kl}
$
can be rewritten as a product of three matrices; that is,
$$
a_{ik}a_{jl}S_{kl}\left(x_n\right)
=  a_{ik}S_{kl}a_{jl}
=  a_{ik}S_{kl}\left(a^T \right)_{lj}
\equiv a\cdot S\cdot a^T .
$$
$a^T$ stands for the transposed matrix; that is,
$(a^T)_{ij}=a_{ji}$.
$$
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      -x_1x_2 & -x_2^2 \\
      x_1^2   & x_1x_2
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      \cos \varphi  & -\sin \varphi \\
      \sin \varphi & \cos \varphi
    \end{array}
  \right)=
$$

\smallskip

$$
  =\left(
    \begin{array}{cc}
      -x_1 x_2 \cos \varphi + x_1^2 \sin \varphi &
        -x_2^2 \cos \varphi + x_1 x_2 \sin \varphi \\
      x_1 x_2 \sin \varphi + x_1^2 \cos \varphi &
        x_2^2 \sin \varphi + x_1 x_2 \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      \cos \varphi  & -\sin \varphi \\
      \sin \varphi & \cos \varphi
    \end{array}
  \right)=
$$

\smallskip

$$
  =\left(\!\!\!
    \begin{array}{cc}
      \cos \varphi
        \left(-x_1 x_2 \cos \varphi + x_1^2 \sin \varphi\right)+\!&\!
      -\sin \varphi
        \left(-x_1 x_2 \cos \varphi + x_1^2 \sin \varphi\right)+\\
      \quad +\sin \varphi
        \left(-x_2^2 \cos \varphi + x_1 x_2 \sin \varphi\right)\!&\!
      \quad +\cos \varphi
        \left(-x_2^2 \cos \varphi + x_1 x_2 \sin \varphi\right)\\
      \\[1ex]
      \cos \varphi
        \left(x_1 x_2 \sin \varphi + x_1^2 \cos \varphi\right)+\!&\!
      -\sin \varphi
        \left(x_1 x_2 \sin \varphi + x_1^2 \cos \varphi\right)+\\
      \quad +\sin \varphi
        \left(x_2^2 \sin \varphi + x_1 x_2 \cos \varphi\right)\!&\!
      \quad +\cos \varphi
        \left(x_2^2 \sin \varphi + x_1 x_2 \cos \varphi\right)
    \end{array}
  \!\right)=
$$

\smallskip

$$
  =\left(
    \begin{array}{cc}
      x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) + &
      2x_1 x_2 \sin \varphi \cos \varphi \\
      \qquad + \left( x_1^2-x_2^2 \right) \sin \varphi \cos \varphi &
      \qquad - x_1^2 \sin^2 \varphi - x_2^2 \cos^2 \varphi \\
      \\[1ex]
      2x_1 x_2 \sin \varphi \cos \varphi + &
      -x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) - \\
      \qquad + x_1^2 \cos^2 \varphi + x_2^2 \sin^2 \varphi &
      \quad -\left(x_1^2-x_2^2\right) \sin \varphi \cos \varphi
    \end{array}
  \right)
$$

Let us now perform the ``inner'' transform
$$
  x'_i =a_{ij}x_j \Longrightarrow
  \begin{array}{rcl}
    x'_1 & = & x_1 \cos \varphi + x_2 \sin \varphi \\
    x'_2 & = & -x_1 \sin \varphi + x_2 \cos \varphi .
  \end{array}
$$

Thereby we assume (to be corroborated) that the functional form in the new coordinates are identical
to the functional form of the old coordinates.
A comparison yields
\begin{eqnarray*}
  -x'_1 \,x'_2 & = &
  -\left(x_1 \cos \varphi + x_2 \sin \varphi \right)
  \left(-x_1 \sin \varphi + x_2 \cos \varphi \right) = \\
  & = &
  -\left(
    -x_1^2 \sin \varphi \cos \varphi +
      x_2^2 \sin \varphi \cos \varphi -
      x_1 x_2 \sin^2 \varphi + x_1 x_2 \cos^2 \varphi
  \right) = \\
  & = &
  x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) +
    \left(x_1^2 - x_2^2\right) \sin \varphi \cos \varphi \\
  (x'_1)^2  & = &
    \left(x_1 \cos \varphi + x_2 \sin \varphi \right)
    \left(x_1 \cos \varphi + x_2 \sin \varphi \right) = \\
  & = & x_1^2 \cos^2 \varphi + x_2^2 \sin^2 \varphi +
    2 x_1 x_2 \sin \varphi \cos \varphi \\
  (x'_2)^2  & = &
    \left(-x_1 \sin \varphi + x_2 \cos \varphi \right)
    \left(-x_1 \sin \varphi + x_2 \cos \varphi \right) = \\
  & = & x_1^2 \sin^2 \varphi + x_2^2 \cos^2 \varphi -
    2 x_1 x_2 \sin \varphi \cos \varphi
\end{eqnarray*}
and hence
$$S' (x'_1,x'_2)=\left(
    \begin{array}{cc}
      -x'_1x'_2 & -(x'_2)^2 \\
      (x'_1)^2   & x'_1x'_2
    \end{array}
\right)
$$ is invariant with respect to rotations by angles $\varphi$, yielding the new basis
$\{ (\cos \varphi ,-\sin \varphi
),(\sin
\varphi ,\cos \varphi )\}$.


Incidentally,
as has been stated earlier, $S(x)$ can be written as the product of two invariant tensors $b_i(x)$ and $c_j(x)$:
$$S_{ij}(x)=b_i(x)c_j(x),$$
with
$
b(x_1,x_2)=(-x_2,x_1),
$ and
$
c(x_1,x_2)=(x_1,x_2)
$.
This can be easily checked by comparing the components:
\begin{eqnarray*}
b_1c_1&=&-x_1x_2 = S_{11},\\
b_1c_2&=&-x_2^2 = S_{12},\\
b_2c_1&=&x_1^2 = S_{21},\\
b_2c_2&=&x_1x_2 = S_{22}.\\
\end{eqnarray*}

Under rotations, $b$ and $c$  transform into
\begin{eqnarray*}
a_{ij}b_j&=&
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      -x_2\\
       x_1
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      -x_2\cos \varphi +x_1 \sin \varphi \\
      x_2\sin \varphi +x_1 \cos \varphi
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      -x'_2 \\
      x'_1
    \end{array}
  \right)    \\
 a_{ij}c_j&=&
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      x_1\\
       x_2
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      x_1\cos \varphi +x_2 \sin \varphi \\
      -x_1\sin \varphi +x_2 \cos \varphi
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      x'_1    \\
      x'_2
    \end{array}
  \right) .
\end{eqnarray*}
%Mit beliebigen Tensoren erster Stufe $x=(x_1,x_2)$
%\"uberschnitten, ergeben sich skalare Invarianten:
%\begin{eqnarray*}
%b_ix_i&=&-x_2x_1+x_1x_2=0\\
%c_ix_i&=&x_1^2+x_2^2.
%\end{eqnarray*}

This factorization of $S$ is nonunique, since
Eq.
(\ref{2012-m-ch-tensor-etotccon1factorized})
uses a different factorization; also, $S$ is decomposible into, for example,
$$S(x_1,x_2)=
  \left(
    \begin{array}{cc}
      -x_1x_2 & -x_2^2 \\
      x_1^2   & x_1x_2
    \end{array}
  \right)     =
  \left(
    \begin{array}{cc}
      -x_2^2 \\
      x_1 x_2
    \end{array}
  \right)
\otimes \left({x_1\over x_2},1\right).
$$



\eexample
}



\section{The Kronecker symbol $\delta$}
\index{delta tensor}
For vector spaces of dimension $n$ the totally symmetric Kronecker symbol $\delta$,
sometimes referred to
as the delta symbol $\delta$--tensor, can be defined by
\begin{equation}
\delta_{i_1 i_2\cdots i_k}
=
\left\{
\begin{array}{rl}
+1&\textrm{ if }  i_1 = i_2 = \cdots = i_k \\
0&\textrm{ otherwise (that is, some indices are not identical).}
\end{array}
\right.
\end{equation}

Note that, with the Einstein summation convention,
\index{Einstein summation convention}
\begin{equation}
\begin{split}
\delta_{ij} a_j    = a_j  \delta_{ij} =
\delta_{i1} a_1
+
\delta_{i2} a_2
+
\cdots
+
\delta_{in} a_n  =a_i
,  \\
\delta_{ji} a_j = a_j  \delta_{ji} =
\delta_{1i} a_1
+
\delta_{2i} a_2
+
\cdots
+
\delta_{ni} a_n  =a_i
.
\end{split}
\end{equation}

\section{The Levi-Civita symbol $\varepsilon$}
\index{Levi-Civita symbol}
\index{antisymmetric tensor}
For vector spaces of dimension $n$ the totally antisymmetric Levi-Civita symbol $\varepsilon$, sometimes referred to
as the Levi-Civita symbol $\varepsilon$--tensor, can be defined by the number of permutations of its indices; that is,
\begin{equation}
\varepsilon_{i_1 i_2\cdots i_k}
=
\left\{
\begin{array}{rl}
+1&\textrm{ if } (i_1 i_2\ldots i_k) \textrm{ is an {\em even} permutation of } (1,2,\ldots k)\\
-1&\textrm{ if } (i_1 i_2\ldots i_k) \textrm{ is an {\em odd} permutation of } (1,2,\ldots k)\\
0&\textrm{ otherwise (that is, some indices are identical).}
\end{array}
\right.
\label{2014-m-ch-lcs}
\end{equation}
Hence, $\varepsilon_{i_1 i_2\cdots i_k}$ stands for the  the sign of the permutation in the case of a permutation, and zero otherwise.

{
\color{blue}
\bexample

In two dimensions,
$$\varepsilon_{ij}\equiv
\left(
\begin{array}{rrrr}
\varepsilon_{11}&\varepsilon_{12}\\
\varepsilon_{21}&\varepsilon_{22}
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0&1\\
-1&0
\end{array}
\right)
.
$$
\eexample
}

In threedimensional Euclidean space,
the cross product, or vector product
\index{cross product}
\index{vector product}
of two vectors
${\bf x}\equiv x_i$
and
${\bf y}\equiv y_i$
can be written as
${\bf x} \times {\bf y}\equiv \varepsilon_{ijk}x_jy_k$.

{\color{OliveGreen}
\bproof
For a direct proof, consider, for arbirtrary threedimensional vectors ${\bf x}$ and ${\bf y}$,
and by enumerating all nonvanishing terms; that is, all permutations,
\begin{equation}
\begin{split}
{\bf x} \times {\bf y}\equiv \varepsilon_{ijk}x_jy_k
\equiv
\begin{pmatrix}
\varepsilon_{123}x_2y_3 + \varepsilon_{132}x_3y_2 \\
\varepsilon_{213}x_1y_3 + \varepsilon_{231}x_3y_1 \\
\varepsilon_{312}x_2y_3 + \varepsilon_{321}x_3y_2
\end{pmatrix}   \\
=
\begin{pmatrix}
\varepsilon_{123}x_2y_3 - \varepsilon_{123}x_3y_2 \\
-\varepsilon_{123}x_1y_3 + \varepsilon_{123}x_3y_1 \\
\varepsilon_{123}x_2y_3 - \varepsilon_{123}x_3y_2
\end{pmatrix}
=
\begin{pmatrix}
x_2y_3 - x_3y_2 \\
-x_1y_3 + x_3y_1 \\
x_2y_3 - x_3y_2
\end{pmatrix}
.
\end{split}
\end{equation}
\eproof
}

\section{The nabla, Laplace, and D'Alembert operators}
\index{nabla operator}

The {\em nabla operator}
\begin{equation}
\nabla =\left(
\frac{\partial }{\partial X_1},
\frac{\partial }{\partial X_2},
\ldots ,
\frac{\partial }{\partial X_n}
\right).
\end{equation}
is a vector differential operator in an $n$-dimensional vector space $\frak V$.
In index notation, $\nabla_i  =\partial_i =\partial_{X^i}$.

The nabla operator transforms in the following manners:
$\nabla_i  =\partial_i =\partial_{X^i}$ transforms like a {\em covariant} basis vector
[compare with Eqs.~(\ref{2012-m-ch-tlcbv}) and (\ref{2001-mu-tensor-tl2nl})], since
\begin{equation}
\partial_i =
\frac{\partial }{\partial X^i}
=
\frac{\partial {X'}^j}{\partial X^i}
\;
\frac{\partial }{\partial {X'}^j}
=
\frac{\partial {X'}^j}{\partial X^i}
\;
\partial'_i
=
{{(a^{-1})}_i}^j
\partial'_i
=
J_{ij}
\partial'_i,
\end{equation}
where $J_{ij}$ stands for the {\em  Jacobian matrix} defined in Eq.~(\ref{2013-m-t-jm}).
\index{Jacobian matrix}

As very similar calculation demonstrates that $\partial^i=\frac{\partial }{\partial X_i}$ transforms like a {\em contravariant} vector.


In three dimensions and in the standard Cartesian basis,
\begin{equation}
\nabla = \left(
\frac{\partial }{\partial X_1},
\frac{\partial }{\partial X_2},
\frac{\partial }{\partial X_3}
\right)
={\bf e}_1\frac{\partial }{\partial X_1}
+{\bf e}_2\frac{\partial }{\partial X_2}
+{\bf e}_3\frac{\partial }{\partial X_3}
.
\end{equation}


It is often used to define basic differential operations;
in particular, (i) to denote the {\em gradient} of a scalar field $f(X_1,X_2,X_3)$ (rendering a vector field with respect to a particular basis),
(ii) the {\em divergence} of a vector field ${\bf v}(X_1,X_2,X_3)$
(rendering a scalar field with respect to a particular basis), and
(iii) the {\em curl} (rotation) of a vector field  ${\bf v}(X_1,X_2,X_3)$ (rendering a vector field with respect to a particular basis)
as follows:
\index{gradient}
\index{divergence}
\index{curl}
\begin{eqnarray}
\textrm{grad } f &=& \nabla f = \left(
\frac{\partial f}{\partial X_1},
\frac{\partial f}{\partial X_2},
\frac{\partial f}{\partial X_3}
\right)  ,\\
\textrm{div }  {\bf v} &=& \nabla \cdot {\bf v} =
\frac{\partial v_1}{\partial X_1}+
\frac{\partial v_2}{\partial X_2}+
\frac{\partial v_3}{\partial X_3}
  ,\\
\textrm{rot } {\bf v} &=& \nabla \times {\bf v} = \left(
\frac{\partial v_3}{\partial X_2}-
\frac{\partial v_2}{\partial X_3}
,
\frac{\partial v_1}{\partial X_3}-
\frac{\partial v_3}{\partial X_1}
,
\frac{\partial v_2}{\partial X_1}-
\frac{\partial v_1}{\partial X_2}
\right)          \\
&\equiv& \varepsilon_{ijk} \partial_j v_k.
\end{eqnarray}

The {\em Laplace operator}
\index{Laplace operator}
is defined by
\begin{equation}
\Delta = \nabla^2= \nabla \cdot \nabla =
\frac{\partial^2 }{\partial^2 X_1}+
\frac{\partial^2 }{\partial^2 X_2}+
\frac{\partial^2 }{\partial^2 X_3}
.
\end{equation}

In special relativity and electrodynamics,  as well as in  wave theory and quantized field theory, with the Minkowski space-time
of dimension four
(referring to the metric tensor with the signature ``$\pm ,\pm ,\pm ,\mp$''),
the {\em D'Alembert operator}
\index{D'Alembert operator}
is defined by the Minkowski metric $\eta = {\rm diag} (1,1,1,-1)$
\begin{equation}
\Box  = \partial_i \partial^i
=
\eta_{ij}  \partial^i \partial^j=
\nabla^2- \frac{\partial^2 }{\partial^2 t}=
\nabla \cdot \nabla - \frac{\partial^2 }{\partial^2 t}=
\frac{\partial^2 }{\partial^2 X_1}+
\frac{\partial^2 }{\partial^2 X_2}+
\frac{\partial^2 }{\partial^2 X_3}- \frac{\partial^2 }{\partial^2 t}
.
\end{equation}



\section{Some tricks and examples}

There are some tricks which are commonly used.
Here, some of them are enumerated:

\begin{itemize}
\item[(i)]
Indices which appear as internal sums can be renamed arbitrarily
(provided their name is not already taken by some other index).
That is, $a_ib^i=a_jb^j$ for arbitrary $a,b,i,j$.
\item[(ii)]
With the Euclidean metric, $\delta_{ii}=n$.
\item[(iii)]
$\frac{\partial X^i }{\partial X^j}=\delta^i_j$.
\item[(iv)]
With the Euclidean metric, $\frac{\partial X^i }{ \partial X^i}=n$.
\item[(v)]
$\varepsilon_{ij}\delta_{ij}=-\varepsilon_{ji}\delta_{ij}=-\varepsilon_{ji}\delta_{ji}=(i \leftrightarrow j)=-\varepsilon_{ij}\delta_{ij}=0$,
since $a=-a$ implies $a=0$;
likewise, $\varepsilon_{ij}x_i x_j=0$.
In general, the Einstein summations $s_{ij\ldots }a_{ij\ldots}$ over objects $s_{ij\ldots }$ which are {\em symmetric} with respect to index exchanges
over objects $a_{ij\ldots}$ which are {\em antisymmetric}  with respect to index exchanges yields zero.
\item[(vi)]
For threedimensional vector spaces ($n=3$)  and the Euclidean metric,
the {\em Grassmann identity} holds:
\index{Grassmann identity}
\begin{equation}
 \varepsilon_{ijk}\varepsilon_{klm}
=  \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}.
\label{2011-m-egi}
\end{equation}
{\color{OliveGreen}
\bproof
For the sake of a proof, consider
\begin{equation}
\begin{split}
{\bf x} \times ({\bf y} \times {\bf z}) \equiv \\
\textrm{in index notation}\\
x_j  \varepsilon_{ijk} y_l z_m\varepsilon_{klm}   =
x_j y_l z_m  \varepsilon_{ijk} \varepsilon_{klm}   \equiv \\
\textrm{in coordinate notation}\\
\begin{pmatrix}
x_1 \\ x_2\\ x_3
\end{pmatrix}
\times
\left[
\begin{pmatrix}
y_1 \\y_2\\y_3
\end{pmatrix}
\times
\begin{pmatrix}
z_1 \\z_2\\z_3
\end{pmatrix}
\right] =
\begin{pmatrix}
x_1 \\ x_2\\ x_3
\end{pmatrix}
\times
\begin{pmatrix}
y_2z_3 -y_3z_2 \\
y_3z_1 -y_1z_3 \\
y_1z_2 -y_2z_1 \\
\end{pmatrix}
 =  \\
\begin{pmatrix}
x_2 (y_1z_2 -y_2z_1) - x_3(y_3z_1 -y_1z_3) \\
x_3 (y_2z_3 -y_3z_2) - x_1(y_1z_2 -y_2z_1) \\
x_1 (y_3z_1 -y_1z_3) - x_2(y_2z_3 -y_3z_2)
\end{pmatrix}
 =  \\
\begin{pmatrix}
x_2 y_1z_2 -x_2y_2z_1 - x_3y_3z_1 +x_3y_1z_3 \\
x_3 y_2z_3 -x_3y_3z_2 - x_1y_1z_2 +x_1y_2z_1 \\
x_1 y_3z_1 -x_1y_1z_3 - x_2y_2z_3 +x_2y_3z_2
\end{pmatrix}
 =  \\
\begin{pmatrix}
y_1 (x_2z_2 + x_3z_3) - z_1(x_2y_2 + x_3y_3) \\
y_2 (x_3z_3 + x_1z_1) - z_2(x_1y_1 + x_3y_3) \\
y_3 (x_1z_1 + x_2z_2) - z_3(x_1y_1 + x_2y_2)
\end{pmatrix}
\end{split}
\end{equation}
The ``incomplete''  dot products can be completed through addition and subtraction of the same term, respectively; that is,
\begin{equation}
\begin{split}
\begin{pmatrix}
y_1(x_1z_1 + x_2z_2 + x_3z_3) - z_1(x_1y_1 + x_2y_2 + x_3y_3)   \\
y_2(x_1z_1 + x_2z_2 + x_3z_3) - z_2(x_1y_1 + x_2y_2 + x_3y_3)     \\
y_3(x_1z_1 + x_2z_2 + x_3z_3) - z_3(x_1y_1 + x_2y_2 + x_3y_3)       \\
\end{pmatrix}
\equiv   \\
\textrm{in vector notation}\\
{\bf y} \left( {\bf x} \cdot {\bf z}\right)
-
{\bf z} \left( {\bf x} \cdot {\bf y}\right)
\equiv   \\
\textrm{in index notation}\\
x_j y_l z_m \left(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}\right).
\end{split}
\end{equation}
\eproof
}

\item[(vii)]
For threedimensional vector spaces ($n=3$) and the Euclidean metric,\\
$\vert a\times b \vert=
\sqrt{\varepsilon_{ijk}\varepsilon_{ist}a_ja_sb_kb_t}  =
\sqrt{\vert a\vert^2
\vert b\vert^2
-(a\cdot b)^2}=\sqrt{{\rm det}
\left(
\begin{array}{cc}
a\cdot a&a\cdot b\\
a\cdot b&b\cdot b
\end{array}\right)}= \vert a\vert
\vert b\vert \sin \theta_{ab}.$
\item[(viii)]
Let $u,v\equiv X_1',X_2'$ be two parameters associated with an
orthonormal Cartesian basis $\{(0,1),(1,0)\}$, and let
$\Phi :(u,v)\mapsto {\Bbb R}^3$
be a mapping from some area of ${\Bbb R}^2$ into a twodimensional
surface of ${\Bbb R}^3$. Then the metric tensor is given by
$g_{ij}=
{\partial \Phi^k \over \partial X'^i}
{\partial \Phi^m \over \partial X'^j} \delta_{km}.$

\end{itemize}






{
\color{blue}
\bexample

Consider the following examples in threedimensional vector space.
Let $r^2 = \sum_{i=1 }^3 x_i^2$.

\begin{enumerate}
\item
\begin{equation}
\begin{split}
  \partial_jr =  \partial_j \sqrt{\sum_ix_i^2} =
  \frac{1}{2}\frac{1}{\sqrt{\sum_ix_i^2}}\,2x_j =
  \frac{x_j}{r}
\end{split}
\end{equation}
By using the chain  rule one obtains
\begin{equation}
\begin{split}
  \partial_jr^\alpha =
  \alpha r^{\alpha-1}\left(\partial_jr\right) =
  \alpha r^{\alpha-1}\left(\frac{x_j}{r}\right)=
  \alpha r^{\alpha-2}x_j
\end{split}
\label{2011-m-eet1}
\end{equation}
and thus $\nabla r^\alpha = \alpha r^{\alpha-2}{\bf x}$.


\item
\begin{equation}
\begin{split}
  \partial_j \log r=\frac{1}{r}\left(\partial_jr\right)
\end{split}
\end{equation}
With $
  \partial_jr = \frac{x_j}{r}
$  derived earlier in Eq. (\ref{2011-m-eet1}) one obtains
$
 \partial_j \log r= \frac{1}{r}\frac{x_j}{r}=
  \frac{x_j}{r^2}
$,
and thus $\nabla  \log r =\frac{{\bf x}}{r^2}$.

\item
\begin{equation}
\begin{split}
  \partial_j
  \left[
    \left(
      \sum_i\left(x_i-a_i\right)^2
    \right)^{-\frac{1}{2}}+
    \left(
      \sum_i\left(x_i+a_i\right)^2
    \right)^{-\frac{1}{2}}
  \right]=
\\
  = -\frac{1}{2}\left[\frac{1}{\left(\sum_i\left(x_i-a_i\right)^2\right)^
    \frac{3}{2}}\,2\left(x_j-a_j\right)
  +\frac{1}{\left(\sum_i\left(x_i+a_i\right)^2\right)^\frac{3}{2}}\,
    2\left(x_j+a_j\right)\right]= \\
 -\left(\sum_i\left(x_i-a_i\right)^2\right)^{-\frac{3}{2}}\left(x_j-a_j\right)-
    \left(\sum_i\left(x_i+a_i\right)^2\right)^{-\frac{3}{2}}\left(x_j+a_j\right)   .
\end{split}
\end{equation}

\item
For three dimensions and for $r \neq 0$,
\begin{equation}
\nabla \bigl({{\bf r} \over r^3} \bigr)\equiv
  \partial_i\left(\frac{r_i}{r^3}\right)=
  \frac{1}{r^3}\underbrace{\partial_i r_i}_{=3}+
  r_i\left(-3\frac{1}{r^4}\right)\left(\frac{1}{2r}\right)2r_i=
  3\frac{1}{r^3}-3\frac{1}{r^3}=0 .
\label{2011-m-eet2}
\end{equation}


\item  With this solution (\ref{2011-m-eet2}) one obtains, for three dimensions and $r \neq 0$,
\begin{equation}
\Delta \bigl({1 \over r} \bigr)\equiv
  \partial_i\partial_i\frac{1}{r}=\partial_i\left(-\frac{1}{r^2}\right)
  \left(\frac{1}{2r}\right)2r_i=-\partial_i\frac{r_i}{r^3}=0   .
\end{equation}


\item  With the earlier solution (\ref{2011-m-eet2}) one obtains
\begin{equation}
\begin{split}
\Delta \bigl({{\bf r} {\bf p} \over r^3} \bigr)\equiv \\
  \partial_i\partial_i\frac{r_jp_j}{r^3}=
%****** unklar: siehe Vorlagen
    \partial_i
    \left[
      \frac{p_i}{r^3}+r_jp_j\left(-3\frac{1}{r^5}\right)r_i
    \right]= \\
  = p_i\left(-3\frac{1}{r^5}\right)r_i+
    p_i\left(-3\frac{1}{r^5}\right)r_i+ \\
   +r_jp_j
    \left[
      \left(15\frac{1}{r^6}\right)
      \left(\frac{1}{2r}\right)2r_i
    \right]r_i+
    r_jp_j\left(-3\frac{1}{r^5}\right)
    \underbrace{\partial_i r_i}_{=3}= \\
  = r_i p_i \frac{1}{r^5}(-3-3+15-9)=0
\end{split}
\end{equation}



\item    With $r\neq 0$ and constant $\bf p$ one obtains
\marginnote{Note that, in three dimensions, the Grassmann identity (\ref{2011-m-egi})
 $\varepsilon_{ijk}\varepsilon_{klm}       =          \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$
holds.}
\begin{equation}
\begin{split}
  \nabla \times ({\bf p} \times \frac{{\bf r}}{r^3})
 \equiv
  \varepsilon_{ijk}\partial_j\varepsilon_{klm}p_l\frac{r_m}{r^3}=
  p_l \varepsilon_{ijk} \varepsilon_{klm}
  \left[\partial_j\frac{r_m}{r^3}\right]  \\
 = p_l
    \varepsilon_{ijk}\varepsilon_{klm}
  \left[
    \frac{1}{r^3}\partial_j r_m + r_m
    \left(-3\frac{1}{r^4}\right)\left(\frac{1}{2r}\right)2r_j
  \right]  \\
  = p_l\varepsilon_{ijk}\varepsilon_{klm}
  \left[
    \frac{1}{r^3}\delta_{jm} - 3\frac{r_j r_m}{r^5}
  \right]  \\
  = p_l(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl})
  \left[
    \frac{1}{r^3}\delta_{jm} - 3\frac{r_j r_m}{r^5}
  \right]  \\
  = p_i \underbrace{\left(3\frac{1}{r^3}-3\frac{1}{r^3}\right)}_{=0}-
  p_j
  \Biggl({1\over {r^3}}
    \underbrace{{\partial_j r_i}}_{=\delta_{ij}}-
    3\frac{r_j r_i}{r^5}
  \Biggr)  \\
  = -\frac{{\bf p}}{r^3}+3\frac{\left({\bf r} {\bf p}\right){\bf r}}{r^5}
.
\end{split}
\end{equation}



\item
\begin{equation}
\begin{split}
{\nabla} \times({\nabla }\Phi )\\
\equiv
\varepsilon_{ijk} \partial_j \partial_k \Phi \\ =
\varepsilon_{ikj} \partial_k \partial_j \Phi  \\=
\varepsilon_{ikj} \partial_j \partial_k \Phi  \\=
-\varepsilon_{ijk} \partial_j \partial_k \Phi =0.
\end{split}
\end{equation}
This is due to the fact that $\partial_j \partial_k$ is  symmetric, whereas
$\varepsilon_{ijk}$ ist totally antisymmetric.


\item        For a proof that
$({{\bf x} } \times {{\bf y}})\times {{\bf z}}
\neq
{{\bf x} } \times ({{\bf y}}\times {{\bf z}})$ consider
\begin{equation}
\begin{split}
({{\bf x} } \times {{\bf y}})\times {{\bf z}}\\
\equiv
\varepsilon_{ijl}
\varepsilon_{jkm}
x_k y_m z_l\\=
-\varepsilon_{ilj}
\varepsilon_{jkm}
x_k y_m z_l\\ =
-(\delta_{ik}\delta_{lm}-
\delta_{im}\delta_{lk})
x_k y_m z_l\\ =
-x_i {{\bf y}}\cdot {{\bf z}}+
y_i {{\bf x}}\cdot {{\bf z}}.
\end{split}
\end{equation}
{\it versus}
\begin{equation}
\begin{split}
{{\bf x} } \times ({{\bf y}}\times {{\bf z}})\\
\equiv
\varepsilon_{ilj}
\varepsilon_{jkm}
x_l y_k z_m\\ =
(\delta_{ik}\delta_{lm}-
\delta_{im}\delta_{lk})
x_l y_k z_m\\ =
y_i {{\bf x}}\cdot {{\bf z}}-
z_i {{\bf x}}\cdot {{\bf y}}.
\end{split}
\end{equation}





\item
Let ${\bf w} = { {{\bf p}} \over r} $ with  $p_i=p_i\left(t  - {r \over c}\right)$,
whereby   $t$ and $c$  are constants. Then,
\begin{eqnarray*}
  \mbox{div}{\bf w}& = &
\nabla \cdot {\bf w}\\
\equiv \partial_i w_i & = & \partial_i
  \left[
    \frac{1}{r} p_i \left(t-\frac{r}{c}\right)
  \right]= \\
  & = & \left(-\frac{1}{r^2}\right)\left(\frac{1}{2r}\right)
    2r_i p_i+
    \frac{1}{r}p_i'\left(-\frac{1}{c}\right)
    \left(\frac{1}{2r}\right)2 r_i= \\
  & = & -\frac{r_i p_i}{r^3}-\frac{1}{c r^2}p_i' r_i
.
\end{eqnarray*}
Hence,
$
  \mbox{div}{\bf w}=\nabla \cdot {\bf w}=
  -\left(\frac{{\bf r} {\bf p}}{r^3}+\frac{{\bf r} {\bf p}'}{c r^2}\right)
$.



\begin{eqnarray*}
\mbox{rot}{\bf w}& =& \nabla \times {\bf w}  \\
  \varepsilon_{ijk}\partial_j w_k & = &
   \equiv  \varepsilon_{ijk}
    \left[
      \left(-\frac{1}{r^2}\right)\left(\frac{1}{2r}\right)
      2 r_j p_k +
      \frac{1}{r}p_k'
      \left(-\frac{1}{c}\right)\left(\frac{1}{2r}\right)2r_j
    \right]= \\
  & = & -\frac{1}{r^3}\varepsilon_{ijk}r_j p_k -\frac{1}{cr^2}
    \varepsilon_{ijk}r_j p_k' = \\
  & \equiv & -\frac{1}{r^3}\left({\bf r} \times {\bf p}\right)-
    \frac{1}{cr^2}\left({\bf r} \times {\bf p}'\right)   .
\end{eqnarray*}

\item
Let us verify  some specific examples of Gauss' (divergence) theorem,
\index{Gauss' theorem}
stating that the outward flux of a vector field through a closed surface
is equal to the volume integral of the divergence of the region inside the surface.
That is, the sum of all sources subtracted by the sum of all sinks represents the net flow out of a region or volume of threedimensional space:
\begin{equation}
\int \limits_V \nabla \cdot {\bf w} \, dv   =\int \limits_{F_V} {\bf w} \cdot d{\bf f}
.   \label{2011-m-gauss}
\end{equation}

Consider the vector field ${\bf w} = (4x, -2y^2, z^2)$
and the (cylindric) volume bounded by the planes  $z=0$ und $z=3$,
as well as by the surface
$x^2 + y^2 = 4$.


Let us first look at the left hand side $\int \limits_V \nabla \cdot {\bf w} \, dv $
of Eq. (\ref{2011-m-gauss}):
$$
  \nabla {\bf w}=\textrm{div } {\bf w}=4-4y+2z
$$
\begin{eqnarray*}
  \Longrightarrow \int \limits_V \! \textrm{div } {\bf w} dv& = &
  \int \limits_{z=0}^3 \! dz \int \limits_{x=-2}^2 \!\! dx
  \int \limits_{y=-\sqrt{4-x^2}}^{\sqrt{4-x^2}} \!\!\!dy\,
    \left(4-4y+2z\right)= \\
  & & \mbox{cylindric coordinates: }
  \Biggl[
    \begin{array}{rcl}
      x & = & r \cos \varphi \\
      y & = & r \sin \varphi \\
%***** \"z-Koordinatentransformation\" hinzugefuegt
      z & = & z
    \end{array}
  \Biggr] \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \int \limits_0^{2\pi} \!\! d\varphi \left(4-4r \sin \varphi+2z\right)= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r \, dr
  \left(4 \varphi +4r\cos\varphi+2\varphi z\right)
  \Biggl|_{\varphi=0}^{2 \pi}= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \left(8 \pi +4r+4 \pi z -4r\right)= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \left(8 \pi +4 \pi z\right) \\
  & = & 2 \left( 8 \pi z +4 \pi \frac{z^2}{2}\right)\Biggl|_{z=0}^{z=3}=
    2 (24+18) \pi = 84 \pi
\end{eqnarray*}

Now consider the right hand side $\int \limits_F {\bf w} \cdot d{\bf f}$
of Eq. (\ref{2011-m-gauss}).
The surface consists of three  parts:
the lower plane $F_1$ of the cylinder is characterized by $z=0$;
the upper plane $F_2$  of the cylinder is characterized by  $z=3$;
the surface on the side of the zylinder $F_3$
 is characterized by   $x^2+y^2=4$.
$d {\bf f}$ must be normal to these surfaces, pointing outwards; hence
 \begin{eqnarray*}
  F_1 : \int \limits_{{\cal F}_1} {\bf w} \cdot d{\bf f}_1  & = &
    \int \limits_{{\cal F}_1}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2=0
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        0 \\
        0 \\
        -1
      \end{array}
    \right)
    \, d\, x d\, y = 0 \\
  F_2 : \int \limits_{{\cal F}_2} {\bf w} \cdot d{\bf f}_2 & = &
    \int \limits_{{\cal F}_2}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2=9
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        0 \\
        0 \\
        1
      \end{array}
    \right)
    \, d\, x d\, y = \\
  & = & 9 \int \limits_{K_{r=2}} \!\! d\, f=9 \cdot 4 \pi=36 \pi \\
  F_3 : \int \limits_{{\cal F}_3} {\bf w} \cdot d{\bf f}_3 & = &
    \int \limits_{{\cal F}_3}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2
      \end{array}
    \right)
    \left(
      \frac{\partial {\bf x}}{\partial \varphi} \times
      \frac{\partial {\bf x}}{\partial z}
    \right)
    \, d\varphi \, dz \quad (r=2=\mbox{const.})
\end{eqnarray*}
$$
  \frac{\partial {\bf x}}{\partial \varphi} =
  \left(
    \begin{array}{c}
      -r \sin \varphi \\
       r \cos \varphi \\
       0
    \end{array}
  \right)=
  \left(
    \begin{array}{c}
      -2 \sin \varphi \\
       2 \cos \varphi \\
       0
    \end{array}
  \right); \enspace
  \frac{\partial {\bf x}}{\partial z} =
  \left(
    \begin{array}{c}
      0 \\
      0 \\
      1
    \end{array}
  \right)
$$
$$
   \Longrightarrow
  \left(
    \frac{\partial {\bf x}}{\partial \varphi} \times
    \frac{\partial {\bf x}}{\partial z}
  \right) =
  \left(
    \begin{array}{c}
      2 \cos \varphi \\
      2 \sin \varphi \\
      0
    \end{array}
  \right)
$$
\begin{eqnarray*}
  \Longrightarrow
  F_3 & = & \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \int \limits_{z=0}^3 \!\! dz
    \left(
      \begin{array}{c}
        4 \cdot 2 \cos \varphi \\
        -2(2 \sin \varphi)^2 \\
        z^2
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        2 \cos \varphi \\
        2 \sin \varphi \\
        0
      \end{array}
    \right)= \\
    &= & \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \int \limits_{z=0}^3 \!\! dz
    \left(16 \cos^2 \varphi -16 \sin^3 \varphi\right) = \\
  & = & 3 \cdot 16 \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \left( \cos^2\varphi - \sin^3 \varphi \right) = \\
  & = &
    \Biggl[
      \begin{array}{rcl}
        \int \cos^2 \varphi \, d\varphi & = & \frac{\varphi}{2}+
          \frac{1}{4} \sin 2 \varphi \\
        \int \sin^3 \varphi \, d\varphi & = & -\cos \varphi+
          \frac{1}{3} \cos^3 \varphi
      \end{array}
    \Biggr] \, = \\
    & = & 3 \cdot 16
    \Biggl\{
      \frac{2 \pi}{2}-
      \underbrace
        {\left[
          \left(1+\frac{1}{3}\right)-\left(1+\frac{1}{3}\right)
        \right]}
      _{=0}
    \Biggr\}=48 \pi
\end{eqnarray*}
For the flux through the surfaces one thus obtains
$$ \oint \limits_F {\bf w} \cdot d{\bf f}=F_1+F_2+F_3=84 \pi .$$





\item
Let us verify  some specific examples of Stokes' theorem in three dimensions,
\index{Stokes' theorem}
stating that
\begin{equation}
\int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f}   =\oint \limits_{{\cal C}_{\cal F}} {\bf b} \cdot d{\bf s}
.   \label{2011-m-stokes}
\end{equation}

Consider the vector field ${\bf b} = (yz, -xz, 0)$
and the volume bounded by spherical cap
formed by the plane at $z = a / \sqrt{2}$ of a sphere of radius $a$ centered around the origin.

Let us first look at the left hand side $\int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f} $
of Eq. (\ref{2011-m-stokes}):
$$
  {\bf b}=
  \left(
    \begin{array}{c}
      yz \\
      -xz \\
      0
    \end{array}
  \right)
  \Longrightarrow
  \textrm{rot } {\bf b} = \nabla \times {\bf b} =
  \left(
    \begin{array}{c}
      x \\
      y \\
      -2z
    \end{array}
  \right)
$$
Let us transform this into spherical coordinates:
$$
  {\bf x}=
  \left(
    \begin{array}{c}
      r\sin \theta \cos \varphi \\
      r\sin \theta \sin \varphi \\
      r\cos \theta
    \end{array}
  \right)
$$
$$
  \Rightarrow
  \frac{\partial {\bf x}}{\partial \theta}=
  r
  \left(
    \begin{array}{c}
      \cos \theta \cos \varphi \\
      \cos \theta \sin \varphi \\
      -\sin \theta
    \end{array}
  \right);\quad
  \frac{\partial {\bf x}}{\partial \varphi}=
  r
  \left(
    \begin{array}{c}
      -\sin \theta \sin \varphi \\
      \sin \theta \cos \varphi \\
      0
    \end{array}
  \right)
$$
$$
  d{\bf f}=
  \left(
    \frac{\partial {\bf x}}{\partial \theta} \times
    \frac{\partial {\bf x}}{\partial \varphi}
  \right)
  d\theta \, d\varphi=
  r^2
  \left(
    \begin{array}{c}
      \sin^2 \theta \cos \varphi \\
      \sin^2 \theta \sin \varphi \\
      \sin \theta \cos \theta
    \end{array}
  \right)
  d\theta \, d\varphi \label{eqn:1.14.1}
$$
$$
  \nabla \times {\bf b}=
  r
  \left(
    \begin{array}{c}
      \sin \theta \cos \varphi \\
      \sin \theta \sin \varphi \\
      -2\cos \theta
    \end{array}
  \right)
$$

\begin{eqnarray*}
  \int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f} & = &
  \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
  \int \limits_{\varphi=0}^{2 \pi} \!\! d\varphi \, a^3
  \left(
    \begin{array}{c}
      \sin \theta \cos \varphi \\
      \sin \theta \sin \varphi \\
      -2\cos \theta
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      \sin^2 \theta \cos \varphi \\
      \sin^2 \theta \sin \varphi \\
      \sin \theta \cos \theta
    \end{array}
  \right)= \\
  & = & a^3
  \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
  \int \limits_{\varphi=0}^{2 \pi} \!\! d\varphi
  \Biggl[
    \sin^3\theta
    \underbrace{\left(\cos^2\varphi+\sin^2\varphi\right)}_{=1}-
      2\sin\theta\cos^2\theta
  \Biggr] = \\
  & = & 2 \pi a^3
  \left[
    \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
    \left(1-\cos^2\theta\right)\sin\theta-
    2\int \limits_{\theta=0}^{\pi/4} \!\! d\theta
    \sin \theta \cos^2 \theta
  \right] = \\
  & = & 2 \pi a^3
  \int \limits_{\theta=0}^{\pi/4}d\theta
  \sin \theta \left(1-3\cos^2\theta\right) = \\
  && \left[
    \begin{array}{l}
      \mbox{transformation of variables: } \\
      \cos \theta = u \Rightarrow du=-\sin \theta d\theta
      \Rightarrow d\theta=-\frac{du}{\sin \theta}
    \end{array}
  \right] \\
  & = & 2 \pi a^3
  \int \limits_{\theta=0}^{\pi/4}(-du)\left(1-3u^2\right)=
    2 \pi a^3
    \left( \frac{3u^3}{3}-u \right)\Biggr|_{\theta=0}^{\pi/4}= \\
  & = & 2 \pi a^3
    \left(\cos^3\theta-\cos\theta \right)\Biggr|_{\theta=0}^{\pi/4}=
    2 \pi a^3
    \left(\frac{2\sqrt{2}}{8}-\frac{\sqrt{2}}{2} \right) = \\
  & = & \frac{2 \pi a^3}{8}\left(-2\sqrt{2}\right)=
    -\frac{\pi a^3 \sqrt{2}}{2}
\end{eqnarray*}


Now consider the right hand side $\oint \limits_{{\cal C}_{\cal F}} {\bf b} \cdot d{\bf s}$
of Eq. (\ref{2011-m-stokes}).
The radius $r'$ of the circle  surface
$\{(x,y,z) \mid x,y \in {\Bbb R} ,z=a/\sqrt{2}\}$ bounded by the sphere with radius $a$
is determined by
$ a^2 =(r')^2 +(a/ \sqrt{2})^2$; hence, $r' =a/\sqrt{2}$.
The curve of integration ${\cal C}_{\cal F}$ can be parameterized by
$$\{(x,y,z) \mid
x={a\over \sqrt{2}} \cos \varphi,
y={a\over \sqrt{2}} \sin \varphi,
z={a\over \sqrt{2}}\}.$$
Therefore,
$$
  {\bf x} = a
  \left(
    \begin{array}{c}
      \frac{1}{\sqrt2}\cos\varphi \\[1ex]
      \frac{1}{\sqrt2}\sin\varphi \\[1ex]
      \frac{1}{\sqrt2}
    \end{array}
  \right)=
  \frac{a}{\sqrt{2}}
  \left(
    \begin{array}{c}
      \cos\varphi \\
      \sin\varphi \\
      1
    \end{array}
  \right)
\in {\cal C}_{\cal F}
$$
Let us transform this into polar coordinates:
$$
  d{\bf s}=\frac{d{\bf x}}{d\varphi} \,d\varphi =
  \frac{a}{\sqrt{2}}
  \left(
    \begin{array}{c}
      -\sin\varphi \\
      \cos\varphi \\
      0
    \end{array}
  \right) d\varphi
$$
$$
  {\bf b}=
  \left(
    \begin{array}{c}
      \frac{a}{\sqrt{2}}\sin\varphi\cdot\frac{a}{\sqrt{2}} \\
      -\frac{a}{\sqrt{2}}\cos\varphi\cdot\frac{a}{\sqrt{2}} \\
      0
    \end{array}
  \right)=
  \frac{a^2}{2}
  \left(
    \begin{array}{c}
      \sin\varphi \\
      -\cos\varphi \\
      0
    \end{array}
  \right)
$$
Hence the circular integral is given by
$$
  \oint \limits_{{\cal C}_F} {\bf b} \cdot d{\bf s}=
  \frac{a^2}{2}\frac{a}{\sqrt{2}}
  \int \limits_{\varphi=0}^{2 \pi}
  \underbrace
    {\left(-\sin^2\varphi-\cos^2\varphi\right)}
  _{=-1}
  \, d\varphi =
  -\frac{a^3}{2\sqrt{2}}2 \pi=-\frac{a^3 \pi}{\sqrt{2}}
.
$$




\end{enumerate}

\eexample
}

\section{Some common misconceptions}

\subsection{Confusion between component representation and ``the real thing''}
Given a particular basis, a tensor is uniquely characterized by its components.
However, without reference to a particular basis, any components are just blurbs.

Example (wrong!): a type-1 tensor (i.e., a vector) is given by
$(1,2)$.

Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a rank-1 tensor (i.e., a vector) is given by
$(1,2)$.


\subsection{A matrix is a tensor}

See the above section.
{
\color{blue}
\bexample
Example (wrong!): A matrix is  a  tensor of type (or rank) 2.
\eexample
}
Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a matrix represents a type-2 tensor.
The matrix components are the tensor components.

Also, for non-orthogonal bases, covariant, contravariant, and mixed tensors correspond to different matrices.


\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright \decofourleft
%\aldine X \decoone c
\floweroneright
% \aldineleft ] \decosix g \leafleft
% \aldineright Y \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
% \decofourright b \floweroneleft
}
\end{center}


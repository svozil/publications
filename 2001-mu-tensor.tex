%%tth:\begin{html}<LINK REL=STYLESHEET HREF="/~svozil/ssh.css">\end{html}
\documentclass[prl,preprint,amsfonts,12pt]{revtex4}
\usepackage{graphicx}
%\documentstyle[amsfonts]{article}
 \RequirePackage{times}
\RequirePackage{courier}
\RequirePackage{mathptm}
%\renewcommand{\baselinestretch}{1.3}
\begin{document}

%\def\mathfrak{\cal }
%\def\Bbb{\bf }
%\sloppy




\title{Tensors as  multilinear forms\\
Handout ``Methoden der Theoretischen Physik-\"Ubungen''}
\author{Karl Svozil}
 \email{svozil@tuwien.ac.at}
\homepage{http://tph.tuwien.ac.at/~svozil}
\affiliation{Institut f\"ur Theoretische Physik, University of Technology Vienna,
Wiedner Hauptstra\ss e 8-10/136, A-1040 Vienna, Austria}

\begin{abstract}
Tensors are defined as multilinear forms on vector spaces
\end{abstract}


\pacs{45.10.Na,02.70}
\keywords{Tensor calculus}

\maketitle

%\section{Definitions}

\section{Notation}

Consider the vector space ${\Bbb R}^D$ of dimension $D$,
a basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_D\}$ consisting of
$D$ basis vectors ${\bf e}_i$,
and $n$ arbitrary vectors
$x_1,x_2,\ldots ,x_n\in {\Bbb R}^D$
with vector components
$X^i_1,X^i_2,\ldots ,X^i_n\in {\Bbb R}$.

{\em Tensor fields} define tensors in every point of ${\Bbb R}^D$ separately.
In general, with respect to a particular basis, the components of a tensor field
depend on the coordinates.


We adopt Einstein's summation convention to sum over equal indices
(one pair with a superscript and a subscript).
Sometimes, sums are written out explicitly.


In what follows, the notations
`` $x\cdot y$'',
`` $(x,y)$'' and
`` $\langle x\mid y\rangle $'' will be used synonymously for the {\em
scalar product}.
Note, however, that the notation `` $x\cdot y$''
may be a little bit misleading; e.g. in the case of the ``pseudo-Euclidean'' metric
 ${\rm diag}(+,+,+,\cdots ,+,-)$.

For a more systematic treatment, see for instance Klingbeil
\cite{Klingbeil}
and Dirschmid
\cite{Dirschmid}.
.


\section{Multilinear form}

A {\em multilinear form}
\begin{equation}
\alpha :{\Bbb R}^n \mapsto {\Bbb R}
\end{equation}
is a map satisfying
\begin{eqnarray}
\alpha ( x_1,x_2,\ldots , A x^1_i+ B x^2_i,\ldots ,x_n)
&=&
A\alpha ( x_1,x_2,\ldots , x^1_i,\ldots ,x_n) \nonumber \\
&&  \qquad +
B\alpha ( x_1,x_2,\ldots , x^2_i,\ldots ,x_n)
\end{eqnarray}
for every one of its (multi-)arguments.


\section{Covariant tensors}
A tensor of rank $n$
\begin{equation}
\alpha:{\Bbb R}^n \mapsto {\Bbb R}
\end{equation}
is a multilinear form
\begin{equation}
\alpha ( x_1,x_2,\ldots ,x_n)=
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1 X^{i_2}_2\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_n}).
\end{equation}
The
\begin{equation}
A_{{i_1}{i_2}\cdots {i_n}}=\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_n})
\end{equation}
 are the
{\em components} of the tensor $\alpha $ with respect to the basis
${\mathfrak B}$.

\subsubsection{Question: how many components are there?}
{Answer: $D^n$.}

\subsubsection{Question: proof that tensors are multilinear forms.}
{Answer:}
by insertion.
\begin{eqnarray}
&&\alpha ( x_1,x_2,\ldots , Ax^1_j+Bx_j^2,\ldots ,x_n)
\nonumber \\
&=&
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1X^{i_2}_2\ldots  [A(X^1)^{i_j}_j+B(X^2)^{i_j}_j]\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_n})
\nonumber \\
&=& A
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1X^{i_2}_2\ldots  (X^1)^{i_j}_j\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_n})
\nonumber \\
&&\quad +
B
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1X^{i_2}_2\ldots  (X^2)^{i_j}_j\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_n})
\nonumber \\
&=& A \alpha ( x_1,x_2,\ldots , x^1_j,\ldots ,x_n)+
B \alpha ( x_1,x_2,\ldots , x_j^2,\ldots ,x_n)\nonumber
\end{eqnarray}

\subsection{Basis transformations}
Let
${\mathfrak B}$
and
${\mathfrak B'}$
be two arbitrary bases of
${\Bbb R}^D$.
Then ervery vector ${\bf e}'_i$ of
${\mathfrak B'}$
can be represented as linear combination of basis vectors from
${\mathfrak B}$:
\begin{equation}
{\bf e}'_i=\sum_{j=1}^D a_i^j {\bf e}_j, \qquad i=1,\ldots , D  .
\label{2001-mu-tensors}
\end{equation}
(Formally, we may treat  ${\bf e}'_i$ and ${\bf e}_i$
as scalar variables $e'_i$ and $e_j$, respectively; such that
$
a_i^j ={\partial { e}'_i \over \partial { e}_j}
$.)


Consider an arbitrary vector $x\in {\Bbb R}^D$
with components $X^i$ with respect to the basis
${\mathfrak B}$
and   ${X'}^i$  with respect to the basis
${\mathfrak B'}$:
\begin{equation}
x
=\sum_{i=1}^D X^i {\bf e}_i
=\sum_{i=1}^D {X'}^i {\bf e}'_i
.
\end{equation}
Insertion into (\ref{2001-mu-tensors}) yields
\begin{equation}
x
=\sum_{i=1}^D X^i {\bf e}_i
=\sum_{i=1}^D {X'}^i {\bf e}'_i
=\sum_{i=1}^D {X'}^i \sum_{j=1}^D a_i^j {\bf e}_j=
\sum_{i=1}^D\left[\sum_{j=1}^D a_i^j{X'}^i \right] {\bf e}_j.
\end{equation}
A comparison of coefficient yields the transformation laws of vector components
\begin{equation}
X^j   = \sum_{j=1}^D a_i^j{X'}^i.
\end{equation}
The matrix $a=\{a_i^j\}$ is called the {\em transformation matrix}.
In terms of the coordinates $X^j$, it can be expressed as
\begin{equation}
a_i^j ={\partial X^j \over \partial X'_i}  .
\label{2001-mu-tensor-tl1}
\end{equation}


A similar argument using
\begin{equation}
{\bf e}_i=\sum_{j=1}^D {a'}_i^j {\bf e}'_j, \qquad i=1,\ldots , D
\end{equation}
yields the inverse transformation laws
\begin{equation}
{X'}^j   = \sum_{j=1}^D {a'}_i^j{X}^i.
\end{equation}
(Again, formally, we may treat  ${\bf e}'_i$ and ${\bf e}_i$
as scalar variables $e'_i$ and $e_j$, respectively; such that
${a'}_i^j ={\partial {e}_i \over \partial { e}'_j}$.)
Thereby,
\begin{equation}
{\bf e}_i=\sum_{j=1}^D {a'}_i^j {\bf e}'_j
=\sum_{j=1}^D {a'}_i^j \sum_{k=1}^D a_j^k {\bf e}_k
=\sum_{j=1}^D \sum_{k=1}^D [{a'}_i^j a_j^k] {\bf e}_k,
\end{equation}
which, due to the linear independence of the basis vectors ${\bf e}_i$ od ${\mathfrak B}$,
is only satisfied if
\begin{equation}
{a'}_i^j a_j^k =\delta_i^k
\qquad
{\rm or}
\qquad
a'a={\Bbb I}.
\end{equation}
That is, $a'$ is the inverse matrix of $a$.
In terms of the coordinates $X^j$, it can be expressed as
\begin{equation}
{a'}_i^j ={\partial {X'}^j \over \partial X_i}  .
\label{2001-mu-tensor-tl2}
\end{equation}

\subsection{Transformation of Tensor components}

Because of multilinearity (!) and by insertion into
(\ref{2001-mu-tensors}),
\begin{eqnarray}
&&\alpha ( {\bf e}'_{j_1},{\bf e}'_{j_2},\ldots ,{\bf e}'_{j_n})=
\alpha \left(
\sum_{i_1=1}^D a_{j_1}^{i_1} {\bf e}_{i_1},
\sum_{i_2=1}^D a_{j_2}^{i_2} {\bf e}_{i_2},
\ldots ,
\sum_{i_n=1}^D a_{j_n}^{i_n} {\bf e}_{i_n}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots a_{j_n}^{i_n} \alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_n})
\end{eqnarray}
or
\begin{equation}
A'_{{j_1}{j_2}\cdots {j_n}}=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots a_{j_n}^{i_n} A_{i_1 i_2\ldots i_n}.
\end{equation}


\section{Contravariant tensors}

\subsection{Definition of contravariant basis}

Consider again a covariant basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_D\}$ consisting of
$D$ basis vectors ${\bf e}_i$.
We shall define now a {\em contravariant} basis
${\mathfrak B^\ast}=\{{\bf e}^1,{\bf e}^2,\ldots ,{\bf e}^D\}$ consisting of
$D$ basis vectors ${\bf e}^i$
by the requirement that the scalar product obeys
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j\equiv ({\bf e}_i,{\bf e}^j)\equiv \langle {\bf e}_i\mid {\bf e}^j\rangle
 =\left\{
 \begin{array}{l}
1 \mbox{ if } i=j \\
0 \mbox{ if } i\neq j  \\
\end{array}
 \right. .
\label{2001-mu-tensors0}
\end{equation}
To distinguish elements of the two bases, the covariant vectors are denoted by {\em subscripts},
whereas the contravariant vectors are denoted by {\em superscripts}.
The last term ${\bf e}_i\cdot {\bf e}^j\equiv ({\bf e}_i,{\bf e}_j)\equiv \langle {\bf e}_i\mid {\bf e}_j\rangle $
recalls different notations of the scalar product.

The entire tensor formalism developed so far can be applied to define {\em contravariant} tensors
as multinear forms
\begin{equation}
\beta:{\Bbb R}^n \mapsto {\Bbb R}
\end{equation}
by
\begin{equation}
\beta ( x^1,x^2,\ldots ,x^n)=
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
\Xi_{i_1}^1 \Xi_{i_2}^2\ldots \Xi_{i_n}^n
\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_n}).
\end{equation}
The
\begin{equation}
B^{{i_1}{i_2}\cdots {i_n}}=\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_n})
\end{equation}
 are the
{\em components} of the contravariant tensor $\beta $ with respect to the basis
${\mathfrak B}^\ast$.

\subsection{Connection between the transformation of covariant and contravariant entities}

Because of linearity, we can make the Ansatz
\begin{equation}
{{\bf e}'}^j=\sum_ib_i^j{\bf e}^i,
\end{equation}
where $\{b_i^j\}=b$ is
the transformation matrix associated with the contravariant basis.
How is $b$ related to $a$,
the transformation matrix associated with the covariant basis?

By exploiting (\ref{2001-mu-tensors0}) one can find the connection between
the transformation of covariant and contravariant basis elements and thus
tensor components.
\begin{equation}
\delta_i^j= {{\bf e}'}_i\cdot {{\bf e}'}^j=(a_i^k{\bf e}_k)\cdot (b_l^j{\bf e}^l)=a_i^kb_l^j {\bf e}_k\cdot {\bf e}^l=a_i^kb_l^j \delta_k^l
=a_i^kb_k^j,
\end{equation}
and
\begin{equation}
b=a^{-1} =a'.
\end{equation}
The entire argument concerning transformations of covariant tensors and components
can be carried through to the contravariant case.
Hence, the contravariant components transform as
\begin{eqnarray}
&&\beta ( {{\bf e}'}^{j_1},{{\bf e}'}^{j_2},\ldots ,{{\bf e}'}^{j_n})=
\beta \left(
\sum_{i_1=1}^D {a'}^{j_1}_{i_1} {\bf e}^{i_1},
\sum_{i_2=1}^D {a'}^{j_2}_{i_2} {\bf e}^{i_2},
\ldots ,
\sum_{i_n=1}^D {a'}^{j_n}_{i_n} {\bf e}^{i_n}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
{a'}^{j_1}_{i_1}{a'}^{j_2}_{i_2}\cdots {a'}^{j_n}_{i_n} \beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_n})
\end{eqnarray}
or
\begin{equation}
B'^{{j_1}{j_2}\cdots {j_n}}=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
{a'}^{j_1}_{i_1}{a'}^{j_2}_{i_2}\cdots {a'}^{j_n}_{i_n} B^{i_1 i_2\ldots i_n}.
\end{equation}

\section{Orthonormal bases}
For orthonormal bases,
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j
\Longleftrightarrow
{\bf e}_i= {\bf e}^i  ,
\end{equation}
and thus the two bases are identical
\begin{equation}
{\mathfrak B}={\mathfrak B}^\ast
\end{equation}
and  formally any distinction between covariant and contravariant vectors becomes
irrelevant. Conceptually, such a distinction persists, though.



\section{Invariant tensors and physical motivation}

\section{Metric tensor}

Metric tensors are defined in metric vector spaces.
A metric vector space (sometimes also refered to
as ``vector space with metric'' or ``geometry'')
is a vector space with inner or scalar product.

This includes (pseudo-) Euclidean spaces with indefinite metric.
(I.e., the distance needs not be positive or zero.)


\subsection{Definition inner or scalar product}


The {\em scalar} or {\em inner product},
is a symmetric bilinear functional
 ${\Bbb R}^D\times {\Bbb R}^D\mapsto {\Bbb R}$
such that
\begin{itemize}
\item
$(x+y,z)=(x,z)+(y,z)$ for all $x,y,z \in {\Bbb R}^D$;
\item
$(x,y+z)=(x,y)+(x,z)$ for all $x,y,z \in {\Bbb R}^D$;
\item
$(\alpha x,y)=\alpha (x,y)$ for all $x,y \in {\Bbb R}^D, \alpha \in {\Bbb R}$;
\item
$(x,\alpha y)=\alpha (x,y)$ for all $x,y \in {\Bbb R}^D, \alpha \in {\Bbb R}$;
\item
$(x,y)={(y,x)} $ for all $x,y \in {\Bbb R}^D$
\end{itemize}
Axioms 1 and 3 assert that the scalar product is linear in the first variable.
Axioms 2 and 4 assert that the scalar product is linear in the second variable.
Axiom 5 asserts the bilinear function is symmetric.


\subsection{Definition metric}

A {\em metric} is a functional ${\Bbb R}^D\mapsto {\Bbb R}$
with the following properties
\begin{itemize}
\item
$\vert\vert x -y \vert\vert =0 \; \Longleftrightarrow \; x = y$,
\item
$\vert\vert x- y\vert\vert  =\vert\vert  x- y\vert\vert $  (symmetry),
\item
$\vert\vert x-z\vert\vert  \le \vert\vert x- y\vert\vert  + \vert\vert  y- z\vert\vert $  (triangle
inequality).
\end{itemize}



\subsection{Construction of a metric from a scalar product by metric tensor}

The {\em metric} tensor is defined by  the scalar product
\begin{equation}
g_{ij}={\bf e}_i\cdot {\bf e}_j\equiv ({\bf e}_i, {\bf e}_j)\equiv \langle {\bf e}_i, {\bf e}_j\rangle .
\end{equation}
and
\begin{equation}
g^{ij}={\bf e}^i\cdot {\bf e}^j\equiv ({\bf e}^i, {\bf e}^j)\equiv \langle {\bf e}^i, {\bf e}^j\rangle .
\end{equation}
Likewise,
\begin{equation}
g^i_{j}={\bf e}^i\cdot {\bf e}_j\equiv ({\bf e}^i, {\bf e}_j)\equiv \langle {\bf e}^i, {\bf e}_j\rangle =\delta^i_j .
\end{equation}
Note that it easy to change a covarant tensor into a contravariant and {\em vice versa}
by the application of a metric tensor.
This can be seen as follows.
Because of linearity, any contravariant basis vector ${\bf e}^i$
can be written as a linear sum of covariant basis vectors:
\begin{equation}
{\bf e}^i=A^{ij}{\bf e}_j.
\end{equation}
Then,
\begin{equation}
g^{ik} ={\bf e}^i\cdot {\bf e}^k =(A^{ij}{\bf e}_j)\cdot {\bf e}^k=A^{ij}({\bf e}_j\cdot {\bf e}^k)=A^{ij}\delta_j^k=A^{ik}
\end{equation}
and thus
\begin{equation}
{\bf e}^i=g^{ij}{\bf e}_j
\end{equation}
and
\begin{equation}
{\bf e}_i=g_{ij}{\bf e}^j.
\end{equation}


Question: Show that, for orthonormal basis, the metric tensor can be
represented as a Kronecker delta function in all basis (form invariant);
i.e.,
$\delta_{ij},\delta^i_j,\delta_i^j,\delta^{ij}$.

Question: Why is $g$ a tensor? Show its multilinearity.

\subsection{What can the metric tensor do for us?}

Most often it is used to raise/lower the indices; i.e.,
to change from contravariant to covariant and conversely from covariant
to contravariant.

In the previous section, the metric tensor has been derived from the scalar product.
The converse is true as well.
The metric tensor represents the scalar product between vectors: let
$x=X^i{\bf e}_i \in {\Bbb R}^D$ and $y=Y^j{\bf e}_j \in {\Bbb R}^D$ be two vectors.
Then ("$^T$" stands for the transpose),
\begin{equation}
x\cdot y\equiv (x,y)\equiv \langle x\mid y\rangle
= X^i {\bf e}_i\cdot Y^j {\bf e}_j
= X^iY^j {\bf e}_i\cdot  {\bf e}_j
=X^iY^j g_{ij}= X^T g Y.
\end{equation}

It also characterizes the length of a vector: in the above
equation, set $y=x$. Then,
\begin{equation}
x\cdot x\equiv (x,x)\equiv \langle x\mid x\rangle
=X^iX^j g_{ij}\equiv X^T g X,
\end{equation}
and thus
\begin{equation}
\vert\vert  x\vert\vert  =\sqrt{X^iX^j g_{ij}}= \sqrt{X^T g X}.
\end{equation}


The square of an infinitesimal vector $ds =\{dx^i\}$ is
\begin{equation}
(d s)^2  = g_{ij}dx^idx^j= dx^T g dx.
\end{equation}


Question: Prove that $\vert\vert  x\vert\vert $ mediated by $g$ is
indeed a metric.

\subsection{Transformation of the metric tensor}

Insertion into the definitions and coordinate transformations
(\ref{2001-mu-tensor-tl1})
and
(\ref{2001-mu-tensor-tl2})
yields

\begin{equation}
g_{ij}={\bf e}_i\cdot {\bf e}_j
={a'}^l_i{\bf e}'_l\cdot {a'}^m_j{\bf e}'_m
={a'}^l_i {a'}^m_j {\bf e}'_l\cdot {\bf e}'_m
= {a'}^l_i {a'}^m_j {g'}_{lm}
= {\partial {X'}^l\over \partial X^i}{\partial {X'}^m\over \partial X^j} {g'}_{lm}
.
\end{equation}

Conversely,
\begin{equation}
g'_{ij}={\bf e}'_i\cdot {\bf e}'_j
={a}^l_i{\bf e}_l\cdot {a}^m_j{\bf e}_m
={a}^l_i {a}^m_j {\bf e}_l\cdot {\bf e}_m
= {a}^l_i {a}^m_j {g}_{lm}
= {\partial {X}^l\over \partial {X'}^i}{\partial {X}^m\over \partial {X'}^j} {g}_{lm}
.
\end{equation}


If the geometry (i.e., the basis) is locally orthonormal, ${g}_{lm}=\delta_{lm}$,
then
$g'_{ij}={\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}$.



\subsection{Examples}

For a more systematic treatment, see for instance Snapper\&Troyer \cite{snapper-troyer}.


\subsubsection{$D$-dimensional Euclidean space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{D\; {\rm times}})
\end{equation}

One application in physics is quantum mechanics,
where $D$ stands for the dimension of a complex Hilbert space.
All definitions can be easily adopted to accommodate the complex numbers.
E.g., axiom 5 of the scalar product becomes
$(x,y)=(x,y)^\ast$, where `` $^\ast$ '' stands for complex conjugation.
Axiom 4 of the scalar product becomes
$(x,\alpha y)=\alpha^\ast (x,y)$.

\subsubsection{Lorentz plane}


\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (1,-1)
\end{equation}

\subsubsection{Minkowski space of dimension $D$}

In this case the metric tensor is called the Minkowski metric and is often denoted by  ``$\eta$'':
\begin{equation}
\eta \equiv \{\eta_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{D-1\; {\rm times}},-1)
\end{equation}


One application in physics is the theory of special relativity,
where $D=4$.
Alexandrov's theorem states that the mere requirement of the preservation of
zero distance (i.e., lightcones), combined with bijectivity of the transformation law
yields the Lorentz transformations
(\cite{alex1,alex2,alex3,alex-col,borchers-heger}
are original articles reviewed in \cite{benz,lester};
see also
\cite{svozil-2001-convention} for an elementary proof).



\subsubsection{Negative Euclidean space of dimension $D$}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{-1,-1,\ldots ,-1}_{D\; {\rm times}})
\end{equation}

\subsubsection{Artinian four-space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (+1,+1,-1 ,-1)
\end{equation}



\subsubsection{General relativity}

In general relativity, the metric tensor $g$ is linked to the energy-mass distribution.
There, it appears as the primary concept when compared to the scalar product.
In the case of zero gravity, $g$ is just the  Minkowski metric (often denoted by  ``$\eta$'')
${\rm diag} (1,1,1,-1) $ corresponding to ``flat'' space-time.

The best known non-flat metric is the Schwarzschild metric
\begin{equation}
g\equiv \left(
\matrix{
(1-2m/r)^{-1}&0&0&0\cr
0&r^2&0&0\cr
0&0&r^2\sin^2 \theta &0\cr
0&0&0&- \left( 1-{2m/r}\right)
}
\right)
\end{equation}
with respect to the spherical space-time coordinates $r,\theta ,\phi ,t$.


\subsubsection{Computation of the metric tensor of the ball}
Consider the transformation from the standard orthonormal
three-dimensional ``cartesian'' coordinates
$X_1=x$,
$X_2=y$,
$X_3=z$,
into spherical coordinates
$X_1'=r$,
$X_2'=\theta$,
$X_3'=\varphi$.
In terms of  $r,\theta , \varphi$, the cartesian coordinates can be written as
$X_1=r \sin \theta \cos \varphi \equiv X_1' \sin X_2' \cos X_3' $,
$X_2=r \sin \theta \sin \varphi \equiv X_1'\sin X_2' \sin X_3' $,
$X_3=r \cos \theta  \equiv X_1'\cos X_2' $.
Furthermore,  since we are dealing with the cartesian orthonormal basis,
$g_{ij}=\delta_{ij}$; hence finally
\begin{equation}
g'_{ij}= {\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}
\equiv {\rm diag}(1,r^2,r^2\sin^2 \theta ),
\end{equation}
and
\begin{equation}
(ds)^2 =(dr)^2+r^2(d\theta )^2+r^2\sin^2 \theta (d\varphi )^2.
\end{equation}

The expression $(ds)^2 =(dr)^2+r^2(d\varphi )^2$
for polar coordinates ($D=2$) is obtained by setting $\theta = \pi/4 $ and $d\theta =0$.

\subsubsection{Computation of the metric tensor of the Moebius strip}
Parameter representation of the Moebius strip:
\begin{equation}
\Phi (u,v) =\left(
\begin{array}{c}
(1+v\cos (\frac{u}{2}))\sin u \\
(1+v\cos (\frac{u}{2}))\cos u \\
v\sin (\frac{u}{2})
\end{array}
\right)
\end{equation}
with
$u\in [0,2\pi ]$ represents the position of the point on the circle,
and $v\in [-a,a]$ $a>0$, where $2a$ is the ``width'' of the Moebius strip.


\begin{eqnarray}
\Phi _{v}&=&\frac{\partial \Phi }{\partial v}=\allowbreak \left(
\begin{array}{c}
\cos \frac{1}{2}u\sin u \\
\cos \frac{1}{2}u\cos u \\
\sin \frac{1}{2}u
\end{array}
\right)
 \\
\Phi _{u}&=&\frac{\partial \Phi }{\partial u}=\allowbreak \left(
\begin{array}{c}
-\frac{1}{2}v\sin \frac{1}{2}u\sin u+\left( 1+v\cos \frac{1}{2}u\right) \cos
u \\
-\frac{1}{2}v\sin \frac{1}{2}u\cos u-\left( 1+v\cos \frac{1}{2}u\right) \sin
u \\
\frac{1}{2}v\cos \frac{1}{2}u
\end{array}
\right)
\end{eqnarray}


\begin{eqnarray}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial du}
&=&\allowbreak \left(
\begin{array}{c}
\cos \frac{1}{2}u\sin u \\
\cos \frac{1}{2}u\cos u \\
\sin \frac{1}{2}u
\end{array}
\right) ^{T}\left(
\begin{array}{c}
-\frac{1}{2}v\sin \frac{1}{2}u\sin u+\left( 1+v\cos \frac{1}{2}u\right) \cos
u \\
-\frac{1}{2}v\sin \frac{1}{2}u\cos u-\left( 1+v\cos \frac{1}{2}u\right) \sin
u \\
\frac{1}{2}v\cos \frac{1}{2}u
\end{array}
\right)
\nonumber \\
&=&
-\frac{1}{2}\left( \cos \frac{1}{2}u\sin ^{2}u\right) v\sin \frac{1}{2}u-%
\frac{1}{2}\left( \cos \frac{1}{2}u\cos ^{2}u\right) v\sin \frac{1}{2}u
\nonumber \\
&&
+%
\frac{1}{2}\left( \sin \frac{1}{2}u\right) v\cos \frac{1}{2}u=\allowbreak 0
\end{eqnarray}

\begin{eqnarray}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial v}&=&\allowbreak \left(
\begin{array}{c}
\cos \frac{1}{2}u\sin u \\
\cos \frac{1}{2}u\cos u \\
\sin \frac{1}{2}u
\end{array}
\right) ^{T}\left(
\begin{array}{c}
\cos \frac{1}{2}u\sin u \\
\cos \frac{1}{2}u\cos u \\
\sin \frac{1}{2}u
\end{array}
\right)
\nonumber \\
&=&
\cos ^{2}\frac{1}{2}u\sin ^{2}u+\cos ^{2}\frac{1}{2}u\cos ^{2}u+\sin ^{2}%
\frac{1}{2}u=\allowbreak 1
\end{eqnarray}


\begin{eqnarray}
(\frac{\partial \Phi }{\partial u})^{T}\frac{\partial \Phi }{\partial u}&=&\allowbreak \left(
\begin{array}{c}
-\frac{1}{2}v\sin \frac{1}{2}u\sin u+\left( 1+v\cos \frac{1}{2}u\right) \cos
u \\
-\frac{1}{2}v\sin \frac{1}{2}u\cos u-\left( 1+v\cos \frac{1}{2}u\right) \sin
u \\
\frac{1}{2}v\cos \frac{1}{2}u
\end{array}
\right) ^{T}\left(
\begin{array}{c}
-\frac{1}{2}v\sin \frac{1}{2}u\sin u+\left( 1+v\cos \frac{1}{2}u\right) \cos
u \\
-\frac{1}{2}v\sin \frac{1}{2}u\cos u-\left( 1+v\cos \frac{1}{2}u\right) \sin
u \\
\frac{1}{2}v\cos \frac{1}{2}u
\end{array}
\right)
\nonumber \\
&=&
\frac{1}{4}v^{2}\sin ^{2}\frac{1}{2}u\sin ^{2}u+\cos
^{2}u+2\left( \cos ^{2}u\right) v\cos \frac{1}{2}u+\left( \cos ^{2}u\right)
v^{2}\cos ^{2}\frac{1}{2}u
\nonumber \\
&&
+\frac{1}{4}v^{2}\sin ^{2}\frac{1}{2}u\cos
^{2}u
+\sin ^{2}u+2\left( \sin ^{2}u\right) v\cos \frac{1}{2}u+\left( \sin
^{2}u\right) v^{2}\cos ^{2}\frac{1}{2}u
\nonumber \\
&&
+\frac{1}{4}v^{2}\cos ^{2}\frac{1}{2}%
u =\allowbreak \frac{1}{4}v^{2}+v^{2}\cos ^{2}\frac{1}{2}u+1+2v\cos \frac{%
1}{2}u
\nonumber \\
&=&(1+v\cos (\frac{u}{2}))^{2}+\frac{1}{4}v^{2}
\end{eqnarray}


Thus the metric tensor is given by
\begin{equation}
g'_{ij}
= {\partial {X}^s\over \partial {X'}^i}{\partial {X}^t\over \partial {X'}^j}g_{st}=
= {\partial {X}^s\over \partial {X'}^i}{\partial {X}^t\over \partial {X'}^j}\delta_{st}\equiv
\left(
\begin{array}{cc}
\Phi _{u}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{u} \\
\Phi _{v}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{v}
\end{array}
\right) ={\rm diag}\left(
(1+v\cos (\frac{u}{2}))^{2}+\frac{1}{4}v^{2} , 1\right).
\end{equation}



\section{Invariant tensors and physical motivation}

What makes some touples (or matrix, or tensor components in general)  of
numbers or scalar functions a tensor? It is the
interpretation of the scalars as tensor components {\em with respect to
a particular basis}. In another basis, if we were talking about the same
tensor, the tensor components; i.e., the numbers or scalar functions
would be different.

The tensor components are scalars and can thus be treated as scalars.
For instance, due to commutativity and associativity, one can exchange
their order. (Notice, though, that this is generally not the case for
differential operators such as $\partial_i=\partial / \partial x^i$.)

A {\em form invariant} tensor with respect to  certain transformations
is a tensor which retains
the same functional form if the transformations are performes; i.e.,
if the basis changes accordingly.
That is, numbers are mapped into the same numbers (not just any
numbers).
Functions remain the same but with the new parameter components as
arguement. For instance; $4\mapsto 4$ and $f(X_1,X_2,X_3)\mapsto
f(X'_1,X'_2,X'_3)$.
If a tensor is invariant with respect to one transformation, it need not
be invariant with respect to another transformation, or with respect to
changes of the scalar product; i.e., the metric.

Nevertheless, totally symmetric (antisymmetric) tensors remain totally
symmetric (antisymmetric) in all cases:
\begin{eqnarray}
A_{i_1i_2 \ldots i_si_t\ldots i_n}
=
A_{i_1i_2 \ldots i_ti_s\ldots i_n}
&\Longrightarrow&
A'_{j_1i_2 \ldots j_s j_t\ldots j_n}
=
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots
a_{j_s}^{i_s}a_{j_t}^{i_t}\cdots
a_{j_n}^{i_n} A_{i_1 i_2\ldots i_s i_t\ldots  i_n}
\nonumber \\ &&\qquad
=
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots
a_{j_s}^{i_s}a_{j_t}^{i_t}\cdots
a_{j_n}^{i_n} A_{i_1 i_2\ldots i_t i_s\ldots  i_n}
\nonumber \\ &&\qquad
=
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots
a_{j_t}^{i_t}a_{j_s}^{i_s}\cdots
a_{j_n}^{i_n} A_{i_1 i_2\ldots i_t i_s\ldots  i_n}
\nonumber \\ &&\qquad
=
A'_{j_1i_2 \ldots j_t j_s\ldots j_n}
\\
A_{i_1i_2 \ldots i_si_t\ldots i_n}
=
-A_{i_1i_2 \ldots i_ti_s\ldots i_n}
=
&\Longrightarrow&
A'_{j_1i_2 \ldots j_s j_t\ldots j_n}
=
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots
a_{j_s}^{i_s}a_{j_t}^{i_t}\cdots
a_{j_n}^{i_n} A_{i_1 i_2\ldots i_s i_t\ldots  i_n}
\nonumber \\ &&\qquad
=
-a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots
a_{j_s}^{i_s}a_{j_t}^{i_t}\cdots
a_{j_n}^{i_n} A_{i_1 i_2\ldots i_t i_s\ldots  i_n}
\nonumber \\ &&\qquad
=
-a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots
a_{j_t}^{i_t}a_{j_s}^{i_s}\cdots
a_{j_n}^{i_n} A_{i_1 i_2\ldots i_t i_s\ldots  i_n}.
\nonumber \\ &&\qquad
=
-A'_{j_1i_2 \ldots j_t j_s\ldots j_n}
\end{eqnarray}


In physics, it would be nice if the natural laws could be written into a
form which does not depend on the particular reference frame or  basis
used.
Form invariance thus is a gratifying physical feature, reflecting the
{\em symmetry} against changes of coordinated and bases.
Therefore, physicists tend to be crazy to write down everything in a
form invariant manner.
One strategy to accomplishe this to start out with form invariant
tensors and compose everything from them. This method guarantees form
invarince (at least in the 0'th order).

%There exists totally symmetric (antisymmetric) tensors which are form
%invariant under all basis and metric changes.
%The symmetric tensor is  associated with the Kronecker delta
%\begin{equation}
%\delta_i^j =
%\delta^i_j =
%\left\{
% \begin{array}{l}
%1 \mbox{ if } i=j \\
%0 \mbox{ if } i\neq j  \\
%\end{array}
% \right. .
%\end{equation}
%This can be easily seen by evaluating
%${\delta'}_{j_1}^{j_2} = a_{j_1}^{i_1}a^{j_2}_{i_2}\delta_{i_1}^{i_2} =
%a_{j_1}^{i}a^{j_2}_i={\delta}_{j_1}^{j_2}$
%
%The antisymmetric tensor is  associated with
%\begin{equation}
%\epsilon_{12\cdots n} = 1, \qquad
%\epsilon_{i_1i_2\cdots i_si_t \cdots i_n} =  -\epsilon_{i_1i_2\cdots
%i_ti_s \cdots i_n}.
%\end{equation}
%


\section{Some tricks}

There are some tricks which are commonly used.
Here, some of them are enumerated:

\begin{itemize}
\item
Indices which appear as internal sums can be renamed arbitrarily
(provided their name is not already taken by some other index).
\item
With the euclidean metric, $\delta_{ii}=D$.
\item
$\partial X^i /\partial X^j=\delta^i_j$.
\item
With the euclidean metric, $\partial X^i /\partial X^i=D$.
\item
For $D=3$ and the euclidean metric,
the {\em Grassmann identity} holds:\\
 $\varepsilon_{ijk}\varepsilon_{klm}
=  \delta_{il}\delta_{jm}-\delta_{iml}\delta_{jl}$.
\item
For $D=3$ and the euclidean metric,\\
$\vert a\times b \vert=
\sqrt{\varepsilon_{ijk}\varepsilon_{ist}a_ja_sb_kb_t}  =
\sqrt{\vert a\vert^2
\vert b\vert^2
-(a\cdot b)^2}=\sqrt{{\rm det}
\left(
\begin{array}{cc}
a\cdot a&a\cdot b\\
a\cdot b&b\cdot b
\end{array}\right)}= \vert a\vert
\vert b\vert \sin \theta_{ab}.$
\item
Let $u,v\equiv X_1',X_2'$ be two parameters associated with an
orthonormal cartesian basis $\{(0,1),(1,0)\}$ and let
$\Phi :(u,v)\mapsto {\Bbb R}^3$
be a mapping from some area of ${\Bbb R}^2$ into a twodimensional
surface of ${\Bbb R}^3$. Then the metric tensor is given by
$g_{ij}=
{\partial \Phi^k \over \partial X'^i}
{\partial \Phi^m \over \partial X'^j} \delta_{km}.$

\end{itemize}

\section{Some common misconceptions}

\subsection{Confusion between component representation and ``the real thing''}
Given a particular basis, a tensor is uniquely characterized by its components.
However, without reference to a particular basis, any components are just blurbs.

Example (wrong!): a rank-1 tensor (i.e., a vector) is given by
$(1,2)$.

Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a rank-1 tensor (i.e., a vector) is given by
$(1,2)$.


\subsection{A matrix is a tensor}

See the above section.

Example (wrong!): A matrix is  a  tensor of rank 2.

Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a matrix represents a rank-2 tensor.
The matrix components are the tensor components.

\subsection{Decomposition of tensors}

Although a tensor of rank $n$ transforms like the tensor product of $n$ tensors of rank 1,
not all rank-$n$ tensors can be decomposed into a single
tensor product of $n$ tensors of rank 1.

Nevertheless, any rank-$n$ tensor  can be decomposed into
the sum of $D^n$
tensor products of $n$ tensors of rank 1.

\subsection{Form invariance of tensors}

Although form invariance is a gratifying feature,
a tensor (field) needs not be form invariant.
Indeed,
while
\begin{equation}
S\equiv \left( \matrix {  x_2^2 & -x_1x_2  \cr
                    - x_1x_2          & x_1^2 \cr} \right)
\end{equation}
is a form invariant tensor field with respect to   the basis $\{(0,1),(1,0)\}$
and orthogonal transformations (rotations around the origin)
\begin{equation}
\left( \matrix { \cos \varphi & \sin \varphi  \cr
                         -\sin \varphi & \cos \varphi  \cr} \right),
\end{equation}
\begin{equation}
T\equiv \left( \matrix { x_2^2 & x_1x_2  \cr
                    x_1x_2          & x_1^2 \cr} \right)
\end{equation}
is not (please verify).
This, however, does not mean that
$T$ is not a respectable tensor field; its just not form invariant under rotations.

Note that the tensor product of form invariant tensors is again a form invariant tensor.

 \bibliography{/mytex/svozil}
 \bibliographystyle{unsrt}

\end{document}
\begin{eqnarray}


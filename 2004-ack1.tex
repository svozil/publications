%%tth:\begin{html}<LINK REL=STYLESHEET HREF="/~svozil/ssh.css">\end{html}
\documentstyle[a4]{article}
\RequirePackage{graphicx}
%\documentstyle[amsfonts]{article}
%\RequirePackage{times}
%\RequirePackage{courier}
%\RequirePackage{mathptm}
%\renewcommand{\baselinestretch}{1.3}
%
% begin style Alex


\newenvironment{Alex}
   {\begin{flushleft}
        \textbf{Alex:} \em}%
   {$\Box$ \end{flushleft}}

% end style Alex

\newenvironment{Karl}
   {\begin{flushleft}
        \textbf{Karl:} \em}%
   {$\Box$ \end{flushleft}}

% end style Alex


\begin{document}

%\def\frak{\cal }
%\def\Bbb{\bf }
\sloppy



\title{How to acknowledge hypercomputation?}
\author{Authors\\
 {\small Institute}}
\date{ }
\maketitle

%\begin{flushright}
%{\scriptsize http://tph.tuwien.ac.at/$\widetilde{\;\;}\,$svozil/publ/2000-cesena.$\{$htm,ps,tex$\}$}
%\end{flushright}

\begin{abstract}
Besides attempts to break the Karp-Cook Thesis by various speedups
there are efforts to conceptualize hypercomputation,
mostly in the framework of some advanced physical theory such as relativity theory
and quantum mechanics.
Rather than discussing these scenarios in detail,
this paper discusses the feasability of operationalizable
verifications and tractable verifications of claims
that certain agents or oracles transcend Turing computability and
recursive function theory.
\end{abstract}


\section{On black boxes which are hypercomputers}

Already in 1958, Davis \cite[p. 11]{davis-58}
sets the stage of the following discussion by pointing out
 {\em `` $\ldots$ how can we ever exclude the possibility of our
 presented,
 some day (perhaps by some extraterrestrial visitors), with a (perhaps
 extremely complex) device or ``oracle'' that ``computes'' a
 non computable function?''
 }
While this may have been a remote, amusing issue in the days written,
the advancement of physical theory in the past decades
has made necessary
a careful evaluation of the possibilities and options for
verification and falsification of certain claims that a concrete physical system
``computes''   a  non computable function.

\begin{Alex}
should we focus on the computation of functions or on decision
problems? Decision problems are easier to handle as the format of
the output (yes or no) is unproblematic.
\end{Alex}

\subsection{Conceptualization of black box}

Device or agent or oracle which one knows nothing about, has no
rational understanding (in the traditional algorithmic sense) of
the intrinsic working.

\begin{Alex}
The job of the oracle is to solve decision problems, i.e. to give
only the answer {\em yes} or {\em no}. This is the traditional use
of oracles in recursion- and complexity theory.
\end{Alex}

\subsection{Conceptualization of interface}

Input/output interface facilitating symbolic (eg. binary) exchange
with the black box.


\section{Tests}

\subsection{Tractability of the verification}

Tractable means verifiability by ``low'' (polynomial?)
degree of resources (time, memory space).

\subsection{Translation of solved problems into hard ones}

Example: prime factorization, RSA


\subsection{NP-complete cases}

Conjecture: by operational means it is not possible to go beyond
tests of hyper-NP-completeness.

\begin{Alex}
Even for an NP-complete problem (say SAT) it is not trivial to
verify that a hypercomputer solves the problem in polynomial time.
Without insight into the internal structure of the hypercomputer
we cannot obtain a {\em proof} of polynomial time computation,
which is an {\em asymptotic} result. Even here we rely on
experiments to test a large number (how large actually?) of
problems. A central problem consists in the right selection of
problem sequences. If the selection is based on random generators
we merely obtain results on average complexity, which would not be
significant.
\end{Alex}

\subsection{Harder cases with tractable verification}

Do there exist (decision) problems which are harder
than the known NP-complete cases,
possible having no recursively enumerable solution and proof methods,
whose results nevertheless are tractable verifiable?

\begin{Alex}
Yes I think so. We just have to choose another scenario. Feed in
worst-case sentences of decision problems from which you know the
solution. Here we may even choose an undecidable problem. E.G. we
may feed the hypercomputer sequences of Diophantine equations
which are unsolvable (i.e. we know that they are unsolvable). We
may increase the quality of the test, by using reductions of other
undecidable problems to Hilbert's tenth problem. Clearly {\em
falsification} is the simpler job: If the hypercomputer gives a
wrong answer on {\em one} problem of such a sequence (by producing
the answer yes) we have shown that his principle of computation is
wrong. To avoid the trivial case where the hypercomputer is a
stupid machinery which only outputs {\em no}, we must mix the
sequence with a solvable one using random generators. Still it
might be the case that the hypercomputer knows all our sequences
and also the codes of our random generators. So when and how do we
{\em verify} the hypercomputer, i.e. when are we convinced of his
``hyper-capabilities''? Is very hard to define a scientific
criterion. Basically the problem of the {\em organization of
experiments} is as hard as in the NP-case. The general problem
remains the same: {\em verifying} a decision algorithm requires
the code, i.e. the internal structure, {\em falsifying} it is
possible by experiments.
\end{Alex}

\subsection{Is provability necessary, what does one gain?}

\subsection{Can two or more hard problems interfere and be
 combined to form a tractable test?}

\begin{Alex}
What is the meaning of {\em interference of problems}? Of course
an oracle for one untractable problem may give a tractable test
for another untractable problem, but this is trivial by reduction.
\end{Alex}

\begin{Karl}
As an analogy, consider Deutsch's problem as one of the first problems which
quantum computers could solve effectively.
Consider a function that take a single (classical) bit into a single (classical) bit.
There are four such functions $f_1,\ldots ,f_4$, corresponding to all variations.
One can specify or ``prepare'' a function bitwise, or alternatively,
one may specify it by requiring that, for instance, such a function
acquires different values on different inputs, such as $f(0)\neq f(1)$.
Thereby, we may, even in principle, learn nothing about the individual functional values alone.
% cf http://people.ccmr.cornell.edu/~mermin/qcomp/chap2.pdf
\end{Karl}

\begin{Karl}
\subsection{Generation of the first bits of  random sequences}
By implementation of Chaitin's ``algorithm'' to compute
Chaitin's $\Omega$  \cite{chaitin:01}
or variants thereof \cite{calude:94}, it would be principle to compute the first bits
of random sequences.
Such random sequences could in principle be subject to the usual
tests of stochasticity \cite{svozil-qct,nbstandards-test}.
\end{Karl}

\bibliography{svozil}
\bibliographystyle{unsrt}
\end{document}

%%tth:\begin{html}<LINK REL=STYLESHEET HREF="/~svozil/ssh.css">\end{html}
\documentclass[prl,preprint,showpacs,showkeys,amsfonts]{revtex4}
\usepackage{graphicx}
%\documentstyle[amsfonts]{article}
 \RequirePackage{times}
%\RequirePackage{courier}
\RequirePackage{mathptm}
%\renewcommand{\baselinestretch}{1.3}
\begin{document}

%\def\mathfrak{\cal }
%\def\Bbb{\bf }
%\sloppy




\title{Tensors as  multilinear forms\\
Handout ``Methoden der Theoretischen Physik-\"Ubungen''}
\author{Karl Svozil}
 \email{svozil@tuwien.ac.at}
\homepage{http://tph.tuwien.ac.at/~svozil}
\affiliation{Institut f\"ur Theoretische Physik, University of Technology Vienna,
Wiedner Hauptstra\ss e 8-10/136, A-1040 Vienna, Austria}

\begin{abstract}
Tensors are defined as multilinear forms on vector spaces
\end{abstract}


\pacs{45.10.Na,02.70}
\keywords{Tensor calculus}

\maketitle

%\section{Definitions}

\section{Notation}

Consider the vector space ${\Bbb R}^D$ of dimension $D$,
a basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_D\}$ consisting of
$D$ basis vectors ${\bf e}_i$,
and $n$ arbitrary vectors
$x_1,x_2,\ldots ,x_n\in {\Bbb R}^D$
with vector components
$X^i_1,X^i_2,\ldots ,X^i_n\in {\Bbb R}$.

{\em Tensor fields} define tensors in every point of ${\Bbb R}^D$ separately.
In general, with respect to a particular basis, the components of a tensor field
depend on the coordinates.


We adopt Einstein's summation convention to sum over equal indices
(one pair with a superscript and a subscript).
Sometimes, sums are written out explicitly.


In what follows, the notations
`` $x\cdot y$'',
`` $(x,y)$'' and
`` $\langle x\mid y\rangle $'' will be used synonymously for the {\em
scalar product}.
Note, however, that the notation `` $x\cdot y$''
may be a little bit misleading; e.g. in the case of the ``pseudo-Euclidean'' metric
 ${\rm diag}(+,+,+,\cdots ,+,-)$.

For a more systematic treatment, see for instance Klingbeil
\cite{Klingbeil}
and Dirschmid
\cite{Dirschmid}.
.


\section{Multilinear form}

A {\em multilinear form}
\begin{equation}
\alpha :{\Bbb R}^n \mapsto {\Bbb R}
\end{equation}
is a map satisfying
\begin{eqnarray}
\alpha ( x_1,x_2,\ldots , A x^1_i+ B x^2_i,\ldots ,x_n)
&=&
A\alpha ( x_1,x_2,\ldots , x^1_i,\ldots ,x_n) \nonumber \\
&&  \qquad +
B\alpha ( x_1,x_2,\ldots , x^2_i,\ldots ,x_n)
\end{eqnarray}
for every one of its (multi-)arguments.


\section{Covariant tensors}
A tensor of rank $n$
\begin{equation}
\alpha:{\Bbb R}^n \mapsto {\Bbb R}
\end{equation}
is a multilinear form
\begin{equation}
\alpha ( x_1,x_2,\ldots ,x_n)=
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1 X^{i_2}_2\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_n}).
\end{equation}
The
\begin{equation}
A_{{i_1}{i_2}\cdots {i_n}}=\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_n})
\end{equation}
 are the
{\em components} of the tensor $\alpha $ with respect to the basis
${\mathfrak B}$.

\subsubsection{Question: how many components are there?}
{Answer: $D^n$.}

\subsubsection{Question: proof that tensors are multilinear forms.}
{Answer:}
by insertion.
\begin{eqnarray}
&&\alpha ( x_1,x_2,\ldots , Ax^1_j+Bx_j^2,\ldots ,x_n)
\nonumber \\
&=&
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1X^{i_2}_2\ldots  [A(X^1)^{i_j}_j+B(X^2)^{i_j}_j]\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_n})
\nonumber \\
&=& A
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1X^{i_2}_2\ldots  (X^1)^{i_j}_j\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_n})
\nonumber \\
&&\quad +
B
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
X^{i_i}_1X^{i_2}_2\ldots  (X^B)^{i_j}_j\ldots X^{i_n}_n
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_n})
\nonumber \\
&=& A \alpha ( x_1,x_2,\ldots , x^1_j,\ldots ,x_n)+
B \alpha ( x_1,x_2,\ldots , x_j^2,\ldots ,x_n)\nonumber
\end{eqnarray}

\subsection{Basis transformations}
Let
${\mathfrak B}$
and
${\mathfrak B'}$
be two arbitrary bases of
${\Bbb R}^D$.
Then ervery vector ${\bf e}'_i$ of
${\mathfrak B'}$
can be represented as linear combination of basis vectors from
${\mathfrak B}$:
\begin{equation}
{\bf e}'_i=\sum_{j=1}^D a_i^j {\bf e}_j, \qquad i=1,\ldots , D .
\label{2001-mu-tensors}
\end{equation}

Consider an arbitrary vector $x\in {\Bbb R}^D$
with components $X^i$ with respect to the basis
${\mathfrak B}$
and   ${X'}^i$  with respect to the basis
${\mathfrak B'}$:
\begin{equation}
x
=\sum_{i=1}^D X^i {\bf e}_i
=\sum_{i=1}^D {X'}^i {\bf e}'_i
.
\end{equation}
Insertion into (\ref{2001-mu-tensors}) yields
\begin{equation}
x
=\sum_{i=1}^D X^i {\bf e}_i
=\sum_{i=1}^D {X'}^i {\bf e}'_i
=\sum_{i=1}^D {X'}^i \sum_{j=1}^D a_i^j {\bf e}_j=
\sum_{i=1}^D\left[\sum_{j=1}^D a_i^j{X'}^i \right] {\bf e}_j.
\end{equation}
A comparison of coefficient yields the transformation laws of vector components
\begin{equation}
X^j   = \sum_{j=1}^D a_i^j{X'}^i.
\end{equation}
The matrix $a=\{a_i^j\}$ is called the {\em transformation matrix}.

A similar argument using
\begin{equation}
{\bf e}_i=\sum_{j=1}^D {a'}_i^j {\bf e}'_j, \qquad i=1,\ldots , D
\end{equation}
yields the inverse transformation laws
\begin{equation}
{X'}^j   = \sum_{j=1}^D {a'}_i^j{X}^i.
\end{equation}
Thereby,
\begin{equation}
{\bf e}_i=\sum_{j=1}^D {a'}_i^j {\bf e}'_j
=\sum_{j=1}^D {a'}_i^j \sum_{k=1}^D a_j^k {\bf e}_k
=\sum_{j=1}^D \sum_{k=1}^D [{a'}_i^j a_j^k] {\bf e}_k,
\end{equation}
which, due to the linear independence of the basis vectors ${\bf e}_i$ od ${\mathfrak B}$,
is only satisfied if
\begin{equation}
{a'}_i^j a_j^k =\delta_i^k
\qquad
{\rm or}
\qquad
a'a={\Bbb I}.
\end{equation}
That is, $a'$ is the inverse matrix of $a$.

\subsection{Transformation of Tensor components}

Because of multilinearity (!) and by insertion into
(\ref{2001-mu-tensors}),
\begin{eqnarray}
&&\alpha ( {\bf e}'_{j_1},{\bf e}'_{j_2},\ldots ,{\bf e}'_{j_n})=
\alpha \left(
\sum_{i_1=1}^D a_{j_1}^{i_1} {\bf e}_{i_1},
\sum_{i_2=1}^D a_{j_2}^{i_2} {\bf e}_{i_2},
\ldots ,
\sum_{i_n=1}^D a_{j_n}^{i_n} {\bf e}_{i_n}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots a_{j_n}^{i_n} \alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_n})
\end{eqnarray}
or
\begin{equation}
A'_{{j_1}{j_2}\cdots {j_n}}=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
a_{j_1}^{i_1}a_{j_2}^{i_2}\cdots a_{j_n}^{i_n} A_{i_1 i_2\ldots i_n}.
\end{equation}


\section{Contravariant tensors}

\subsection{Definition of contravariant basis}

Consider again a covariant basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_D\}$ consisting of
$D$ basis vectors ${\bf e}_i$.
We shall define now a {\em contravariant} basis
${\mathfrak B^\ast}=\{{\bf e}^1,{\bf e}^2,\ldots ,{\bf e}^D\}$ consisting of
$D$ basis vectors ${\bf e}^i$
by the requirement that the scalar product obeys
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j\equiv ({\bf e}_i,{\bf e}_j)\equiv \langle {\bf e}_i\mid {\bf e}_j\rangle
 =\left\{
 \begin{array}{l}
1 \mbox{ if } i=j \\
0 \mbox{ if } i\neq j  \\
\end{array}
 \right. .
\label{2001-mu-tensors0}
\end{equation}
To distinguish elements of the two bases, the covariant vectors are denoted by {\em subscripts},
whereas the contravariant vectors are denoted by {\em superscripts}.
The last term ${\bf e}_i\cdot {\bf e}^j\equiv ({\bf e}_i,{\bf e}_j)\equiv \langle {\bf e}_i\mid {\bf e}_j\rangle $
recalls different notations of the scalar product.

The entire tensor formalism developed so far can be applied to define {\em contravariant} tensors
as multinear forms
\begin{equation}
\beta:{\Bbb R}^n \mapsto {\Bbb R}
\end{equation}
by
\begin{equation}
\beta ( x^1,x^2,\ldots ,x^n)=
\sum_{i_1=1}^D
\sum_{i_2=1}^D
\cdots
\sum_{i_n=1}^D
\Xi_{i_1}^1 \Xi_{i_2}^2\ldots \Xi_{i_n}^n
\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_n}).
\end{equation}
The
\begin{equation}
B^{{i_1}{i_2}\cdots {i_n}}=\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_n})
\end{equation}
 are the
{\em components} of the contravariant tensor $\beta $ with respect to the basis
${\mathfrak B}^\ast$.

\subsection{Connection between the transformation of covariant and contravariant entities}

Because of linearity, we can make the Ansatz
\begin{equation}
{{\bf e}'}^j=\sum_ib_i^j{\bf e}^i,
\end{equation}
where $\{b_i^j\}=b$ is
the transformation matrix associated with the contravariant basis.
How is $b$ related to $a$,
the transformation matrix associated with the covariant basis?

By exploiting (\ref{2001-mu-tensors0}) one can find the connection between
the transformation of covariant and contravariant basis elements and thus
tensor components.
\begin{equation}
\delta_i^j= {{\bf e}'}_i\cdot {{\bf e}'}^j=(a_i^k{\bf e}_k)\cdot (b_l^j{\bf e}^l)=a_i^kb_l^j {\bf e}_k\cdot {\bf e}^l=a_i^kb_l^j \delta_k^l
=a_i^kb_k^j,
\end{equation}
and
\begin{equation}
b=a^{-1} =a'.
\end{equation}
The entire argument concerning transformations of covariant tensors and components
can be carried through to the contravariant case.
Hence, the contravariant components transform as
\begin{eqnarray}
&&\beta ( {{\bf e}'}^{j_1},{{\bf e}'}^{j_2},\ldots ,{{\bf e}'}^{j_n})=
\beta \left(
\sum_{i_1=1}^D {a'}^{j_1}_{i_1} {\bf e}^{i_1},
\sum_{i_2=1}^D {a'}^{j_2}_{i_2} {\bf e}^{i_2},
\ldots ,
\sum_{i_n=1}^D {a'}^{j_n}_{i_n} {\bf e}^{i_n}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
{a'}^{j_1}_{i_1}{a'}^{j_2}_{i_2}\cdots {a'}^{j_n}_{i_n} \beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_n})
\end{eqnarray}
or
\begin{equation}
B'^{{j_1}{j_2}\cdots {j_n}}=
\sum_{i_1=1}^D\sum_{i_2=1}^D\cdots \sum_{i_n=1}^D
{a'}^{j_1}_{i_1}{a'}^{j_2}_{i_2}\cdots {a'}^{j_n}_{i_n} B^{i_1 i_2\ldots i_n}.
\end{equation}

\section{Orthonormal bases}
For orthonormal bases,
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j
\Longleftrightarrow
{\bf e}_i= {\bf e}^i  ,
\end{equation}
and thus the two bases are identical
\begin{equation}
{\mathfrak B}={\mathfrak B}^\ast
\end{equation}
and  formally any distinction between covariant and contravariant vectors becomes
irrelevant. Conceptually, such a distinction persists, though.


\section{Metric tensor}

Metric tensors are defined in metric vector spaces.
A metric vector space (sometimes also refered to
as ``vector space with metric'' or ``geometry'')
is a vector space with inner or scalar product.

This includes (pseudo-) Euclidean spaces with indefinite metric.
(I.e., the distance needs not be positive or zero.)


\subsection{Definition inner or scalar product}


The {\em scalar} or {\em inner product},
is a symmetric bilinear functional
 ${\Bbb R}^D\times {\Bbb R}^D\mapsto {\Bbb R}$
such that
\begin{itemize}
\item
$(x+y,z)=(x,z)+(y,z)$ for all $x,y,z \in {\Bbb R}^D$;
\item
$(x,y+z)=(x,y)+(x,z)$ for all $x,y,z \in {\Bbb R}^D$;
\item
$(\alpha x,y)=\alpha (x,y)$ for all $x,y \in {\Bbb R}^D, \alpha \in {\Bbb R}$;
\item
$(x,\alpha y)=\alpha (x,y)$ for all $x,y \in {\Bbb R}^D, \alpha \in {\Bbb R}$;
\item
$(x,y)={(y,x)} $ for all $x,y \in {\Bbb R}^D$
\end{itemize}
Axioms 1 and 3 assert that the scalar product is linear in the first variable.
Axioms 2 and 4 assert that the scalar product is linear in the second variable.
Axiom 5 asserts the bilinear function is symmetric.


\subsection{Definition metric}

A {\em metric} is a functional ${\Bbb R}^D\mapsto {\Bbb R}$
with the following properties
\begin{itemize}
\item
$\vert\vert x -y \vert\vert =0 \; \Longleftrightarrow \; x = y$,
\item
$\vert\vert x- y\vert\vert  =\vert\vert  x- y\vert\vert $  (symmetry),
\item
$\vert\vert x-z\vert\vert  \le \vert\vert x- y\vert\vert  + \vert\vert  y- z\vert\vert $  (triangle
inequality).
\end{itemize}



\subsection{Construction of a metric from a scalar product by metric tensor}

The {\em metric} tensor is defined by  the scalar product
\begin{equation}
g_{ij}={\bf e}_i\cdot {\bf e}_j\equiv ({\bf e}_i, {\bf e}_j)\equiv \langle {\bf e}_i, {\bf e}_j\rangle .
\end{equation}
and
\begin{equation}
g^{ij}={\bf e}^i\cdot {\bf e}^j\equiv ({\bf e}^i, {\bf e}^j)\equiv \langle {\bf e}^i, {\bf e}^j\rangle .
\end{equation}
Likewise,
\begin{equation}
g^i_{j}={\bf e}^i\cdot {\bf e}_j\equiv ({\bf e}^i, {\bf e}_j)\equiv \langle {\bf e}^i, {\bf e}_j\rangle .
\end{equation}
Note that it easy to change a covarant tensor into a contravariant and {\em vice versa}
by the application of a metric tensor.
This can be seen as follows.
Because of linearity, any contravariant basis vector ${\bf e}^i$
can be written as a linear sum of covariant basis vectors:
\begin{equation}
{\bf e}^i=A^{ij}{\bf e}_j.
\end{equation}
Then,
\begin{equation}
g^{ik} ={\bf e}^i\cdot {\bf e}^k =(A^{ij}{\bf e}_j)\cdot {\bf e}^k=A^{ij}({\bf e}_j\cdot {\bf e}^k)=A^{ij}\delta_j^k=A^{ik}
\end{equation}
and thus
\begin{equation}
{\bf e}^i=g^{ij}{\bf e}_j
\end{equation}
and
\begin{equation}
{\bf e}_i=g_{ij}{\bf e}^j.
\end{equation}


Question: Show that, for orthonormal basis, the metric tensor can be
represented as a Kronecker delta function in all basis (form invariant);
i.e.,
$\delta_{ij},\delta^i_j,\delta_i^j,\delta^{ij}$.

Question: Why is $g$ a tensor? Show is multilinearity.

\subsection{What can the metric tensor do for us?}

Most often it is used to raise/lower the indices; i.e.,
to change from contravariant to covariant and conversely from covariant
to contravariant.

In the previous section, the metric tensor has been derived from the scalar product.
The converse is true as well.
The metric tensor represents the scalar product between vectors: let
$x=X^i{\bf e}_i \in {\Bbb R}^D$ and $y=Y^j{\bf e}_j \in {\Bbb R}^D$ be two vectors.
Then,
\begin{equation}
x\cdot y\equiv (x,y)\equiv \langle x\mid y\rangle
= X^i {\bf e}_i\cdot Y^j {\bf e}_j
= X^iY^j {\bf e}_i\cdot  {\bf e}_j
=X^iY^j g_{ij}= X^T g Y.
\end{equation}

It also characterizes the length of a vector: in the above
equation, set $y=x$. Then ("$^T$" stands for the transpose),
\begin{equation}
x\cdot x\equiv (x,x)\equiv \langle x\mid x\rangle
=X^iX^j g_{ij}\equiv X^T g X,
\end{equation}
and thus
\begin{equation}
\vert\vert  x\vert\vert  =\sqrt{X^iX^j g_{ij}}= \sqrt{X^T g X}.
\end{equation}

666666666666666666666666666

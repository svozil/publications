\chapter{Divergent series}
\index{divergent series}
\label{2011-m-ch-ds}
In this final chapter we will consider {\em divergent series}, which,
as has already been mentioned earlier,
seem to have been ``invented by the devil'' \cite{Hardy:1949}.
Unfortunately such series occur
very often in physical situations;
for instance in celestial mechanics or in quantum field theory
\cite{Boyd99thedevil,PhysRev.85.631,PhysRevD.57.1144,PhysRevD.62.076001},
and one may wonder with Abel why, {\em ``for the most part,
it is true that the results are correct, which is very strange''  }
\cite{rousseau-2004}.
On the other hand,
there appears to be another view on diverging series,
a view that has been expressed by Berry as follows
\cite{berry-92}:
{\em ``$\ldots$ an asymptotic series $\ldots$ is a compact encoding of a function,
and its divergence should be regarded not as a deficiency but as a source of information about the function.''
}

\section{Convergence and divergence}
Let us first define {\em convergence} in the context of series.
\index{convergence}
A series
\begin{equation}
\sum_{j=0}^\infty a_j =a_0+a_1+a_2+\cdots
\end{equation}
is said to converge to the {\em sum}
$s$, if the {\em partial sum}
\begin{equation}
s_n=  \sum_{j=0}^n a_j =a_0+a_1+a_2+\cdots + a_n
\end{equation}
tends to a finite limit $s$ when $n\rightarrow \infty$;
otherwise it is said to be divergent.
\index{divergence}

One of the most prominent series is the
Leibniz series~\cite{leibnitz-1860,moore-1938,Hardy:1949,everest-2003}
\begin{equation}
s = \sum_{j=0}^\infty (-1)^j=1-1+1-1+1-\cdots ,
\label{2009-fiftyfifty-1s}
\end{equation}
whose summands may be -- inconsistently
-- ``rearranged,''
yielding
\begin{equation*}
\begin{split}
\textrm{ either }
1-1+1-1+1-1+\cdots = (1-1)+(1-1)+(1-1)-\cdots =0\\
\textrm{ or }
1-1+1-1+1-1+\cdots = 1+(-1+1)+ (-1+1) +\cdots =1.
\end{split}
\end{equation*}
Note that, by {\em Riemann's rearrangement theorem},
\index{Riemann rearrangement theorem}
even convergent series  which do not absolutely converge
(i.e., $\sum_{j=0}^n a_j$ converges but $\sum_{j=0}^n \left| a_j \right|$ diverges)
can converge to any arbitrary (even infinite) value
by permuting (rearranging) the (ratio of) positive and negative terms
(the series of which must both be divergent).

The Leibniz series is a particular case $q=-1$ of a {\em geometric series}
\index{geometric series}
\begin{equation}
s = \sum_{j=0}^\infty q^j=1+q+q^2+q^3+ \cdots  =1+q s
\label{2009-fiftyfifty-1sgs}
\end{equation}
which, since $s=1+qs$, for $\vert q\vert <1$, converges  to
\begin{equation}
s= \sum_{j=0}^\infty q^j= \frac{1}{1-q}.
\label{2012-fiftyfifty-1sgscont}
\end{equation}
One way to sum the Leibnitz series is by ``continuing''
Eq. (\ref{2012-fiftyfifty-1sgscont})
for arbitrary $q\neq 1$, thereby defining the
{\em Abel sum}
\index{Abel sum}
\begin{equation}
\sum_{j=0}^\infty (-1)^j \stackrel{{\rm A}}{=} \frac{1}{1-(-1)} = \frac{1}{2}.
\label{2012-fiftyfifty-1sgscont1}
\end{equation}

Another divergent series,
which can be obtained by formally expanding  the square of the Abel sum of the
Leibnitz series $s^2 \stackrel{{\rm A}}{=} (1+x)^{-2}$
around $0$ and inserting $x=1$
\cite{Kline-83}
is
\begin{equation}
s^2  =
\left(\sum_{j=0}^\infty (-1)^j\right)
\left(\sum_{k=0}^\infty (-1)^k\right)
= \sum_{j=0}^\infty (-1)^{j+1} j = 0 + 1-2+3-4+5-\cdots
.
\label{2009-fiftyfifty-1s1}
\end{equation}
In the same sense as the Leibnitz series, this yields the Abel sum $s^2\stackrel{{\rm A}}{=}1/4$.


Note that the sequence of its partial sums $s^2_n=\sum_{j=0}^n (-1)^{j+1} j $
yield every integer once; that is,
$s^2_0 =0$,
$s^2_1 =0+1=1$,
$s^2_2 =0+1-2=-1$,
$s^2_3 =0+1-2+3=2$,
$s^2_4 =0+1-2+3-4-2$,
$\ldots$,
$s^2_n =-\frac{n}{2}$ for even $n$,
and
$s^2_n =-\frac{n+1}{2}$ for odd $n$.
It thus establishes a strict one-to-one mapping
$s^2: {\Bbb N} \mapsto {\Bbb Z}$
of the natural numbers onto the integers.


\section{Euler differential equation}
\index{Euler differential equation}

In what follows we demonstrate that divergent series may make sense, in the way Abel
wondered.
That is, we shall show that the first partial sums of divergent series
may yield ``good'' approximations of the exact result; and
that, from a certain point onward, more terms contributing to the
sum  might worsen the approximation rather an make it better -- a situation totally different
from convergent series, where more terms always result in better approximations.

Let us, with Rousseau,
for the sake of demonstration of the former situation,
consider the {\em Euler differential equation}
\index{Euler differential equation}
\begin{equation}
\begin{split}
\left(x^2 \frac{d}{dx} +1\right) y(x) = {x},\;\textrm{ or }\;
\left(\frac{d}{dx} +\frac{1}{x^2}\right) y(x) = \frac{1}{x}.
\end{split}
\label{2011-m-ch-dsee}
\end{equation}

We shall solve this equation by two methods:  we shall, on the one hand,
present a divergent series solution, and on the other hand, an exact solution.
Then we shall  compare the series approximation to the exact solution by considering
the difference.

A series solution of the Euler differential equation can be given by
\begin{equation}
{y_s} (x) = \sum_{j=0}^\infty (-1)^j j! x^{j+1}.
\label{2011-m-ch-dseess}
\end{equation}
That (\ref{2011-m-ch-dseess})
solves  (\ref{2011-m-ch-dsee})
can be seen by inserting the former into the latter; that is,
\begin{equation}
\begin{split}
\left(x^2 \frac{d}{dx} +1\right) \sum_{j=0}^\infty (-1)^j j! x^{j+1} = {x},  \\
\sum_{j=0}^\infty (-1)^j (j+1)! x^{j+2} + \sum_{j=0}^\infty (-1)^j j! x^{j+1} = {x},  \\
\qquad \textrm{[change of variable in the first sum: } j \rightarrow j-1\textrm{ ]}\\
\sum_{j=1}^\infty (-1)^{j-1} (j+1-1)! x^{j+2-1} + \sum_{j=0}^\infty (-1)^j j! x^{j+1} = {x},  \\
\sum_{j=1}^\infty (-1)^{j-1} j! x^{j+1} + x +\sum_{j=1}^\infty (-1)^j j! x^{j+1} = {x},  \\
x +\sum_{j=1}^\infty (-1)^j\left[(-1)^{-1} +1\right] j! x^{j+1}    = {x},  \\
x +\sum_{j=1}^\infty (-1)^j \underbrace{\left[ -1  +1\right]}_{=0} j! x^{j+1}    = {x},  \\
x    = {x}.  \\
\end{split}
\label{2011-m-ch-dsee111}
\end{equation}


On the other hand, an exact solution can be found by {\em quadrature;}
that is, by explicit integration (see, for instance, Chapter one of Ref. \cite{birkhoff-Rota-48}).
Consider the homogenuous first order differential equation
\begin{equation}
\begin{split}
\left(\frac{d}{dx} + p(x)  \right)  y(x)=0,\\
\textrm{ or }
\frac{dy(x)}{dx} = - p(x)   y(x) ,
\\ \textrm{ or }
\frac{dy(x)}{ y(x) } = - p(x) dx .
\end{split}
\label{2011-m-ch-dseeqsh}
\end{equation}
Integrating both sides yields
\begin{equation}
\begin{split}
\log \vert y(x)\vert =  - \int p(x) dx +C \; \textrm{, or } \;
\vert y(x)\vert =K e^{- \int p(x) dx},
\end{split}
\label{2011-m-ch-dseedi}
\end{equation}
where $C$ is some constant, and $K=e^C$.
Let $P(x)={ \int p(x) dx}$. Hence, heuristically,
$y(x)e^{P(x)}$ is constant, as can also be seen by explicit differentiation of $y(x)e^{P(x)}$; that is,
\begin{equation}
\begin{split}
\frac{d}{dx}y(x)e^{P(x)}
=  e^{P(x)}\frac{dy(x)}{dx}   +  y(x)\frac{d}{dx} e^{P(x)}\\
\qquad =
e^{P(x)}\frac{dy(x)}{dx}   +  y(x)  p(x) e^{P(x)}\\
\qquad =
e^{P(x)}\left(\frac{d}{dx}   +   p(x) \right)y(x)\\
\qquad = 0
\end{split}
\label{2011-m-ch-dseeed}
\end{equation}
if and, since $ e^{P(x)}\neq 0$,
only if $y(x)$ satisfies the homogenuous equation  (\ref{2011-m-ch-dseeqsh}).
Hence,
\begin{equation}
\begin{split}
y(x)  =c e^{- \int p(x) dx} \textrm { is the solution of}\\
\left(\frac{d}{dx} + p(x)  \right)  y(x)=0
\end{split}
\label{2011-m-ch-dseesoh}
\end{equation}
for some constant $c$.



Similarly, we can again find a solution to the inhomogenuos  first order differential equation
\begin{equation}
\begin{split}
\left(\frac{d}{dx} + p(x)  \right)  y(x)+ q(x)=0,      \\
\textrm{ or }\left(\frac{d }{dx}  + p(x) \right)   y(x)= - q(x)
\end{split}
\label{2011-m-ch-dseeinh}
\end{equation}
by differentiating the function $y(x)e^{P(x)}=y(x)e^{ \int p(x) dx}$;
that is,
\begin{equation}
\begin{split}
\frac{d }{dx}   y(x)e^{  \int p(x) dx} =
e^{  \int p(x) dx}\frac{d }{dx}   y(x) + p(x) e^{ \int p(x) dx}  y(x) \\
\qquad
=  e^{  \int p(x) dx}\underbrace{\left(\frac{d }{dx}    + p(x) \right)   y(x)}_{=q(x)}  \\
=   - e^{  \int p(x) dx} q(x).
\end{split}
\label{2011-m-ch-dsee1213}
\end{equation}
Hence, for some constant $y_0$ and some $a,b$,
we must have, by integration,
\begin{equation}
\begin{split}
\int_b^x \frac{d }{dt}   y(t)e^{  \int_a^t p(s) ds} dt  =
y(x)e^{  \int_a^x p(t) dt} \\
= y_0 -  \int_b^x e^{ \int_a^t p(s) ds }q(t) dt,\\
\textrm{ and hence }
y(x)= y_0e^{ - \int_a^x p(t) dt}  -  e^{ - \int_a^x p(t) dt} \int_b^x e^{ \int_a^t p(s) ds} q(t) dt
.
\end{split}
\label{2011-m-ch-dseekh}
\end{equation}
If $a=b$, then $y(b)= y_0$.

Coming back to the Euler differential equation and identifying
$p(x)=1/x^2$ and $q(x) = - 1/x$ we obtain, up to a constant,  with $b=0$ and arbitrary constant $a\neq 0$,
\begin{equation}
\begin{split}
y(x)=
-  e^{ - \int_a^x \frac{ dt}{t^2}} \int_0^x e^{ \int_a^t \frac{ ds}{s^2}} \left(-\frac{ 1}{t}\right) dt
\\
=
  e^{ - \left. \left(-\frac{1}{t}\right )\right|_a^x} \int_0^x e^{  \left. -\frac{1}{s}\right|_a^t} \left(\frac{ 1}{t}\right) dt
\\
=
  e^{ \frac{1}{x} -  \frac{1}{a} } \int_0^x e^{  -\frac{1}{t} +  \frac{1}{a} } \left(\frac{ 1}{t}\right) dt
\\
=
  e^{ \frac{1}{x}}\underbrace{e^{ -  \frac{1}{a} + \frac{1}{a}}}_{=e^0=1} \int_0^x e^{ -\frac{1}{t} } \left(\frac{ 1}{t}\right) dt
\\
=
  e^{ \frac{1}{x}}e^{ -  \frac{1}{a} } \int_0^x e^{ -\frac{1}{t} }e^{ \frac{1}{a} } \left(\frac{ 1}{t}\right) dt
\\
=
e^{   \frac{1}{x}} \int_0^x  \frac{ e^{ - \frac{1}{t}}}{t} dt
\\
=
 \int_0^x  \frac{ e^{ \frac{1}{x} -\frac{1}{t}}}{t} dt.
\end{split}
\label{2011-m-ch-dseeeesola}
\end{equation}
With a change of the integration variable
\begin{equation}
\begin{split}
\frac{\xi}{x} = \frac{1}{t}-\frac{1}{x}
, \; \textrm{ and thus }  \;
\xi = \frac{x}{t}-1
, \;  \textrm{ and } \;
t =  \frac{x}{1+ \xi}
, \\
\frac{dt}{d\xi } =  -\frac{x}{(1+ \xi)^2}  , \;
\textrm{ and thus }  \;
dt =  -\frac{x}{(1+ \xi)^2} d\xi, \;    \\
\textrm{ and thus }  \;
\frac{dt}{t } =   \frac{-\frac{x}{(1+ \xi)^2}}{\frac{x}{1+ \xi}} d\xi
=   -\frac{ d\xi}{1+ \xi},
\end{split}
\label{2011-m-ch-dseeans}
\end{equation}
the integral (\ref{2011-m-ch-dseeeesola}) can be rewritten as
\begin{equation}
\begin{split}
y(x)= \int_{\infty}^0
\left(-\frac{e^{-\frac{\xi}{x}}}{1+\xi}\right) d\xi
= \int_0^\infty
\frac{e^{-\frac{\xi}{x}}}{1+\xi} d\xi .
\end{split}
\label{2011-m-ch-dseefasol}
\end{equation}
It is proportional to the
{\em Stieltjes Integral}
\index{Stieltjes Integral}
\cite{Bender-Orszag,Boyd99thedevil}
\begin{equation}
S(x)= \int_0^\infty
\frac{e^{-\xi}}{1+x\xi} d\xi .
\label{2012-m-ch-ds-si}
\end{equation}

Note that whereas the series solution $y_s(x)$ diverges for all nonzero $x$,
the solution $y(x)$ in (\ref{2011-m-ch-dseefasol})
converges and is well defined for all $x\ge 0$.

Let us now estimate the absolute difference between $y_{s_k}(x)$
which represents the partial sum ``$y_s(x)$ truncated
after the $k$th term'' and $y(x)$; that is, let us consider
\begin{equation}
\begin{split}
\vert y(x) - y_{s_k}(x) \vert=
\left\vert \int_0^\infty
\frac{e^{-\frac{\xi}{x}}}{1+\xi} d\xi
-
\sum_{j=0}^k (-1)^j j! x^{j+1} \right\vert .
\end{split}
\label{2011-m-ch-dseeanest}
\end{equation}

For any $x\ge 0$ this difference can be estimated \cite{rousseau-2004} by  a bound from above
\begin{equation}
\left| R_k(x)\right|
\stackrel{{\tiny \textrm{ def }}}{=}
\vert y(x) - y_{s_k}(x) \vert
\le
k! x^{k+1},
\label{2011-m-ch-dseeest}
\end{equation}
that is, this difference between the exact solution $y(x)$ and the diverging partial series
$y_{s_k}(x)$ is smaller than the first neglected term; and all subsequent ones.

{\color{OliveGreen}
\bproof

For a proof, observe that,
since
a partial {\em geometric series}
\index{geometric series}
is the sum of all the numbers in a geometric progression up to a certain power;
that is,
\begin{equation}
\sum_{k=0}^n r^k =   1+r+r^2+ \cdots +r^k+ \cdots +r^n .
\label{2011-m-ch-dsee124567}
\end{equation}
By multiplying both sides with $1-r$,
the sum (\ref{2011-m-ch-dsee124567}) can be rewritten as
\begin{equation}
\begin{split}
(1-r) \sum_{k=0}^n r^k=
(1-k) (1+ r+r^2+ \cdots +r^k+ \cdots +r^n)\\
=1+ r+r^2+ \cdots +r^k+ \cdots +r^n -
r(1+r+r^2+ \cdots +r^k+ \cdots +r^n +r^{n}) \\
=1+ r+r^2+ \cdots +r^k+ \cdots +r^n -
(r+r^2+ \cdots +r^k+ \cdots +r^n +r^{n+1}) \\
= 1-r^{n+1}
,
\end{split}
\end{equation}
and, since the middle terms all cancel out,
\begin{equation}
\sum_{k=0}^n r^k =  \frac{1-r^{n+1}}{1-r},
\;
\textrm{ or }
\;
\sum_{k=0}^{n-1} r^k =  \frac{1-r^{n}}{1-r}  =  \frac{1}{1-r} - \frac{r^{n}}{1-r}
.
\label{2011-m-ch-dsee12}
\end{equation}
Thus, for $r=-\zeta$, it is true that
\begin{equation}
\begin{split}
\frac{1}{1+\zeta}=\sum_{k=0}^{n-1} (-1)^k \zeta^k + (-1)^n \frac{\zeta^n}{1+\zeta}.
\end{split}
\label{2011-m-ch-dsee13}
\end{equation}
Thus
\begin{equation}
\begin{split}
f(x) =\int_0^\infty \frac{e^{-\frac{\zeta}{x}}}{1+\zeta}d\zeta \\
\qquad =
\int_0^\infty  e^{-\frac{\zeta}{x}}\left(
\sum_{k=0}^{n-1} (-1)^k \zeta^k + (-1)^n \frac{\zeta^n}{1+\zeta}
\right)
d\zeta \\
\qquad =
\sum_{k=0}^{n-1}\int_0^\infty (-1)^k \zeta^k  e^{-\frac{\zeta}{x}}   d\zeta
 +
\int_0^\infty (-1)^n \frac{\zeta^ne^{-\frac{\zeta}{x}}}{1+\zeta}
d\zeta  .
\end{split}
\label{2011-m-ch-dsee14}
\end{equation}
Since
\begin{equation}
k!= \Gamma(k+1)=    \int_0^\infty z^k e^{-z} dz,
\label{2011-m-ch-dsee15}
\end{equation}
one obtains
\begin{equation}
\begin{split}
\int_0^\infty  \zeta^k  e^{-\frac{\zeta}{x}}   d\zeta     \\
\qquad \textrm{[substitution:  }z=\frac{\zeta}{x}, d \zeta =x dz  \textrm{ ]  } \\
\qquad = \int_0^\infty x^{k+1} z^k  e^{-z}   dz
\\
\qquad =  x^{k+1} k! ,
\end{split}
\label{2011-m-ch-dsee16}
\end{equation}
and hence
\begin{equation}
\begin{split}
f(x)  =
\sum_{k=0}^{n-1}\int_0^\infty (-1)^k \zeta^k  e^{-\frac{\zeta}{x}}   d\zeta
 +
\int_0^\infty (-1)^n \frac{\zeta^ne^{-\frac{\zeta}{x}}}{1+\zeta}
d\zeta  \\
\qquad =
\sum_{k=0}^{n-1}  (-1)^k x^{k+1} k!
 +
\int_0^\infty (-1)^n \frac{\zeta^ne^{-\frac{\zeta}{x}}}{1+\zeta}
d\zeta  \\
\qquad =
f_n(x)  +R_n(x),
\end{split}
\label{2011-m-ch-dsee17}
\end{equation}
where $f_n(x)$ represents the partial sum of the power series, and $R_n(x)$ stands for the remainder,
the difference between $f(x)$ and $f_n(x)$.
The absolute of the remainder can be estimated by
\begin{equation}
\begin{split}
\left| R_n(x)\right|
=
\int_0^\infty  \frac{\zeta^n e^{-\frac{\zeta}{x}}}{1+\zeta} d\zeta \\
\qquad \le
\int_0^\infty  \zeta^n e^{-\frac{\zeta}{x}} d\zeta  \\
\qquad = n! x^{n+1}.
\end{split}
\label{2011-m-ch-dsee18}
\end{equation}\eproof
}


\subsection{Borel's resummation method -- ``The Master forbids it''}

In what follows we shall again follow Christiane Rousseau's treatment \cite{rousseau-2004}
and use a resummation method
\marginnote{For more resummation techniques, please see Chapter 16 of
\cite{Kleinert-Schulte}
%http://users.physik.fu-berlin.de/~kleinert/b8/psfiles/16.pdf
}
invented by Borel \cite{Borel1899}
\marginnote{{\em ``The idea that a function could be determined by a divergent asymptotic series was a foreign one to the nineteenth century mind.
Borel, then an unknown young man, discovered that his summation method gave the ``right'' answer for many classical divergent series.
He decided to make a pilgrimage to Stockholm to see Mittag-Leffler, who was the recognized lord of complex analysis.
Mittag-Leffler listened politely to what Borel had to say and then,
 placing his hand upon the complete works by Weierstrass, his teacher, he said in Latin,
``The Master forbids it.''
A tale of Mark Kac,''} quoted (on page 38) by \cite{reed-sim4}}
to obtain the exact convergent solution
(\ref{2011-m-ch-dseefasol})
of the Euler differential equation  (\ref{2011-m-ch-dsee})
from the divergent series solution (\ref{2011-m-ch-dseess}).
First we can rewrite a suitable infinite series by an integral representation,
thereby using the integral representation of the factorial (\ref{2011-m-ch-dsee15})
as follows:
\begin{equation}
\begin{split}
\sum_{j=0}^\infty
a_j
=
\sum_{j=0}^\infty
a_j  \frac{j!}{j!}
=
\sum_{j=0}^\infty
  \frac{a_j}{j!}  j!
\\
=
\sum_{j=0}^\infty
  \frac{a_j}{j!}  \int_0^\infty t^j e^{-t} dt
\stackrel{{\rm B}}{=}
\int_0^\infty \left(\sum_{j=0}^\infty   \frac{a_j t^j}{j!}\right)   e^{-t} dt
.
\end{split}
\label{2012-m-ch-dsborel}
\end{equation}
A series  $\sum_{j=0}^\infty   a_j $
is {\em Borel summable}
\index{Borel summable}
if $\sum_{j=0}^\infty   \frac{a_j t^j}{j!}$ has a non-zero radius of convergence,
if it can be extended along the positive real axis, and if the integral
(\ref{2012-m-ch-dsborel}) is convergent.
This integral is called the
{\em Borel sum}
\index{Borel sum}
of the series.

In the case of the series solution of the Euler differential equation, $a_j = (-1)^j j! x^{j+1}$
[cf. Eq. (\ref{2011-m-ch-dseess})].
Thus,
\begin{equation}
 \sum_{j=0}^\infty   \frac{a_j t^j}{j!}
=
 \sum_{j=0}^\infty   \frac{(-1)^j j! x^{j+1} t^j}{j!}
=
 x\sum_{j=0}^\infty   (-xt)^j =\frac{x}{1+xt},
\label{2012-m-ch-dsborelee}
\end{equation}
and therefore, with the substitionion $xt=\zeta$, $dt= \frac{d\zeta}{x}$
\begin{equation}
 \sum_{j=0}^\infty  (-1)^j j! x^{j+1}
\stackrel{{\rm B}}{=}
\int_0^\infty \sum_{j=0}^\infty   \frac{a_j t^j}{j!}   e^{-t} dt
 =
\int_0^\infty \frac{x}{1+xt}   e^{-t} dt
 =
\int_0^\infty \frac{e^{-\frac{\zeta}{x}}}{1+\zeta}    d\zeta,
\label{2012-m-ch-dsborelee1}
\end{equation}
which is the exact solution (\ref{2011-m-ch-dseefasol})
of the Euler differential equation  (\ref{2011-m-ch-dsee}).

We can also find the Borel sum (which in this case is equal to the Abel sum)
of the Leibniz series (\ref{2009-fiftyfifty-1s})
by
\begin{equation}
\begin{split}
s= \sum_{j=0}^\infty (-1)^j
\stackrel{{\rm B}}{=}
\int_0^\infty \left(\sum_{j=0}^\infty   \frac{(-1)^j t^j}{j!}\right)   e^{-t} dt  \\
=
\int_0^\infty \left(\sum_{j=0}^\infty   \frac{(- t)^j}{j!}\right)   e^{-t} dt
=
\int_0^\infty    e^{-2t} dt  \\
[\textrm{variable substitution } 2t = \zeta, dt = \frac{1}{2} d \zeta ]\\
=
\frac{1}{2}
\int_0^\infty    e^{-\zeta } d\zeta      \\
=
\frac{1}{2}
\left.     \left(-e^{-\zeta }\right) \right|_{\zeta=0}^\infty
=
\frac{1}{2} \left(- e^{-\infty} + e^{-0}\right) = \frac{1}{2}
.
\end{split}
\end{equation}


A similar calculation for $s^2$ defined in Eq.
(\ref{2009-fiftyfifty-1s1})
yields
\begin{equation}
\begin{split}
s^2= \sum_{j=0}^\infty (-1)^{j+1} j = (-1) \sum_{j=1}^\infty (-1)^j j
\stackrel{{\rm B}}{=}
-\int_0^\infty \left(\sum_{j=1}^\infty   \frac{(-1)^j j t^j}{j!}\right)   e^{-t} dt  \\
=
-\int_0^\infty \left(\sum_{j=1}^\infty   \frac{(-t)^j}{(j-1)!}\right)   e^{-t} dt  \\
=
-\int_0^\infty \left(\sum_{j=0}^\infty   \frac{(-t)^{j+1}}{j!}\right)   e^{-t} dt  \\
=
-\int_0^\infty (-t) \left(\sum_{j=0}^\infty   \frac{(-t)^j}{j!}\right)   e^{-t} dt  \\
=
-\int_0^\infty  (-t)  e^{-2t} dt  \\
[\textrm{variable substitution } 2t = \zeta, dt = \frac{1}{2} d \zeta ]\\
=
\frac{1}{4}
\int_0^\infty  \zeta  e^{-\zeta } d\zeta      \\
=
\frac{1}{4}
\Gamma (2)
=
\frac{1}{4} 1! = \frac{1}{4}
,
\end{split}
\end{equation}
which is again equal to the Abel sum.
\index{Abel sum}



\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright
%\decofourleft
%\aldine X \decoone c
 \floweroneright
% \aldineleft ]
% \decosix
%\leafleft
% \aldineright  w  \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
%\decofourright
% \floweroneleft
}
\end{center}

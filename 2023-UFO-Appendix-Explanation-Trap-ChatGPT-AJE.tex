%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Explanation trap through conceptual and theoretical overreach}
\label{2023-UFO-Appendix-Explanation-Trap} % Always give a unique label
\label{2023-UFO-part-Perception-flight-characteristics-to}
\index{overreach}
\index{explanation trap}
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head


%\abstract*{When a sentient being encounters a phenomenon that is far beyond its current scientific understanding, it cannot fully grasp it. If the being still tries to explain the phenomenon, it will fall into a state of overreach, resulting in a situation known as an ``explanation trap."}


%\abstract{When a sentient being encounters a phenomenon that is far beyond its current scientific understanding, it cannot fully grasp it. If the being still tries to explain the phenomenon, it will fall into a state of overreach, resulting in a situation known as an ``explanation trap."}




\section{The concept of the progression of science according to Kuhn and Lakatos}

Thomas Samuel Kuhn argued~\cite{kuhn} that science does not have uniform development but instead goes through phases of ``normal'' and ``revolutionary.'' Normal science is focused on ``puzzle-solving'' with familiar methods and problems, leading to the accumulation of puzzle solutions. Revolutionary science, on the other hand, involves a revision of current beliefs, hard-core assumptions, and practices, leading to a ``Kuhn-loss'' where previous achievements may no longer be valid~\cite{sep-thomas-kuhn}.

Imre Lakatos proposed modifying Kuhn's sociopsychological tools with logico-methodological tools. In Lakatos' view, a research program, not isolated theories, is the unit of appraisal. Each theory within a program shares the same beliefs and hard-core assumptions and is surrounded by auxiliary hypotheses. If a theory is refuted, criticism is first directed at the auxiliary hypotheses and not the hard core, whose proponents tend to protect and immunize. Modifications are guided by the hard-core's implicit heuristic principles~\cite{sep-lakatos}.

Lakatos~\cite{lakatosch,lakatos_1978} would call a research program ``progressive'' if the associated new theory resolves empirical anomalies, is independently testable (falsifiable), and progresses empirically if new predictions are confirmed: It expands the empirical realm to previously unknown physical phenomena and technology.

One could consider Lakatos' observations as an extension and modification of Kuhn's view, with an emphasis on the importance of using logico-methodological terms, particularly regarding the auxiliary hypotheses ``protecting the hard core.'' Additionally, Lakatos emphasized that the hegemony of beliefs, hard-core assumptions, and practices is not as complete as Kuhn believed: there were always competing programs or paradigms.


\section{Succession of semantically incoherent scientific research programs}

Both Kuhn and Lakatos (as well as to some degree Paul Feyerabend~\cite{feyerabend,fey-philpapers1,fey-philpapers2}) might agree that
\begin{enumerate}
\item
for prolonged periods, there exist beliefs, hard-core assumptions, and practices that constitute a dominant scientific programme;
\item
any such dominant scientific program
\begin{enumerate}
\item
consists of core semantical concepts that translate into theoretical, syntactic formalizations;
\item
eventually will be overturned by another scientific program;
\item
paradigms are incommensurable~\cite{sep-incommensurability}:
the semantical concepts of competing or successive scientific programs are un(cor)related (whereas their theoretical, syntactic formalizations might, in some approximations, coincide).
\end{enumerate}
\end{enumerate}

Therefore, we are effectively dealing with a temporal succession of scientific research programs with entirely distinct, incommensurable and mutually inconsistent
concepts. No ``semantic convergence'' can be recognized.
Moreover, it is possible that this succession is nonunique, and different researchers or groups of investigators or civilizations may have pursued various strands of concepts and research programs.

\section{Conceptual and theoretical overreach}

I am ready to formulate my main hypothesis, which I call ``conceptual and theoretical overreach:''
\begin{svgraybox}
An agent or individual who is pursuing a particular research program may not be capable of comprehending or reconstructing phenomena and technology associated with a research program that is more than one step ahead. Therefore, the attempt to understand advanced technology from a civilization that is more than one scientific revolution ahead is bound to fail.
\end{svgraybox}

Indeed, it might not be totally unreasonable to assume that consulting the contemporary so-called ``experts''---such as
theoretical physicists or rocket scientists---might have an adverse effect on some preliminary, tentative,
and incomplete understanding of the phenomena. These ``experts'' are biased and ``ego-invested''
toward their respective field of expertise, leading them to emphasize their current thinking.
In Reich's Segmental Armouring Theory terms, the respective and respected experts sometimes carry and apply their current expertise like a vendor's tray, surrounded by it like an impenetrable armor of alleged wisdom.

In most cases, involving very advanced technology or the evaluation of what might be a progressive research program,
the thinking and belief system of the ``contemporary experts'' will be inappropriate, if not outrightly distracting and
wrong---a waste of opportunity costs. To illustrate, imagine asking a shaman medicine man of Borneo to explain a World War II (WWII) airplane flying over his head.

\section{Explanation trap}

The understanding of the motion and propulsion of UFOs may remain elusive due to the limitations of our current means and concepts. Imposing an explanation without proper understanding can lead to the entrenchment of erroneous models and ideas, despite a general lack of comprehension. This circumstance will be called, for a lack of better terms, an ``explanation trap.''

On the scale of individuals, the ``mind projection fallacy'' is a related concept introduced by physicist and
Bayesian statistician Edwin Thompson Jaynes~\cite{jaynes-89}.
It refers to the tendency of individuals to project their own subjective mental processes onto the ``outside objective''
(or, more nominalistically:
one's conceptions or labels referring to one's concepts and categories of external)
world and to assume that others have similar mental processes.

Jaynes argued that this fallacy can lead to misunderstandings and incorrect conclusions about the nature of reality and the actions of others. For example, individuals may assume that other people see the world in the same way that they do, or that objects have inherent properties when in reality, these are just interpretations that are dependent on their individual perception and experiences.

Jaynes believed that this fallacy is a result of individuals not realizing that their own mental processes are subjective, and therefore, their perceptions of the world are also subjective. This fallacy can also contribute to the illusion of determinism, where individuals believe that events and outcomes are determined by some objective, external factor, when in reality, the outcomes are dependent on the subject's interpretation and understanding.

To avoid the explanation trap and the mind projection fallacy, it is suggested to adopt an analytical approach called ``evenly suspended attention,'' inspired by Freud~\cite{Freud-itp,Freud-itpe}. This approach involves observing the phenomenon without preconceived ideas or biases and allowing the observations to settle in, even without immediate explanations or category formations.

This also includes, in particular, recognition and estimation of one's ignorance:
It is important to recognize and acknowledge when we do not know something.

\section{Examples}


Let me present some examples or illustrations of the overreach principle.


% https://www.thoughtco.com/what-are-clarkes-laws-2699067

In the second edition of his book ``Profiles of the Future: An Inquiry into the Limits of the Possible,'' Arthur C. Clarke formulated Clarke's third law~\cite{Clarke2000Jan}:
``Any sufficiently advanced technology is indistinguishable from magic.''
This can be seen as an early recognition of conceptual overreach: some technology may be so advanced
that certain recipients, if left on their own without outside explanations, are hopelessly overwhelmed.

\subsection{Stone-age tribe fleetingly encountering an airplane}

For instance, imagine a Melanesian indigenous tribe fleetingly encountering an airplane during WWII.
It is likely that tribe members would seek guidance first from their elders, authorities, or shaman,
who would all be completely unable to grasp what was happening.
If those elders, authorities, or shaman were to deny the existence of phenomena such as airplanes,
this could easily trigger the formation of a ``cargo cult'' as a reaction.

\subsection{Isaac Newton trying to comprehend the Global Positioning System}

For a historic example,
Lakatos mentions conceptual variations of the theory of gravitation~\cite{lakatosch}:
\begin{enumerate}
\item
Ptolemy's idea of epicicles was geometric.
\item
Newton's concept of gravitational force was mechanical.
\item
Einstein's idea of curved space-time was geometric.
\end{enumerate}

Although all ``advanced'' theories contain each other's formalizations and predictions at a certain resolution and with finite approximations, there is no semantic consistency. The future of gravity theories is uncertain \cite{isqg-cc, isqg}.
Currently, the study of gravitation (relativity theory) and quantization, particularly quantum field theory, are treated as separate subjects. However, once these two areas are combined, the resulting ``unified'' theory may bear little resemblance to either starting point.

\subsection{Operation Overcast/Paperclip}

Operation Overcast/Paperclip was a US effort to acquire advanced technology and knowledge from defeated Nazi Germany after WWII.
The operation was successful in integrating technological advances that were within the reach of contemporary means and concepts. The Soviet equivalent, Operation Osoaviakhim, aimed to achieve the same goal by recruiting and evacuating German specialists to the Soviet Union. The British and Australian effort was named Operation Matchbox. Additionally, there were similar efforts, such as the Alsos Mission and the Russian Alsos, that focused mainly on acquiring German nuclear technology.
\index{Paperclip}
\index{Overcast}

\subsection{Science overreach framed as cargo cult}

As discussed in Section~\ref{2023-UFO-part-Speculation-cargo-cults-ldl}, which deals with possible contemporary cargo cults,
we, as a civilization, might suffer from an incomprehensible phenomenon in a similar way that indigenous people reflected upon our technology after WWII.



\section{Absorption of alien technology}
\label{2023-UFO-part-Perception-flight-characteristics-aat}



Here is a conjecture: when faced with alien craft and debris, the US military-industrial complex attempted to absorb alien technology,
similar to the German technology of the Nazi era.


\index{ATIC}
\index{Air Technical Intelligence Center}

As noted in Section~\ref{2023-UFO-part-Perception-types-USA-Sign},
ATIC, the Air Technical Intelligence Centre at Wright-Patterson Air Force Base, Ohio, was responsible for handling Project Blue Book investigations~\cite[Chapter~3]{Ruppelt2011May}.
On July 1, 1961, the Air Technical Intelligence Centre (ATIC) became the Foreign Technology Division (FTD).
It has been rebranded several times and is currently called the National Air and Space Intelligence Center (NASIC). I am sure it will soon change its name again.

As speculated in Sections~\ref{2023-UFO-part-Perception-types-USA-baa}
and~\ref{2023-UFO-part-Perception-crash-retreivals-where},
such efforts may have been delegated to what is now referred to as carve-out USAPs mentioned in Section~\ref{2023-UFO-part-Perception-crash-retreivals--cousap}.
This may have resulted in some progress~\cite{Corso1998Jun} if the respective technologies were not ``too advanced,'' but failed in central areas such as propulsion and ``anti-gravity'' or inertia modification.


\section{Anti-gravity might not be a feasible hypothesis}
\label{2023-UFO-part-Perception-flight-characteristics-ag}

I spare the reader with canonical arguments that
anti-gravity goes against our current understanding of physics and gravity.
The laws of physics, as described by Einstein's theory of general relativity,
state that gravity is a property of mass and energy that attracts other masses and energies.
There is currently no experimental evidence that contradicts this theory and no widely accepted alternative
theory that includes the existence of anti-gravity.
Additionally, the concept of anti-gravity raises many theoretical problems and lacks a solid theoretical framework:
it might require some sort of ``negative or inverse curvature,''
maybe also associated with ``negative mass or energy (density).''
Until there is experimental evidence or a well-developed theoretical framework for anti-gravity,
it remains an unsupported and implausible hypothesis.

I also do not want to discuss in detail special solutions of Einstein's field equations
allowing faster-than-light motion,
often referred to as the Alcubierre drive~\cite{0264-9381-11-5-001,Lentz2021,Fell_2021}.\index{Alcubierre drive}
Other solutions of Einstein's field equations require ``exotic matter'' resulting in
wormholes (Einstein?Rosen bridges), which are hypothetical structures connecting disparate points in spacetime~\cite{Misner-Wheeler-Thorne}.


I would like to emphasize that anti-gravity may not solve the reported UFO flight characteristic category of sudden changes in direction.
It would simply introduce a counteracting gravitational force that could, for instance, counteract the pull toward the center of the Earth,
for example, but would not address the sudden changes in direction.
This would enable a craft to hover over some point steadily or slowly cruise at a constant altitude.
However, it would not explain sudden, discontinuous changes in motion such as directional changes or changes in velocity:
even if we had ``anti-gravity propulsion,'' any such propulsion would still adhere to Newton's second law of motion
stating that the acceleration of an object is directly proportional to the net force applied to it and inversely proportional to its mass.
In simple terms, this law states that the more force applied to an object, the greater its acceleration will be.
Moreover, the greater an object's mass is, the less its acceleration will be for a given force.
Conversely, the smaller an object's mass is, the greater its acceleration will be for a given force.

Thus, even with access to anti-gravity propulsion, producing discontinuous changes in motion would require either applying an excessive amount of force in the
desired direction of motion or reducing the craft's mass or inertia in that direction.
This holds true relative to the validity of Newton's second law of motion.
(These statements are relative to the validity of Newton's second law of motion.)

Therefore, I suggest focusing on changing inertia, or mass, instead of concentrating on anti-gravity. To accomplish this, we must strive to comprehend mass, or at the very least, have a grasp of what we currently do not understand about inertia and mass.


\section{Can inertia be changed?}
\label{2023-UFO-part-Perception-flight-characteristics-ci}


\subsection{Variety of inertial mass: bare masses and field energy}

With the caveats in mind that I may suffer some overreach, I would like to suggest that
maybe we should direct our attention to inertial motion, or to inertia in general.

Inertia is a property of an object by which it continues in its state of rest or uniform motion in a straight line,
unless acted upon by an external force. Any object would not know how to move without inertia. Operationally, it is a measure of an object's resistance to change its velocity when a force is applied. Going forward, the terms ``inertia'' and ``mass'' or ``inertial mass'' will be used interchangeably, understanding that mass is a
(linear) measure of inertia. This holds true unless a distinction between ``gravitational mass'' and ``inertial mass'' is necessary
and the equivalence principle of relativity theory---our current theory of gravitation is based upon---needs to be reviewed.


The concept of inertia was first described by Isaac Newton in his laws of motion:
Newton's second law states that the acceleration $a$ of an object is proportional to the force $F$
applied to it and inversely proportional to its inertial mass $m$; that is, $a = F / m$.
Hence, an object without inertial mass would ``not know how to'' experience acceleration, as dividing by zero is undefined.
An object with very little inertial mass would experience very large acceleration, as a very small denominator (and a
given  numerator) results is a very large number.

Furthermore, in Newton's law of gravity, the gravitational mass of an object determines the strength of its gravitational force.
It is not possible to create an object that has a gravitational force without gravitational mass,
Therefore, eliminating mass would result in the elimination of gravity as well~\cite{Wilczek2003}.

Therefore, manipulation or modulation of inertia, which is a measure of an object's resistance to change its velocity and direction,
will immediately impact the object's flight characteristics.
If, for instance, it is possible to change the inertial mass of some object to almost zero, then a very tiny force renders huge
changes in the object's motion: it will be able to ``zigzag'' without much effort, as its inertial mass is almost zero.

An immediate counterargument to such a possibility is built into relativity theory's assumption of equivalence between inertial and gravitational mass:
inertial masses are ``nonnegotiable.'' If they were, then the equivalence principle would not hold,
and one could not postulate a single metric ``for everybody.''

At the end of the 19th century, physicists such as Henri Poincar\'e, Max Abraham, Fritz Hasen\"ohrl,
and Albert Einstein realized that the electromagnetic field must be included in the calculation of inertial mass---indeed, that it is a form thereof~\cite{Boughn2011Aug}.
They assumed linear proportionality between mass and electromagnetic field energy, resulting in the equation $m = k E / c^2$, where $c$ is the speed of light in a vacuum.
Einstein was the first to calculate the correct proportionality factor, which was $k=1$~\cite{Einstein1905Jan,Einstein2005Apr}.

Currently, the conversion of field energy into mass is regularly demonstrated in particle accelerators.
In high-energy electron-positron collisions, many particles often emerge from the event, and the total mass of these particles can be thousands of times greater than the mass of the original electron and positron.
This shows that mass can be created from energy in a physical process~\cite{Wilczek2003}.

Therefore, there is a component of mass whose origin is in the field energy,
and another component that is not.
We shall call this latter component the ``bare mass.'' Therefore, a hypothetical bare mass is a term used in quantum field theory (QFT)
for the mass a particle would have if it were not influenced by interactions with other particles or fields.


In reality, particles always interact with other particles and fields---otherwise
they would permanently remain hidden or cloaked relative to objects made from interacting matter.
Those interactions can cause the masses to shift from their bare mass value.
This shift in mass is known as a ``dressed mass,''
and it is the mass that we observe in experiments.
The dressed mass of a particle can be larger or smaller than its bare mass,
depending on the strength and type of interactions it experiences.

In QFT, the bare masses of particles are used as input parameters in theories and models,
and the dressed masses are calculated from the bare masses and the interactions between particles.
The concept of bare masses is important for understanding the behavior of particles in QFT
and for making predictions about particle interactions and properties.

\subsection{Dressed mass from field energy of the strong and electromagnetic interactions}

So how does mass from fields come about in our present semantic conception (aka narrative)?
In answering this question, we will closely follow Frank Wilczek's accounts~\cite{Wilczek2003,Wilczek_2012}.
We shall see that, according to the contemporary hard-core assumptions, there are two origins of mass: one from fields that are conceptualized by
particles such as photons and gluons, and one from the ``Higgs mechanism.''


The mass of ordinary matter comes from atoms, which are primarily composed of nuclei made of protons and neutrons.
The electrons contribute little mass to atoms.
It is now well established that protons and neutrons are made of quarks and gluons,
meaning that the majority of matter's mass can be traced back to quarks and gluons.

Electrons occupy the shells of atoms and are charged particles that interact with the charged particles in the nucleus via electromagnetism.
The quantum theory of electromagnetism, called quantum electrodynamics (QED),
describes these interactions between charged particles by the exchange of virtual photons.

As stated earlier, most of the mass of matter comes from the heavy particles in the nucleus,
the protons and neutrons, rather than from the electromagnetic field.
These particles are made of quarks, and the stability of the entire nucleon,
made of neutral neutrons and positive protons, is maintained by a force much stronger than electrostatic repulsion,
with a range on the order of the nucleus diameter.
The quantum theory of this force is called quantum chromodynamics (QCD).

At its core, QCD is similar to QED but on a larger scale both in the number of field quanta exchanges and in the strength of the interaction.
Therefore, we refer to QCD as a formalization of the ``strong interaction.''
In QED, there is only one type of charge, electric charge, while QCD has three types of charge referred to as ``colors.''
Instead of one photon in QED, QCD has eight color gluons that respond to different color charges or convert one into another.

According to QCD, the confinement force between quarks is attractive and increases as the distance between them increases.
This confinement force becomes extremely strong as the quarks are separated by large distances,
and is responsible for confining quarks within hadrons (such as protons and neutrons).

In other words, as quarks are separated by a large distance,
the attractive confinement force between them becomes very strong
(relative to possible electromagnetic repulsion between same polarities),
preventing them from moving away from each other.
This confinement gives hadrons their stability and structure,
and is a key aspect of QCD and our understanding of the strong nuclear force.

QCT is about a force that is much stronger than QED at sufficiently large distances (of the tiny scale of the nucleus),
and therefore has to account for most of the field-related mass of matter.
The respective QED calculations of hadron masses,
including the masses of protons and neutrons, using both QCD and QED should settle ``most'' of the nucleon masses, and
 tell us the origin of ``most'' masses.
(They cannot settle electron, muon, quark or neutrino masses though; or why the photon is massless.)

Our computations construct massive particles using building blocks-quarks
and gluons-that, even if these particles themselves are massless.
How can that be?

Quantum mechanics dictates that when a bare color charge is placed in empty space,
it will generate a cloud of virtual color gluons.
These color gluon fields themselves carry color charge and produce additional soft radiation.
This creates a self-sustaining enhancement, resulting in the growth of a large color cloud around the small color charge.
(I shall not go into detail why asymptotic freedom and QED limit these ``avalanche-like'' processes.)

In any case, starting from hypothetical zero ``bare masses'' of all particles involved---in particular, the electron and its respective anti-particle
called positron, and quarks---suggests that all masses come from the electro- and chromodynamical fields encoded by photon and gluon exchanges.
This is the first source of energy and thus inertial mass.

We may subsume these concepts of an ``emergent'' inertial mass by fields as follows~\cite{Wilczek_2012}:

Classical mechanics views mass as an inherent and fundamental property of matter. However, in modern physics, there is no equivalent fundamental concept, and mass is seen as an emergent property
that approximately arises from elementary ``charged'' constituents.

The main contributors to the mass of standard matter are color gluons and quarks, as well as electrons and photons, which  a much smaller impact.
All other factors---including bare masses covered next---are considered insignificant.

For completeness, it is worth mentioning that the contribution to the dressed mass from the weak interaction---responsible, for instance,
for the beta decay of neutrons into protons, electrons and electron antineutrinos---in the standard model~\cite{cottingham_greenwood_2007}
of quantum field theory (conceptualized by an exchange of W and Z bosons) is small.

\subsection{Dressed mass by the Higgs mechanism}

The understanding of the origin of mass is by no means complete, or---at least to me---satisfactory.
We have achieved some tentative understanding
of the origin of ``most'' of the mass of ordinary matter, but not of all of it. The values
of the   W and Z particles, the leptons (electron, muon, tau), and the quark masses, in particular, remain deeply mysterious even in our most advanced
speculations about unification and string theory.

In my opinion, the Brout-Englert-Higgs (BEH) mechanism has been an ad hoc assumption---a deus ex machina of sorts---that has been suggested to resolve those issues to some degree.
Through BEH, a particle can acquire a mass that is not associated with electroweak and strong field energy.

The Higgs field is supposed to be all-pervasive, permeating all of space.
As particles interact with the Higgs field, this interaction causes mass or inertia;
analogous to a ball in molasses:
the ball acquires inertia or  mass---resistance to (change of) movement---as the molasse slows it down.
In that way, an additional interaction (per massive particle)
can ``explain'' inertia not residing in the electroweak and strong interactions (and
their respective fields).

Particles that interact more with the Higgs field have a greater mass,
while particles such as photons that do not interact with it have no mass.
The Higgs field is represented by a particle known as the Higgs boson
which was discovered in 2012 by the ATLAS~\cite{ATLASLHCHIGGS2012} and CMS~\cite{CMSLHCHIGGS2012}
experiments using CERN's Large Hadron Collider (LHC)~\cite{ATLAS_Higgs-2022,CMS_Higgs-2022,ATLAS-CERN}.
Despite those additional interactions---one per particle acquiring mass through interacting with it---the Higgs boson
has been modeled as ``characterless,'' with zero spin, no electric charge and no strong force interaction.




\subsection{Changes in inertia by radiative corrections and charged particles between conducting plates}

It has been suggested that the Casimir effect has an influence on physical parameters such as the velocity of light or mass (i.e., inertia). This is because parallel plates impose boundary constraints on the cloud of virtual photons, resulting in a discretization of field modes perpendicular to the plates. These, in turn, affect the radiative corrections. Calculations have shown that this effect leads to a decrease in observed masses and anomalous magnetic moments~\cite{1995-mass,PhysRevD.34.1429}, as well as an increase in the velocity of light in a vacuum~\cite{scharnhorst,0305-4470-26-8-024,Scharnhorst-1998,milonni,Chiao:02}.


\subsection{Other suggestions involving zero point fluctuations of quantized fields}

There are varieties of suggestions for modifying the quantum mechanical vacuum state---in particular, its refractive index through varying permittivity or permeability---through radiative corrections or gravity from zero-point-fluctuations~\cite{dirac-aether, Sakharov_2000, Rueda_1998, Puthoff_1989, Haisch_2001, Bombelli_2005, Davis_2006, Robertson_2008}.

\subsection{Mach's principle as an alternative explanation for inertia}

For the sake of completeness, I mention the hypothesis that the inertial forces experienced by a body in nonuniform motion are determined by the quantity and distribution of matter in the universe that interacts with that body. This is often referred to as ``Mach's principle''~\cite{Barbour1995Aug}.

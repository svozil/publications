\chapter{Tensors}
\label{ch:t}


What follows is a ``corollary,'' or rather an expansion and extension,
of what has been presented in the previous chapter; in particular, with regards to dual vector spaces
(page
\pageref{2011-m-dvs}),
and the tensor product
(page
\pageref{2011-m-tensorp}).

\section{Notation}

Let us consider the vector space ${\Bbb R}^n$ of dimension $n$;
a basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n\}$ consisting of
$n$ basis vectors ${\bf e}_i$,
and $k$ arbitrary vectors
${\bf x}_1,{\bf x}_2,\ldots ,{\bf x}_k\in {\Bbb R}^n$;
the vector ${\bf x}_i$ having the vector components
$X^i_1,X^i_2,\ldots ,X^i_k\in {\Bbb R}$.


Please note again that,
just like any tensor (field), the tensor product ${\bf z} = {\bf x} \otimes {\bf y}$     has three equivalent  representations:
\begin{itemize}
\item[(i)]
as the scalar coordinates $X^iY^j$ with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
this form is often used in the theory of (general) relativity;
\item[(ii)]
as the quasi-matrix $z^{ij}  =X^iY^j$, whose components $z^{ij}$ are
defined with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
this form is often used in classical (as compared to quantum) mechanics and electrodynamics;
\item[(iii)]
as a quasi-vector or ``flattened matrix'' defined by the Kronecker product
${\bf z} = (X^1  {\bf y}, X^2  {\bf y}, \ldots , X^n  {\bf y})=
(X^1  { Y}^1, \ldots ,X^1  { Y}^n, \ldots  ,X^n  { Y}^1, \ldots , X^n  { Y}^n)
$. Again, the scalar coordinates $X^iY^j$ are defined
with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded.
\index{Kronecker product}
This latter form is often used in (few-partite) quantum mechanics.
\end{itemize}
In all three cases, the pairs $X^iY^j$  are properly represented by distinct mathematical entities.


{\em Tensor fields} define tensors in every point of ${\Bbb R}^n$ separately.
In general, with respect to a particular basis, the components of a tensor field
depend on the coordinates.


We adopt Einstein's summation convention to sum over equal indices
(a pair with a superscript and a subscript).
Sometimes, sums are written out explicitly.


In what follows, the notations
``$x\cdot y$'',
``$(x,y)$'' and
``$\langle x\mid y\rangle $'' will be used synonymously for the {\em
scalar product}
or
{\em inner product}.
\index{scalar product}
\index{inner product}
Note, however, that the ``dot notation $x\cdot y$''
may be a little bit misleading; for example, in the case of the ``pseudo-Euclidean'' metric
represented by the matrix
 ${\rm diag}(+,+,+,\cdots ,+,-)$, it is no more the standard Euclidean dot product
${\rm diag}(+,+,+,\cdots ,+,+)$.

For a more systematic treatment, see for instance Klingbeil's
or Dirschmid's introductions
\cite{Klingbeil,Dirschmid}.


\section{Multilinear form}

A {\em multilinear form}
\begin{equation}
\alpha :{ \frak V }^k \mapsto {\Bbb R} \text{ or } {\Bbb C}
\end{equation}
is a map from (multiple) arguments ${\bf x}_i$ which are elements of some vector space  $\frak V$
into some scalars in ${\Bbb R}$ or ${\Bbb C}$,  satisfying
\begin{equation}
\begin{split}
\alpha ( {\bf x}_1,{\bf x}_2,\ldots , A {\bf y}+ B {\bf z}, \ldots ,{\bf x}_k)
=
A\alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf y}, \ldots ,{\bf x}_k)   \\
   \qquad +
B\alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf z}, \ldots ,{\bf x}_k)
\end{split}
\end{equation}
for every one of its (multi-)arguments.

In what follows we shall concentrate on {\em real-valued} multilinear forms which map
$k$ vectors in
${\Bbb R}^n$
into
${\Bbb R}$.


\section{Covariant tensors}
Let  ${\bf x}_i =\sum_{j_i=1}^n X^{j_i}_i {\bf e}_{j_i} =X^{j_i}_i {\bf e}_{j_i}$
be some vector in (i.e., some element of) an $n$--dimensional vector space ${\frak V}$ labelled by an index $i$.
A tensor of rank $k$
\index{rank of tensor}
\index{tensor rank}
\index{tensor type}
\begin{equation}
\alpha:{ \frak V }^k \mapsto {\Bbb R}
\end{equation}
is a multilinear form
\begin{equation}
\alpha ( {\bf x}_1,{\bf x}_2,\ldots ,{\bf x}_k)=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_1}_1 X^{i_2}_2\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k}).
\end{equation}
The
\begin{equation}
A_{{i_1}{i_2}\cdots {i_k}}
\stackrel{{\tiny \textrm{ def }}}{=}
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k})
\end{equation}
 are the
{\em components} or
{\em coordinates}
of the tensor $\alpha $ with respect to the basis
${\mathfrak B}$.

Note that a tensor of type (or rank) $k$ in $n$-dimensional vector space has $n^k$  coordinates.

{\color{OliveGreen}
\bproof
To prove that tensors are multilinear forms, insert
\begin{equation}
\begin{split}
 \alpha ( {\bf x}_1,{\bf x}_2,\ldots , A{\bf x}^1_j+B{\bf x}_j^2,\ldots ,{\bf x}_k)
\\
=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  [A(X^1)^{i_j}_j+B(X^2)^{i_j}_j]\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
= A
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  (X^1)^{i_j}_j\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
\quad +
B
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  (X^2)^{i_j}_j\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
=
A \alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf x}^1_j,\ldots ,{\bf x}_k)+
B \alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf x}_j^2,\ldots ,{\bf x}_k)\nonumber
\end{split}
\end{equation}
\eproof
}

\subsection{Basis transformations}
Let
${\mathfrak B}$
and
${\mathfrak B'}$
be two arbitrary bases of
${\Bbb R}^n$.
Then every vector ${\bf e}'_i$ of
${\mathfrak B'}$
can be represented as linear combination of basis vectors from
${\mathfrak B}$:
\begin{equation}
{\bf e}'_i=\sum_{j=1}^n {a_i}^j {\bf e}_j, \qquad i=1,\ldots , n  .
\label{2001-mu-tensors}
\end{equation}
%Formally, one may treat  ${\bf e}'_i$ and ${\bf e}_i$ as scalar variables $e'_i$ and $e_j$, respectively; such that ${a_i}^j ={\partial { e}'_i \over \partial { e}_j}$.


Consider an arbitrary vector ${\bf x} \in {\Bbb R}^n$
with components $X^i$ with respect to the basis
${\mathfrak B}$
and   ${X'}^i$  with respect to the basis
${\mathfrak B'}$:
\begin{equation}
{\bf x}
=\sum_{i=1}^n X^i {\bf e}_i
=\sum_{i=1}^n {X'}^i {\bf e}'_i
.
\end{equation}
Insertion into (\ref{2001-mu-tensors}) yields
\begin{equation}
\begin{split}
{\bf x}
=\sum_{i=1}^n X^i {\bf e}_i
=\sum_{i=1}^n {X'}^i {\bf e}'_i
=\sum_{i=1}^n {X'}^i \sum_{j=1}^n {a_i}^j {\bf e}_j\\
=
\sum_{i=1}^n\left[\sum_{j=1}^n {a_i}^j{X'}^i \right] {\bf e}_j
=
\sum_{j=1}^n\left[ \sum_{i=1}^n {a_i}^j{X'}^i \right] {\bf e}_j
=
\sum_{i=1}^n\left[ \sum_{j=1}^n {a_j}^i{X'}^j \right] {\bf e}_i
.
\end{split}
\end{equation}
A comparison of coefficient
(and a renaming of the
indices $i \leftrightarrow j$)
yields the transformation laws of vector components
\begin{equation}
X^j   = \sum_{i=1}^n {a_i}^j{X'}^i.
\label{2012-m-ch-di-choic}
\end{equation}
The matrix $a=\{{a_i}^j\}$ is called the {\em transformation matrix}.
In terms of the coordinates $X^j$, it can be expressed as
\begin{equation}
{a_i}^j =\frac{  X^j }{  X'^i}  ,
\label{2001-mu-tensor-tl1}
\end{equation}
assuming that the coordinate transformations are linear.
If the basis transformations involve nonlinear coordinate changes -- such as from the
Cartesian to the polar or spherical coordinates discussed later -- we have to employ
\begin{equation}
dX^j   = \sum_{i=1}^n {a_i}^j \,d{X'}^i  ,
\label{2012-m-ch-di-choic11}
\end{equation}
as well as
\begin{equation}
{a_i}^j ={\partial X^j \over \partial X'^i}   .
\label{2001-mu-tensor-tl11}
\end{equation}


A similar argument using
\begin{equation}
{\bf e}_i=\sum_{j=1}^n {{a'}_i}^j {\bf e}'_j, \qquad i=1,\ldots , n
\label{2012-m-ch-tlcbv}
\end{equation}
yields the inverse transformation laws
\begin{equation}
{X'}^j   = \sum_{i=1}^n {{a'}_i}^j{X}^i.
\end{equation}
%Again, formally, we may treat  ${\bf e}'_i$ and ${\bf e}_i$  as scalar variables $e'_i$ and $e_j$, respectively; such that ${{a'}_i}^j ={\partial {e}_i \over \partial { e}'_j}$.
Thereby,
\begin{equation}
{\bf e}_i=\sum_{j=1}^n {{a'}_i}^j {\bf e}'_j
=\sum_{j=1}^n {{a'}_i}^j \sum_{k=1}^n {a_j}^k {\bf e}_k
=\sum_{j=1}^n \sum_{k=1}^n [{{a'}_i}^j {a_j}^k] {\bf e}_k,
\end{equation}
which, due to the linear independence of the basis vectors ${\bf e}_i$ of ${\mathfrak B}$,
is only satisfied if
\begin{equation}
{{a'}_i}^j {a_j}^k =\delta_i^k
\qquad
{\rm or}
\qquad
a'a={\Bbb I}.
\end{equation}
That is, $a'$ is the inverse matrix of $a$.
In terms of the coordinates $X^j$, it can be expressed as
[see also the {\em Jacobian matrix}
\index{Jacobian matrix}
$
J_{ij}$ defined in Eq.~\ref{2013-m-t-jm}]
\begin{equation}
{{a'}_i}^j =\frac{ {X'}^j }{   X^i}
\label{2001-mu-tensor-tl2}
\end{equation}
for linear   coordinate transformations and
\begin{equation}
d{X'}^j   = \sum_{i=1}^n {{a'}_i}^j\, d{X}^i,
\end{equation}
as well as
\begin{equation}
{{a'}_i}^j ={\partial {X'}^j \over \partial X^i}
\label{2001-mu-tensor-tl2nl}
\end{equation}
else.

\subsection{Transformation of tensor components}

Because of multilinearity  and by insertion into
(\ref{2001-mu-tensors}),
\begin{eqnarray}
&&\alpha ( {\bf e}'_{j_1},{\bf e}'_{j_2},\ldots ,{\bf e}'_{j_k})=
\alpha \left(
\sum_{i_1=1}^n {a_{j_1}}^{i_1} {\bf e}_{i_1},
\sum_{i_2=1}^n {a_{j_2}}^{i_2} {\bf e}_{i_2},
\ldots ,
\sum_{i_k=1}^n {a_{j_k}}^{i_k} {\bf e}_{i_k}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots {a_{j_k}}^{i_k} \alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k})
\label{2012-m-ch-tensor-etotc1}
\end{eqnarray}
or
\begin{equation}
A'_{{j_1}{j_2}\cdots {j_k}}=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots {a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_k}.
\label{2011-m-tvtcov}
\end{equation}


\section{Contravariant tensors}

\subsection{Definition of contravariant basis}

Consider again a covariant basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n\}$ consisting of
$n$ basis vectors ${\bf e}_i$.
Just as on page \pageref{2011-m-Dualbasis} earlier, we shall define a {\em contravariant} basis
${\mathfrak B^\ast}=\{{\bf e}^1,{\bf e}^2,\ldots ,{\bf e}^n\}$ consisting of
$n$ basis vectors ${\bf e}^i$
by the requirement that the scalar product obeys
\begin{equation}
\delta_i^j =  {\bf e}^i\cdot{\bf e}_j\equiv ({\bf e}^i,{\bf e}_j)\equiv \langle {\bf e}^i\mid {\bf e}_j\rangle
 =\left\{
 \begin{array}{l}
1 \mbox{ if } i=j \\
0 \mbox{ if } i\neq j  \\
\end{array}
 \right. .
\label{2001-mu-tensors0}
\end{equation}



To distinguish elements of the two bases, the covariant vectors are denoted by {\em subscripts},
whereas the contravariant vectors are denoted by {\em superscripts}.
The last terms $ {\bf e}^i\cdot{\bf e}_j\equiv ({\bf e}^i,{\bf e}_j)\equiv \langle {\bf e}^i\mid {\bf e}_j\rangle  $
recall different notations of the scalar product.

Again, note that (the coordinates of) the dual basis vectors of an orthonormal basis can be coded identically
as  (the coordinates of) the original basis vectors; that is,
in this case,
(the coordinates of) the dual basis vectors are just rearranged as the transposed form of the original basis vectors.


The entire tensor formalism developed so far can be transferred and applied to define {\em contravariant} tensors
as multinear forms
\begin{equation}
\beta:{ \frak V^\ast }^k \mapsto {\Bbb R}
\end{equation}
by
\begin{equation}
\beta ( {\bf x}^1,{\bf x}^2,\ldots ,{\bf x}^k)=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
\Xi_{i_1}^1 \Xi_{i_2}^2\ldots \Xi_{i_k}^k
\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k}).
\end{equation}
The
\begin{equation}
B^{{i_1}{i_2}\cdots {i_k}}=\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k})
\label{2011-m-tvtcontrav}
\end{equation}
 are the
{\em components} of the contravariant tensor $\beta $ with respect to the basis
${\mathfrak B}^\ast$.


More generally,
suppose ${\frak V}$ is an $n$-dimensional vector space, and
${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is a basis of  ${\frak V}$;
if $g_{ij}$ is the {\em metric tensor},
\index{metric tensor}
the dual basis is defined by
\begin{equation}
g(f_i^*, f_j)=g( f^i,f_j)={\delta^i}_{j},
\end{equation}
where again  ${\delta^i}_{j}$    is Kronecker delta function, which is defined
\index{Kronecker delta function}
\begin{equation}
\delta_{ij} =\begin{cases}
0  &\text{ for }i\neq j , \\
1  &\text{ for }i = j.
\end{cases}
\end{equation}
regardless of the order of indices, and regardless of whether these indices represent covariance and contravariance.

\subsection{Connection between the transformation of covariant and contravariant entities}

Because of linearity, we can make the formal {\it Ansatz}
\begin{equation}
{{\bf e}'}^j=\sum_i{b_i}^j{\bf e}^i,
\end{equation}
where $\left[{b_i}^j\right] = b$ is
the transformation matrix associated with the contravariant basis.
How is $b$ related to $a$,
the transformation matrix associated with the covariant basis?

By exploiting (\ref{2001-mu-tensors0}) one can find the connection between
the transformation of covariant and contravariant basis elements and thus
tensor components; that is,
\begin{equation}
\delta_i^j= {{\bf e}'}_i\cdot {{\bf e}'}^j=({a_i}^k{\bf e}_k)\cdot ({b_l}^j{\bf e}^l)={a_i}^k {b_l}^j {\bf e}_k\cdot {\bf e}^l={a_i}^k{b_l}^j \delta_k^l
={a_i}^k {b_k}^j,
\end{equation}
and thus
\begin{equation}
b=a^{-1} =a',\textrm{ and } {{\bf e}'}^j=\sum_i{(a^{-1})_i}^j{\bf e}^i=\sum_i{ a'_i}^j{\bf e}^i.
\label{2012-m-ch-tensor-tocontrav}
\end{equation}
The argument concerning transformations of covariant tensors and components
can be carried through to the contravariant case.
Hence, the contravariant components transform as
\begin{eqnarray}
&&\beta ( {{\bf e}'}^{j_1},{{\bf e}'}^{j_2},\ldots ,{{\bf e}'}^{j_k})=
\beta \left(
\sum_{i_1=1}^n {a'_{i_1}}^{j_1} {\bf e}^{i_1},
\sum_{i_2=1}^n {a'_{i_2}}^{j_2} {\bf e}^{i_2},
\ldots ,
\sum_{i_k=1}^n {a'_{i_k}}^{j_k} {\bf e}^{i_k}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a'_{i_1}}^{j_1}{a'_{i_2}}^{j_2}\cdots {a'_{i_k}}^{j_k} \beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k})
 \label{2012-m-ch-tensor-etotccon1}
\end{eqnarray}
or
\begin{equation}
B'^{{j_1}{j_2}\cdots {j_k}}=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a'_{i_1}}^{j_1}{a'_{i_2}}^{j_2}\cdots {a'_{i_k}}^{j_k} B^{i_1 i_2\ldots i_k}.
 \label{2012-m-ch-tensor-etotccon2}
\end{equation}


\section{Orthonormal bases}
For orthonormal bases of $n$-dimensional Hilbert space,
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j
\textrm { if and only if }
{\bf e}_i= {\bf e}^i  \textrm { for all } 1\le i,j \le n.
\end{equation}
Therefore, the vector space and its dual vector space are ``identical''
in the sense that the coordinate tuples representing their bases are identical
(though relatively transposed).
That is, besides transposition, the two bases are identical
\begin{equation}
{\mathfrak B}\equiv {\mathfrak B}^\ast
\end{equation}
and  formally any distinction between covariant and contravariant vectors becomes
irrelevant. Conceptually, such a distinction persists, though.
In this sense, we might ``forget about the difference between
covariant and contravariant orders.''



\section{Invariant tensors and physical motivation}

\section{Metric tensor}

Metric tensors are defined in metric vector spaces.
A metric vector space (sometimes also refered to
as ``vector space with metric'' or ``geometry'')
is a vector space with some inner or scalar product.
This includes (pseudo-) Euclidean spaces with indefinite metric.
(I.e., the distance needs not be positive or zero.)




\subsection{Definition metric}
\index{metric tensor}
\index{metric}
\label{2011-m-metrict}

A {\em metric} $g$ is a functional ${\Bbb R}^n\times{\Bbb R}^n\mapsto {\Bbb R}$
with the following properties:
\begin{itemize}
\item
$g$ is symmetric; that is, $g(x,y)=g(y,x)$;
\item
$g$ is bilinear; that is,
$g(
\alpha x + \beta y, z)
= \alpha g( x,z) + \beta g(y, z)
$ (due to symmetry $g$ is also bilinear in the second argument);
\item
$g$ is nondegenerate; that is,
for every $x\in {\frak V}$, $x\neq 0$, there exists a
$y\in {\frak V}$ such that $g(x,y)\neq 0$.
\end{itemize}



\subsection{Construction of a metric from a scalar product by metric tensor}

In particular cases, the {\em metric} tensor may be defined {\it via}  the scalar product
\begin{equation}
g_{ij}={\bf e}_i\cdot {\bf e}_j\equiv ({\bf e}_i, {\bf e}_j)\equiv \langle {\bf e}_i \mid {\bf e}_j\rangle .
\end{equation}
and
\begin{equation}
g^{ij}={\bf e}^i\cdot {\bf e}^j\equiv ({\bf e}^i, {\bf e}^j)\equiv \langle {\bf e}^i \mid {\bf e}^j\rangle .
\end{equation}
By definition of the (dual) basis in Eq. (\ref{2011-m-Dualbasis-e3}) on page \pageref{2011-m-Dualbasis-e3},
\begin{equation}
{g^i}_{j}
= {\bf e}^i  {\bf e}_j
= g^{il}{\bf e}_l \cdot {\bf e}_j
=g^{il}g_{lj}
={\delta^i}_j ,
\end{equation}
which is a reflection of the covariant and contravariant metric tensors being inverse,
since the basis and the associated dual basis is inverse (and {\it vice versa}).
Note that it is possible to change a covariant tensor into a contravariant one and {\em vice versa}
by the application of a metric tensor.
This can be seen as follows.
Because of linearity, any contravariant basis vector ${\bf e}^i$
can be written as a linear sum of covariant (transposed, but we do not mark transposition here) basis vectors:
\begin{equation}
{\bf e}^i=A^{ij}{\bf e}_j.
\end{equation}
Then,
\begin{equation}
g^{ik} ={\bf e}^i\cdot {\bf e}^k =(A^{ij}{\bf e}_j)\cdot {\bf e}^k=A^{ij}({\bf e}_j\cdot {\bf e}^k)=A^{ij}\delta_j^k=A^{ik}
\end{equation}
and thus
\begin{equation}
{\bf e}^i=g^{ij}{\bf e}_j
\end{equation}
and
\begin{equation}
{\bf e}_i=g_{ij}{\bf e}^j.
\end{equation}


For orthonormal bases, the metric tensor can be
represented as a Kronecker delta function, and thus  remains form invariant.
Moreover, its covariant and contravariant components are identical; that is,
$\delta_{ij}=\delta^i_j=\delta_i^j=\delta^{ij}$.



\subsection{What can the metric tensor do for us?}

Most often it is used to raise or lower the indices; that is,
to change from contravariant to covariant and conversely from covariant
to contravariant.
{
\color{blue}
\bexample
For example,
\begin{equation}
{\bf x} =
X^i {\bf e}_i = X^i g_{ij} {\bf e}^j   = X_j {\bf e}^j,
\end{equation}
and hence $X_j = X^i g_{ij}$.
\eexample
}


In the previous section, the metric tensor has been derived from the scalar product.
The converse is true as well.
In Euclidean space with the dot (scalar, inner) product
the metric tensor represents the scalar product between vectors: let
${\bf x}=X^i{\bf e}_i \in {\Bbb R}^n$ and ${\bf y}=Y^j{\bf e}_j \in {\Bbb R}^n$ be two vectors.
Then ("$T$" stands for the transpose),
\begin{equation}
{\bf x}\cdot {\bf y}\equiv ({\bf x},{\bf y})\equiv \langle {\bf x}\mid {\bf y}\rangle
= X^i {\bf e}_i\cdot Y^j {\bf e}_j
= X^iY^j {\bf e}_i\cdot  {\bf e}_j
=X^iY^j g_{ij}= X^T g Y.
\end{equation}

It also characterizes the length of a vector: in the above
equation, set ${\bf y}={\bf x}$. Then,
\begin{equation}
{\bf x}\cdot {\bf x}\equiv ({\bf x},{\bf x})\equiv \langle {\bf x}\mid {\bf x}\rangle
=X^iX^j g_{ij}\equiv X^T g X,
\end{equation}
and thus
\begin{equation}
\vert\vert  x\vert\vert  =\sqrt{X^iX^j g_{ij}}= \sqrt{X^T g X}.
\end{equation}


The square of an infinitesimal vector $ds =\{d{\bf x}^i\}$ is
\begin{equation}
(d s)^2  = g_{ij}d{\bf x}^id{\bf x}^j= d{\bf x}^T g d{\bf x}.
\end{equation}


Question: Prove that $\vert\vert  {\bf x}\vert\vert $ mediated by $g$ is
indeed a metric; that is, that
$g$ represents a {\em bilinear functional}
$g({\bf x},{\bf y}) ={\bf x}^i{\bf y}^j g_{ij}$ that is {\em symmetric}; that is,
$g({\bf x},{\bf y}) = g({\bf x},{\bf y})$
and {\em nondegenerate}; that is, for any nonzero vector ${\bf x}\in {\frak V}$,   ${\bf x}\neq 0$,
there is some  vector  ${\bf y}\in {\frak V}$, so that  $g({\bf x},{\bf y}) \neq 0$.
\index{metric}

\subsection{Transformation of the metric tensor}

Insertion into the definitions and coordinate transformations
(\ref{2012-m-ch-tlcbv}) as well as      (\ref{2001-mu-tensor-tl2})
yields
\begin{equation}
g_{ij}={\bf e}_i\cdot {\bf e}_j
={a'_i}^l{\bf e'}_l\cdot {a'_j}^m{\bf e'}_m
={a'_i}^l {a'_j}^m {\bf e'}_l\cdot {\bf e'}_m
= {a'_i}^l {a'_j}^m {g'}_{lm}
= {\partial {X'}^l\over \partial X^i}{\partial {X'}^m\over \partial X^j} {g'}_{lm}
.
\label{2011-m-emtdc}
\end{equation}

Conversely,  (\ref{2001-mu-tensors}) as well as    (\ref{2001-mu-tensor-tl1})
yields
\begin{equation}
g'_{ij}={\bf e}'_i\cdot {\bf e}'_j
={a_i}^l{\bf e}_l\cdot {a_j}^m{\bf e}_m
={a_i}^l {a_j}^m {\bf e}_l\cdot {\bf e}_m
= {a_i}^l {a_j}^m {g}_{lm}
= {\partial {X}^l\over \partial {X'}^i}{\partial {X}^m\over \partial {X'}^j} {g}_{lm}
.
\end{equation}


If the geometry (i.e., the basis) is locally orthonormal, ${g}_{lm}=\delta_{lm}$,
then
$g'_{ij}={\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}$.


In terms of the
{\em  Jacobian matrix}
\index{Jacobian matrix}
\begin{equation}
J\equiv
J_{ij}={\partial {X'}^i\over \partial {X}^j }
\equiv
\begin{pmatrix}
{\partial {X'}^1\over \partial {X}^1}&\cdots&{\partial {X'}^1\over \partial {X}^n}\\
\vdots&\ddots &\vdots\\
{\partial {X'}^n\over \partial {X}^1}&\cdots&{\partial {X'}^n\over \partial {X}^n}
\end{pmatrix} ,
\label{2013-m-t-jm}
\end{equation}
the metric tensor in Eq. (\ref{2011-m-emtdc})
can be rewritten as
\begin{equation}
g = J^T g' J
\equiv g_{ij}= J_{li}J_{mj}g'_{lm}
.
\label{2011-m-emtdcJ}
\end{equation}
If the manifold is embedded into an Euclidean space,
then $g'_{lm}=\delta_{lm}$
and  $g = J^T  J $.

The metric tensor and the Jacobian (determinant)
are thus related by
\begin{equation}
\textrm{det }g = (\textrm{det }J^T) (\textrm{det } g')(\textrm{det } J)
.
\label{2011-m-emtdcJd}
\end{equation}

\subsection{Examples}

In what follows a few metrics are enumerated and briefly commented.
For a more systematic treatment, see, for instance, Snapper and Troyer's {\em Metric Affine geometry} \cite{snapper-troyer}.


\subsection*{$n$-dimensional Euclidean space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{n\; {\rm times}})
\end{equation}

One application in physics is quantum mechanics,
where $n$ stands for the dimension of a complex Hilbert space.
Some definitions can be easily adopted to accommodate the complex numbers.
E.g., axiom 5 of the scalar product becomes
$(x,y)=\overline{(x,y)}$, where ``$\overline{(x,y)}$'' stands for complex conjugation of $(x,y)$.
Axiom 4 of the scalar product becomes
$(x,\alpha y)=\overline{\alpha} (x,y)$.

\subsection*{Lorentz plane}


\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (1,-1)
\end{equation}

\subsection*{Minkowski space of dimension $n$}

In this case the metric tensor is called the
{\em Minkowski metric}
\index{Minkowski metric}
and is often denoted by  ``$\eta$'':
\begin{equation}
\eta \equiv \{\eta_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{n-1\; {\rm times}},-1)
\label{2012-m-ch-tensor-minspn}
\end{equation}


One application in physics is the theory of special relativity,
where $D=4$.
Alexandrov's theorem states that the mere requirement of the preservation of
zero distance (i.e., lightcones), combined with bijectivity (one-to-oneness) of the transformation law
yields the Lorentz transformations
\cite{alex1,alex2,alex3,alex-col,borchers-heger,benz,lester,svozil-2001-convention}.



\subsection*{Negative Euclidean space of dimension $n$}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{-1,-1,\ldots ,-1}_{n\; {\rm times}})
\end{equation}

\subsection*{Artinian four-space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (+1,+1,-1 ,-1)
\end{equation}



\subsection*{General relativity}

In general relativity, the metric tensor $g$ is linked to the energy-mass distribution.
There, it appears as the primary concept when compared to the scalar product.
In the case of zero gravity, $g$ is just the  Minkowski metric (often denoted by  ``$\eta$'')
${\rm diag} (1,1,1,-1) $ corresponding to ``flat'' space-time.

The best known non-flat metric is the Schwarzschild metric
\begin{equation}
g
\equiv
\begin{pmatrix}
(1-2m/r)^{-1}&0&0&0\\
0&r^2&0&0\\
0&0&r^2\sin^2 \theta &0\\
0&0&0&- \left( 1-{2m/r}\right)
\end{pmatrix}
\end{equation}
with respect to the spherical space-time coordinates $r,\theta ,\phi ,t$.

{
\color{blue}
\bexample
\subsection*{Computation of the metric tensor of the ball}
Consider the transformation from the standard orthonormal
three-dimensional ``Cartesian'' coordinates
$X_1=x$,
$X_2=y$,
$X_3=z$,
into spherical coordinates
(for a definition of spherical coordinates, see also page \pageref{2011-m-spericalcoo})
\index{spherical coordinates}
$X_1'=r$,
$X_2'=\theta$,
$X_3'=\varphi$.
In terms of  $r,\theta , \varphi$, the Cartesian coordinates can be written as
\begin{equation}
\begin{split}
 X_1=r \sin \theta \cos \varphi \equiv X_1' \sin X_2' \cos X_3'  , \\
 X_2=r \sin \theta \sin \varphi \equiv X_1'\sin X_2' \sin X_3'  ,    \\
 X_3=r \cos \theta  \equiv X_1'\cos X_2'  .
\end{split}
\end{equation}
Furthermore,  since we are dealing with the Cartesian orthonormal basis,
$g_{ij}=\delta_{ij}$; hence finally
\begin{equation}
g'_{ij}= {\partial {X}^l\over \partial {X'}^i}{\partial {X}_l\over \partial {X'}^j}
\equiv {\rm diag}(1,r^2,r^2\sin^2 \theta ),
\end{equation}
and
\begin{equation}
(ds)^2 =(dr)^2+r^2(d\theta )^2+r^2\sin^2 \theta (d\varphi )^2.
\end{equation}

The expression $(ds)^2 =(dr)^2+r^2(d\varphi )^2$
for polar coordinates in two dimensions (i.e., $n=2$) is obtained by setting $\theta = \pi/2 $ and $d\theta =0$.

\subsection*{Computation of the metric tensor of the Moebius strip}
The parameter representation of the Moebius strip is
\begin{equation}
\Phi (u,v) =\left(
\begin{array}{c}
(1+v\cos \frac{u}{2})\sin u \\
(1+v\cos \frac{u}{2})\cos u \\
v\sin \frac{u}{2}
\end{array}
\right),
\end{equation}
where
$u\in [0,2\pi ]$ represents the position of the point on the circle,  and where $2a>0$ is the ``width'' of the Moebius strip,
and where $v\in [-a,a]$.


\begin{equation}
\begin{split}
\Phi _{v}=\frac{\partial \Phi }{\partial v}=\allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}
 \\
\Phi _{u}=\frac{\partial \Phi }{\partial u}=\allowbreak
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
\end{split}
\end{equation}


\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial u}
=  \allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}^{T}
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
\\
=
-\frac{1}{2}\left( \cos \frac{u}{2}\sin ^{2}u\right) v\sin \frac{u}{2}-%
\frac{1}{2}\left( \cos \frac{u}{2}\cos ^{2}u\right) v\sin \frac{u}{2}
\\
+%
\frac{1}{2}\sin \frac{u}{2} v\cos \frac{u}{2}=\allowbreak 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial v}
=\allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}^{T}
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}
 \\
=
\cos ^{2}\frac{u}{2}\sin ^{2}u+\cos ^{2}\frac{u}{2}\cos ^{2}u+\sin ^{2}%
\frac{u}{2}=\allowbreak 1
\end{split}
\end{equation}


\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial u})^{T}\frac{\partial \Phi }{\partial u}
=
\\
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos
u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin
u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}^{T} \cdot
\\
\cdot
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos
u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin
u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
 \\
=
\frac{1}{4}v^{2}\sin ^{2}\frac{u}{2}\sin ^{2}u+\cos
^{2}u+2 v \cos ^{2}u \cos \frac{u}{2}+
v^{2}\cos ^{2}u  \cos ^{2}\frac{u}{2}
\\
+\frac{1}{4}v^{2}\sin ^{2}\frac{u}{2}\cos^{2}u
+\sin ^{2}u+2  v\sin ^{2}u\cos \frac{u}{2}+ v^{2} \sin^{2}u\cos ^{2}\frac{u}{2}
 \\
+\frac{1}{4}v^{2}\cos ^{2}\frac{1}{2}%
u =\allowbreak \frac{1}{4}v^{2}+v^{2}\cos ^{2}\frac{u}{2}+1+2v\cos \frac{%
1}{2}u
 \\
=\left(1+v\cos \frac{u}{2}\right)^{2}+\frac{1}{4}v^{2}
\end{split}
\end{equation}


Thus the metric tensor is given by
\begin{equation}
\begin{split}
g'_{ij}
= {\partial {X}^s\over \partial {X'}^i}{\partial {X}^t\over \partial {X'}^j}g_{st}
= {\partial {X}^s\over \partial {X'}^i}{\partial {X}^t\over \partial {X'}^j}\delta_{st}\\
\quad \equiv
\left(
\begin{array}{cc}
\Phi _{u}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{u} \\
\Phi _{v}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{v}
\end{array}
\right) ={\rm diag}\left(
\left(1+v\cos \frac{u}{2}\right)^{2}+\frac{1}{4}v^{2} , 1\right).
\end{split}
\end{equation}

\eexample
}





\section{General tensor}

A (general) Tensor $T$ can be defined as a multilinear form  on the
$r$-fold product of a vector space ${\frak V}$, times the
$s$-fold product of the dual vector space ${\frak V}^\ast$;
that is,
\begin{equation}
T: \left( {\frak V} \right)^r \times \left( {\frak V}^\ast \right)^s
=
\underbrace{{\frak V}\times \cdots \times {\frak V}}_{r\textrm{ \scriptsize copies}}
\times
\underbrace{{\frak V}^\ast \times \cdots \times {\frak V}^\ast}_{s\textrm{ \scriptsize copies}}
\mapsto {\Bbb F}
,
 \label{2012-m-ch-tensor-gdt}
\end{equation}
where, most commonly, the scalar field
${\Bbb F}$
will be identified with the set ${\Bbb R}$ of reals,
or with the set ${\Bbb C}$ of complex numbers.
Thereby,
$r$ is called the
{\em covariant order}, and
\index{covariant order}
$s$ is called the
{\em contravariant order}
\index{contravariant order}
of $T$.
A tensor of covariant order $r$ and contravariant order $s$
is then pronounced a tensor of
{\em type} (or {\em rank})
\index{tensor rank}
\index{tensor type}
$(r,s)$.
By convention, covariant indices are denoted by {\em subscripts},
whereas the contravariant indices  are denoted by {\em superscripts}.

With the standard, ``inherited'' addition and scalar multiplication,
the set ${\frak T}_r^s$ of all tensors of type $(r,s)$
forms a linear vector space.


Note that a tensor of type $(1,0)$ is called  a
{\em covariant vector}
\index{covariant vector},
or just a
{\em vector}.
\index{vector}
A tensor of type $(0,1)$ is called a
{\em contravariant vector}. \index{contravariant vector}

Tensors can change their type by the invocation of the {\em metric tensor}.
That is, a covariant tensor (index) $i$ can be made into a contravariant tensor (index) $j$
by summing over the index $i$ in a product involving the tensor and $g^{ij}$.
Likewise,  a contravariant tensor (index) $i$ can be made into a covariant tensor (index) $j$
by summing over the index $i$ in a product involving the tensor and $g_{ij}$.


Under basis or other linear transformations,
covariant tensors with index $i$ transform by summing over this index with (the transformation matrix) ${a_i}^j$.
Contravariant tensors with index $i$ transform by summing over this index with the inverse (transformation matrix)  ${(a^{-1})_i}^j$.

\section{Decomposition of tensors}

Although a tensor of type (or rank) $n$ transforms like the tensor product of $n$ tensors of type 1,
not all type-$n$ tensors can be decomposed into a single
tensor product of $n$ tensors of type (or rank) 1.

Nevertheless,
by a generalized Schmidt decomposition (cf. page \pageref{2011-m-Schmidtdecomposition}),
any type-$2$ tensor  can be decomposed into
the sum of
tensor products of two tensors of type 1.

\section{Form invariance of tensors}

A tensor (field) is
form invariant  with respect to some basis change
\index{form invariance}
if its representation in the new basis has the same form as in the old basis.
For instance, if the ``12122--component'' $T_{12122} (x)$ of the tensor $T$
with respect to the old basis and old coordinates $x$   equals some function $f(x)$ (say, $f(x)=x^2$),
then, a necessary condition for $T$ to be form invariant is that, in terms of the new basis,
that component  $T'_{12122} (x')$  equals the same function $f(x')$ as before, but in the new coordinates $x'$.
A sufficient condition for form invariance of $T$ is that {\em all}
coordinates or components of $T$ are form invariant in that way.


Although form invariance is a gratifying feature for the reasons explained shortly,
a tensor (field) needs not necessarily
be form invariant with respect to all or even any (symmetry) transformation(s).



A physical motivation for the use of form invariant tensors can be given as follows.
What makes some tuples (or matrix, or tensor components in general)  of
numbers or scalar functions a tensor? It is the
interpretation of the scalars as tensor components {\em with respect to
a particular basis}. In another basis, if we were talking about the same
tensor, the tensor components; that is, the numbers or scalar functions,
would be different.
Pointedly stated, the tensor coordinates represent some
encoding of a multilinear function with respect to a particular basis.

Formally, the tensor coordinates are numbers; that is, scalars,
which are grouped together in vector touples or matrices or whatever form we consider useful.
As the tensor coordinates are scalars, they can be treated as scalars.
For instance, due to commutativity and associativity, one can exchange
their order. (Notice, though, that this is generally not the case for
differential operators such as $\partial_i=\partial / \partial {\bf x}^i$.)

A {\em form invariant} tensor with respect to  certain transformations
is a tensor which retains
the same functional form if the transformations are performed; that is,
if the basis changes accordingly.
That is, in this case,
the functional form of mapping numbers or coordinates or other entities remains unchanged, regardless of the coordinate change.
Functions remain the same but with the new parameter components as
arguement. For instance; $4\mapsto 4$ and $f(X_1,X_2,X_3)\mapsto
f(X'_1,X'_2,X'_3)$.

Furthermore, if a tensor is invariant with respect to one transformation, it need not
be invariant with respect to another transformation, or with respect to
changes of the scalar product; that is, the metric.

Nevertheless, totally symmetric (antisymmetric) tensors remain totally
symmetric (antisymmetric) in all cases:
$$
A_{i_1i_2 \ldots i_si_t\ldots i_k}
=
A_{i_1i_2 \ldots i_ti_s\ldots i_k}
$$
implies
$$
\begin{array}{l}
A'_{j_1i_2 \ldots j_s j_t\ldots j_k}
=
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_s i_t\ldots  i_k}
 \\  \qquad
=
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
  \\  \qquad
=
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_t}}^{i_t}{a_{j_s}}^{i_s}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
  \\  \qquad
=
A'_{j_1i_2 \ldots j_t j_s\ldots j_k}    .
\end{array}
$$
Likewise,
$$
A_{i_1i_2 \ldots i_si_t\ldots i_k}
=
-A_{i_1i_2 \ldots i_ti_s\ldots i_k}
$$
implies
$$
\begin{array}{l}
A'_{j_1i_2 \ldots j_s j_t\ldots j_k}
=
{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_s i_t\ldots  i_k}
 \\  \qquad
=
-{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
 \\ \qquad
=
-{a_{j_1}}^{i_1}{a_{j_2}}^{i_2}\cdots
{a_{j_t}}^{i_t}{a_{j_s}}^{i_s}\cdots
{a_{j_k}}^{i_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}.
  \\  \qquad
=
-A'_{j_1i_2 \ldots j_t j_s\ldots j_k}  .
\end{array}
$$


In physics, it would be nice if the natural laws could be written into a
form which does not depend on the particular reference frame or  basis
used.
Form invariance thus is a gratifying physical feature, reflecting the
{\em symmetry} against changes of coordinates and bases.

After all, physicists want the formalization of their fundamental laws not to artificially depend on,
say, spacial directions, or on some particular basis, if there is no physical reason why this should be so.
Therefore, physicists tend to be crazy to write down everything in a
form invariant manner.

One strategy to accomplish  form invariance  is to start out with form invariant
tensors and compose -- by tensor products and index reduction -- everything from them. This method guarantees form
invarince.


{
\color{blue}
\bexample
Indeed,  for the sake of demonstration, consider the following two factorizable tensor fields:
while
\begin{equation}
{S}(x)=
\begin{pmatrix}
  {  x}_2  \\
- {  x}_1
\end{pmatrix}
\otimes
\begin{pmatrix}
   {  x}_2  \\
 - {  x}_1
\end{pmatrix}^T
=
\left(    {  x}_2 ,- {  x}_1  \right)^T
\otimes
\left(    {  x}_2 ,- {  x}_1  \right)
\equiv
\begin{pmatrix}
   {  x}_2^2          & -{ x}_1{x}_2  \\
 - {  x}_1{  x}_2     & { x}_1^2
\end{pmatrix}
\label{2012-m-ch-tensor-etotccon1factorized}
\end{equation}
is a form invariant tensor field with respect to the basis $\{(0,1),(1,0)\}$
and orthogonal transformations (rotations around the origin)
\begin{equation}
\begin{pmatrix}
  \cos \varphi & \sin \varphi  \\
 -\sin \varphi & \cos \varphi
\end{pmatrix}
,
\end{equation}
\begin{equation}
{ T}(x)=
\begin{pmatrix}
{  x}_2  \\
{  x}_1
\end{pmatrix}
\otimes
\begin{pmatrix}
{  x}_2  \\
{  x}_1 \end{pmatrix}^T
=
\left(    {  x}_2 ,  {  x}_1  \right)^T
\otimes
\left(    {  x}_2 ,  {  x}_1  \right)
\equiv
\begin{pmatrix}
{  x}_2^2 & { x}_1{  x}_2  \\
{  x}_1{  x}_2          & { x}_1^2
\end{pmatrix}
\end{equation}
is not.

This can be proven by considering the single factors from which $S$ and $T$ are composed.
Eqs. (\ref{2012-m-ch-tensor-etotc1})-(\ref{2011-m-tvtcov})
and
(\ref{2012-m-ch-tensor-etotccon1})-(\ref{2012-m-ch-tensor-etotccon2})
show that the form
invariance of the factors implies the form invariance of the tensor products.

For instance, in our example, the factors $\left(    {  x}_2 ,- {  x}_1  \right)^T$
of $S$ are invariant, as they transform as
$$
\begin{pmatrix} \cos \varphi & \sin \varphi  \\
                         -\sin \varphi & \cos \varphi
\end{pmatrix}
\begin{pmatrix}
{  x}_2  \\
 - {  x}_1
\end{pmatrix}
=
\begin{pmatrix}
 {  x}_2 \cos \varphi  - x_1 \sin \varphi  \\
            - x_2 \sin \varphi         - {  x}_1 \cos \varphi
\end{pmatrix}
=
\begin{pmatrix}
{  x}_2'  \\
 - {  x}_1'
\end{pmatrix},
$$
where the transformation of the coordinates
$$
\begin{pmatrix}
{  x}_1'  \\
  {  x}_2'
\end{pmatrix}
=
\begin{pmatrix}
 \cos \varphi & \sin \varphi  \\
   -\sin \varphi & \cos \varphi
\end{pmatrix}
\begin{pmatrix}
 {  x}_1  \\
 {  x}_2
\end{pmatrix}
=
\begin{pmatrix}
{  x}_1 \cos \varphi  + x_2 \sin \varphi  \\
- x_1 \sin \varphi         + {  x}_2 \cos \varphi
\end{pmatrix}
$$
has been used.


Note that  the notation identifying tensors of type (or rank) two with matrices,
creates an ``artifact'' insofar as the transformation of the ``second index'' must then be represented by
the exchanged multiplication order, together with the transposed transformation matrix;
that is,
$$
a_{ik}a_{jl}A_{kl}
=  a_{ik}A_{kl}a_{jl}
=  a_{ik}A_{kl}\left(a^T \right)_{lj}
\equiv a\cdot A\cdot a^T .
$$
Thus for a
transformation  of
the transposed touple  $\left(    {  x}_2 ,- {  x}_1  \right)$
we must consider the {\em transposed} transformation matrix arranged {\em after} the factor; that is,
$$
\left(   {  x}_2 , - {  x}_1 \right)
\begin{pmatrix}  \cos \varphi & -\sin \varphi  \\
  \sin \varphi & \cos \varphi
\end{pmatrix}
=
\left(
{  x}_2 \cos \varphi  - x_1 \sin \varphi ,
 - x_2 \sin \varphi         - {  x}_1 \cos \varphi
\right)
=
\left(
{  x}_2'  ,
 - {  x}_1'
\right).
$$



In contrast, a similar calculation shows that the factors
$\left(    {  x}_2 ,  {  x}_1  \right)^T$
of $T$ do not transform invariantly.
However, noninvariance with respect to certain transformations does not imply that
$T$ is not a valid, ``respectable'' tensor field; it is just not form invariant under rotations.
\eexample
}

Nevertheless, note that, while the tensor product of form invariant tensors is again a form invariant tensor,  not every form
invariant tensor might be decomposed into products of form invariant tensors.

{
\color{blue}
\bexample
Let
$\vert + \rangle  \equiv   (0,1)$
and
$\vert - \rangle  \equiv   (1,0)$.
For a nondecomposable tensor, consider the sum of two-partite tensor products (associated with two ``entangled'' particles)
\index{entanglement}
Bell state (cf. Eq. (\ref{2005-hp-ep12s1v}) on page \pageref{2005-hp-ep12s1v}) in the standard basis     \index{Bell state}
\begin{equation}
\begin{split}
\vert \Psi^-\rangle = \frac{1}{\sqrt{2}}\left(\vert +-\rangle   - \vert -+\rangle  \right)   \\
\qquad \equiv  \left( 0,\frac{1}{\sqrt{2}},- \frac{1}{\sqrt{2}} ,  0 \right)     \\
\qquad \equiv  \frac{1}{2}
\begin{pmatrix}
0&0&0&0\\
0&1&-1&0\\
0&-1&1&0\\
0&0&0&0
\end{pmatrix}
.
\end{split}
\label{2011-m-bellstatenondec}
\end{equation}
\marginnote{$\vert \Psi^-\rangle $,   together with the other three Bell states
$\vert \Psi^+\rangle = \frac{1}{\sqrt{2}}\left(\vert +-\rangle   + \vert -+\rangle  \right) $,
$\vert \Phi^+\rangle = \frac{1}{\sqrt{2}}\left(\vert --\rangle   + \vert ++\rangle  \right) $,
and
$\vert \Phi^-\rangle = \frac{1}{\sqrt{2}}\left(\vert --\rangle   - \vert ++\rangle  \right) $,
forms an orthonormal basis of ${\Bbb C}^4$.
}

Why is $\vert \Psi^-\rangle$ not decomposable?
In order to be able to answer this question
(see alse Section \ref{2012-m-c-fdvs-entanglement} on page \pageref{2012-m-c-fdvs-entanglement}), consider
the most general two-partite state
\begin{equation}
\vert \psi \rangle
=
\psi_{--}\vert -- \rangle
+
\psi_{-+}\vert -+ \rangle
+
\psi_{+-}\vert +- \rangle
+
\psi_{++}\vert ++ \rangle
,
\end{equation}
with $\psi_{ij}\in {\Bbb C}$,
and compare it to the most general state obtainable through products of single-partite states
$\vert \phi_1\rangle  = \alpha_-  \vert - \rangle    + \alpha_+  \vert + \rangle$,
and
$\vert \phi_2\rangle  = \beta_-  \vert - \rangle    + \beta_+  \vert + \rangle$
with $\alpha_{i}, \beta_i \in {\Bbb C}$;
that is,
\begin{equation}
\begin{split}
\vert \phi \rangle  =\vert \phi_1\rangle    \vert \phi_2\rangle   \\
\qquad =
(\alpha_-  \vert - \rangle    + \alpha_+  \vert + \rangle )
(\beta_-  \vert - \rangle    + \beta_+  \vert + \rangle )   \\
\qquad  =\alpha_- \beta_- \vert -- \rangle    + \alpha_-\beta_+  \vert -+ \rangle +
\alpha_+ \beta_- \vert +- \rangle    + \alpha_+\beta_+  \vert ++ \rangle. \\
\end{split}
\end{equation}
Since the two-partite basis states
\begin{equation}
\begin{split}
\vert -- \rangle  \equiv (1,0,0,0)
,\\
\vert -+ \rangle    \equiv (0,1,0,0)
,\\
\vert +- \rangle     \equiv (0,0,1,0)
,\\
\vert ++ \rangle      \equiv (0,0,0,1)
\end{split}
\end{equation}
are linear independent (indeed, orthonormal),
a comparison of $\vert \psi \rangle  $ with  $\vert \phi \rangle$ yields
\begin{equation}
\begin{split}
\psi_{--}=  \alpha_- \beta_-
,\\
\psi_{-+}=   \alpha_-\beta_+
,\\
\psi_{+-}=  \alpha_+ \beta_-
,\\
\psi_{++}= \alpha_+\beta_+
.
\end{split}
\end{equation}
Hence,
$\psi_{--}/ \psi_{-+} =   \beta_- / \beta_+ =   \psi_{+-} / \psi_{++}$,
and thus a necessary and sufficient condition for a two-partite quantum state to be decomposable
into a product of single-particle quantum states is that its amplitudes obey
 \begin{equation}
\psi_{--}\psi_{++}  =  \psi_{-+}   \psi_{+-} .
\end{equation}
This is not satisfied for the Bell state $\vert \Psi^-\rangle$ in Eq. (\ref{2011-m-bellstatenondec}),
because in this case $\psi_{--}=\psi_{++} =0$
and  $ \psi_{-+} = - \psi_{+-} =1/\sqrt{2}$.
Such nondecomposability is in physics referred to as {\em entanglement}
\cite{CambridgeJournals:1737068,CambridgeJournals:2027212,schrodinger}.
\index{entanglement}

Note also that $\vert \Psi^-\rangle$ is a {\em singlet state},
as it is form invariant under the following generalized rotations in two-dimensional complex Hilbert subspace; that is,
(if you do not believe this please check yourself)
\begin{equation}
\begin{split}
\vert + \rangle =
e^{ i{\frac{\varphi}{2}} }
\left(
\cos \frac{\theta}{2} \vert +'  \rangle
-
\sin \frac{\theta}{2} \vert -'   \rangle
\right),
\\
 \vert - \rangle =
e^{ -i{\frac{\varphi}{2}} }
\left(
\sin \frac{\theta}{2} \vert +'   \rangle
+
\cos \frac{\theta}{2} \vert -'  \rangle
\right)
\end{split}
\end{equation}
in the spherical coordinates $\theta , \varphi$ defined on page \pageref{2011-m-spericalcoo},
but it cannot be composed or written as a product of a {\em single} (let alone form invariant) two-partite tensor product.

\eexample
}


%There exists totally symmetric (antisymmetric) tensors which are form
%invariant under all basis and metric changes.
%The symmetric tensor is  associated with the Kronecker delta
%\begin{equation}
%\delta_i^j =
%\delta^i_j =
%\left\{
% \begin{array}{l}
%1 \mbox{ if } i=j \\
%0 \mbox{ if } i\neq j  \\
%\end{array}
% \right. .
%\end{equation}
%This can be easily seen by evaluating
%${\delta'}_{j_1}^{j_2} = {a_{j_1}}^{i_1}a^{j_2}_{i_2}\delta_{i_1}^{i_2} =
%a_{j_1}^{i}a^{j_2}_i={\delta}_{j_1}^{j_2}$
%
%The antisymmetric tensor is  associated with
%\begin{equation}
%\epsilon_{12\cdots n} = 1, \qquad
%\epsilon_{i_1i_2\cdots i_si_t \cdots i_k} =  -\epsilon_{i_1i_2\cdots
%i_ti_s \cdots i_k}.
%\end{equation}
%

In order to prove form invariance of a constant tensor,
one has to transform the tensor according to the standard transformation laws
(\ref{2011-m-tvtcov}) and (\ref{2011-m-tvtcontrav}), and compare the result with the input;
that is, with the untransformed, original, tensor.
This is sometimes referred to as the ``outer transformation.''


In order to prove form invariance of a tensor field,
one has to additionally transform the spatial coordinates on which the field depends;
that is, the arguments of that field; and then compare.
This is sometimes referred to as the ``inner transformation.''
This will become clearer with the following example.

{
\color{blue}
\bexample


Consider again the tensor field defined
earlier in Eq.
(\ref{2012-m-ch-tensor-etotccon1factorized}),
but let us not choose the ``elegant''
ways of proving form invariance by factoring; rather we explicitly
consider the transformation of all the  components
$$S_{ij}(x_1,x_2)
=
\begin{pmatrix}
 -x_1x_2 & - x_2^2  \\
 x_1^2 & x_1x_2
\end{pmatrix}
$$
with respect to the standard basis  $\{(1,0), (0,1)\}$.

Is $S$ form invariant with respect to rotations around the origin?
That is, $S$ should be form invariant with repect to transformations
$x_i' = a_{ij} x_j$
with
$$
a_{ij}=\begin{pmatrix}
 \cos \varphi & \sin \varphi  \\
  -\sin \varphi & \cos \varphi
\end{pmatrix}.
$$


Consider the ``outer'' transformation first.
As has been pointed out earlier,
the term on the right hand side in $
S_{ij}'= a_{ik}a_{jl}S_{kl}
$
can be rewritten as a product of three matrices; that is,
$$
a_{ik}a_{jl}S_{kl}\left(x_n\right)
=  a_{ik}S_{kl}a_{jl}
=  a_{ik}S_{kl}\left(a^T \right)_{lj}
\equiv a\cdot S\cdot a^T .
$$
$a^T$ stands for the transposed matrix; that is,
$(a^T)_{ij}=a_{ji}$.
$$
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      -x_1x_2 & -x_2^2 \\
      x_1^2   & x_1x_2
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      \cos \varphi  & -\sin \varphi \\
      \sin \varphi & \cos \varphi
    \end{array}
  \right)=
$$

\smallskip

$$
  =\left(
    \begin{array}{cc}
      -x_1 x_2 \cos \varphi + x_1^2 \sin \varphi &
        -x_2^2 \cos \varphi + x_1 x_2 \sin \varphi \\
      x_1 x_2 \sin \varphi + x_1^2 \cos \varphi &
        x_2^2 \sin \varphi + x_1 x_2 \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      \cos \varphi  & -\sin \varphi \\
      \sin \varphi & \cos \varphi
    \end{array}
  \right)=
$$

\smallskip

$$
  =\left(\!\!\!
    \begin{array}{cc}
      \cos \varphi
        \left(-x_1 x_2 \cos \varphi + x_1^2 \sin \varphi\right)+\!&\!
      -\sin \varphi
        \left(-x_1 x_2 \cos \varphi + x_1^2 \sin \varphi\right)+\\
      \quad +\sin \varphi
        \left(-x_2^2 \cos \varphi + x_1 x_2 \sin \varphi\right)\!&\!
      \quad +\cos \varphi
        \left(-x_2^2 \cos \varphi + x_1 x_2 \sin \varphi\right)\\
      \\[1ex]
      \cos \varphi
        \left(x_1 x_2 \sin \varphi + x_1^2 \cos \varphi\right)+\!&\!
      -\sin \varphi
        \left(x_1 x_2 \sin \varphi + x_1^2 \cos \varphi\right)+\\
      \quad +\sin \varphi
        \left(x_2^2 \sin \varphi + x_1 x_2 \cos \varphi\right)\!&\!
      \quad +\cos \varphi
        \left(x_2^2 \sin \varphi + x_1 x_2 \cos \varphi\right)
    \end{array}
  \!\right)=
$$

\smallskip

$$
  =\left(
    \begin{array}{cc}
      x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) + &
      2x_1 x_2 \sin \varphi \cos \varphi \\
      \qquad + \left( x_1^2-x_2^2 \right) \sin \varphi \cos \varphi &
      \qquad - x_1^2 \sin^2 \varphi - x_2^2 \cos^2 \varphi \\
      \\[1ex]
      2x_1 x_2 \sin \varphi \cos \varphi + &
      -x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) - \\
      \qquad + x_1^2 \cos^2 \varphi + x_2^2 \sin^2 \varphi &
      \quad -\left(x_1^2-x_2^2\right) \sin \varphi \cos \varphi
    \end{array}
  \right)
$$

Let us now perform the ``inner'' transform
$$
  x'_i =a_{ij}x_j \Longrightarrow
  \begin{array}{rcl}
    x'_1 & = & x_1 \cos \varphi + x_2 \sin \varphi \\
    x'_2 & = & -x_1 \sin \varphi + x_2 \cos \varphi .
  \end{array}
$$

Thereby we assume (to be corroborated) that the functional form in the new coordinates are identical
to the functional form of the old coordinates.
A comparison yields
\begin{eqnarray*}
  -x'_1 \,x'_2 & = &
  -\left(x_1 \cos \varphi + x_2 \sin \varphi \right)
  \left(-x_1 \sin \varphi + x_2 \cos \varphi \right) = \\
  & = &
  -\left(
    -x_1^2 \sin \varphi \cos \varphi +
      x_2^2 \sin \varphi \cos \varphi -
      x_1 x_2 \sin^2 \varphi + x_1 x_2 \cos^2 \varphi
  \right) = \\
  & = &
  x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) +
    \left(x_1^2 - x_2^2\right) \sin \varphi \cos \varphi \\
  (x'_1)^2  & = &
    \left(x_1 \cos \varphi + x_2 \sin \varphi \right)
    \left(x_1 \cos \varphi + x_2 \sin \varphi \right) = \\
  & = & x_1^2 \cos^2 \varphi + x_2^2 \sin^2 \varphi +
    2 x_1 x_2 \sin \varphi \cos \varphi \\
  (x'_2)^2  & = &
    \left(-x_1 \sin \varphi + x_2 \cos \varphi \right)
    \left(-x_1 \sin \varphi + x_2 \cos \varphi \right) = \\
  & = & x_1^2 \sin^2 \varphi + x_2^2 \cos^2 \varphi -
    2 x_1 x_2 \sin \varphi \cos \varphi
\end{eqnarray*}
and hence
$$S' (x'_1,x'_2)=\left(
    \begin{array}{cc}
      -x'_1x'_2 & -(x'_2)^2 \\
      (x'_1)^2   & x'_1x'_2
    \end{array}
\right)
$$ is invariant with respect to basis rotations $$\{ (\cos \varphi ,-\sin \varphi
),(\sin
\varphi ,\cos \varphi )\}$$.


Incidentally,
as has been stated earlier, $S(x)$ can be written as the product of two invariant tensors $b_i(x)$ and $c_j(x)$:
$$S_{ij}(x)=b_i(x)c_j(x),$$
with
$
b(x_1,x_2)=(-x_2,x_1),
$ and
$
c(x_1,x_2)=(x_1,x_2)
$.
This can be easily checked by comparing the components:
\begin{eqnarray*}
b_1c_1&=&-x_1x_2 = S_{11},\\
b_1c_2&=&-x_2^2 = S_{12},\\
b_2c_1&=&x_1^2 = S_{21},\\
b_2c_2&=&x_1x_2 = S_{22}.\\
\end{eqnarray*}

Under rotations, $b$ and $c$  transform into
\begin{eqnarray*}
a_{ij}b_j&=&
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      -x_2\\
       x_1
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      -x_2\cos \varphi +x_1 \sin \varphi \\
      x_2\sin \varphi +x_1 \cos \varphi
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      -x'_2 \\
      x'_1
    \end{array}
  \right)    \\
 a_{ij}c_j&=&
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      x_1\\
       x_2
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      x_1\cos \varphi +x_2 \sin \varphi \\
      -x_1\sin \varphi +x_2 \cos \varphi
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      x'_1    \\
      x'_2
    \end{array}
  \right) .
\end{eqnarray*}
%Mit beliebigen Tensoren erster Stufe $x=(x_1,x_2)$
%\"uberschnitten, ergeben sich skalare Invarianten:
%\begin{eqnarray*}
%b_ix_i&=&-x_2x_1+x_1x_2=0\\
%c_ix_i&=&x_1^2+x_2^2.
%\end{eqnarray*}

This factorization of $S$ is nonunique, since
Eq.
(\ref{2012-m-ch-tensor-etotccon1factorized})
uses a different factorization; also, $S$ is decomposible into, for example,
$$S(x_1,x_2)=
  \left(
    \begin{array}{cc}
      -x_1x_2 & -x_2^2 \\
      x_1^2   & x_1x_2
    \end{array}
  \right)     =
  \left(
    \begin{array}{cc}
      -x_2^2 \\
      x_1 x_2
    \end{array}
  \right)
\otimes \left({x_1\over x_2},1\right).
$$



\eexample
}



\section{The Kronecker symbol $\delta$}
\index{delta tensor}
For vector spaces of dimension $n$ the totally symmetric Kronecker symbol $\delta$,
sometimes referred to
as the delta symbol $\delta$--tensor, can be defined by
\begin{equation}
\delta_{i_1 i_2\cdots i_k}
=
\left\{
\begin{array}{rl}
+1&\textrm{ if }  i_1 = i_2 = \cdots = i_k \\
0&\textrm{ otherwise (that is, some indices are not identical).}
\end{array}
\right.
\end{equation}

\section{The Levi-Civita symbol $\varepsilon$}
\index{Levi-Civita symbol}
\index{antisymmetric tensor}
For vector spaces of dimension $n$ the totally antisymmetric Levi-Civita symbol $\varepsilon$, sometimes referred to
as the Levi-Civita symbol $\varepsilon$--tensor, can be defined by the number of permutations of its indices; that is,
\begin{equation}
\varepsilon_{i_1 i_2\cdots i_k}
=
\left\{
\begin{array}{rl}
+1&\textrm{ if } (i_1 i_2\ldots i_k) \textrm{ is an {\em even} permutation of } (1,2,\ldots k)\\
-1&\textrm{ if } (i_1 i_2\ldots i_k) \textrm{ is an {\em odd} permutation of } (1,2,\ldots k)\\
0&\textrm{ otherwise (that is, some indices are identical).}
\end{array}
\right.
\end{equation}
Hence, $\varepsilon_{i_1 i_2\cdots i_k}$ stands for the  the sign of the permutation in the case of a permutation, and zero otherwise.

{
\color{blue}
\bexample

In two dimensions,
$$\varepsilon_{ij}\equiv
\left(
\begin{array}{rrrr}
\varepsilon_{11}&\varepsilon_{12}\\
\varepsilon_{21}&\varepsilon_{22}
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0&1\\
-1&0
\end{array}
\right)
.
$$
\eexample
}

In three dimensional Euclidean space,
the cross product, or vector product
\index{cross product}
\index{vector product}
of two vectors
${\bf a}\equiv a_i$
and
${\bf b}\equiv b_i$
can be written as
${\bf a} \times {\bf b}\equiv \varepsilon_{ijk}a_jb_k$.

\section{The nabla, Laplace, and D'Alembert operators}
\index{nabla operator}

The {\em nabla operator}
\begin{equation}
\nabla =\left(
\frac{\partial }{\partial x_1},
\frac{\partial }{\partial x_2},
\ldots ,
\frac{\partial }{\partial x_n}
\right).
\end{equation}
is a vector differential operator in an $n$-dimensional vector space $\frak V$.
In index notation, $\nabla_i  =\partial_i =\partial_{x_i}$.

In three dimensions and in the standard Cartesian basis,
\begin{equation}
\nabla = \left(
\frac{\partial }{\partial x_1},
\frac{\partial }{\partial x_2},
\frac{\partial }{\partial x_3}
\right)
={\bf e}_1\frac{\partial }{\partial x_1}
+{\bf e}_2\frac{\partial }{\partial x_2}
+{\bf e}_3\frac{\partial }{\partial x_3}
.
\end{equation}

It is often used to define basic differential operations;
in particular, (i) to denote the {\em gradient} of a scalar field $f(x_1,x_2,x_3)$ (rendering a vector field with respect to a particular basis),
(ii) the {\em divergence} of a vector field ${\bf v}(x_1,x_2,x_3)$
(rendering a scalar field with respect to a particular basis), and
(iii) the {\em curl} (rotation) of a vector field  ${\bf v}(x_1,x_2,x_3)$ (rendering a vector field with respect to a particular basis)
as follows:
\index{gradient}
\index{divergence}
\index{curl}
\begin{eqnarray}
\textrm{grad } f &=& \nabla f = \left(
\frac{\partial f}{\partial x_1},
\frac{\partial f}{\partial x_2},
\frac{\partial f}{\partial x_3}
\right)  ,\\
\textrm{div }  {\bf v} &=& \nabla \cdot {\bf v} =
\frac{\partial v_1}{\partial x_1}+
\frac{\partial v_2}{\partial x_2}+
\frac{\partial v_3}{\partial x_3}
  ,\\
\textrm{rot } {\bf v} &=& \nabla \times {\bf v} = \left(
\frac{\partial v_3}{\partial x_2}-
\frac{\partial v_2}{\partial x_3}
,
\frac{\partial v_1}{\partial x_3}-
\frac{\partial v_3}{\partial x_1}
,
\frac{\partial v_2}{\partial x_1}-
\frac{\partial v_1}{\partial x_2}
\right)          \\
&\equiv& \varepsilon_{ijk} \partial_j v_k.
\end{eqnarray}

The {\em Laplace operator}
\index{Laplace operator}
is defined by
\begin{equation}
\Delta = \nabla^2= \nabla \cdot \nabla =
\frac{\partial^2 }{\partial^2 x_1}+
\frac{\partial^2 }{\partial^2 x_2}+
\frac{\partial^2 }{\partial^2 x_3}
.
\end{equation}

In special relativity and electrodynamics,  as well as in  wave theory and quantized field theory, with the Minkowski space-time
of dimension four
(referring to the metric tensor with the signature ``$\pm ,\pm ,\pm ,\mp$''),
the {\em D'Alembert operator}
\index{D'Alembert operator}
is defined by the Minkowski metric $\eta = {\rm diag} (1,1,1,-1)$
\begin{equation}
\Box  = \partial_i \partial^i
=
\eta_{ij}  \partial^i \partial^j=
\nabla^2- \frac{\partial^2 }{\partial^2 t}=
\nabla \cdot \nabla - \frac{\partial^2 }{\partial^2 t}=
\frac{\partial^2 }{\partial^2 x_1}+
\frac{\partial^2 }{\partial^2 x_2}+
\frac{\partial^2 }{\partial^2 x_3}- \frac{\partial^2 }{\partial^2 t}
.
\end{equation}



\section{Some tricks and examples}

There are some tricks which are commonly used.
Here, some of them are enumerated:

\begin{itemize}
\item[(i)]
Indices which appear as internal sums can be renamed arbitrarily
(provided their name is not already taken by some other index).
That is, $a_ib^i=a_jb^j$ for arbitrary $a,b,i,j$.
\item[(ii)]
With the Euclidean metric, $\delta_{ii}=n$.
\item[(iii)]
$\frac{\partial X^i }{\partial X^j}=\delta^i_j$.
\item[(iv)]
With the Euclidean metric, $\frac{\partial X^i }{ \partial X^i}=n$.
\item[(v)]
For threedimensional vector spaces ($n=3$)  and the Euclidean metric,
the {\em Grassmann identity} holds:
\index{Grassmann identity}
\begin{equation}
 \varepsilon_{ijk}\varepsilon_{klm}
=  \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}.
\label{2011-m-egi}
\end{equation}
\item[(vi)]
For threedimensional vector spaces ($n=3$) and the Euclidean metric,\\
$\vert a\times b \vert=
\sqrt{\varepsilon_{ijk}\varepsilon_{ist}a_ja_sb_kb_t}  =
\sqrt{\vert a\vert^2
\vert b\vert^2
-(a\cdot b)^2}=\sqrt{{\rm det}
\left(
\begin{array}{cc}
a\cdot a&a\cdot b\\
a\cdot b&b\cdot b
\end{array}\right)}= \vert a\vert
\vert b\vert \sin \theta_{ab}.$
\item[(vii)]
Let $u,v\equiv X_1',X_2'$ be two parameters associated with an
orthonormal Cartesian basis $\{(0,1),(1,0)\}$, and let
$\Phi :(u,v)\mapsto {\Bbb R}^3$
be a mapping from some area of ${\Bbb R}^2$ into a twodimensional
surface of ${\Bbb R}^3$. Then the metric tensor is given by
$g_{ij}=
{\partial \Phi^k \over \partial X'^i}
{\partial \Phi^m \over \partial X'^j} \delta_{km}.$

\end{itemize}






{
\color{blue}
\bexample

Consider the following examples in three-dimensional vector space.
Let $r^2 = \sum_{i=1 }^3 x_i^2$.

\begin{enumerate}
\item
\begin{equation}
\begin{split}
  \partial_jr =  \partial_j \sqrt{\sum_ix_i^2} =
  \frac{1}{2}\frac{1}{\sqrt{\sum_ix_i^2}}\,2x_j =
  \frac{x_j}{r}
\end{split}
\end{equation}
By using the chain  rule one obtains
\begin{equation}
\begin{split}
  \partial_jr^\alpha =
  \alpha r^{\alpha-1}\left(\partial_jr\right) =
  \alpha r^{\alpha-1}\left(\frac{x_j}{r}\right)=
  \alpha r^{\alpha-2}x_j
\end{split}
\label{2011-m-eet1}
\end{equation}
and thus $\nabla r^\alpha = \alpha r^{\alpha-2}{\bf x}$.


\item
\begin{equation}
\begin{split}
  \partial_j \log r=\frac{1}{r}\left(\partial_jr\right)
\end{split}
\end{equation}
With $
  \partial_jr = \frac{x_j}{r}
$  derived earlier in Eq. (\ref{2011-m-eet1}) one obtains
$
 \partial_j \log r= \frac{1}{r}\frac{x_j}{r}=
  \frac{x_j}{r^2}
$,
and thus $\nabla  \log r =\frac{{\bf x}}{r^2}$.

\item
\begin{equation}
\begin{split}
  \partial_j
  \left[
    \left(
      \sum_i\left(x_i-a_i\right)^2
    \right)^{-\frac{1}{2}}+
    \left(
      \sum_i\left(x_i+a_i\right)^2
    \right)^{-\frac{1}{2}}
  \right]=
\\
  = -\frac{1}{2}\left[\frac{1}{\left(\sum_i\left(x_i-a_i\right)^2\right)^
    \frac{3}{2}}\,2\left(x_j-a_j\right)
  +\frac{1}{\left(\sum_i\left(x_i+a_i\right)^2\right)^\frac{3}{2}}\,
    2\left(x_j+a_j\right)\right]= \\
 -\left(\sum_i\left(x_i-a_i\right)^2\right)^{-\frac{3}{2}}\left(x_j-a_j\right)-
    \left(\sum_i\left(x_i+a_i\right)^2\right)^{-\frac{3}{2}}\left(x_j+a_j\right)   .
\end{split}
\end{equation}

\item
\begin{equation}
\begin{split}
\nabla \bigl({{\bf r} \over r^3} \bigr)\equiv
\\  \partial_i\left(\frac{r_i}{r^3}\right)=
  \frac{1}{r^3}\underbrace{\partial_i r_i}_{=3}+
  r_i\left(-3\frac{1}{r^4}\right)\left(\frac{1}{2r}\right)2r_i=
  3\frac{1}{r^3}-3\frac{1}{r^3}=0 .
\end{split}
\label{2011-m-eet2}
\end{equation}


\item  With the earlier solution (\ref{2011-m-eet2}) one obtains, for $r \neq 0$,
\begin{equation}
\begin{split}
\Delta \bigl({1 \over r} \bigr)\equiv
\\
  \partial_i\partial_i\frac{1}{r}=\partial_i\left(-\frac{1}{r^2}\right)
  \left(\frac{1}{2r}\right)2r_i=-\partial_i\frac{r_i}{r^3}=0   .
\end{split}
\end{equation}


\item  With the earlier solution (\ref{2011-m-eet2}) one obtains
\begin{equation}
\begin{split}
\Delta \bigl({{\bf r} {\bf p} \over r^3} \bigr)\equiv \\
  \partial_i\partial_i\frac{r_jp_j}{r^3}=
%****** unklar: siehe Vorlagen
    \partial_i
    \left[
      \frac{p_i}{r^3}+r_jp_j\left(-3\frac{1}{r^5}\right)r_i
    \right]= \\
  = p_i\left(-3\frac{1}{r^5}\right)r_i+
    p_i\left(-3\frac{1}{r^5}\right)r_i+ \\
   +r_jp_j
    \left[
      \left(15\frac{1}{r^6}\right)
      \left(\frac{1}{2r}\right)2r_i
    \right]r_i+
    r_jp_j\left(-3\frac{1}{r^5}\right)
    \underbrace{\partial_i r_i}_{=3}= \\
  = r_i p_i \frac{1}{r^5}(-3-3+15-9)=0
\end{split}
\end{equation}



\item    With $r\neq 0$ and constant $\bf p$ one obtains
\marginnote{Note that, in three dimensions, the Grassmann identity (\ref{2011-m-egi})
 $\varepsilon_{ijk}\varepsilon_{klm}       =          \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$
holds.}
\begin{equation}
\begin{split}
  \nabla \times ({\bf p} \times \frac{{\bf r}}{r^3})
 \equiv
  \varepsilon_{ijk}\partial_j\varepsilon_{klm}p_l\frac{r_m}{r^3}=
  p_l \varepsilon_{ijk} \varepsilon_{klm}
  \left[\partial_j\frac{r_m}{r^3}\right]  \\
 = p_l
    \varepsilon_{ijk}\varepsilon_{klm}
  \left[
    \frac{1}{r^3}\partial_j r_m + r_m
    \left(-3\frac{1}{r^4}\right)\left(\frac{1}{2r}\right)2r_j
  \right]  \\
  = p_l\varepsilon_{ijk}\varepsilon_{klm}
  \left[
    \frac{1}{r^3}\delta_{jm} - 3\frac{r_j r_m}{r^5}
  \right]  \\
  = p_l(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl})
  \left[
    \frac{1}{r^3}\delta_{jm} - 3\frac{r_j r_m}{r^5}
  \right]  \\
  = p_i \underbrace{\left(3\frac{1}{r^3}-3\frac{1}{r^3}\right)}_{=0}-
  p_j
  \Biggl({1\over {r^3}}
    \underbrace{{\partial_j r_i}}_{=\delta_{ij}}-
    3\frac{r_j r_i}{r^5}
  \Biggr)  \\
  = -\frac{{\bf p}}{r^3}+3\frac{\left({\bf r} {\bf p}\right){\bf r}}{r^5}
.
\end{split}
\end{equation}



\item
\begin{equation}
\begin{split}
{\nabla} \times({\nabla }\Phi )\\
\equiv
\varepsilon_{ijk} \partial_j \partial_k \Phi \\ =
\varepsilon_{ikj} \partial_k \partial_j \Phi  \\=
\varepsilon_{ikj} \partial_j \partial_k \Phi  \\=
-\varepsilon_{ijk} \partial_j \partial_k \Phi =0.
\end{split}
\end{equation}
This is due to the fact that $\partial_j \partial_k$ is  symmetric, whereas
$\varepsilon_{ijk}$ ist totally antisymmetric.


\item        For a proof that
$({{\bf x} } \times {{\bf y}})\times {{\bf z}}
\neq
{{\bf x} } \times ({{\bf y}}\times {{\bf z}})$ consider
\begin{equation}
\begin{split}
({{\bf x} } \times {{\bf y}})\times {{\bf z}}\\
\equiv
\varepsilon_{ijl}
\varepsilon_{jkm}
x_k y_m z_l\\=
-\varepsilon_{ilj}
\varepsilon_{jkm}
x_k y_m z_l\\ =
-(\delta_{ik}\delta_{lm}-
\delta_{im}\delta_{lk})
x_k y_m z_l\\ =
-x_i {{\bf y}}\cdot {{\bf z}}+
y_i {{\bf x}}\cdot {{\bf z}}.
\end{split}
\end{equation}
{\it versus}
\begin{equation}
\begin{split}
{{\bf x} } \times ({{\bf y}}\times {{\bf z}})\\
\equiv
\varepsilon_{ilj}
\varepsilon_{jkm}
x_l y_k z_m\\ =
(\delta_{ik}\delta_{lm}-
\delta_{im}\delta_{lk})
x_l y_k z_m\\ =
y_i {{\bf x}}\cdot {{\bf z}}-
z_i {{\bf x}}\cdot {{\bf y}}.
\end{split}
\end{equation}





\item
Let ${\bf w} = { {{\bf p}} \over r} $ with  $p_i=p_i\left(t  - {r \over c}\right)$,
whereby   $t$ and $c$  are constants. Then,
\begin{eqnarray*}
  \mbox{div}{\bf w}& = &
\nabla \cdot {\bf w}\\
\equiv \partial_i w_i & = & \partial_i
  \left[
    \frac{1}{r} p_i \left(t-\frac{r}{c}\right)
  \right]= \\
  & = & \left(-\frac{1}{r^2}\right)\left(\frac{1}{2r}\right)
    2r_i p_i+
    \frac{1}{r}p_i'\left(-\frac{1}{c}\right)
    \left(\frac{1}{2r}\right)2 r_i= \\
  & = & -\frac{r_i p_i}{r^3}-\frac{1}{c r^2}p_i' r_i
.
\end{eqnarray*}
Hence,
$
  \mbox{div}{\bf w}=\nabla \cdot {\bf w}=
  -\left(\frac{{\bf r} {\bf p}}{r^3}+\frac{{\bf r} {\bf p}'}{c r^2}\right)
$.



\begin{eqnarray*}
\mbox{rot}{\bf w}& =& \nabla \times {\bf w}  \\
  \varepsilon_{ijk}\partial_j w_k & = &
   \equiv  \varepsilon_{ijk}
    \left[
      \left(-\frac{1}{r^2}\right)\left(\frac{1}{2r}\right)
      2 r_j p_k +
      \frac{1}{r}p_k'
      \left(-\frac{1}{c}\right)\left(\frac{1}{2r}\right)2r_j
    \right]= \\
  & = & -\frac{1}{r^3}\varepsilon_{ijk}r_j p_k -\frac{1}{cr^2}
    \varepsilon_{ijk}r_j p_k' = \\
  & \equiv & -\frac{1}{r^3}\left({\bf r} \times {\bf p}\right)-
    \frac{1}{cr^2}\left({\bf r} \times {\bf p}'\right)   .
\end{eqnarray*}

\item
Let us verify  some specific examples of Gauss' (divergence) theorem,
\index{Gauss' theorem}
stating that the outward flux of a vector field through a closed surface
is equal to the volume integral of the divergence of the region inside the surface.
That is, the sum of all sources subtracted by the sum of all sinks represents the net flow out of a region or volume of threedimensional space:
\begin{equation}
\int \limits_V \nabla \cdot {\bf w} \, dv   =\int \limits_F {\bf w} \cdot d{\bf f}
.   \label{2011-m-gauss}
\end{equation}

Consider the vector field ${\bf w} = (4x, -2y^2, z^2)$
and the (cylindric) volume bounded by the planes  $z=0$ und $z=3$,
as well as by the surface
$x^2 + y^2 = 4$.


Let us first look at the left hand side $\int \limits_V \nabla \cdot {\bf w} \, dv $
of Eq. (\ref{2011-m-gauss}):
$$
  \nabla {\bf w}=\textrm{div } {\bf w}=4-4y+2z
$$
\begin{eqnarray*}
  \Longrightarrow \int \limits_V \! \textrm{div } {\bf w} dv& = &
  \int \limits_{z=0}^3 \! dz \int \limits_{x=-2}^2 \!\! dx
  \int \limits_{y=-\sqrt{4-x^2}}^{\sqrt{4-x^2}} \!\!\!dy\,
    \left(4-4y+2z\right)= \\
  & & \mbox{cylindric coordinates: }
  \Biggl[
    \begin{array}{rcl}
      x & = & r \cos \varphi \\
      y & = & r \sin \varphi \\
%***** \"z-Koordinatentransformation\" hinzugefuegt
      z & = & z
    \end{array}
  \Biggr] \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \int \limits_0^{2\pi} \!\! d\varphi \left(4-4r \sin \varphi+2z\right)= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r \, dr
  \left(4 \varphi +4r\cos\varphi+2\varphi z\right)
  \Biggl|_{\varphi=0}^{2 \pi}= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \left(8 \pi +4r+4 \pi z -4r\right)= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \left(8 \pi +4 \pi z\right) \\
  & = & 2 \left( 8 \pi z +4 \pi \frac{z^2}{2}\right)\Biggl|_{z=0}^{z=3}=
    2 (24+18) \pi = 84 \pi
\end{eqnarray*}

Now consider the right hand side $\int \limits_F {\bf w} \cdot d{\bf f}$
of Eq. (\ref{2011-m-gauss}).
The surface consists of three  parts:
the lower plane $F_1$ of the cylinder is characterized by $z=0$;
the upper plane $F_2$  of the cylinder is characterized by  $z=3$;
the surface on the side of the zylinder $F_3$
 is characterized by   $x^2+y^2=4$.
$d {\bf f}$ must be normal to these surfaces, pointing outwards; hence
 \begin{eqnarray*}
  F_1 : \int \limits_{{\cal F}_1} {\bf w} \cdot d{\bf f}_1  & = &
    \int \limits_{{\cal F}_1}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2=0
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        0 \\
        0 \\
        -1
      \end{array}
    \right)
    \, d\, x d\, y = 0 \\
  F_2 : \int \limits_{{\cal F}_2} {\bf w} \cdot d{\bf f}_2 & = &
    \int \limits_{{\cal F}_2}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2=9
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        0 \\
        0 \\
        1
      \end{array}
    \right)
    \, d\, x d\, y = \\
  & = & 9 \int \limits_{K_{r=2}} \!\! d\, f=9 \cdot 4 \pi=36 \pi \\
  F_3 : \int \limits_{{\cal F}_3} {\bf w} \cdot d{\bf f}_3 & = &
    \int \limits_{{\cal F}_3}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2
      \end{array}
    \right)
    \left(
      \frac{\partial {\bf x}}{\partial \varphi} \times
      \frac{\partial {\bf x}}{\partial z}
    \right)
    \, d\varphi \, dz \quad (r=2=\mbox{const.})
\end{eqnarray*}
$$
  \frac{\partial {\bf x}}{\partial \varphi} =
  \left(
    \begin{array}{c}
      -r \sin \varphi \\
       r \cos \varphi \\
       0
    \end{array}
  \right)=
  \left(
    \begin{array}{c}
      -2 \sin \varphi \\
       2 \cos \varphi \\
       0
    \end{array}
  \right); \enspace
  \frac{\partial {\bf x}}{\partial z} =
  \left(
    \begin{array}{c}
      0 \\
      0 \\
      1
    \end{array}
  \right)
$$
$$
   \Longrightarrow
  \left(
    \frac{\partial {\bf x}}{\partial \varphi} \times
    \frac{\partial {\bf x}}{\partial z}
  \right) =
  \left(
    \begin{array}{c}
      2 \cos \varphi \\
      2 \sin \varphi \\
      0
    \end{array}
  \right)
$$
\begin{eqnarray*}
  \Longrightarrow
  F_3 & = & \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \int \limits_{z=0}^3 \!\! dz
    \left(
      \begin{array}{c}
        4 \cdot 2 \cos \varphi \\
        -2(2 \sin \varphi)^2 \\
        z^2
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        2 \cos \varphi \\
        2 \sin \varphi \\
        0
      \end{array}
    \right)= \\
    &= & \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \int \limits_{z=0}^3 \!\! dz
    \left(16 \cos^2 \varphi -16 \sin^3 \varphi\right) = \\
  & = & 3 \cdot 16 \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \left( \cos^2\varphi - \sin^3 \varphi \right) = \\
  & = &
    \Biggl[
      \begin{array}{rcl}
        \int \cos^2 \varphi \, d\varphi & = & \frac{\varphi}{2}+
          \frac{1}{4} \sin 2 \varphi \\
        \int \sin^3 \varphi \, d\varphi & = & -\cos \varphi+
          \frac{1}{3} \cos^3 \varphi
      \end{array}
    \Biggr] \, = \\
    & = & 3 \cdot 16
    \Biggl\{
      \frac{2 \pi}{2}-
      \underbrace
        {\left[
          \left(1+\frac{1}{3}\right)-\left(1+\frac{1}{3}\right)
        \right]}
      _{=0}
    \Biggr\}=48 \pi
\end{eqnarray*}
For the flux through the surfaces one thus obtains
$$ \oint \limits_F {\bf w} \cdot d{\bf f}=F_1+F_2+F_3=84 \pi .$$





\item
Let us verify  some specific examples of Stokes' theorem in three dimensions,
\index{Stokes' theorem}
stating that
\begin{equation}
\int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f}   =\oint \limits_{{\cal C}_{\cal F}} {\bf b} \cdot d{\bf s}
.   \label{2011-m-stokes}
\end{equation}

Consider the vector field ${\bf b} = (yz, -xz, 0)$
and the (cylindric) volume bounded by spherical cap
formed by the plane at $z = a / \sqrt{2}$ of a sphere of radius $a$ centered around the origin.

Let us first look at the left hand side $\int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f} $
of Eq. (\ref{2011-m-stokes}):
$$
  {\bf b}=
  \left(
    \begin{array}{c}
      yz \\
      -xz \\
      0
    \end{array}
  \right)
  \Longrightarrow
  \textrm{rot } {\bf b} = \nabla \times {\bf b} =
  \left(
    \begin{array}{c}
      x \\
      y \\
      -2z
    \end{array}
  \right)
$$
Let us transform this into spherical coordinates:
$$
  {\bf x}=
  \left(
    \begin{array}{c}
      r\sin \theta \cos \varphi \\
      r\sin \theta \sin \varphi \\
      r\cos \theta
    \end{array}
  \right)
$$
$$
  \Rightarrow
  \frac{\partial {\bf x}}{\partial \theta}=
  r
  \left(
    \begin{array}{c}
      \cos \theta \cos \varphi \\
      \cos \theta \sin \varphi \\
      -\sin \theta
    \end{array}
  \right);\quad
  \frac{\partial {\bf x}}{\partial \varphi}=
  r
  \left(
    \begin{array}{c}
      -\sin \theta \sin \varphi \\
      \sin \theta \cos \varphi \\
      0
    \end{array}
  \right)
$$
$$
  d{\bf f}=
  \left(
    \frac{\partial {\bf x}}{\partial \theta} \times
    \frac{\partial {\bf x}}{\partial \varphi}
  \right)
  d\theta \, d\varphi=
  r^2
  \left(
    \begin{array}{c}
      \sin^2 \theta \cos \varphi \\
      \sin^2 \theta \sin \varphi \\
      \sin \theta \cos \theta
    \end{array}
  \right)
  d\theta \, d\varphi \label{eqn:1.14.1}
$$
$$
  \nabla \times {\bf b}=
  r
  \left(
    \begin{array}{c}
      \sin \theta \cos \varphi \\
      \sin \theta \sin \varphi \\
      -2\cos \theta
    \end{array}
  \right)
$$

\begin{eqnarray*}
  \int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f} & = &
  \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
  \int \limits_{\varphi=0}^{2 \pi} \!\! d\varphi \, a^3
  \left(
    \begin{array}{c}
      \sin \theta \cos \varphi \\
      \sin \theta \sin \varphi \\
      -2\cos \theta
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      \sin^2 \theta \cos \varphi \\
      \sin^2 \theta \sin \varphi \\
      \sin \theta \cos \theta
    \end{array}
  \right)= \\
  & = & a^3
  \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
  \int \limits_{\varphi=0}^{2 \pi} \!\! d\varphi
  \Biggl[
    \sin^3\theta
    \underbrace{\left(\cos^2\varphi+\sin^2\varphi\right)}_{=1}-
      2\sin\theta\cos^2\theta
  \Biggr] = \\
  & = & 2 \pi a^3
  \left[
    \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
    \left(1-\cos^2\theta\right)\sin\theta-
    2\int \limits_{\theta=0}^{\pi/4} \!\! d\theta
    \sin \theta \cos^2 \theta
  \right] = \\
  & = & 2 \pi a^3
  \int \limits_{\theta=0}^{\pi/4}d\theta
  \sin \theta \left(1-3\cos^2\theta\right) = \\
  && \left[
    \begin{array}{l}
      \mbox{transformation of variables: } \\
      \cos \theta = u \Rightarrow du=-\sin \theta d\theta
      \Rightarrow d\theta=-\frac{du}{\sin \theta}
    \end{array}
  \right] \\
  & = & 2 \pi a^3
  \int \limits_{\theta=0}^{\pi/4}(-du)\left(1-3u^2\right)=
    2 \pi a^3
    \left( \frac{3u^3}{3}-u \right)\Biggr|_{\theta=0}^{\pi/4}= \\
  & = & 2 \pi a^3
    \left(\cos^3\theta-\cos\theta \right)\Biggr|_{\theta=0}^{\pi/4}=
    2 \pi a^3
    \left(\frac{2\sqrt{2}}{8}-\frac{\sqrt{2}}{2} \right) = \\
  & = & \frac{2 \pi a^3}{8}\left(-2\sqrt{2}\right)=
    -\frac{\pi a^3 \sqrt{2}}{2}
\end{eqnarray*}


Now consider the right hand side $\oint \limits_{{\cal C}_{\cal F}} {\bf b} \cdot d{\bf s}$
of Eq. (\ref{2011-m-stokes}).
The radius $r'$ of the circle  surface
$\{(x,y,z) \mid x,y \in {\Bbb R} ,z=a/\sqrt{2}\}$ bounded by the sphere with radius $a$
is determined by
$ a^2 =(r')^2 +{a\over 2}$; hence, $r' =a/\sqrt{2}$.
The curve of integration ${\cal C}_{\cal F}$ can be parameterized by
$$\{(x,y,z) \mid
x={a\over \sqrt{2}} \cos \varphi,
y={a\over \sqrt{2}} \sin \varphi,
z={a\over \sqrt{2}}\}.$$
Therefore,
$$
  {\bf x} = a
  \left(
    \begin{array}{c}
      \frac{1}{\sqrt2}\cos\varphi \\[1ex]
      \frac{1}{\sqrt2}\sin\varphi \\[1ex]
      \frac{1}{\sqrt2}
    \end{array}
  \right)=
  \frac{a}{\sqrt{2}}
  \left(
    \begin{array}{c}
      \cos\varphi \\
      \sin\varphi \\
      1
    \end{array}
  \right)
\in {\cal C}_{\cal F}
$$
Let us transform this into polar coordinates:
$$
  d{\bf s}=\frac{d{\bf x}}{d\varphi} \,d\varphi =
  \frac{a}{\sqrt{2}}
  \left(
    \begin{array}{c}
      -\sin\varphi \\
      \cos\varphi \\
      0
    \end{array}
  \right) d\varphi
$$
$$
  {\bf b}=
  \left(
    \begin{array}{c}
      \frac{a}{\sqrt{2}}\sin\varphi\cdot\frac{a}{\sqrt{2}} \\
      -\frac{a}{\sqrt{2}}\cos\varphi\cdot\frac{a}{\sqrt{2}} \\
      0
    \end{array}
  \right)=
  \frac{a^2}{2}
  \left(
    \begin{array}{c}
      \sin\varphi \\
      -\cos\varphi \\
      0
    \end{array}
  \right)
$$
Hence the circular integral is given by
$$
  \oint \limits_{{\cal C}_F} {\bf b} \cdot d{\bf s}=
  \frac{a^2}{2}\frac{a}{\sqrt{2}}
  \int \limits_{\varphi=0}^{2 \pi}
  \underbrace
    {\left(-\sin^2\varphi-\cos^2\varphi\right)}
  _{=-1}
  \, d\varphi =
  -\frac{a^3}{2\sqrt{2}}2 \pi=-\frac{a^3 \pi}{\sqrt{2}}
.
$$




\end{enumerate}

\eexample
}

\section{Some common misconceptions}

\subsection{Confusion between component representation and ``the real thing''}
Given a particular basis, a tensor is uniquely characterized by its components.
However, without reference to a particular basis, any components are just blurbs.

Example (wrong!): a type-1 tensor (i.e., a vector) is given by
$(1,2)$.

Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a rank-1 tensor (i.e., a vector) is given by
$(1,2)$.


\subsection{A matrix is a tensor}

See the above section.
{
\color{blue}
\bexample
Example (wrong!): A matrix is  a  tensor of type (or rank) 2.
\eexample
}
Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a matrix represents a type-2 tensor.
The matrix components are the tensor components.

Also, for non-orthogonal bases, covariant, contravariant, and mixed tensors correspond to different matrices.


\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright \decofourleft
%\aldine X \decoone c
\floweroneright
% \aldineleft ] \decosix g \leafleft
% \aldineright Y \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
% \decofourright b \floweroneleft
}
\end{center}


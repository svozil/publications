\chapter{Special functions of mathematical physics}
\label{2011-m-ch-sf}


Special functions\marginnote{This chapter follows several approaches:
%\cite{lebedev:1965:sft,Wilf,bell-specfun,Temme:1996:SFI,Temme-11612,andrews:1999:sfu,Kuznetsov,Kisil}}
\cite{lebedev:1965:sft,Wilf,bell-specfun,andrews:1999:sfu,Kuznetsov,Kisil}}
\marginnote{For reference, consider
\cite{abramowitz:1964:hmf,Brych-HBSF,Gradshteyn}}   often arise as solutions of differential equations; for instance as eigenfunctions
of differential operators in quantum mechanics.
Sometimes they occur  after several {\em separation of variables}
and substitution steps have transformed the physical problem into something manageable.
For instance, we might start out with some linear partial differential equation like the wave equation,
then separate the space from time coordinates,
then separate the radial from the angular components,
and finally, separate the two angular parameters.
After we have done that, we end up with several separate differential equations of the Liouville form;
among them the Legendre differential equation leading us to the Legendre polynomials.

In what follows, a particular class of special functions will be considered.
These functions are all special cases of the
{\em hypergeometric function},
\index{hypergeometric function}
which is the solution of the
{\em hypergeometric differential equation}.
\index{hypergeometric differential equation}
The hypergeometric function exhibits a high degree of
``plasticity,''
as many elementary analytic functions can be expressed by it.

First, as a prerequisite, let us define the gamma function.
Then we proceed to second order Fuchsian differential equations;
\index{Fuchsian equation}
followed by rewriting a Fuchsian  differential equation
into a hypergeometric differential equation.
Then we study the hypergeometric function as a solution to the
hypergeometric differential equation.
Finally, we mention some particular hypergeometric functions, such as the
Legendre orthogonal polynomials, and others.

Again, if not mentioned otherwise, we shall restrict our attention to
second order differential equations.
Sometimes -- such as for the Fuchsian class -- a generalization is possible but
not very relevant for physics.

\section{Gamma function}
\index{gamma function}

The gamma function $\Gamma (x)$ is an extension of the factorial (function)  $n!$ because
it generalizes the ``classical'' factorial, which is defined on the natural numbers,
to real or complex arguments (different from the negative integers and from zero); that is,
\begin{equation}
\Gamma (n+1) = n! \textrm { for } n \in {\Bbb N}
\textrm{, or }
\Gamma (n) = (n-1)! \textrm { for } n \in {\Bbb N}-0
.
\label{2011-m-ch-sfgamman}
\end{equation}

Let us first define the
{\em shifted factorial}
\index{shifted factorial}
or, by another naming,
the
{\em Pochhammer symbol}
\index{Pochhammer symbol}
\begin{equation}
\begin{split}
(a)_0\stackrel{{\tiny \textrm{ def }}}{=}1,
\\
(a)_n\stackrel{{\tiny \textrm{ def }}}{=}
a(a+1)\cdots (a+n-1)
,
\end{split}
\label{2011-m-ch-sfsf}
\end{equation}
where $n>0$ and $a$ can be any real or complex number.
If $a$ is a natural number greater than zero, $(a)_n=
\frac{\Gamma (a+n)}{\Gamma(a)}$.
Note that
$(a)_1=a$ and $(a)_2=a(a+1)$,
and so on.


With this definition of the shifted factorial,
\begin{equation}
\begin{split}
 z ! ( z +1)_ n
=1\cdot 2 \cdots  z  \cdot ( z +1)(( z +1)+1)\cdots (( z +1)+ n -1)  \\
\qquad
=1\cdot 2 \cdots  z  \cdot ( z +1)(  z +2)\cdots ( z + n ) \\
\qquad
= ( z + n )! ,
\\
\textrm{ or }
 z !
= \frac{( z + n )!}{  ( z +1)_ n }.
\end{split}
\label{2011-m-ch-sfsf1}
\end{equation}

Since
\begin{equation}
\begin{split}
( z + n )!
=( n + z )!     \\
\qquad
= 1\cdot 2 \cdots  n  \cdot ( n +1)( n +2)\cdots ( n + z )   \\
\qquad
=  n ! \cdot ( n +1)( n +2)\cdots ( n + z )   \\
\qquad
= n !( n +1)_ z  ,
\end{split}
\end{equation}
we can rewrite Eq. (\ref{2011-m-ch-sfsf1}) into
\begin{equation}
 z !
= \frac{ n !( n +1)_ z  }{  ( z +1)_ n }
=   \frac{ n !  n ^ z  }{  ( z +1)_ n }  \frac{( n +1)_ z  }{  n ^ z }
.
\label{2011-m-ch-sfsf2}
\end{equation}

The latter factor, for large $n$, converges as
\index{order of}
\marginnote{Again, just as on page \pageref{2018-mm-ch-di-otof}, ``$O(x)$'' means ``of the order of $x$'' or ``absolutely bound by'' in the following way:
if $g(x)$ is a positive function,  then $f(x)=O(g(x))$ implies that there exist a positive real number $m$
such that $\vert f(x) \vert < m g(x)$.}

\begin{equation}
%\begin{split}
\begin{split}
\frac{( n +1)_ z  }{  n ^ z }   =
\frac{( n +1)(( n +1)+1)\cdots (( n +1)+ z -1)}{  n ^ z }  \\
=
\underbrace{\frac{( n +1)}{n}
\frac{( n +2)}{n}
\cdots
\frac{( n +z)}{n}}_{z \text{ factors}}
  \\
%\qquad
=
\frac{ n ^ z  +O( n ^{ z -1})}{  n ^ z }
%\qquad
=
\frac{ n ^ z }{  n ^ z }  +
\frac{O( n ^{ z -1})}{ n ^ z }
%\qquad
=
1  + O( n ^{-1})
\stackrel{ n  \rightarrow \infty}{\longrightarrow} 1.
\end{split}
%\end{split}
\end{equation}
In this limit, Eq. (\ref{2011-m-ch-sfsf2})  can be written as
\begin{equation}
 z !
= \lim_{ n \rightarrow \infty}  z !
=  \lim_{ n \rightarrow \infty} \frac{ n !  n ^ z  }{  ( z +1)_ n }
.
\label{2011-m-ch-sfsf3}
\end{equation}

Hence, for all $z\in {\Bbb C}$ which are not equal to a negative integer
--
that is,  $z \not\in\left\{-1,-2,\ldots\right\}$
--
we can, in analogy to the ``classical factorial,''
define   a ``factorial function shifted by one'' as
\begin{equation}
\Gamma ( z + 1)
\stackrel{{\tiny \textrm{ def }}}{=} \lim_{ n \rightarrow \infty} \frac{ n !  n ^ z  }{  ( z +1)_ n }.
\label{2011-m-ch-sfsfdga}
\end{equation}
That is,
$\Gamma (z+1)$ has been redefined to
allow  an analytic continuation of the ``classical'' factorial $z!$ for $z \in \mathbb{N}$:
in (\ref{2011-m-ch-sfsfdga}) $z$ just appears in an exponent and in the argument of a shifted factorial.

At the same time basic properties of the factorial are maintained:
because for very large $n$ and constant $z$ (i.e., $z\ll n$),
$(z+n)\approx n$, and
\begin{equation}
\begin{split}
\Gamma ( z  )
=  \lim_{ n \rightarrow \infty} \frac{ n !  n ^ { z -1}  }{  ( z )_n }
\\ \qquad
=  \lim_{ n \rightarrow \infty} \frac{ n !  n ^ { z -1}  }{  z(z+1)\cdots (z+n-1) }
\\
=
\lim_{ n \rightarrow \infty} \frac{ n !  n ^ { z -1}  }{  z(z+1)\cdots (z+n-1) }
\underbrace{\left(\frac{z+n}{z+n}\right)}_{1}
\\ \qquad
=
\lim_{ n \rightarrow \infty} \frac{ n !  n ^ { z -1} (z+n) }{  z(z+1)\cdots (z+n) }
\\
=
\frac{1}{z} \lim_{ n \rightarrow \infty} \frac{ n !  n ^ z  }{  ( z +1)_n }
=
\frac{1}{z} \Gamma ( z +1 )
.
\label{2011-m-ch-sfsfdga234}
\end{split}
\end{equation}
This implies that
\begin{equation}
\Gamma ( z + 1)
=  z
\Gamma ( z  ) .
\label{2011-m-ch-sfsfdgas1ind}
\end{equation}
Note that, since
\begin{equation}
(1)_n =1 (1+1) (1+2)\cdots (1+n-1)= n! ,
\end{equation}
Eq. (\ref{2011-m-ch-sfsfdga})  yields
\begin{equation}
\Gamma ( 1)
=  \lim_{ n \rightarrow \infty} \frac{ n !  n ^ 0  }{  (  1)_ n }
=  \lim_{ n \rightarrow \infty} \frac{ n !    }{  n !}  =1
.
\label{2011-m-ch-sfsfdgas1}
\end{equation}
By induction, Eqs.
(\ref{2011-m-ch-sfsfdgas1})
and
(\ref{2011-m-ch-sfsfdgas1ind})
yield
$\Gamma ( n +1 ) =n!$ for $n\in {\Bbb N}$.

We state without proof that, for complex numbers $z$
with positive real parts $\Re z >0 $,  $\Gamma (z)$ can be defined by an integral
representation as
\begin{equation}
\Gamma ( z )
\stackrel{{\tiny \textrm{ def }}}{=}
\int_0^\infty t^{z-1}e^{-t}dt
\label{2017-m-ch-sf-edgamma}
.
\end{equation}
{\color{OliveGreen}
\bproof
Note that Eq. (\ref{2011-m-ch-sfsfdgas1ind}) can be derived from this integral representation
of $\Gamma(z)$
by partial integration; that is [with $u=t^z$ and $v'=\exp (-t)$, respectively],
\begin{equation}
\begin{split}
\Gamma ( z+1 )
=
\int_0^\infty t^{z}e^{-t}dt
\\
=
\underbrace{\left. -t^{z}e^{-t}  \right|_0^\infty }_{=0}
-  \left[- \int_0^\infty \left( \frac{d }{dt}t^{z}\right) e^{-t} dt \right]
\\
=
\int_0^\infty z t^{z-1} e^{-t} dt
\\
=
z\int_0^\infty  t^{z-1} e^{-t} dt = z \Gamma ( z )
.
\end{split}
\end{equation}
Therefore, Eq.~(\ref{2017-m-ch-sf-edgamma})
can be verified for $z\in \mathbb{N}$ by complete induction. The induction basis
$z=1$  can be directly evaluated:
\begin{equation}
\Gamma ( 1 )
\int_0^\infty \underbrace{t^{0}}_{=1} e^{-t}dt = \left. - e^{-t} \right|_0^\infty
= - \underbrace{e^{-\infty}}_{=0} - \underbrace{(- e^{0})}_{=-1} =1.
\end{equation}
% http://www.math.uconn.edu/~kconrad/blurbs/analysis/stirling.pdf
\eproof
}

We also mention the following formul\ae:
\begin{equation}
\begin{split}
\Gamma \left( \frac{1}{2} \right)
=\int_0^\infty  \frac{1}{\sqrt{t}} e^{-t} dt \\
\text{[variable substitution: }
u=\sqrt{t}, t=u^2, dt=2u\,du\textrm{]}\\
=\int_0^\infty  \frac{1}{u} e^{-u^2} 2u \, du
=2\int_0^\infty   e^{-u^2} du
=\int_{-\infty}^\infty   e^{-u^2} du
=\sqrt{\pi } ,
\end{split}
\end{equation}
where the Gaussian integral~(\ref{2018-m-ch-di-gi}) on page~\pageref{2018-m-ch-di-gi}  has been used.
Furthermore, more generally, without proof
\begin{equation}
\Gamma \left( \frac{n}{2}\right)=\sqrt{\pi }\frac{( n-2)!!}{2^{(n-1)/2}} \textrm{, for }n>0; \textrm {  and }
\end{equation}
\begin{equation}
\text{Euler's reflection formula }\; \Gamma ( x)\Gamma ( 1-x) =\frac{\pi}{\sin (\pi x)} .
\end{equation}



Here, the
{\em  double factorial}
\index{double factorial} is defined by
\begin{equation}
n!!=
\begin{cases}
1   & \text{if } n=-1,0,  \\
2\cdot 4\cdots (n-2)\cdot n&\text{for even }   n= 2k ,  k\in \mathbb{N} \\
1\cdot 3 \cdots (n-2) \cdot  n &\text{for odd }  n= 2k-1 ,  k\in \mathbb{N} .
\end{cases}
\end{equation}

Note that the even and odd cases can be respectively rewritten as
\begin{equation}
\begin{split}
\text{for even }   n= 2k ,  k\ge 1:
n!!= 2\cdot 4\cdots (n-2)\cdot n\\
[n= 2k, k\in\mathbb{N}] =  (2k)!!
  =  \prod_{i=1}^k (2i)= 2^{k} \prod_{i=1}^k i\\
  =  2^{k}\cdot  1\cdot 2\cdots (k-1)\cdot k = 2^k\, k!
\\
\text{for odd }  n= 2k-1 ,  k\ge 1:
n!!= 1\cdot 3 \cdots (n-2) \cdot  n \\
[n= 2k-1, k\in\mathbb{N}] =(2k-1)!!
 =  \prod_{i=1}^k (2i-1) \\
  = 1\cdot 3 \cdots  (2k-1)  \underbrace{\frac{(2k)!!}{(2k)!!}}_{=1}    \\
  = \frac{1\cdot 2 \cdots (2k-2) \cdot  (2k-1)\cdot  (2k)}{(2k)!!}   =  \frac{(2k)!}{2^k \,k!}\\
  = \frac{k! (k+1)(k+2) \cdots [(k+1)+k-2][(k+1)+k-1] }{2^k \,k!}
  =  \frac{(k+1)_k }{2^k }
\end{split}
\end{equation}


% https://www.math.unl.edu/~sdunbar1/ProbabilityTheory/Lessons/StirlingsFormula/SurveyStirlingsFormula/surveystirlingsformula.pdf

Stirling's formula~\cite{Namias-1986}
\index{Stirling's formula}
[again, $O(x)$ means ``of the order of $x$'']
\begin{equation}
\begin{split}
\log n! = n \log n -n + O(\log (n))\textrm{, or }   \\
n! \stackrel{n\rightarrow \infty}{\longrightarrow} \sqrt{2\pi n} \left(\frac{n}{e}\right)^n
\textrm{, or, more generally, }\\
\Gamma (x) = \sqrt{\frac{2\pi }{x}}\left(\frac{x}{e}\right)^x \left( 1+ O\left(\frac{1}{x}\right) \right)
\end{split}
\end{equation}
is stated without proof.

\section{Beta function}
\index{beta function}
The  {\em beta function,}
also called the  {\em Euler integral of the first kind,} is a special function defined by
\index{Euler integral}
\begin{equation}
B(x,y)=\int_0^1 t^{x-1}(1-t)^{y-1} dt =\frac{\Gamma (x) \Gamma (y)}{\Gamma (x+y)} \textrm{ for } \Re x, \Re y >0
\label{2011-m-ch-sf-beta}
\end{equation}
No proof of the identity of the two representations in terms of an integral, and of $\Gamma$-functions is given.


\section{Fuchsian differential equations}
\index{Fuchsian equation}

Many differential equations of theoretical physics are Fuchsian equations.
We shall, therefore, study this class in some generality.


\subsection{Regular,  regular singular, and irregular singular point}
Consider the homogeneous differential equation   [Eq. (\ref{2011-m-ch-sl1}) on page \pageref{2011-m-ch-sl1} is inhomogeneous]
\begin{equation}
{\cal L}_x y(x) =   a_2(x) \frac{d^2}{dx^2}y(x)  +  a_1(x) \frac{d}{dx}y(x)+   a_0(x) y(x)
 =
0.
\label{2011-m-ch-sf-fc1}
\end{equation}
If $a_0(x)$, $a_1(x)$ and $a_2(x)$ are analytic at some point $x_0$ and in its neighborhood,
and if $a_2(x_0)\neq 0$
at $x_0$, then
 $x_0$
is called an {\em ordinary point}, or {\em regular point}.
\index{ordinary point}
\index{regular point}
We  state without proof that in this case the solutions around $x_0$ can be expanded as power series.
In this case we can  divide equation (\ref{2011-m-ch-sf-fc1}) by $a_2(x)$ and rewrite it
\begin{equation}
\frac{1}{a_2(x)}{\cal L}_x y(x) =   \frac{d^2}{dx^2}y(x) +   p_1  (x)    \frac{d}{dx}y(x)+    p_2  (x)   y(x)
 =
0,
\label{2011-m-ch-sf-fc2}
\end{equation}
with
$  p_1  (x)  =    a_1(x) / a_2(x)
$
and
$ p_2 (x) =     a_0(x) / a_2(x)
$.

If, however, $a_2(x_0)= 0$ and $a_1(x_0)$ or $a_0(x_0)$ are nonzero, then the $x_0$ is called
{\em singular point}
\index{singular point} of (\ref{2011-m-ch-sf-fc1}).
In the simplest case $a_2(x)$ has a {\em simple zero} at $x_0$:
then both $  p_1  (x)  $ and $  p_2  (x)  $
in (\ref{2011-m-ch-sf-fc2})
have at most simple poles.


Furthermore, for reasons disclosed later
-- mainly motivated by the possibility to write the solutions as power series --
a point $x_0$ is called a
{\em regular singular point}
\index{regular singular point}
of Eq. (\ref{2011-m-ch-sf-fc1})
if
\begin{equation}
\begin{split}
\lim_{x\rightarrow x_0} \left[ (x-x_0)\frac{ a_1(x)}{a_2(x)} \right]
=
\lim_{x\rightarrow x_0} \left[ (x-x_0) p_1(x) \right]
\textrm{, as well as }\\
\lim_{x\rightarrow x_0} \left[ (x-x_0)^2 \frac{a_0(x)}{a_2(x)} \right]
=
\lim_{x\rightarrow x_0} \left[ (x-x_0)^2 p_2(x) \right]
\end{split}
\end{equation}
both exist.
If anyone of these limits does not exist, the singular point is
an
{\em irregular singular point}.
\index{irregular singular point}

A linear ordinary differential equation is called {\em Fuchsian,}
or {\em Fuchsian differential equation}
generalizable to arbitrary order $n$ of differentiation
\begin{equation}
\left[ \frac{d^n}{dx^n} + p_1  (x) \frac{d^{n-1}}{dx^{n-1}} + \cdots  + p_{n-1}  (x)  \frac{d}{dx}+    p_n  (x) \right]
y(x)
 =
0,
\label{2011-m-ch-sf-fc2gc}
\end{equation}
if every singular point, including infinity,
is regular, meaning that  $p_k (x)$ has at most poles of order $k$.


A  very important case is a Fuchsian of the second order
(up to second derivatives occur).
In this case,
we suppose that the coefficients in (\ref{2011-m-ch-sf-fc2})
satisfy the following conditions:
\begin{itemize}
\item
$p_1  (x)$ has at most {\em single poles}, and
\item
$p_2 (x)$ has at most  {\em double poles}.
\end{itemize}

The simplest realization of this case is for
$
a_2(x)= a(x-x_0)^2
$,
$
a_1(x)= b(x-x_0)
$,
$
a_0(x)= c
$
for some constant $a,b,c \in {\Bbb C}$.


\subsection{Behavior at infinity}
\label{2012-m-ch-feainfty}
In order to cope with infinity $z=\infty$
let us transform the Fuchsian equation $w'' +p_1(z)w' +p_2(z)w=0$
into the new variable $t={1\over z}$.

\begin{equation}
\begin{split}
t={1\over z},\ z={1\over t},\ u(t)\stackrel{{\tiny \textrm{ def }}}
=
w\left({1\over t}\right)=w(z)
\\
{dt\over dz}=-{1\over z^2}=-t^2\text{ and }
{dz\over dt}=-{1\over t^2}
\textrm{; therefore }
{d\over dz}
=   {dt\over dz} {d\over dt} =
-t^2{d\over dt}
\\
{d^2\over dz^2}=-t^2{d\over dt}\left(-t^2{d\over dt}\right)=
-t^2\left(-2t{d\over dt}-t^2{d^2\over dt^2}\right)=
2t^3{d\over dt}+t^4{d^2\over dt^2}
\\
w'(z)={d\over dz}w(z)=-t^2{d\over dt}u(t)=
-t^2u'(t)
\\
w''(z)={d^2\over dz^2}w(z)=
\left(2t^3{d\over dt}+t^4
{d^2\over dt^2}\right)u(t)=2t^3u'(t)+t^4u''(t)
\end{split}
\end{equation}

Insertion into the Fuchsian equation $w''+p_1(z)w'+p_2(z)w=0 $ yields
\begin{equation}
   2t^3u'+t^4u''+p_1\left({1\over t}\right)(-t^2u')+
   p_2\left({1\over t}\right)u=0,
\end{equation}
and hence,
\begin{equation}
    u''+\left[{2\over t}-{p_1\left({1\over t}\right)
   \over t^2}\right]u'+{p_2\left({1\over t}\right)\over t^4}u=0.
\end{equation}
From
\begin{equation}
\tilde p_1(t)\stackrel{{\tiny \textrm{ def }}}{=} \left[{2\over t}-
{p_1\left({1\over t}\right)\over t^2}\right]
\label{2016-m-ch-sf-idp1}
\end{equation}
and
\begin{equation}
\displaystyle \tilde p_2(t)\stackrel{{\tiny \textrm{ def }}}{=} {p_2\left({1\over t}\right)\over t^4}
\label{2016-m-ch-sf-idp2}
\end{equation}
follows the form of the rewritten differential equation
\begin{equation}
u''+\tilde p_1(t)u'+\tilde p_2(t)u=0.
\end{equation}
A necessary criterion for this equation to be Fuchsian is that $0$ is an ordinary,
or at least a regular singular, point.

Note that, for infinity to be a regular singular point,
$\tilde p_1(t)$ must have at most a pole of the order of $ t^{-1}$,
and
$\tilde p_2(t)$ must have at most a pole of the order of $t^{-2}$
at $t=0$.
Therefore,
$(1/t) p_1(1/t) = z p_1(z)$
as well as
$(1/t^2) p_2(1/t) = z^2 p_2(z)$
must both be analytic functions as $t\rightarrow 0$,
or $z\rightarrow \infty$.
This will be an important finding for the following arguments.


\subsection{Functional form of the coefficients in Fuchsian differential equations}

The functional form of the coefficients $p_1(x)$ and $p_2(x)$,
resulting from the assumption of merely regular singular points can be estimated as follows.

First, let us start with {\em poles at finite complex numbers}.
Suppose there are $k$ finite poles.
[The behavior of
$p_1(x)$ and $p_2(x)$ at
infinity will be treated later.]
Therefore, in Eq. (\ref{2011-m-ch-sf-fc2}),
the coefficients must be of the form
\begin{equation}
\begin{split}
p_1  (x)  = \frac{P_1(x)}{\prod_{j=1}^k (x-x_j)} ,\\
\textrm{and }
p_2 (x) =  \frac{P_2(x)}{\prod_{j=1}^k (x-x_j)^2} ,
\end{split}
\label{2011-m-ch-sf-eforp}
\end{equation}
where the $x_1,\ldots ,x_k$ are $k$ the (regular singular) points of the poles,
and $P_1(x)$ and $P_2(x)$ are {\em entire functions;}
that is, they are analytic (or, by another wording, holomorphic)
over the whole complex plane formed by $\{x \mid x \in {\Bbb C} \}$.



Second, consider possible {\em poles at infinity.}
Note that the requirement that infinity is regular singular will restrict the possible growth of
$p_1(x)$
as well as
$p_2(x)$
and thus, to a lesser degree, of
$P_1(x)$
as well as
$P_2(x)$.

As has been shown earlier,
because of the requirement that infinity is regular singular, as $x$ approaches infinity,
$p_1(x)x$
as well as
$p_2(x)x^2$
must both be analytic.
Therefore,
$p_1(x)$ cannot grow faster than $\vert x \vert^{-1}$,
and
$p_2(x)$ cannot grow faster than $\vert x \vert^{-2}$.

Consequently, by (\ref{2011-m-ch-sf-eforp}),
as $x$ approaches infinity,
$
P_1 (x) = p_1 (x) \prod_{j=1}^k (x-x_j)
$
does not grow faster than $\vert x \vert^{k-1}$ -- which in turn means that $P_1 (x)$ is bounded by some constant times $\vert x \vert^{k-1}$.
Furthmore,
$
P_2 (x) = p_2(x)\prod_{j=1}^k (x-x_j)^2
$
%\label{2012-m-ch-sf-eforp12}
does not grow faster than $\vert x \vert^{2k-2}$
-- which in turn means that $P_2 (x)$ is bounded by some constant times $\vert x \vert^{2k-2}$.


Recall that both $P_1(x)$ and $P_2(x)$ are {\em entire functions.}
Therefore, because of the {\em generalized Liouville theorem} \cite{Greene}
(mentioned on page \pageref{2012-m-ch-ca-lt}),
\index{Liouville theorem}
\index{generalized Liouville theorem}
\label{2014-m-ch-sf-glt}
both
$P_1(x)$ and $P_2(x)$ must be polynomials
%-- that is, polynomials of the form $\frac{R(x)}{Q(x)}$, where $Q(x)$ is not identically zero --
of degree of at most $k-1$ and $2k-2$, respectively.
\index{rational function}

Moreover, by using
{\em partial fraction decomposition}
\index{partial fraction decomposition}
\cite{KristenssonC3}
of the rational functions
(that is, the quotient of polynomials of the form $\frac{R(x)}{Q(x)}$, where $Q(x)$ is not identically zero)
in terms of their pole factors $(x-x_j)$,
we obtain the general form of the coefficients
\begin{equation}
\begin{split}
p_1 (x)  = \sum_{j=1}^k \frac{A_j}{x-x_j},\\
\textrm{and }
p_2(x) = \sum_{j=1}^k \left[ \frac{B_j}{(x-x_j)^2}   +  \frac{C_j}{x-x_j} \right]
,
\end{split}
\label{2012-m-ch-sf-eforp12359}
\end{equation}
with constant $A_j, B_j, C_j \in {\Bbb C}$.
The resulting Fuchsian differential equation
is called
{\em Riemann differential equation}.
\index{Riemann differential equation}



Although we have considered an arbitrary finite number of poles,
for reasons that are unclear to this author,
in physics, we are mainly concerned
with two poles (i.e., $k=2$)
at finite points, and one at infinity.

The {\em hypergeometric differential equation} is a {\em Fuchsian differential equation}
which has at most {\em three regular singularities}, including infinity,
at \cite{Kuznetsov}  $0$, $1$, and $\infty$.

\subsection{Frobenius method: solution by power series}

Now let us get more concrete about the solution of Fuchsian equations.
\index{power series solution}
Thereby the general strategy is to transform an ordinary differential equation
into a system of (coupled) linear equations as follows:
It turns out that the solutions of Fuchsian differential equations can be
expanded as {\em power series}, so that the differentiations can be performed explicitly.
The unknow coefficients of these power series  which ``encode the solutions'' are then
obtained by utilizing the linear independence of different powers in these series.
Thereby every factor
multiplied by the powers in these series is enforced to vanish separately.

{
\color{blue}
\bexample
In order to obtain a feeling for power series solutions of differential equations,
consider the ``first order'' Fuchsian equation  \cite{larson-edwards-calculus}
\begin{equation}
y'-\lambda  y=0.
\label{2011-m-ch-sf-pss1}
\end{equation}
Make the {\it Ansatz}, also known as
{\em Frobenius method}
\index{Frobenius method}
\cite{arfken05},
 that the solution can be expanded into a power series of the form
\begin{equation}
y(x)=\sum_{j=0}^\infty a_j x^j.
\end{equation}
Then,  the second term of Eq. (\ref{2011-m-ch-sf-pss1}) is
$-\lambda   \sum_{j=0}^\infty a_j x^j$, whereas the first term   can be written as
\begin{equation}
\begin{split}
\left(\frac{d}{dx}  \sum_{j=0}^\infty a_j x^j\right) =
 \sum_{j=0}^\infty ja_j x^{j-1}=
 \sum_{j=1}^\infty ja_j x^{j-1}\\
=
 \sum_{m=j-1=0}^\infty (m+1)a_{m+1} x^{m}=
 \sum_{ j =0}^\infty (j+1)a_{j+1} x^{j}.
\end{split}
\label{2011-m-ch-sf-pss2iuzuiz}
\end{equation}
Therefore, by comparing the coefficients of $x^j$,  for $n\ge 0$,
\begin{equation}
\begin{split}
(j+1)a_{j+1}= \lambda   a_j\textrm{, or }\\
a_{j+1}= \frac{\lambda   a_j}{j+1} =a_0 \frac{\lambda ^{j+1}}{(j+1)!}\textrm{, and }\\
a_{j }= a_0 \frac{\lambda ^{j }}{j!}
 .
\end{split}
\end{equation}
Therefore,
\begin{equation}
y(x)=\sum_{j=0}^\infty a_0 \frac{\lambda ^{j }}{j!} x^j=a_0 \sum_{j=0}^\infty \frac{(\lambda  x)^{j }}{j!} =a_0 e^{\lambda  x}.
\end{equation}

\eexample
}

%\subsection{Laurent series {\it Ansatz}}
In the Fuchsian case let us consider the following {\it Frobenius Ansatz}
to expand the solution as a {\em generalized power series} around a regular singular point $x_0$,
which can be  motivated by Eq. (\ref{2011-m-ch-sf-eforp}), and by the {\em Laurent series expansion}
\index{Laurent series}
(\ref{011-m-ch-ca-else1})--(\ref{011-m-ch-ca-else2}) on
page \pageref{011-m-ch-ca-else1}:
\begin{equation}
\begin{split}
  p_1  (x)  = \frac{A_1(x)}{x-x_0}=\sum_{j=0}^\infty \alpha_j (x-x_0)^{j-1} \textrm{  for } 0 < \vert x-x_0 \vert < r_1,\\
 p_2 (x) = \frac{A_2(x)}{(x-x_0)^2}=\sum_{j=0}^\infty \beta_j (x-x_0)^{j-2} \textrm{  for } 0 < \vert x-x_0 \vert < r_2,\\
y(x)=  (x-x_0)^{\sigma} \sum_{l=0}^\infty  (x-x_0)^{l} w_l
=  \sum_{l=0}^\infty (x-x_0)^{l + \sigma} w_l \textrm{, with } w_0\neq 0
,
\end{split}
\label{2011-m-ch-sf-pss2}
\end{equation}
where $A_1(x)= [(x-x_0) a_1(x)]/a_2(x)$
and $A_2(x)= [(x-x_0)^2 a_0(x)]/a_2(x)$.
Eq. (\ref{2011-m-ch-sf-fc2})
then becomes
\begin{equation*}
\begin{split}
\frac{d^2}{dx^2}y(x) +   p_1  (x)    \frac{d}{dx}y(x)+    p_2  (x)   y(x)     =   0,    \\
\left[\frac{d^2}{dx^2}  + \sum_{j=0}^\infty \alpha_j (x-x_0)^{j-1}  \frac{d}{dx} + \sum_{j=0}^\infty \beta_j (x-x_0)^{j-2}\right]
\sum_{l=0}^\infty w_l (x-x_0)^{l + \sigma}     =   0,    \\
\sum_{l=0}^\infty ({l + \sigma})({l + \sigma-1}) w_l (x-x_0)^{l + \sigma -2}\qquad \qquad \\
\qquad + \left[  \sum_{l=0}^\infty ({l + \sigma}) w_l (x-x_0)^{l + \sigma -1}\right]  \sum_{j=0}^\infty \alpha_j (x-x_0)^{j-1} \qquad \qquad
\\
\qquad + \left[\sum_{l=0}^\infty w_l (x-x_0)^{l + \sigma}\right] \sum_{j=0}^\infty \beta_j (x-x_0)^{j-2}
    =   0,    \\
(x-x_0)^{\sigma-2}  \sum_{l=0}^\infty (x-x_0)^{l}
\Bigg[
({l + \sigma})({l + \sigma-1})   w_l  \qquad \qquad \\
\qquad + ({l + \sigma})   w_l    \sum_{j=0}^\infty \alpha_j (x-x_0)^{j}
%\\
% \qquad
+  w_l  \sum_{j=0}^\infty \beta_j (x-x_0)^{j}
\Bigg]
    =   0,
\\
(x-x_0)^{\sigma-2} \left[ \sum_{l=0}^\infty
({l + \sigma})({l + \sigma-1})   w_l (x-x_0)^{l}\right. \qquad \qquad \\
\qquad +  \sum_{l=0}^\infty ({l + \sigma})   w_l    \sum_{j=0}^\infty \alpha_j (x-x_0)^{l+j}
%\\
\left.
 +  \sum_{l=0}^\infty  w_l  \sum_{j=0}^\infty \beta_j (x-x_0)^{l+j}  \right]
    =   0.
\end{split}
\end{equation*}
Next, in order  to reach a  common power of $(x-x_0)$,
we perform an index identification  in the second and third summands (where the order of the sums change):
$l=m$ in the first summand, as well as an index shift
$
l+j =m
$, and thus
$j= m-l$.
Since
$l\ge 0$  and  $j\ge 0$, also $m=l+j$ cannot be negative.
Furthermore,
$0\le j = m-l$, so that  $l\le m$.
\begin{equation}
\begin{split}
(x-x_0)^{\sigma-2} \left[  \sum_{l=0}^\infty
({l + \sigma})({l + \sigma-1})   w_l (x-x_0)^{l} \right.    \qquad \qquad
\\
\qquad + \sum_{j=0}^\infty  \sum_{l=0}^\infty ({l + \sigma})   w_l    \alpha_j (x-x_0)^{l+j}  \qquad \qquad
\\
\left.
\qquad + \sum_{j=0}^\infty  \sum_{l=0}^\infty  w_l  \beta_j (x-x_0)^{l+j}  \right]
    =   0,\\
(x-x_0)^{\sigma-2} \left[   \sum_{m=0}^\infty
({m + \sigma})({m + \sigma-1})   w_m (x-x_0)^{m} \right. \qquad \qquad \\
\qquad + \sum_{m=0}^\infty  \sum_{l=0}^m ({l + \sigma})   w_l    \alpha_{m-l} (x-x_0)^{l+m-l}  \qquad \qquad
\\
\left.
\qquad + \sum_{m=0}^\infty  \sum_{l=0}^m  w_l  \beta_{m-l} (x-x_0)^{l+m-l}     \right]
    =   0,\\
(x-x_0)^{\sigma-2}\left\{     \sum_{m=0}^\infty
(x-x_0)^{m} \left[
({m + \sigma})({m + \sigma-1})   w_m \right.\right. \qquad \qquad \\
   \left.  \left.  \qquad + \sum_{l=0}^m  ({l + \sigma})   w_l   \alpha_{m-l}
 +  \sum_{l=0}^m  w_l \beta_{m-l}
\right]
\right\}
    =   0,    \\
(x-x_0)^{\sigma-2}\left\{     \sum_{m=0}^\infty
(x-x_0)^{m} \left[
({m + \sigma})({m + \sigma-1})   w_m
\right.
\right.   \qquad \qquad
 \\
\qquad \qquad +
\left.
\left.
 \sum_{l=0}^m w_l  \left( ({l + \sigma}) \alpha_{m-l}
 + \beta_{m-l}
\right)
\right]
\right\}
    =   0.
\end{split}
\label{2011-m-ch-sf-pss646465}
\end{equation}

If we can divide this equation through  $(x-x_0)^{\sigma-2}$
and exploit the linear independence of the polynomials $(x-x_0)^{m}$,
we obtain an infinite number of equations for the infinite number of coefficients $w_m$
by requiring that all the terms ``inbetween'' the $[\cdots ]$--brackets in Eq. (\ref{2011-m-ch-sf-pss646465})
vanish {\em individually.}
In particular, for $m=0$ and $w_0\neq 0$,
\begin{equation}
\begin{split}
({0+ \sigma})({0 + \sigma-1})   w_0
+
 w_0  \left( ({0 + \sigma}) \alpha_{0} + \beta_{0}\right)
    =   0\\
 f_0(\sigma ) \stackrel{{\tiny \textrm{ def }}}{=} \sigma({\sigma-1}) +  \sigma \alpha_{0} + \beta_{0}           =   0
.
\end{split}
\label{2011-m-ch-sf-pss2sigma}
\end{equation}
The {\em radius of convergence} of the solution will,
\index{radius of convergence}
in accordance with the Laurent series expansion, extend to the next singularity.

Note that in  Eq. (\ref{2011-m-ch-sf-pss2sigma}) we have defined $f_0(\sigma )$ which we will use now.
Furthermore, for successive $m$, and with the definition of
\begin{equation}
f_k(\sigma ) \stackrel{{\tiny \textrm{ def }}}{=} \alpha_k \sigma +\beta_k,
\end{equation}
we obtain the sequence of linear equations
\begin{equation}
\begin{split}
w_0f_0(\sigma ) =0\\
w_1f_0(\sigma +1)+w_0f_1(\sigma )  =0,\\
w_2f_0(\sigma +2)+w_1f_1(\sigma +1) +w_0f_2(\sigma )  =0,\\
\qquad \vdots      \\
w_nf_0(\sigma+ n)   +w_{n-1}f_1(\sigma+ n-1)+ \cdots +w_0f_n(\sigma )  =0.
\end{split}
\label{2011-m-ch-sf-pss2sigmaiter}
\end{equation}
which can be used for an inductive determination of the coefficients $w_k$.

Eq. (\ref{2011-m-ch-sf-pss2sigma}) is a quadratic equation
$   \sigma^2 +\sigma (\alpha_{0}-1 ) +  \beta_{0}  =0$
for the
{\em characteristic exponents}
\index{characteristic exponents}
\begin{equation}
\sigma_{1,2} = \frac{1}{2} \left[1 - \alpha_{0} \pm \sqrt{(1 - \alpha_{0})^2-4  \beta_{0}}\right]
\end{equation}
We state without proof that, if the difference of the characteristic exponents
\begin{equation}
\sigma_1 - \sigma_2  =   \sqrt{(1 - \alpha_{0})^2-4  \beta_{0}}
\end{equation}
is  {\em nonzero} and {\em not} an integer, then the two solutions found from   $\sigma_{1,2}$
through  the generalized series Ansatz  (\ref{2011-m-ch-sf-pss2}) are linear independent.

Intuitively speaking, the Frobenius method ``is in obvious trouble'' to find
the general solution of the Fuchsian equation
if the two characteristic exponents coincide (e.g., $\sigma_1 = \sigma_2$),
but it ``is also in trouble'' to find
the general solution
if  $\sigma_1 - \sigma_2 = m \in {\Bbb N}$;
that is, if, for some positive integer $m$,  $\sigma_1  = \sigma_2 + m > \sigma_2$.
Because in this case, ``eventually''
at $n=m$ in Eq. (\ref{2011-m-ch-sf-pss2sigmaiter}),
we obtain as iterative solution for the coefficient $w_m$ the term
\begin{equation}
\begin{split}
w_m  = - \frac{w_{m-1}f_1(\sigma_2 + m-1)+ \cdots +w_0f_m(\sigma_2  )}{f_0(\sigma_2 + m)}
\\
\qquad  = - \frac{w_{m-1}f_1(\sigma_1-1)+ \cdots +w_0f_m(\sigma_2  )}{\underbrace{f_0(\sigma_1)}_{ = 0}}
.
\end{split}
\end{equation}
That is,  the greater critical exponent $\sigma_1=\sigma_2 + m$   is a solution
of Eq. (\ref{2011-m-ch-sf-pss2sigma}) so that $f_0(\sigma_1)$ in the denominator vanishes.

In these cases the greater charakteristic exponent  $\sigma_1 \ge \sigma_2$
can still be used to find a solution in terms of a power series,
but the smaller characteristic exponent $\sigma_2$ in general cannot.

\subsection{d'Alambert reduction of order}
\index{reduction of order}



If $\sigma_1=\sigma_2+n$ with $n\in {\Bbb Z}$,
then we find only a {\em single} solution of the Fuchsian equation in terms of the power series resulting
from inserting the {\em greater} (or equal) characteristic exponent.
In order to obtain another linear  independent solution we have to employ
a method based on the Wronskian \cite{arfken05}, or
the
d'Alambert reduction \cite[15mm]{Teschl-odr},
which is a general method to obtain another, linear independent solution
\index{d'Alambert reduction}
$y_2(x)$ from an existing particular solution  $y_1(x)$ by the {\it Ansatz}
(no proof is presented here)
\begin{equation}
y_2(x)=y_1(x)\int_x  v(s) ds.
\label{2011-m-ch-sf-dalambansatz}
\end{equation}
Inserting $y_2(x)$ from (\ref{2011-m-ch-sf-dalambansatz}) into the Fuchsian equation (\ref{2011-m-ch-sf-fc2}),
and using the fact that by assumption $y_1(x)$
is a solution of it,
yields
\begin{equation*}
\begin{split}
\frac{d^2}{dx^2}y_2(x) +   p_1  (x)    \frac{d}{dx}y_2(x)+    p_2  (x)   y_2(x)
 =
0, \\
\frac{d^2}{dx^2}y_1(x)\int_x  v(s) ds +   p_1  (x)    \frac{d}{dx}y_1(x)\int_x  v(s) ds+    p_2  (x)   y_1(x)\int_x  v(s) ds
 =
0, \\
\frac{d }{dx }\left\{
\left[\frac{d }{dx }y_1(x)\right]\int_x  v(s) ds
+y_1(x)   v(x)  \right\} \qquad  \\
+   p_1  (x)    \left[\frac{d}{dx}y_1(x)\right] \int_x  v(s) ds + p_1(x)   v(x)
+   p_2 (x)  y_1(x)\int_x  v(s) ds
 =
0, \\
\left[ \frac{d^2 }{dx^2 }y_1(x)\right] \int_x  v(s) ds
+\left[ \frac{d  }{dx  }y_1(x)\right]    v(x)
+\left[ \frac{d  }{dx  }y_1(x)\right]    v(x)
+y_1(x)  \left[ \frac{d  }{dx  } v(x) \right]    \\
+   p_1  (x)    \left[ \frac{d}{dx}y_1(x)\right] \int_x  v(s) ds + p_1  (x)   y_1(x)   v(x)
+   p_2 (x)  y_1(x)\int_x  v(s) ds
 =
0, \\
\left[ \frac{d^2 }{dx^2 }y_1(x)\right] \int_x  v(s) ds
+   p_1  (x)    \left[ \frac{d}{dx}y_1(x)\right] \int_x  v(s) ds
+   p_2 (x)  y_1(x)\int_x  v(s) ds    \quad   \\
+   p_1  (x)   y_1(x)   v(x)
+\left[ \frac{d  }{dx  }y_1(x)\right]    v(x)
+\left[ \frac{d  }{dx  }y_1(x)\right]    v(x)
+y_1(x)  \left[ \frac{d  }{dx  } v(x) \right]
 =
0, \\
\left[ \frac{d^2 }{dx^2 }y_1(x)\right] \int_x  v(s) ds
+   p_1  (x)    \left[ \frac{d}{dx}y_1(x)\right] \int_x  v(s) ds
+   p_2 (x)  y_1(x)\int_x  v(s) ds   \quad     \\
+y_1(x)  \left[ \frac{d  }{dx  } v(x) \right]
+2\left[ \frac{d  }{dx  }y_1(x)\right]    v(x)
 +   p_1  (x)   y_1(x)   v(x)
 =
0, \\
\underbrace{\left\{\left[ \frac{d^2 }{dx^2 }y_1(x)\right]
+   p_1  (x)    \left[ \frac{d}{dx}y_1(x)\right]
+   p_2 (x)  y_1(x)\right\}}_{=0}\int_x  v(s) ds     \qquad   \\
+y_1(x)  \left[ \frac{d  }{dx  } v(x) \right]
+\left\{ 2\left[ \frac{d  }{dx  }y_1(x)\right]
  +   p_1  (x)   y_1(x) \right\}  v(x)
 =
0, \\
 y_1(x)  \left[ \frac{d  }{dx  } v(x) \right]
+\left\{ 2\left[ \frac{d  }{dx  }y_1(x)\right]
  +   p_1  (x)   y_1(x) \right\}  v(x)
 =
0,
\end{split}
\end{equation*}
and finally,
\begin{equation}
      v'(x)  +  v(x)    \left\{ 2 \frac{y'_1(x)}{y_1(x)}    +   p_1  (x)   \right\} = 0.
\label{2011-m-ch-sf-fc2123}
\end{equation}



\subsection{Computation of the characteristic exponent}

 Let
 $w'' +p_1(z)w' +p_2(z)w=0$
be a Fuchsian equation.
From the Laurent series expansion of $p_1(z)$ and $p_2(z)$ with Cauchy's integral formula  we can derive
the following equations, which are helpful in determining the characteristic exponent $\sigma$,
as defined in (\ref{2011-m-ch-sf-pss2sigma}) by
$
\sigma({\sigma-1}) +  \sigma \alpha_{0} + \beta_{0}           =   0
$:
\begin{equation}
\begin{split}
\alpha_0=\lim_{z\rightarrow z_0} (z-z_0)p_1(z),\\
\beta_0=\lim_{z\rightarrow z_0} (z-z_0)^2p_2(z),
\end{split}
\end{equation}
 where $z_0$ is a regular singular point.


{\color{OliveGreen}
\bproof

In order to find {$\alpha_0$}, consider the Laurent series for
\begin{equation}
\begin{split}
   p_1(z)=\sum_{k=-1}^\infty \tilde a_k(z-z_0)^k, \\
\textrm{ with }
   \tilde a_k={1\over 2\pi i}\oint p_1(s)(s-z_0)^{-(k+1)}ds.
\end{split}
\end{equation}
The summands  vanish  for $k<-1$, because $p_1(z)$ has at most a pole of order one at $z_0$.

An index change $n=k+1$, or  $k=n-1$,  as well as a redefinition
$\alpha_n \stackrel{{\tiny \textrm{ def }}}{=} \tilde a_{n-1}$
yields
\begin{equation}
p_1(z) =\sum_{n=0}^\infty \alpha_n(z-z_0)^{n-1},
\end{equation}
where
\begin{equation}
 \alpha_n={\tilde a}_{n-1}={1\over 2\pi i}
\oint p_1(s)(s-z_0)^{-n}ds;
\end{equation}
and, in particular,
\begin{equation}
\alpha_0={1\over2\pi i}\oint p_1(s)ds.
\label{2016-m-ch-sf-al0}
\end{equation}
Because the equation is Fuchsian,  $p_1(z)$ has at most a pole of order one at $z_0$.
Therefore, $(z-z_0) p_1(z)$  is analytic around $z_0$.
By multiplying  $p_1(z)$ with unity $1=(z-z_0)/(z-z_0)$
and insertion into~(\ref{2016-m-ch-sf-al0}) we obtain
\begin{equation}
   \alpha_0={1\over2\pi i}\oint{p_1(s)(s-z_0)\over(s-z_0)}ds.
\end{equation}
{\em Cauchy's integral formula}~(\ref{2012-m-ch-ca-cif}) on page \pageref{2012-m-ch-ca-cif}
yields
\begin{equation}
   \alpha_0=\lim_{s\to z_0}p_1(s)(s-z_0).
\end{equation}

Alternatively we may consider the
{\it Frobenius Ansatz}~(\ref{2011-m-ch-sf-pss2})
$  p_1(z)=\sum_{n=0}^\infty\alpha_n(z-z_0)^{n-1}$
which has again been motivated by the fact that $p_1(z)$ has at most a pole of order one at $z_0$.
Multiplication of this series by $(z-z_0)$ yields
\begin{equation}
   (z-z_0)p_1(z)=\sum_{n=0}^\infty\alpha_n(z-z_0)^n .
\end{equation}
In the limit $z\to z_0$,
\begin{equation}
\alpha_0  = \lim_{z\to z_0}(z-z_0)p_1(z)
.
\end{equation}


Likewise, let us find the expression for $\beta_0$  by considering the Laurent series for
\begin{equation}
\begin{split}
   p_2(z)=\sum_{k=-2}^\infty \tilde b_k(z-z_0)^k \\
\textrm{ with }
   \tilde b_k={1\over 2\pi i}\oint p_2(s)(s-z_0)^{-(k+1)}ds.
\end{split}
\end{equation}
The summands  vanish  for $k<-2$, because $p_2(z)$ has at most a pole of order two at $z_0$.

An index change  $n=k+2$, or  $k=n-2$,  as well as a redefinition
$\beta_n \stackrel{{\tiny \textrm{ def }}}{=} \tilde b_{n-2}$
yields
\begin{equation}
p_2(z) =\sum_{n=0}^\infty \beta_n(z-z_0)^{n-2},
\end{equation}
where
\begin{equation}
 \beta_n={\tilde b}_{n-2}={1\over 2\pi i}
\oint p_2(s)(s-z_0)^{-(n-1)}ds;
\end{equation}
and, in particular,
\begin{equation}
\beta_0={1\over2\pi i}\oint (s-z_0) p_2(s) ds.
\label{2016-m-ch-sf-bl0}
\end{equation}

Because the equation is Fuchsian,  $p_2(z)$ has at most a pole of order two at $z_0$.
Therefore, $(z-z_0)^2 p_2(z)$  is analytic around $z_0$.
By multiplying  $p_2(z)$ with unity $1=(z-z_0)^2/(z-z_0)^2$
and insertion into~(\ref{2016-m-ch-sf-bl0}) we obtain
\begin{equation}
   \beta_0={1\over2\pi i}\oint{p_2(s)(s-z_0)^2\over(s-z_0)}ds .
\end{equation}
{\em Cauchy's integral formula}~(\ref{2012-m-ch-ca-cif}) on page \pageref{2012-m-ch-ca-cif} yields
\begin{equation}
   \beta_0=\lim_{s\to z_0}p_2(s)(s-z_0)^2.
\end{equation}

Again another way to see this is with the
{\it Frobenius Ansatz}~(\ref{2011-m-ch-sf-pss2})   $p_2(z)=\sum_{n=0}^\infty\beta_n(z-z_0)^{n-2}$.
Multiplication with $(z-z_0)^2$, and taking the limit $z\to z_0$, yields
\begin{equation}
   \lim_{z\to z_0}(z-z_0)^2p_2(z)=\beta_n
.
\end{equation}


\eproof
}




{
\color{blue}
\bexample

\subsection{Examples}
Let us consider some examples involving Fuchsian equations of the second order.
\begin{enumerate}

\item
First, we shall prove that $z^2 y''(z) + z y'(z) - y(z) = 0$ is of the Fuchsian type, and compute the solutions with the Frobenius method.

Let us first locate the singularities of
\begin{equation}
y''(z) + \frac{y'(z)}{z} - \frac{y(z)}{z^2} = 0.
\label{2016-m-ch-sf-efmEu}
\end{equation}
One singularity is at the finite point finite $z_0=0$.

In order to analyze the singularity at infinity, we have to transform the equation by $z=1/t$.
First observe that
$p_1(z)= 1/z$
and
$p_2(z)= -1/z^2$.
Therefore, after the transformation, the new coefficients, computed from
(\ref{2016-m-ch-sf-idp1}) and (\ref{2016-m-ch-sf-idp2}),
are
\begin{equation}
\begin{split}
\tilde p_1 (t) = \left( \frac{2}{t} - \frac{t}{t^2} \right) =   \frac{1}{t} , \textrm{ and }\\
\tilde p_2 (t) = \frac{- t^2}{t^4}  = -\frac{ 1}{t^2}
.
\end{split}
\end{equation}
Thereby we effectively regain the original type of equation~(\ref{2016-m-ch-sf-efmEu}).
We can thus treat both singularities at zero and infinity in the same way.

Both singularities are regular, as the coefficients
$p_1$ and $\tilde p_1$  have poles of order 1,
$p_2$ and $\tilde p_2$  have poles of order 2, respectively.
Therefore, the differential equation is Fuchsian.

In order to obtain solutions, let us first compute the  characteristic exponents
by
\begin{equation}
\begin{split}
\alpha_0 = \lim_{z \rightarrow 0} z p_1 (z) = \lim_{z \rightarrow 0} z\frac{1}{z} = 1
 , \textrm{ and }\\
\beta_0 = \lim_{z \rightarrow 0} z^2 p_2 (z) = \lim_{z \rightarrow 0} - z^2\frac{1}{z^2} = - 1
,
\end{split}
\end{equation}
so that, from
(\ref{2011-m-ch-sf-pss2sigma}),
\begin{equation}
\begin{split}
0= f_0(\sigma )  = \sigma({\sigma-1}) +  \sigma \alpha_{0} + \beta_{0}           =
\sigma^2 - \sigma +  \sigma  -1 = \\
= \sigma^2   -1
 , \textrm{ and  thus }
\sigma_{1,2} = \pm 1 .
\end{split}
\end{equation}

The first solution is obtained by insertion of the {\it Frobenius Ansatz}~(\ref{2011-m-ch-sf-pss2}),
in particular,
$y_1(x) =  \sum_{l=0}^\infty (x-x_0)^{l + \sigma} w_l$ with $\sigma_{1} =  1$ and  $x_0=0$
into (\ref{2016-m-ch-sf-efmEu}).
In this case,
\begin{equation}
\begin{split}
x^2 \sum_{l=0}^\infty (l+1)l x^{l - 1} w_l
+
x \sum_{l=0}^\infty (l+1) x^{l} w_l
-
\sum_{l=0}^\infty   x^{l+1} w_l = 0,
\\
\sum_{l=0}^\infty \left[ (l+1)l +  (l+1) - 1 \right] x^{l+1} w_l =
\sum_{l=0}^\infty w_l l (l+2)   x^{l+1} = 0.
\end{split}
\end{equation}
Since
the polynomials are linear independent, we obtain  $ w_l l (l+2) = 0  $ for all $l \ge 0$.
Therefore, for constant $A$,
\begin{equation}
 w_0  = A \textrm{, and }
 w_l  = 0 \textrm{ for } l > 0.
\end{equation}
So, the first solution is $y_1(z) = Az$.

The second solution, computed through the {\it Frobenius Ansatz}~(\ref{2011-m-ch-sf-pss2}),
is obtained by inserting
$y_2(x) =  \sum_{l=0}^\infty (x-x_0)^{l + \sigma} w_l$ with $\sigma_{2} =  -1$
into (\ref{2016-m-ch-sf-efmEu}).
This yields
\begin{equation}
\begin{split}
z^2 \sum_{l=0}^\infty (l-1)(l-2) (x-x_0)^{l - 3} w_l  +\\
+
z \sum_{l=0}^\infty (l-1) (x-x_0)^{l-2} w_l
-
\sum_{l=0}^\infty   (x-x_0)^{l-1} w_l = 0,
\\
\sum_{l=0}^\infty \left[ w_l (l-1)(l-2) + w_l (l-1) - w_l \right] (x-x_0)^{l-1} w_l = 0,
\\
\sum_{l=0}^\infty w_l l (l-2)   (x-x_0)^{l-1}  = 0.
\end{split}
\end{equation}
Since the polynomials are linear independent, we obtain  $ w_l l (l-2) = 0  $ for all $l \ge 0$.
Therefore, for constant $B,C$,
\begin{equation}
\begin{split}
 w_0  = B ,
 w_1  = 0,
 w_2  = C \textrm{, and }
 w_l  = 0 \textrm{ for } l > 2,
\end{split}
\end{equation}
So that the second solution is $y_2(z) = B\frac{1}{z}+ Cz$.

Note that $y_2(z)$ already represents the general solution of (\ref{2016-m-ch-sf-efmEu}).
\marginnote{Most of the coefficients are zero, so no iteration with a ``catastrophic divisions by zero'' occurs here.}
Alternatively we could have started from $y_1(z)$ and applied d'Alambert's {\it Ansatz}
(\ref{2011-m-ch-sf-dalambansatz})--(\ref{2011-m-ch-sf-fc2123}):
\begin{equation}
v' (z) + v (z)\left(\frac{2}{z} + \frac{1}{z}\right) = 0 \textrm{, or } v' (z) = - \frac{3 v (z)}{z}
\end{equation}
yields
\begin{equation}
\frac{d v}{v} = - 3\frac{d z}{z} \textrm{, and }\log v = -3 \log z \textrm{, or } v(z) = z^{-3}.
\end{equation}
Therefore, according to (\ref{2011-m-ch-sf-dalambansatz}),
\begin{equation}
y_2(z) = y_1(z) \int_z  v(s) ds  = A z \left(-\frac{1}{2z^2}\right) = \frac{A'}{z}  .
\end{equation}








\item
Find out whether the following differential equations are Fuchsian,
and enumerate the regular singular points:
\begin{equation}
\begin{split}
zw''+(1-z)w'=0 ,  \\
z^2w''+zw'-\nu ^2 w=0 ,  \\
z^2(1+z)^2w''+2z(z+1)(z+2)w'-4w=0 , \\
2z(z+2)w'' +w' -zw=0.
\end{split}
\end{equation}

{ ad 1:} $\displaystyle zw''+(1-z)w'=0\ \Longrightarrow
\ w''+{(1-z)\over z}w'=0$\\[2ex]
 {$z=0$:}
$$
   \alpha_0=\lim_{z\to0}z{(1-z)\over z}=1,\quad
   \beta_0=\lim_{z\to0}z^2\cdot0=0.
$$
The equation for the characteristic exponent is
$$
   \sigma(\sigma-1)+\sigma\alpha_0+\beta_0=0\Longrightarrow
   \sigma^2-\sigma+\sigma=0\Longrightarrow\sigma_{1,2}=0.
$$

\bigskip

\noindent  {$z=\infty$:} $  z={1\over t}$
$$
   \tilde p_1(t)={2\over t}-{{\left(1-{1\over t}\right)\over{1\over t}}\over
   t^2}={2\over t}-{\left(1-{1\over t}\right)\over t}={1\over t}
   +{1\over t^2}={t+1\over t^2}
$$
$\Longrightarrow$ not Fuchsian.

\bigskip

\noindent {  ad 2:} $\displaystyle z^2w''+zw'-v^2w=0\Longrightarrow
w''+{1\over z}w'-{v^2\over z^2}w=0$.\\[2ex]
 {$z=0$:}
$$
   \alpha_0=\lim_{z\to0}z{1\over z}=1,\quad
   \beta_0=\lim_{z\to0}z^2\left(-{v^2\over z^2}\right)=-v^2.
$$
$$
   \Longrightarrow \sigma^2-\sigma+\sigma-v^2=0\Longrightarrow\sigma_{1,2}=
   \pm v
$$

\bigskip

\noindent  {$z=\infty$:} $  z={1\over t}$
\begin{eqnarray*}
   \tilde p_1(t)&=&{2\over t}-{1\over t^2}t={1\over t}\\
   \tilde p_2(t)&=&{1\over t^4}\left(-t^2v^2\right)=-{v^2\over t^2}
\end{eqnarray*}
$$
   \Longrightarrow u''+{1\over t}u'-{v^2\over t^2}u=0 \Longrightarrow
   \sigma_{1,2}=\pm v
$$
$\Longrightarrow$ Fuchsian equation.

\bigskip

\noindent {  ad 3:}
$$
   z^2(1+z)^2w''+2z(z+1)(z+2)w'-4w=0\Longrightarrow
   w''+{2(z+2)\over z(z+1)}w'-{4\over z^2(1+z)^2}w=0
$$
 {$z=0$:}
$$
   \alpha_0=\lim_{z\to0}z{2(z+2)\over z(z+1)}=4,\quad
   \beta_0=\lim_{z\to0}z^2\left(-{4\over z^2(1+z)^2}\right)=-4.
$$
$$
   \Longrightarrow\sigma(\sigma-1)+4\sigma-4=\sigma^2+3\sigma-4=0
   \Longrightarrow\sigma_{1,2}=
   {-3\pm\sqrt{9+16}\over 2}=\left\{{-4\atop +1}\right.
$$
 {$z=-1$:}
$$
   \alpha_0=\lim_{z\to-1}(z+1){2(z+2)\over z(z+1)}=-2,\quad
   \beta_0=\lim_{z\to-1}(z+1)^2\left(-{4\over z^2(1+z)^2}\right)=-4.
$$
$$
   \Longrightarrow\sigma(\sigma-1)-2\sigma-4=\sigma^2-3\sigma-4=
   0\Longrightarrow\sigma_{1,2}=
   {3\pm\sqrt{9+16}\over 2}=\left\{{+4\atop -1}\right.
$$
 {$z=\infty$:}
\begin{eqnarray*}
   \tilde p_1(t)&=&{2\over t}-{1\over t^2}{2\left({1\over t}+2\right)
                   \over {1\over t}\left({1\over t}+1\right)}=
                   {2\over t}-{2\left({1\over t}+2\right)\over
                   1+t}={2\over t}\left(1-{1+2t\over 1+t}\right)\\
   \tilde p_2(t)&=&{1\over t^4}\left(-{4\over{1\over t^2}
                   \left(1+{1\over t}\right)^2}\right)=-{4\over t^2}
                   {t^2\over(t+1)^2}=-{4\over(t+1)^2}
\end{eqnarray*}
$$
   \Longrightarrow u''+{2\over t}\left(1-{1+2t\over1+t}\right)u'-
   {4\over(t+1)^2}u=0
$$
$$
   \alpha_0=\lim_{t\to0}t{2\over t}\left(1-{1+2t\over1+t}\right)=0,\quad
   \beta_0=\lim_{t\to0}t^2\left(-{4\over (t+1)^2}\right)=0.
$$
$$
   \Longrightarrow\sigma(\sigma-1)=0\Longrightarrow\sigma_{1,2}=
   \left\{{0\atop 1}\right.
$$
$\Longrightarrow$ Fuchsian equation.

\bigskip

\noindent {  ad 4:}
$$
   2z(z+2)w''+w'-zw=0\Longrightarrow w''+{1\over 2z(z+2)}w'-{1\over 2(z+2)}w=0
$$
 {$z=0$:}
$$
   \alpha_0=\lim_{z\to0}z{1\over 2z(z+2)}={1\over4},\quad
   \beta_0=\lim_{z\to0}z^2{-1\over 2(z+2)}=0.
$$
$$
   \Longrightarrow\sigma^2-\sigma+{1\over 4}\sigma=0\Longrightarrow
   \sigma^2-{3\over 4}\sigma=0\Longrightarrow\sigma_1=0,\sigma_2={3\over 4}.
$$
 {$z=-2$:}
$$
   \alpha_0=\lim_{z\to-2}(z+2){1\over 2z(z+2)}=-{1\over 4},\quad
   \beta_0=\lim_{z\to-2}(z+2)^2{-1\over 2(z+2)}=0.
$$
$$
   \Longrightarrow\sigma_1=0,\quad\sigma_2={5\over 4}.
$$
 {$z=\infty$:}
\begin{eqnarray*}
   \tilde p_1(t)&=&{2\over t}-{1\over t^2}\left({1\over 2{1\over t}
                   \left({1\over t}+2\right)}\right)={2\over t}-
                   {1\over 2(1+2t)}\\
   \tilde p_2(t)&=&{1\over t^4}{(-1)\over 2\left({1\over t}+2\right)}=
                   -{1\over 2t^3(1+2t)}
\end{eqnarray*}
$\Longrightarrow$ not a Fuchsian.


\item
Determine the solutions of
$$z^2w''+(3z+1)w'+w=0 $$  around the regular singular points.

The singularities are at $z=0$ and $z=\infty$.

\noindent  {Singularities   at $z=0$:}
$$
   p_1(z)={3z+1\over z^2}=
   {a_1(z)\over z} \textrm{ with }
   a_1(z)=3+{1\over z}
$$
  $p_1(z)$ has a pole of higher order than one; hence this is no Fuchsian equation;
and  $z=0$
is an irregular singular point.

\noindent  {Singularities   at $z=\infty$:}
\begin{itemize}
\item Transformation $\displaystyle z={1\over t}$, $w(z)\to u(t)$:
      $$
         u''(t)+\left[{2\over t}-{1\over t^2}p_1\left({1\over t}\right)
         \right]\cdot u'(t)+{1\over t^4}p_2\left({1\over t}\right)\cdot
         u(t)=0.
      $$
      The new coefficient functions are
      \begin{eqnarray*}
         \tilde p_1(t)&=&{2\over t}-{1\over t^2}p_1\left({1\over t}\right)=
                         {2\over t}-{1\over t^2}(3t+t^2)={2\over t}
                         -{3\over t}-1=-{1\over t}-1\\
         \tilde p_2(t)&=&{1\over t^4}p_2\left({1\over t}\right)=
                         {t^2\over t^4}={1\over t^2}
      \end{eqnarray*}
\item check whether this is a  regular singular point:
      $$
         \begin{array}{lll}
            \displaystyle \tilde p_1(t)=-{1+t\over t}={\tilde a_1(t)\over t} &
            ~~\mbox{ with }~~\tilde a_1(t)=-(1+t)&~\mbox{regular}\\
            \displaystyle \tilde p_2(t)={1\over t^2}={\tilde a_2(t)\over t^2} &
            ~~\mbox{ with }~~\tilde a_2(t)=1&~\mbox{regular}
         \end{array}
      $$
      $\tilde a_1$ and $\tilde a_2$ are regular at $t=0$, hence  this is a  regular singular point.
\item {\it Ansatz} around $t=0$:
the transformed equation is
      \begin{eqnarray*}
         u''(t)+\tilde p_1(t)u'(t)+\tilde p_2(t)u(t)&=&0\\
         u''(t)-\left({1\over t}+1\right)u'(t)+{1\over t^2}u(t)&=&0\\
         t^2 u''(t)-(t+t^2)u'(t)+u(t)&=&0
      \end{eqnarray*}
      The generalized power series is
      \begin{eqnarray*}
         u(t)  &=&\sum_{n=0}^\infty w_n t^{n+\sigma}\\
         u'(t) &=&\sum_{n=0}^\infty w_n (n+\sigma)t^{n+\sigma-1}\\
         u''(t)&=&\sum_{n=0}^\infty w_n (n+\sigma)(n+\sigma-1)
                  t^{n+\sigma-2}
      \end{eqnarray*}
      If we insert this into the transformed differential equation we obtain
      \begin{equation*}
         \begin{split}
           t^2\sum_{n=0}^\infty w_n(n+\sigma)
             (n+\sigma-1)t^{n+\sigma-2}-\\
           \qquad\quad -\ (t+t^2)\sum_{n=0}^\infty
             w_n(n+\sigma)t^{n+\sigma-1}+
             \sum_{n=0}^\infty w_n t^{n+\sigma}=0\\
          \sum_{n=0}^\infty w_n(n+\sigma)(n+\sigma-1)t^{n+\sigma}-
             \sum_{n=0}^\infty w_n(n+\sigma)t^{n+\sigma}-\\
           \qquad\quad -\ \sum_{n=0}^\infty w_n(n+\sigma)
             t^{n+\sigma+1}+\sum_{n=0}^\infty w_n t^{n+\sigma}=0
         \end{split}
      \end{equation*}
      Change of index: $m=n+1$, $n=m-1$ in the third sum yields
      $$
         \sum_{n=0}^\infty w_n\Bigl[(n+\sigma)(n+\sigma-2)+1\Bigr]
         t^{n+\sigma}-
         \sum_{m=1}^\infty w_{m-1}(m-1+\sigma)t^{m+\sigma}=0.
      $$
      In the second sum, substitute $m$ for $n$
      $$
         \sum_{n=0}^\infty w_n\Bigl[(n+\sigma)(n+\sigma-2)+1\Bigr]
         t^{n+\sigma}-
         \sum_{n=1}^\infty w_{n-1}(n+\sigma-1)t^{n+\sigma}=0.
      $$
      We write out explicitly the  $n=0$ term of the first sum
      \begin{equation*}
         \begin{split}
            \displaystyle w_0\Bigl[\sigma(\sigma-2)+1\Bigr]t^\sigma+
               \sum_{n=1}^\infty w_n\Bigl[(n+\sigma)(n+\sigma-2)+1\Bigr]
               t^{n+\sigma}-\\
            \displaystyle \qquad -\ \sum_{n=1}^\infty
               w_{n-1}(n+\sigma-1)t^{n+\sigma}=0.
         \end{split}
      \end{equation*}
      Now we can combine the two sums
      \begin{equation*}
         \begin{split}
         \displaystyle w_0\Bigl[\sigma(\sigma-2)+1\Bigr]t^\sigma+\\
         \displaystyle \quad +\, \sum_{n=1}^\infty
            \Bigl\{w_n\Bigl[(n+\sigma)(n+\sigma-2)+1\Bigr]
            -w_{n-1}(n+\sigma-1)\Bigr\}t^{n+\sigma}=0.
         \end{split}
     \end{equation*}
      The left hand side can only vanish for all  $t$ if the coefficients vanish; hence
      \begin{eqnarray}
         w_0\Bigl[\sigma(\sigma-2)+1\Bigr]&=&0, \label{eqn:5.3.1}\\
         w_n\Bigl[(n+\sigma)(n+\sigma-2)+1\Bigr]-
            w_{n-1}(n+\sigma-1)&=&0. \label{eqn:5.3.2}
      \end{eqnarray}
      ad (\ref{eqn:5.3.1}) for $w_0$:
      \begin{eqnarray*}
         \sigma(\sigma-2)+1&=&0\\
         \sigma^2-2\sigma+1&=&0\\
         (\sigma-1)^2&=&0\quad \Longrightarrow\ \sigma_\infty^{(1,2)}=1
      \end{eqnarray*}
      The characteristic exponent is $\sigma_\infty^{(1)}=
      \sigma_\infty^{(2)}=1$.\medskip\\
      ad (\ref{eqn:5.3.2}) for $w_n$:
      For the coefficients $w_n$ we obtain the recursion formula
      \begin{eqnarray*}
         w_n\Bigl[(n+\sigma)(n+\sigma-2)+1\Bigr]&=&w_{n-1}(n+\sigma-1)\\
         \Longrightarrow\ w_n&=&{n+\sigma-1\over (n+\sigma)(n+\sigma-2)+1}
         w_{n-1}.
      \end{eqnarray*}
      Let us insert $\sigma=1$:
      $$
         w_n={n\over (n+1)(n-1)+1}w_{n-1}={n\over n^2-1+1}w_{n-1}=
         {n\over n^2}w_{n-1}={1\over n}w_{n-1}.
      $$
      We can fix $w_0=1$, hence:
      \begin{eqnarray*}
         w_0&=&1={1\over 1}={1\over 0!}\\
         w_1&=&{1\over 1}={1\over 1!}\\
         w_2&=&{1\over 1\cdot 2}={1\over 2!}\\
         w_3&=&{1\over 1\cdot 2\cdot 3}={1\over 3!}\\
         &\vdots\\
         w_n&=&{1\over 1\cdot 2\cdot 3\cdot\,\cdots\,\cdot n}={1\over n!}
      \end{eqnarray*}
      And finally,
      $$
         u_1(t)=t^\sigma\sum_{n=0}^\infty w_n t^n=t\sum_{n=0}^\infty
         {t^n\over n!}=te^t .
      $$
\item Notice that both characteristic exponents are equal; hence we have to employ  the
d'Alambert reduction
      $$
         u_2(t)=u_1(t)\int\limits_0^t v(s)ds
      $$
      with
      $$
         v'(t)+v(t)\left[2{u_1'(t)\over u_1(t)}+\tilde p_1(t)\right]=0.
      $$
     Insertion of $u_1$ and $\tilde p_1$,
      \begin{eqnarray*}
         u_1(t)&=&te^t\\
         u_1'(t)&=&e^t(1+t)\\
         \tilde p_1(t)&=&-\left({1\over t}+1\right),
      \end{eqnarray*}
      yields
      \begin{eqnarray*}
         v'(t)+v(t)\left(2{e^t(1+t)\over te^t}-{1\over t}-1\right)&=&0\\
         v'(t)+v(t)\left(2{(1+t)\over t}-{1\over t}-1\right)&=&0\\
         v'(t)+v(t)\left({2\over t}+2-{1\over t}-1\right)&=&0\\
         v'(t)+v(t)\left({1\over t}+1\right)&=&0\\
         {dv\over dt}&=&-v\left(1+{1\over t}\right)\\
         {dv\over v}&=&-\left(1+{1\over t}\right)dt
      \end{eqnarray*}
      Upon integration of both sides we obtain
      \begin{eqnarray*}
         \int{dv\over v}&=&-\int\left(1+{1\over t}\right)dt\\
         \log v&=&-(t+\log t)=-t-\log t\\
         v&=&\exp(-t-\log t)=e^{-t}e^{-\log t}={e^{-t}\over t},
      \end{eqnarray*}
      and hence an explicit form of $v(t)$:
      $$
         v(t)={1\over t}e^{-t}.
      $$
      If we insert this into the equation for  $u_2$ we   obtain
      $$
         u_2(t)=te^t\int_0^t{1\over s}e^{-s}ds.
      $$

\item Therefore, with $  t={1\over z}$, $u(t)=w(z)$,
      the two linear independent solutions around the regular singular point at $z=\infty$ are
      \begin{equation}
      \begin{split}
         w_1(z)={1\over z}\exp\left({1\over z}\right)\textrm{, and}\\
         w_2(z)={1\over z}\exp\left({1\over z}\right)
                \int\limits_0^{1\over z}{1\over t}e^{-t}dt.
      \end{split}
      \end{equation}
\end{itemize}

\end{enumerate}
\eexample
}




\section{Hypergeometric function}
\index{hypergeometric function}

\subsection{Definition}
A
{\em hypergeometric series}
\index{hypergeometric series}
is a series
\begin{equation}
\sum_{j=0}^\infty c_j ,
\label{2011-m-ch-sfhserd}
\end{equation}
where the quotients $\frac{c_{j+1}}{c_j}$ are {\em rational functions} (that is, the quotient of two polynomials
$\frac{R(x)}{Q(x)}$, where $Q(x)$ is not identically zero) of $j$, so that they can be factorized by
\index{rational function}
\begin{equation}
\begin{split}
\frac{c_{j+1}}{c_j}
=
\frac{(j+a_1)(j+a_2)\cdots (j+a_p)}{(j+b_1)(j+b_2)\cdots (j+b_q)}
\left(\frac{x}{j+1}\right),
\\
 \textrm{ or } c_{j+1}
=  c_j
\frac{(j+a_1)(j+a_2)\cdots (j+a_p)}{(j+b_1)(j+b_2)\cdots (j+b_q)}
\left(\frac{x}{j+1}\right)
\\
\qquad =
 c_{j-1}
\frac{(j-1+a_1)(j-1+a_2)\cdots (j-1+a_p)}{(j-1+b_1)(j-1+b_2)\cdots (j-1+b_q)}
\times
\\
\times
\frac{(j+a_1)(j+a_2)\cdots (j+a_p)}{(j+b_1)(j+b_2)\cdots (j+b_q)}
\left(\frac{x}{j}\right)
\left(\frac{x}{j+1}\right)
\\
\qquad =
 c_{0}
\frac{a_1 a_2\cdots a_p}{b_1 b_2\cdots b_q}
\cdots
\frac{(j-1+a_1)(j-1+a_2)\cdots (j-1+a_p)}{(j-1+b_1)(j-1+b_2)\cdots (j-1+b_q)}
\times
\\
\times
\frac{(j+a_1)(j+a_2)\cdots (j+a_p)}{(j+b_1)(j+b_2)\cdots (j+b_q)}
\left(\frac{x}{1}\right)
\cdots
\left(\frac{x}{j}\right)
\left(\frac{x}{j+1}\right)
\\
\qquad =
 c_{0}
\frac{(a_1)_{j+1}(a_2)_{j+1}\cdots (a_p)_{j+1}}{(b_1)_{j+1}(b_2)_{j+1}\cdots (b_q)_{j+1}}
\left(\frac{x^{j+1}}{(j+1)!}\right)
.
\end{split}
\label{2011-m-ch-sfhser}
\end{equation}
The factor $j+1$ in the denominator of the first  line of (\ref{2011-m-ch-sfhser})
on the right
yields $(j+1)!$.
If it were not there ``naturally''
we may obtain it by compensating it with a factor $j+1$ in the numerator.

With this iterated ratio~(\ref{2011-m-ch-sfhser}),
the hypergeometric series (\ref{2011-m-ch-sfhserd}) can be written in terms of
{\em shifted factorials},
\index{shifted factorial}
or, by another naming,
the
{\em Pochhammer symbol},
\index{Pochhammer symbol}
as
\begin{equation}
\begin{split}
\sum_{j=0}^\infty c_j
=
 c_0 \sum_{j=0}^\infty  \frac{( a_1)_j( a_2)_j\cdots ( a_p)_j}{( b_1)_j( b_2)_j\cdots ( b_q)_j}
\frac{x^j}{j!}
\\
\qquad =
c_0 {{}_pF_q} \left(
\begin{array}{cc}
a_1,\ldots ,a_p\\
b_1,\ldots ,b_q
\end{array} ; x
\right)     \textrm{, or }\\
\qquad =
c_0 {{}_pF_q} \left(
a_1,\ldots ,a_p;
b_1,\ldots ,b_q
 ; x
\right) .     \\
\end{split}
\label{2011-m-ch-sfhserd1}
\end{equation}

Apart from this definition {\it via}
hypergeometric series, the Gauss {\em hypergeometric function},
or, used synonymously,
the {\em Gauss series}
\index{Gauss series}
\index{Gauss hypergeometric function}
\begin{equation}
\begin{split}
{\;}_2F_1 \left(
\begin{array}{cc}
a ,b\\
c
\end{array} ; x
\right)
={\;}_2F_1 \left(
a ,b;c ; x
\right)
=   \sum_{j=0}^\infty  \frac{( a)_j( b)_j}{(c)_j} \frac{x^j}{j!}
\\
\qquad
=
1+ \frac{ab}{c} x   + \frac{1}{2!}\frac{a(a+1)b(b+1)}{c(c+1)} x^2
+ \cdots
\end{split}
\end{equation}
can be defined as a solution of a {\em Fuchsian differential equation}
which has at most {\em three regular singularities}
at $0$, $1$, and $\infty$.

Indeed, any Fuchsian  equation
with
finite  {regular singularities} at $x_1$ and $x_2$ can be rewritten into the
{\em Riemann differential equation}
\index{Riemann differential equation}
(\ref{2012-m-ch-sf-eforp12359}),
which in turn can be rewritten into the
{\em Gaussian differential equation}
\index{Gaussian differential equation}
or
{\em hypergeometric differential equation}
\index{hypergeometric differential equation}
with
regular singularities
at $0$, $1$, and $\infty$
\cite{hille-69,birkhoff-Rota-48,KristenssonC3}.
\marginnote{The Bessel equation has a regular singular point at $0$, and an irregular singular point at infinity.
\index{Bessel equation}}
This can be demonstrated by rewriting any such equation of the form
\begin{equation}
\begin{split}
 w''(x) + \left( \frac{A_1}{x-x_1}    + \frac{A_2}{x-x_2}
\right) w'(x)
\\ \qquad + \left(  \frac{B_1}{(x-x_1)^2}+\frac{B_2}{(x-x_2)^2} +\frac{C_1}{x-x_1} +\frac{C_2}{x-x_2}
\right)  w(x)  =0
\end{split}
\label{2011-m-ch-sfhserd12}
\end{equation}
through transforming Eq. (\ref{2011-m-ch-sfhserd12}) into the  {\em hypergeometric differential equation}
\index{hypergeometric differential equation}
\begin{equation}
\left[{}\frac{d^2}{dx^2}+ \frac{(a+b+1)x-c}{x(x-1)}\frac{d}{dx}+\frac{ab}{x(x-1)} \right]
{\;}_2F_1(a,b;c;x)=0,
\label{2011-m-ch-sfhserd121eq}
\end{equation}
where the solution is proportional to the Gauss hypergeometric function
\begin{equation}
w(x) \longrightarrow (x-x_1)^{\sigma^{(1)}_1} (x-x_2)^{ \sigma^{(2)}_2} {\;}_2F_1(a,b;c;x),
\end{equation}
 and the variable transform as
\begin{equation}
\begin{split}
x \longrightarrow x = \frac{x-x_1}{x_2-x_1}  \textrm{, with}\\
a=  {\sigma^{(1)}_1}+{\sigma^{(1)}_2}   +{\sigma^{(1)}_\infty},  \\
b=  {\sigma^{(1)}_1}+{\sigma^{(1)}_2}   +{\sigma^{(2)}_\infty} ,\\
c= 1+ {\sigma^{(1)}_1}   -{\sigma^{(2)}_1} .
\end{split}
\label{2011-m-ch-sfhserd121}
\end{equation}
where $\sigma^{(i)}_j$ stands for the $i$th characteristic exponent of the $j$th singularity.


{\color{OliveGreen}
\bproof

Whereas the full transformation from Eq. (\ref{2011-m-ch-sfhserd12})
to the hypergeometric differential equation  (\ref{2011-m-ch-sfhserd121eq}) will not been given, we shall show that
the Gauss hypergeometric function ${\;}_2F_1$ satisfies the hypergeometric differential equation (\ref{2011-m-ch-sfhserd121eq}).

First, define the differential operator
\begin{equation}
\vartheta = x \frac{d}{dx} ,
\label{2011-m-ch-sfhserddovt}
\end{equation}
and observe that
\begin{equation}
\begin{split}
\vartheta (\vartheta +c-1) x^n
=x \frac{d}{dx} \left( x \frac{d}{dx} +c-1\right) x^n\\  \qquad
=x \frac{d}{dx} \left( x n  x^{n-1}+c x^n- x^n\right)\\  \qquad
=x \frac{d}{dx} \left(   n  x^{n}+c x^n- x^n\right)\\     \qquad
=x \frac{d}{dx} \left(  n +c-1\right) x^n\\              \qquad
=n\left(  n +c-1\right) x^n.
\end{split}
\label{2011-m-ch-sfhserddovd1}
\end{equation}

Thus, if we apply  $\vartheta (\vartheta +c-1)$ to ${\;}_2F_1$, then
\begin{equation}
\begin{split}
\vartheta (\vartheta +c-1) {\;}_2F_1 (a,b;c;x)
=  \vartheta (\vartheta +c-1) \sum_{j=0}^\infty  \frac{( a)_j( b)_j}{(c)_j} \frac{x^j}{j!}
\\
=   \sum_{j=0}^\infty  \frac{( a)_j( b)_j}{(c)_j} \frac{j(j+c-1)x^j}{j!}
=   \sum_{j=1}^\infty  \frac{( a)_j( b)_j}{(c)_j} \frac{j(j+c-1)x^j}{j!}
\\
=   \sum_{j=1}^\infty  \frac{( a)_j( b)_j}{(c)_j} \frac{ (j+c-1)x^j}{(j-1)!}
\\
\textrm{[index shift: } j\rightarrow n+1, n=j-1, n\ge 0\textrm{]}
\\
=   \sum_{n=0}^\infty  \frac{( a)_{n+1}( b)_{n+1}}{(c)_{n+1}} \frac{ ({n+1}+c-1)x^{n+1}}{n!}
\\
=  x \sum_{n=0}^\infty  \frac{( a)_{n}(a+n)( b)_{n}(b+n)}{(c)_{n}(c+n)} \frac{ ({n }+c)x^{n}}{n!}
\\
=  x\sum_{n=0}^\infty  \frac{( a)_{n}( b)_{n}}{(c)_{n}} \frac{(a+n) (b+n)x^{n}}{n!}
\\
=   x(\vartheta +a)(\vartheta +b)  \sum_{n=0}^\infty  \frac{( a)_{n}( b)_{n}}{(c)_{n}} \frac{x^{n}}{n!}
=   x(\vartheta +a)(\vartheta +b)  {\;}_2F_1(a,b;c;x)
,
\end{split}
\label{2011-m-ch-sfhserddovd123}
\end{equation}
where we have used
\begin{equation}
\begin{split}
(a+n)x^n = (a+ \vartheta )x^n \text{, and}   \\
(a)_{n+1}=a(a+1)\cdots (a+n-1)(a+n) =  (a)_{n}(a+n).
\end{split}
\end{equation}
Writing out $\vartheta$ in  Eq. (\ref{2011-m-ch-sfhserddovd123}) explicitly yields
\begin{equation}
\begin{split}
\left\{\vartheta (\vartheta +c-1)  -   x(\vartheta +a)(\vartheta +b)\right\}  {\;}_2F_1(a,b;c;x) =0,
   \\
\left\{x \frac{d}{dx} \left(x \frac{d}{dx} +c-1\right)    -   x\left(x \frac{d}{dx}+a\right)\left(x \frac{d}{dx} +b\right) \right\}  {\;}_2F_1(a,b;c;x) =0,
   \\
\left\{ \frac{d}{dx} \left(x \frac{d}{dx} +c-1\right)    -    \left(x \frac{d}{dx}+a\right)\left(x \frac{d}{dx} +b\right) \right\}  {\;}_2F_1(a,b;c;x) =0,
   \\
\left\{ \frac{d}{dx} + x \frac{d^2}{dx^2} +(c-1)\frac{d}{dx}
 -    \left(x^2 \frac{d^2}{dx^2}+ x \frac{d}{dx}+ bx\frac{d}{dx}
\right.
\right.
  \qquad  \\
\left.
\left.
+  ax \frac{d}{dx} +ab\right) \right\}  {\;}_2F_1(a,b;c;x) =0,
   \\
\left\{  \left( x    -x^2 \right)\frac{d^2}{dx^2}
+
\left(
1+c-1-x-x(a+b)
\right)\frac{d}{dx}
+
ab  \right\}  {\;}_2F_1(a,b;c;x) =0
,   \\
\left\{ -   x(x-1)\frac{d^2}{dx^2}
-
\left(
c-x(1+a+b)
\right)\frac{d}{dx}
-
ab \right\}  {\;}_2F_1(a,b;c;x) =0,
   \\
\left\{ \frac{d^2}{dx^2}
+
\frac{x(1+a+b)-c}{x(x-1)}
\frac{d}{dx}
+
\frac{ab}{x(x-1)} \right\}  {\;}_2F_1(a,b;c;x) =0.
\end{split}
\label{2011-m-ch-sfhserddovd1234}
\end{equation}


\eproof
}

\subsection{Properties}

There exist many properties of the hypergeometric series.
In the following, we shall mention a few.


\begin{equation}
{d\over dz}{\;}_2F_1(a,b;c;z)={ab\over c}{\;}_2F_1(a+1,b+1;c+1;z) .
\end{equation}

{\color{OliveGreen}
\bproof

\begin{eqnarray*}
   {d\over dz}{\;}_2F_1(a,b;c;z)&=&{d\over dz}\sum_{n=0}^\infty
                           {(a)_n(b)_n\over(c)_n}{z^n\over n!}=\\
                        &=&\sum_{n=0}^\infty {(a)_n(b)_n\over(c)_n}
                           n{z^{n-1}\over n!}\\
                         &=&\sum_{n=1}^\infty
                           {(a)_n(b)_n\over(c)_n}{z^{n-1}\over(n-1)!}
\end{eqnarray*}
An index shift $n\to m+1$, $m=n-1$, and a subsequent renaming $m \to n$, yields
$$
   {d\over dz}{\;}_2F_1(a,b;c;z)=\sum_{n=0}^\infty
   {(a)_{n+1}(b)_{n+1}\over(c)_{n+1}}{z^n\over n!} .
$$
As
\begin{eqnarray*}
   (x)_{n+1}&=&x(x+1)(x+2)\cdots(x+n-1)(x+n)\\
   (x+1)_n  &=&\phantom{x}(x+1)(x+2)\cdots(x+n-1)(x+n)\\
   (x)_{n+1}&=&x(x+1)_n
\end{eqnarray*}
holds, we obtain
$$
   {d\over dz}{\;}_2F_1(a,b;c;z)=\sum_{n=0}^\infty{ab\over c}
   {(a+1)_n(b+1)_n\over(c+1)_n}{z^n\over n!}=
   {ab\over c}{\;}_2F_1(a+1,b+1;c+1;z).
$$

\eproof
}

We state {\em Euler's integral representation} for $\Re c>0$ and $\Re b>0$  without proof:
\begin{equation}
 {\;}_2F_1(a,b;c;x)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}
\int_0^1 t^{b-1}(1-t)^{c-b-1}(1-xt)^{-a} dt
.
\end{equation}


For  $\Re (c-a-b)>0$, we also state Gauss' theorem
\index{Gauss theorem}
\begin{equation}
 {\;}_2F_1(a,b;c;1)=\sum_{j=0}^\infty \frac{(a)_j(b)_j}{j! (c)_j} =\frac{\Gamma(c)\Gamma(c-a-b)}{\Gamma(c-a)\Gamma(c-b)}.
\end{equation}

{\color{OliveGreen}
\bproof
For a proof, we can set $x=1$ in Euler's integral representation, and the Beta function defined in Eq.
(\ref{2011-m-ch-sf-beta}).
\eproof
}

\subsection{Plasticity}

Some of the most important elementary functions can be expressed as hypergeometric series; most importantly the Gaussian one ${\;}_2F_1$,
which is
sometimes denoted by just $F$.
Let us enumerate a few.
\begin{eqnarray}
e^x
&=&
{\;}_0F_0(-;-;x)
\\
\cos x
&=&
{\;}_0F_1(-;\frac{1}{2};-\frac{x^2}{4})
\\
\sin x
&=&
x
{\;}_0F_1(-;\frac{3}{2};-\frac{x^2}{4})
\\
(1-x)^{-a}
&=&
{\;}_1F_0(a;-  ;x)
\\
\sin^{-1} x
&=&
x
{\;}_2F_1(\frac{1}{2},\frac{1}{2};\frac{3}{2};x^2)
\\
\tan^{-1} x
&=&
x
{\;}_2F_1(\frac{1}{2},1;\frac{3}{2};-x^2)
\\
\log (1 + x)
&=&
x
{\;}_2F_1(1,1;2;-x)
\\
H_{2n}(x)
&=&
\frac{(-1)^n(2n)!}{n!}
{\;}_1F_1(-n;\frac{1}{2}; x^2)
\\
H_{2n+1}(x)
&=&
2x
\frac{(-1)^n(2n+1)!}{n!}
{\;}_1F_1(-n;\frac{3}{2}; x^2)
\\
L_{n}^\alpha (x)
&=&
\left(
\begin{array}{c}
n+\alpha\\
n
\end{array}
\right)
{\;}_1F_1(-n;\alpha +1; x)
\\
P_{n}(x)
&=&   P^{(0, 0 )}_n (x)
=
{\;}_2F_1(-n,n+1; 1;\frac{1- x}{2}) ,
\\
C_{n}^\gamma (x)
&=& \frac{(2\gamma )_n}{\left( \gamma +\frac{1}{2}\right)_n}  P^{(\gamma -\frac{1}{2}, \gamma -\frac{1}{2} )}_n (x)
 ,
\\
T_{n}  (x)
&=& \frac{n!}{\left(  \frac{1}{2}\right)_n}  P^{( -\frac{1}{2},  -\frac{1}{2} )}_n (x)
,
\\
J_{\alpha }  (x)
&=& \frac{\left(\frac{x}{2}\right)^\alpha }{\Gamma (\alpha +1)}
{\;}_0F_1(- ; \alpha +1;-\frac{1 }{4}x^2) ,
\end{eqnarray}
where
$H$ stands for
{\em Hermite polynomials},
\index{Hermite polynomial}
$L$ for
{\em Laguerre polynomials},
\index{Laguerre polynomial}
\begin{equation}
P^{(\alpha, \beta )}_n (x)
=\frac{(\alpha +1)_n}{n!} {\;}_2F_1(-n,n+\alpha +\beta +1; \alpha +1;\frac{1- x}{2})
\end{equation}
for
{\em Jacobi polynomials},
\index{Jacobi polynomial}
$C$ for
{\em Gegenbauer polynomials},
\index{Gegenbauer polynomial}
$T$ for
{\em Chebyshev polynomials},
\index{Chebyshev polynomial}
$P$  for
{\em Legendre polynomials},
\index{Legendre polynomial}
and
$J$  for   the
{\em Bessel functions of the first kind},
\index{Bessel function}
respectively.

{
\color{blue}
\bexample

\begin{enumerate}

\item
Let us prove that
 $$\log (1-z)=-z {\;}_2F_1(1,1,2;z) . $$
Consider
$$
   {\;}_2F_1(1,1,2;z)=\sum_{m=0}^\infty{[(1)_m]^2\over(2)_m}{z^m\over m!}=
   \sum_{m=0}^\infty{[1\cdot2\cdot\,\cdots\,\cdot m]^2\over
   2\cdot(2+1)\cdot\,\cdots\,\cdot(2+m-1)}{z^m\over m!}
$$
With
$$
   (1)_m=1\cdot2\cdot\,\cdots\,\cdot m=m!,\qquad
   (2)_m=2\cdot(2+1)\cdot\,\cdots\,\cdot (2+m-1)=(m+1)!
$$
follows
$$
   {\;}_2F_1(1,1,2;z)=\sum_{m=0}^\infty{[m!]^2\over(m+1)!}
   {z^m\over m!}=\sum_{m=0}^\infty {z^m\over m+1}.
$$
Index shift $k =m+1$
$$
   {\;}_2F_1(1,1,2;z)=\sum_{k=1}^\infty {z^{k-1}\over k}
$$
and hence
$$
   -z{\;}_2F_1(1,1,2;z)=-\sum_{k=1}^\infty{z^k\over k}.
$$
Compare with the series
$$
   \log(1+x)=\sum_{k=1}^\infty(-1)^{k+1}{x^k\over k}\qquad
   \mbox{for}\quad -1<x\leq1
$$
If one substitutes $-x$ for $x$, then
$$
   \log(1-x)=-\sum_{k=1}^\infty{x^k\over k}.
$$
The identity follows from the analytic continuation of $x$ to the complex $z$ plane.


\item
Let us prove that,  because of
$
(a+z)^n
=
\sum_{k=0}^n
\begin{pmatrix}
n\\
k
\end{pmatrix}
 z^ka^{n-k}$,
 $$(1-z)^n={\;}_2F_1(-n,1,1;z). $$

$$
   {\;}_2F_1(-n,1,1;z)=\sum_{i=0}^\infty{(-n)_i(1)_i\over(1)_i}{z^i\over i!}=
   \sum_{i=0}^\infty(-n)_i{z^i\over i!}.
$$
Consider $(-n)_i$
$$
   (-n)_i=(-n)(-n+1)\cdots(-n+i-1).
$$

For  $n\geq 0$
the series stops after a finite number of terms, because the factor
$-n+i-1=0$ for $i=n+1$ vanishes;
hence the sum of $i$ extends only
from $0$ to $n$.
Hence, if we collect the factors
 $(-1)$ which yield $(-1)^i$ we obtain
$$
   (-n)_i=(-1)^in(n-1)\cdots[n-(i-1)]=(-1)^i{n!\over(n-i)!}.
$$
Hence, insertion into the Gauss hypergeometric function yields
$$
   {\;}_2F_1(-n,1,1;z)=\sum_{i=0}^n(-1)^iz^i{n!\over i!(n-i)!}=
   \sum_{i=0}^n{n\choose i}(-z)^i.
$$
This is the binomial series
$$
   (1+x)^n=\sum_{k=0}^n{n\choose k}x^k
$$
with $x=-z$; and hence,
$$
   {\;}_2F_1(-n,1,1;z)=(1-z)^n.
$$




\item
 Let us prove that,  because of $\arcsin x=\sum_{k=0}^\infty
 {(2k)!x^{2k+1}\over 2^{2k}(k!)^2(2k+1)}$,
 $${\;}_2F_1\left({1\over 2},{1\over 2},{3\over 2}; \sin^2 z\right)
 ={z\over \sin z }  .$$

Consider
$$
   {\;}_2F_1\left({1\over 2},{1\over 2},{3\over 2};\sin^2z\right)=
   \sum_{m=0}^\infty{\left[\left({1\over 2}\right)_m\right]^2\over
   \left({3\over 2}\right)_m}{(\sin z)^{2m}\over m!}.
$$
We take
\begin{eqnarray*}
   (2n)!!&=& 2\cdot4\cdot\,\cdots\,\cdot(2n)=n!2^n\\
   (2n-1)!!&=& 1\cdot3\cdot\,\cdots\,\cdot(2n-1)={(2n)!\over2^n n!}
\end{eqnarray*}
Hence
\begin{eqnarray*}
   \left({1\over 2}\right)_m\!\!\!&=\!\!\!&{1\over 2}\cdot
   \left({1\over 2}+1\right)\cdots\left({1\over 2}+m-1\right)=
   {1\cdot3\cdot5\cdots(2m-1)\over2^m}={(2m-1)!!\over2^m}\\
   \left({3\over 2}\right)_m\!\!\!&=\!\!\!&{3\over 2}\cdot
   \left({3\over 2}+1\right)\cdots\left({3\over 2}+m-1\right)=
   {3\cdot5\cdot7\cdots(2m+1)\over2^m}={(2m+1)!!\over2^m}
\end{eqnarray*}
Therefore,
$$
   {\left({1\over 2}\right)_m\over\left({3\over 2}\right)_m}=
   {1\over 2m+1}.
$$
On the other hand,
\begin{eqnarray*}
   (2m)!&=&1\cdot2\cdot3\cdot\,\cdots\,\cdot(2m-1)(2m)=(2m-1)!!(2m)!!=\\
        &=&1\cdot3\cdot5\cdot\,\cdots\,\cdot(2m-1)\cdot
           2\cdot4\cdot6\cdot\,\cdots\,\cdot(2m)=\\
        &=&\left({1\over 2}\right)_m2^m\cdot2^m m!=2^{2m}m!
           \left({1\over 2}\right)_m
   \Longrightarrow\left({1\over 2}\right)_m={(2m)!\over 2^{2m}m!}
\end{eqnarray*}
Upon insertion one obtains
$$
   F\left({1\over 2},{1\over 2},{3\over 2};\sin^2z\right)=
   \sum_{m=0}^\infty{(2m)!(\sin z)^{2m}\over 2^{2m}(m!)^2(2m+1)}.
$$
Comparing with the series for arcsin one finally obtains
$$
   \sin z F\left({1\over 2},{1\over 2},{3\over 2};\sin^2z\right)=
   \arcsin(\sin z)=z.
$$
\end{enumerate}
\eexample
}

\subsection{Four forms}


We state without proof the four forms of the
Gauss hypergeometric function \cite{macrobert:1967:she}.
\begin{eqnarray}
{\;}_2F_1(a,b;c;x)
&=&(1-x)^{c-a-b}{\;}_2F_1(c-a,c-b;c;x)\\
&=&(1-x)^{ -a }{\;}_2F_1\left( a,c-b;c;\frac{x}{x-1}\right)\\
&=&(1-x)^{ -b }{\;}_2F_1\left( b,c-a;c;\frac{x}{x-1}\right).
\end{eqnarray}


\section{Orthogonal polynomials}
\label{2012-m-sf-fs}

Many systems or sequences of functions may serve as a {\em basis of linearly independent functions}
which are capable to ``cover'' -- that is, to approximate -- certain functional classes
\cite{herman-sc,Marcellan}.
We have already encountered at least two such prospective bases [cf. Eq. (\ref{2011-m-fa-e1fc})]:
\begin{equation}
\{1,x,x^2,\ldots ,x^k,\ldots \} \textrm { with } f(x) =\sum_{k=0}^\infty c_k x^k,
\label{2011-m-ch-sfe1}
\end{equation}
and
\begin{equation}
\begin{split}
\left\{e^{ikx} \mid k\in {\Bbb Z} \right\} \quad \textrm{ for } f(x+2\pi)=f(x) \\
\qquad \textrm {  with }
f(x)= \sum _{k=-\infty}^\infty c_k e^{ikx},\\
\qquad  \textrm{  where }
c_k=\frac{1}{2\pi}\int_{-\pi}^\pi f(x) e^{-ikx} dx.
\end{split}
\end{equation}

In order to claim existence of such functional basis systems, let us first define what  {\em orthogonality} means in the  functional context.
\index{orthogonal functions}
Just as for linear vector spaces, we can define an
{\em inner product}
or {\em scalar product}
[cf. also Eq. (\ref{2012-m-ch-sfesp1})]
\index{inner product}
\index{scalar product}
of two real-valued functions $f(x)$ and $g(x)$ by the integral \cite{Wilf}
\begin{equation}
\langle   f \mid g\rangle
=
\int_a^b f(x)g(x) \rho(x) dx
\label{2011-m-ch-sfesp}
\end{equation}
for some suitable {\em weight function} $\rho (x)\ge 0$.  \index{weight function}
Very often, the weight function is set to the identity; that is,
$\rho(x) =\rho =1$.
We notice without proof that $\langle   f \vert g\rangle  $ satisfies all requirements of a scalar product.
A system of functions $\{\psi_0,\psi_1,\psi_2,\ldots ,\psi_k,\ldots \}$
is orthogonal if, for $j\neq k$,
\begin{equation}
\langle   \psi_j \mid \psi_k\rangle
=
\int_a^b \psi_j(x)\psi_k(x) \rho(x) dx
=0.
\label{2011-m-ch-sfespof}
\end{equation}


Suppose, in some generality,
that
 $\{f_0,f_1,f_2,\ldots ,f_k,\ldots \}$
is a sequence of nonorthogonal functions.
Then we can apply a {\em Gram-Schmidt orthogonalization process} to these functions
\index{Gram-Schmidt process}
and thereby obtain orthogonal functions
$\{\phi_0,\phi_1,\phi_2,\ldots ,\phi_k,\ldots \}$
by
\begin{equation}
\begin{split}
\phi_0 (x) =f_0(x), \\
\phi_k (x)
=
f_k(x)
-
\sum_{j=0}^{k-1}\frac{\langle f_k\mid \phi_j \rangle}{\langle \phi_j   \mid \phi_j \rangle}\phi_j (x).
\end{split}
\label{2011-m-ch-sfegs}
\end{equation}
Note that the proof of the {  Gram-Schmidt  process} in the functional
context is analogous to the one in the vector context.

\section{Legendre polynomials}
\label{2013-m-sf-lp}

The polynomial functions in $\{1,x,x^2,\ldots ,x^k,\ldots \}$
are not mutually orthogonal because,
for instance,  with $\rho =1$ and $b=-a=1$,
\begin{equation}
\langle   1 \mid x^2\rangle
=
\int_{a=-1}^{b=1} x^2 dx
=
\left. \frac{x^3}{3} \right|_{x=-1}^{x=1}
=
\frac{2}{3}.
\end{equation}
Hence, by the   Gram-Schmidt  process we obtain
\begin{equation}
\begin{split}
\phi_0(x) =1,       \\
\phi_1(x) = x - \frac{\langle x \mid 1 \rangle}{\langle 1   \mid 1 \rangle}1   \\
\qquad  = x - 0 =x,   \\
\phi_2(x) = x^2 - \frac{\langle x^2 \mid 1 \rangle}{\langle 1   \mid 1 \rangle}1
 - \frac{\langle x^2 \mid x \rangle}{\langle x   \mid x \rangle}x
 \\
\qquad  =  x^2 - \frac{2/3}{2}1 -0x    =    x^2 - \frac{1 }{ 3},\\
\vdots
\end{split}
\end{equation}
If, on top of orthogonality,  we are ``forcing'' a type of ``normalization'' by defining
\begin{equation}
\begin{split}
P_l(x)\stackrel{{\tiny \textrm{ def }}}{=}\frac{\phi_l(x)}{\phi_l(1)},
\\
 \textrm{with }\; P_l(1)  =1,
\end{split}
\end{equation}
then the resulting orthogonal polynomials are the
{\em Legendre polynomials}  $P_l$; in particular,
\index{Legendre polynomials}
\begin{equation}
\begin{split}
P_0(x) =1,\\
P_1(x) =x,\\
P_2(x) =  \left. \left( x^2 - \frac{1 }{ 3}\right)\bigg/ \frac{2 }{ 3}  \right.
       =  \frac{1 }{ 2} \left( 3x^2 - 1\right)
,\\
\vdots
\end{split}
\label{2011-m-ch-sfelp}
\end{equation}
with $P_l(1)=1$, $l={\Bbb N}_0$.


Why should we be interested in orthonormal systems of functions?
Because, as pointed out earlier in the context of hypergeometric functions,
they could be alternatively defined as the eigenfunctions and solutions of certain differential equation,
such as, for instance, the Schr\"odinger equation, which may be subjected to a separation of variables.
For Legendre polynomials the associated differential equation is the  {\em Legendre equation}
\index{Legendre equation}
\begin{equation}
\begin{split}
\left\{
(1-x^2) \frac{d^2}{dx^2}
- 2x  \frac{d}{dx} +l(l+1)
\right\}P_l(x) = 0,
\\
\textrm{or } \left\{  \frac{d }{dx }\left[\left(1-x^2\right)\frac{d }{dx }\right]
+
 l(l+1) \right\}  P_l(x)= 0
\end{split}
\label{2011-m-ch-sfelpede}
\end{equation}
for $l\in {\Bbb N}_0$,
whose Sturm-Liouville form has been mentioned earlier in Table \ref{2011-m-sl-t-varieties}
on page \pageref{2011-m-sl-t-varieties}.
For a proof, we refer to the literature.


\subsection{Rodrigues formula}
\index{Rodrigues formula}

A third alternative definition  of Legendre polynomials
is by the  Rodrigues formula
\begin{equation}
P_l(x)=\frac{1}{2^ll!}\frac{d^l}{dx^l}(x^2-1)^l \textrm{, for }l\in {\Bbb N}_0.
\label{2011-m-ch-sfrf}
\end{equation}
Again, no proof of equivalence will be given.

For even $l$, $P_l(x)=P_l(-x)$ is an even function of $x$,
whereas
for odd $l$, $P_l(x)=-P_l(-x)$ is an odd function of $x$;
that is,
\begin{equation}
P_l(-x)=(-1)^lP_l(x).
\end{equation}
Moreover,
\begin{equation}
P_l(-1)=(-1)^l
\end{equation}
and
\begin{equation}
P_{2k+1}(0)=0.
\end{equation}




{\color{OliveGreen}
\bproof

This can be shown by  the
substitution
 $t=-x$, $dt=-dx$, and insertion into  the  Rodrigues formula:
\begin{eqnarray*}
   P_l(-x)&=&\left.{1\over 2^ll!}{d^l\over du^l}(u^2-1)^l\right|_{u=-x}
   =[u\to-u]=\\
   &=&\left.{1\over(-1)^l}{1\over2^ll!}{d^l\over du^l}(u^2-1)^l
   \right|_{u=x}=(-1)^lP_l(x).
\end{eqnarray*}

Because of the ``normalization'' $P_l(1)=1$ we obtain $$P_l(-1)=(-1)^lP_l(1)=(-1)^l.$$

And as $P_l(-0)=P_l(0)=(-1)^lP_l(0)$, we obtain $P_l(0)=0$ for odd $l$.


\eproof
}



\subsection{Generating function}
\index{generating function}

For
$\vert x\vert<1$ and
$\vert t\vert<1$ the
Legendre polynomials $P_l(x)$ are the coefficients in the Taylor series expansion of the following generating function
\begin{equation}
g(x,t) =\frac{1}{\sqrt{1-2xt+t^2}}=\sum_{l=0}^\infty P_l(x)\, t^l
\end{equation}
 around $t=0$.
No proof is given here.

\subsection{The three term and other recursion formul\ae}
\index{three term recursion formula}

Among other things, generating functions are used for the derivation of certain recursion relations involving Legendre polynomials.

For instance, for $l=1,2,\ldots$, the   three term recursion formula
\begin{equation}
(2l+1)x P_l(x) = (l+1) P_{l+1}(x)+lP_{l-1}(x) ,
\end{equation}
or, by substituting $l-1$ for $l$,  for $l=2,3\ldots$,
\begin{equation}
(2l-1)x P_{l-1}(x) = l P_{l}(x)+(l-1)P_{l-2}(x),
\end{equation}
can be proven as follows.

{\color{OliveGreen}
\bproof

$$
   g(x,t)={1\over\sqrt{1-2tx+t^2}}=\sum_{n=0}^\infty t^nP_n(x)
$$
$$
   {\partial\over\partial t}g(x,t)=-{1\over 2}(1-2tx+t^2)^{-{3\over 2}}
   (-2x+2t)={1\over\sqrt{1-2tx+t^2}}\,{x-t\over1-2tx+t^2}
$$
$$
   {\partial\over\partial t}g(x,t)={x-t\over1-2tx+t^2}
   \sum_{n=0}^\infty t^nP_n(x)=\sum_{n=0}^\infty nt^{n-1}P_n(x)
$$
$$
   (x-t)\sum_{n=0}^\infty t^nP_n(x)-(1-2tx+t^2)
   \sum_{n=0}^\infty nt^{n-1}P_n(x)=0
$$
$$
   \sum_{n=0}^\infty xt^nP_n(x)-
   \sum_{n=0}^\infty t^{n+1}P_n(x)-
   \sum_{n=1}^\infty nt^{n-1}P_n(x)+
$$
$$
   \hspace*{5cm}+\,\sum_{n=0}^\infty 2xnt^nP_n(x)-
   \sum_{n=0}^\infty nt^{n+1}P_n(x)=0
$$
$$
   \sum_{n=0}^\infty (2n+1)xt^nP_n(x)-
   \sum_{n=0}^\infty (n+1)t^{n+1}P_n(x)-
   \sum_{n=1}^\infty nt^{n-1}P_n(x)=0
$$
$$
   \sum_{n=0}^\infty (2n+1)xt^nP_n(x)-
   \sum_{n=1}^\infty nt^{n}P_{n-1}(x)-
   \sum_{n=0}^\infty (n+1)t^nP_{n+1}(x)=0,
$$
$$
   xP_0(x)-P_1(x)+\sum_{n=1}^\infty t^n\Bigl[(2n+1)xP_n(x)-nP_{n-1}(x)
   -(n+1)P_{n+1}(x)\Bigr]=0,
$$
hence
$$
    xP_0(x)-P_1(x)=0, \qquad (2n+1)xP_n(x)-nP_{n-1}(x)-
   (n+1)P_{n+1}(x)=0,
$$
hence
$$
  P_1(x)=xP_0(x), \qquad (n+1)P_{n+1}(x)=
   (2n+1)xP_n(x)-nP_{n-1}(x).
$$

\eproof
}


Let us prove
\begin{equation}
P_{l-1}(x)=P'_l(x)-2xP'_{l-1}(x)+P'_{l-2}(x) .
\end{equation}

{\color{OliveGreen}
\bproof


$$
   g(x,t)={1\over\sqrt{1-2tx+t^2}}=\sum_{n=0}^\infty t^nP_n(x)
$$
$$
   {\partial\over\partial x}g(x,t)=-{1\over 2}(1-2tx+t^2)^{-{3\over 2}}
   (-2t)={1\over\sqrt{1-2tx+t^2}}\,{t\over1-2tx+t^2}
$$
$$
   {\partial\over\partial x}g(x,t)={t\over1-2tx+t^2}
   \sum_{n=0}^\infty t^nP_n(x)=\sum_{n=0}^\infty t^{n}P'_n(x)
$$
$$
   \sum_{n=0}^\infty t^{n+1}P_n(x)=\sum_{n=0}^\infty t^{n}P'_n(x)-
   \sum_{n=0}^\infty 2xt^{n+1}P'_n(x)+\sum_{n=0}^\infty t^{n+2}P'_n(x)
$$
$$
   \sum_{n=1}^\infty t^{n}P_{n-1}(x)=\sum_{n=0}^\infty t^{n}P'_n(x)-
   \sum_{n=1}^\infty 2xt^{n}P'_{n-1}(x)+\sum_{n=2}^\infty t^{n}P'_{n-2}(x)
$$
$$
   tP_0+\sum_{n=2}^\infty t^{n}P_{n-1}(x)=P'_0(x)+tP'_1(x)+
   \sum_{n=2}^\infty t^{n}P'_n(x)-
$$
$$
   \hspace*{5cm}-\,2xtP'_0-\sum_{n=2}^\infty 2xt^{n}P'_{n-1}(x)+
   \sum_{n=2}^\infty t^{n}P'_{n-2}(x)
$$
$$
   P'_0(x)+t\Bigl[P'_1(x)-P_0(x)-2xP'_0(x)\Bigr]+\hspace*{3cm}
$$
$$
   \hspace*{3cm}+\,\sum_{n=2}^\infty t^{n}[P'_n(x)-2xP'_{n-1}(x)+P'_{n-2}(x)-
   P_{n-1}(x)]=0
$$
$$
   P'_0(x)=0\textrm{, hence } P_0(x)={\rm const.}
$$
$$
   P'_1(x)-P_0(x)-2xP'_0(x)=0.
$$
Because of $P'_0(x)=0$ we obtain $P'_1(x)-P_0(x)=0$, hence $P'_1(x)=P_0(x)$, and
$$
   P'_n(x)-2xP'_{n-1}(x)+P'_{n-2}(x)-P_{n-1}(x)=0.
$$
Finally we substitute $n+1$ for $n$:
$$
   P'_{n+1}(x)-2xP'_{n}(x)+P'_{n-1}(x)-P_{n}(x)=0,
$$
hence
$$
    P_n(x)=P'_{n+1}(x)-2xP'_{n}(x)+P'_{n-1}(x).
$$

\eproof
}

Let us prove
\begin{equation}
P'_{l+1}(x)-P'_{l-1}(x)=(2l+1)P_l(x) .
\end{equation}

{\color{OliveGreen}
\bproof

\begin{eqnarray*}
   (n+1)P_{n+1}(x)&=&(2n+1)xP_n(x)-nP_{n-1}(x)\quad\left|
   {d\over dx}\right.\\
   (n+1)P'_{n+1}(x)&=&(2n+1)P_n(x)+(2n+1)xP'_n(x)-nP'_{n-1}(x)
   \quad\Bigl|\cdot\,2\\
   \textrm{(i):}\quad(2n+2)P'_{n+1}(x)&=&2(2n+1)P_n(x)+2(2n+1)xP'_n(x)-2nP'_{n-1}(x)
\end{eqnarray*}
$$
   P'_{n+1}(x)-2xP'_{n}(x)+P'_{n-1}(x)=P_{n}(x)\quad\Bigl|
   \cdot\,(2n+1)
$$
$$
   \textrm{(ii):}\quad (2n+1)P'_{n+1}(x)-2(2n+1)xP'_{n}(x)+(2n+1)P'_{n-1}(x)=
   (2n+1)P_{n}(x)
$$
We subtract (ii) from (i):
$$
   P'_{n+1}(x)+2(2n+1)xP'_n(x)-(2n+1)P'_{n-1}(x)=
$$
$$
   \hspace*{5cm}=\,(2n+1)P_n(x)+2(2n+1)xP'_n(x)-2nP'_{n-1}(x);
$$
hence
$$
   P'_{n+1}(x)-P'_{n-1}(x)=(2n+1)P_n(x).
$$
\eproof
}

\subsection{Expansion in Legendre polynomials}

We state without proof  that square integrable functions $f(x)$
can be written as series of Legendre polynomials
as
\begin{equation}
\begin{split}
 f(x)=\sum_{l=0}^\infty a_lP_l(x),\\
\textrm{with expansion coefficients } a_l={2l+1\over 2}
\int\limits_{-1}^{+1}f(x)P_l(x)dx.
 \end{split}
\end{equation}

{
\color{blue}
\bexample
Let us expand the Heaviside function defined in Eq. (\ref{2011-m-di-edhf})
\index{Heaviside function}
\begin{equation}
H(x)
=
\left\{
\begin{array}{rl}
1&\textrm{ for } x\ge  0\\
0&\textrm{ for } x <  0
\end{array}
\right.
\end{equation}
in terms of Legendre polynomials.

We shall use the recursion formula $(2l+1)P_l=P'_{l+1}-P'_{l-1}$ and rewrite
\begin{eqnarray*}
 a_l &=& {1\over 2}\int\limits_0^1\bigl(P'_{l+1}(x)-
              P'_{l-1}(x)\bigr)dx=
              {1\over 2}\bigl(P_{l+1}(x)-P_{l-1}(x)\bigr)
              \biggr|_{x=0}^1=\\
            &=& {1\over 2}\underbrace{\bigl[P_{l+1}(1)-P_{l-1}(1)\bigr]}
                _{\mbox{$=0$ because of}\atop \mbox{``normalization''}}-
                {1\over 2}\bigl[P_{l+1}(0)-P_{l-1}(0)\bigr]
.
\end{eqnarray*}
Note that $P_n(0)=0$ for {\em odd} $n$; hence $  a_l=0$ for
{\em even} $l\ne0$. We shall treat the case $l=0$ with $P_0(x)=1$ separately.
Upon substituting $2l+1$ for $l$ one obtains
$$
   a_{2l+1}=-{1\over 2}\biggl[P_{2l+2}(0)-P_{2l}(0)\biggr].
$$
We shall next use the formula
$$
   P_l(0)=(-1)^{l\over 2}{l!\over
2^l\left(\left({l\over2}\right)!\right)^2},
$$
and for {\em even} $l\geq 0$ one obtains
\begin{eqnarray*}
   a_{2l+1}&=&-{1\over 2}\left[{(-1)^{l+1}(2l+2)!\over 2^{2l+2}((l+1)!)^2}
              -{(-1)^l(2l)!\over 2^{2l}(l!)^2}\right]=\\
   &=&(-1)^l{(2l)!\over 2^{2l+1}(l!)^2}\left[{(2l+1)(2l+2)\over 2^2(l+1)^2}
               +1\right]=\\
   &=&(-1)^l{(2l)!\over 2^{2l+1}(l!)^2}\left[{2(2l+1)(l+1)\over 2^2(l+1)^2}
               +1\right]=\\
   &=&(-1)^l{(2l)!\over 2^{2l+1}(l!)^2}\left[{2l+1+2l+2\over 2(l+1)}\right]=\\
   &=&(-1)^l{(2l)!\over 2^{2l+1}(l!)^2}\left[{4l+3\over2(l+1)}\right]=\\
   &=&(-1)^l{(2l)!(4l+3)\over 2^{2l+2}l!(l+1)!}\\
   a_0&=&{1\over 2}\int\limits_{-1}^{+1} H (x)\underbrace{P_0(x)}_
               {\mbox{$=1$}}dx={1\over 2}\int\limits_0^1dx={1\over 2};
\end{eqnarray*}
and finally
$$
  H (x)={1\over 2}+\sum_{l=0}^\infty
   (-1)^l{(2l)!(4l+3)\over2^{2l+2}l!(l+1)!}P_{2l+1}(x).
$$
\eexample
}


\section{Associated Legendre polynomial}
\index{associated Legendre polynomial}

Associated Legendre polynomials $P_l^m(x)$ are the solutions of the
{\em general Legendre equation}
\index{general Legendre equation}

\begin{equation}
\begin{split}
\left\{
(1-x^2)\frac{d^2}{dx^2}
-2x  \frac{d }{dx }
+
\left[ l(l+1)-\frac{m^2}{1-x^2} \right] \right\} P_l^m(x)= 0,
\\
\textrm{or } \left[  \frac{d }{dx }\left((1-x^2)\frac{d }{dx }\right)
+
 l(l+1)-\frac{m^2}{1-x^2} \right]  P_l^m(x)= 0
\end{split}
\label{2011-m-ch-sfgle}
\end{equation}
Eq. (\ref{2011-m-ch-sfgle})
reduces to the Legendre equation
 (\ref{2011-m-ch-sfelpede})
on page \pageref{2011-m-ch-sfelpede} for  $m=0$;
hence
\begin{equation}
P_l^0(x)=P_l(x).
\end{equation}
More generally,
by differentiating $m$ times the Legendre equation (\ref{2011-m-ch-sfelpede})
it can be shown that
\begin{equation}
P_l^m(x)=(-1)^m(1-x^2)^\frac{m}{2} \frac{d^m}{dx^m}P_l(x).
\end{equation}
By inserting $P_l(x)$ from the
{\em Rodrigues formula}
\index{Rodrigues formula}
for Legendre polynomials (\ref{2011-m-ch-sfrf})
we obtain
\begin{equation}
\begin{split}
P_l^m(x)=
(-1)^m(1-x^2)^\frac{m}{2} \frac{d^m}{dx^m} \frac{1}{2^ll!}\frac{d^l}{dx^l}(x^2-1)^l
\\
\qquad
=
\frac{(-1)^m(1-x^2)^\frac{m}{2}}{2^ll!} \frac{d^{m+l}}{dx^{m+l}}(x^2-1)^l
.
\end{split}
\end{equation}

In terms of the Gauss hypergeometric function
\index{hypergeometric function}
the associated Legendre polynomials can be generalized to
arbitrary complex indices $\mu$, $\lambda$ and argument $x$ by
\begin{equation}
P^\mu_\lambda (x) =
\frac{1}{\Gamma (1-\mu )}\left(\frac{1+x}{1-x}\right)^\frac{\mu}{2}
{}_2F_1
\left(
-\lambda , \lambda  +1; 1-\mu ;\frac{1-x}{2}
\right).
\end{equation}
No proof is given here.

\section{Spherical harmonics}
\index{spherical harmonics}
\label{2011-m-ch-sfshar}

%http://en.wikipedia.org/wiki/Associated_Legendre_polynomials

Let us define the {\em spherical harmonics} $Y_l^m (\theta ,\varphi )$ by
\begin{equation}
Y_l^m (\theta ,\varphi ) =\sqrt{\frac{(2l+1)(l-m)!}{4\pi (l+m)!} }
P_l^m(\cos \theta )e^{im\varphi }\textrm{ for } -l\le m\le l.
.
\label{2014-m-ch-sf-sh}
\end{equation}

Spherical harmonics are solutions of the differential equation
\begin{equation}
\left\{
\Delta + l(l+1)
\right\}
Y_l^m (\theta ,\varphi ) =0
.
\end{equation}
This equation is what typically remains after separation and ``removal'' of the
radial part of the Laplace equation $\Delta \psi(r,\theta ,\varphi)=0$ in three dimensions
when the problem is invariant (symmetric) under rotations.
\marginnote{Twice continuously differentiable,
complex-valued solutions $u$ of the Laplace equation
$\Delta u =0$
are called
{\em harmonic functions}
\index{harmonic function}
\cite{Axler:1994:HFT}}

%\section{Bessel functions}



\section{Solution of the Schr\"odinger equation for a hydrogen atom}
\index{Schr\"odinger equation}

Suppose Schr\"odinger, in his 1926 {\it annus mirabilis}
--
a year
which seems to have been initiated by a trip to Arosa with `an old girlfriend from Vienna'
(apparently, it was neither his wife Anny who remained in Zurich, nor Lotte, nor Irene nor Felicie \cite{Moore-Schroedinger}),
--
came down from the mountains or from whatever realm he was in -- and
handed you over some partial differential equation  for the hydrogen atom
-- an equation
(note that  the quantum mechanical ``momentum operator'' ${\cal P}$ is identified with  $- i \hslash \nabla$)
\begin{equation}
\begin{split}
  \frac{1}{2\mu } {\cal P} ^2  \psi =\frac{1}{2\mu }\left({\cal P}_x^2 +{\cal P}_y^2 +{\cal P}_z^2\right) \psi = \left( E-V \right)\psi ,\\
\textrm{  or, with } V=-\frac{e^2}{4\pi \epsilon_0 r}, \\
 -\left( \frac{\hslash^2}{2\mu }\Delta +  \frac{e^2}{4\pi \epsilon_0r} \right)\psi ({\bf x})= E\psi, \\
 \textrm{ or }\left[ \Delta + \frac{2\mu }{\hslash^2} \left(\frac{e^2}{4\pi \epsilon_0r} + E \right)\right]\psi ({\bf x})=0,
\end{split}
\label{2011-m-ch-qae-sebf}
\end{equation}
which would later bear his name  -- and asked you if you could be so kind to please solve it for him.
Actually, by Schr\"odinger's own account \cite{ANDP:ANDP19263840404} he handed  over this eigenwert equation to Hermann Klaus Hugo Weyl;
in this instance, he was not dissimilar from Einstein, who seemed to have employed a (human) computist on a very regular basis.
Schr\"odinger might also have hinted that $\mu$, $e$, and $\epsilon_0$ stand for some
\index{reduced mass}
(reduced) mass~\marginnote{
In two-particle situations without external forces it is common to define the reduced mass
$\mu$ by $\frac{1}{\mu} = \frac{1}{m_1} +\frac{1}{m_2}= \frac{m_2+m_1}{m_1m_2}$, or $\mu= \frac{m_1m_2}{m_2+m_1}$,
where $m_1$ and $m_2$ are the masses of the constituent particles, respectively.
In this case, one can identify the electron mass $m_e$ with $m_1$, and the nucleon (proton) mass
$m_p\approx 1836 m_e \gg m_e$ with $m_2$, thereby allowing the approximation $\mu = \frac{m_e m_p}{m_e+m_p} \approx \frac{m_e m_p}{m_p} = m_e$.
},
charge, and the  permittivity of the vacuum, respectively,
$\hslash$ is a constant of (the dimension of) action,
and $E$ is some eigenvalue which must be determined from
the solution of (\ref{2011-m-ch-qae-sebf}).

So, what could you do?
First, observe that the problem is spherical symmetric,
as the potential   just depends on the radius $r=\sqrt{{\bf x}\cdot {\bf x}}$,
and also the Laplace operator $\Delta =
\nabla \cdot \nabla $ allows spherical symmetry.
Thus we could write   the Schr\"odinger equation (\ref{2011-m-ch-qae-sebf})
in terms of spherical coordinates
$(r, \theta ,\varphi )$,
mentioned already as an example of orthogonal curvilinear coordinates in Eq.~(\ref{2018-mm-ch-sc}),
with
\begin{equation}
\begin{split}
x  = r\sin \theta \cos \varphi \text{, }
y = r\sin \theta \sin \varphi \text{, }
z = r\cos \theta  \text{; and}\\
r=\sqrt{x^2+y^2+z^2}\text{, }
\theta = \arccos \left(\frac{z}{r}\right)\text{, }
\varphi=  \arctan \left(\frac{y}{x}\right).
\end{split}
\label{2018-mm-ch-sc1}
\end{equation}
$\theta$ is the polar angle in the $x$--$z$-plane measured
from the $z$-axis, with $0 \le \theta \le \pi$,
and $\varphi $ is  the azimuthal angle in the $x$--$y$-plane, measured
from the $x$-axis with $0 \le \varphi < 2 \pi$.
\index{spherical coordinates}
In terms of spherical coordinates the Laplace operator~(\ref{2018-mm-ch-laplaceocsc})
on page~\pageref{2018-mm-ch-laplaceocsc}
essentially ``decays into'' (that is, consists additively of)
a radial part and an angular part
\begin{equation}
\begin{split}
\Delta =
 \frac{\partial^2}{\partial x^2}
+
\frac{\partial^2}{\partial y^2}
+
\frac{\partial^2}{\partial z^2}
=
 \frac{\partial}{\partial x}
 \frac{\partial}{\partial x}
+
\frac{\partial}{\partial y}
 \frac{\partial}{\partial y}
+
\frac{\partial}{\partial z}
 \frac{\partial}{\partial z}
\\
=
\frac{1}{r^2} \left[ \frac{\partial}{\partial r}\left( r^2\frac{\partial}{\partial r}\right)
% \right. \qquad \quad   \\
+
%\left.
\frac{1}{\sin \theta}   \frac{\partial}{\partial \theta }
\sin \theta \frac{\partial}{\partial \theta }
+
\frac{1}{\sin^2 \theta} \frac{\partial^2}{\partial \varphi^2 }
\right].
\end{split}
\label{2011-m-ch-qae-losc}
\end{equation}


\newcommand{\comm}[1]{}
\comm{
=============================================================================

r[x_,y_,z_] := Sqrt[x^2 + y^2 + z^2];
\[Theta][x_,y_,z_] := ArcCos[z/Sqrt[x^2 + y^2 + z^2]];
\[Phi][x_,y_,z_] := ArcTan[ y / x ];

x[r,\[Theta]_,\[Phi]_] := r*Sin[\[Theta]]*Cos[\[Phi]];
y[r,\[Theta]_,\[Phi]_] :=  r*Sin[\[Theta]]*Sin[\[Phi]];
z[r,\[Theta]_,\[Phi]_] :=  r*Cos[\[Theta]];

(*
FullSimplify[
(
D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]
/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
+
D[ r[x,y,z] , y] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , y] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , y] D[ f[r,\[Theta],\[Phi]] , \[Phi]]
/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
+
D[ r[x,y,z] , z] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , z] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , z] D[ f[r,\[Theta],\[Phi]] , \[Phi]]
/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
)
]
*)

FullSimplify[ (FullSimplify[(
D[ r[x,y,z] , x] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z], x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, r] +
D[\[Theta][x,y,z] , x] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, \[Theta]] +
D[ \[Phi][x,y,z] , x] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, \[Phi]]
+ (***********)
D[ r[x,y,z] , y] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, r] +
D[ \[Theta][x,y,z] , y] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, \[Theta]] +
D[ \[Phi][x,y,z] , y] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, \[Phi]]
+ (***********)
D[ r[x,y,z] , z] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]], \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, r] +
D[ \[Theta][x,y,z] , z] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, \[Theta]] +
D[ \[Phi][x,y,z] , z] D[
FullSimplify[ (D[ r[x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , r] + D[ \[Theta][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Theta]] + D[ \[Phi][x,y,z] , x] D[ f[r,\[Theta],\[Phi]] , \[Phi]]/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]
, \[Phi]]
)]
/. x -> r*Sin[\[Theta]]*Cos[\[Phi]]
/. y -> r*Sin[\[Theta]]*Sin[\[Phi]]
/. z -> r*Cos[\[Theta]]
),Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ]

JacoSp= FullSimplify[ {
 { D[Sqrt[x^2 + y^2 + z^2], x] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]             ,
  D[Sqrt[x^2 + y^2 + z^2], y] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]             ,
 D[Sqrt[x^2 + y^2 + z^2], z] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]              },
{  D[ArcCos[z/Sqrt[x^2 + y^2 + z^2]], x] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]      ,
  D[ArcCos[z/Sqrt[x^2 + y^2 + z^2]], y] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]      ,
 D[ArcCos[z/Sqrt[x^2 + y^2 + z^2]], z] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]     },
{  D[ ArcTan[ y / x ], x] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]                  ,
  D[ ArcTan[ y / x ], y] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]                  ,
 D[ ArcTan[ y / x ], z] /. x -> r*Sin[\[Theta]]*Cos[\[Phi]] /. y -> r*Sin[\[Theta]]*Sin[\[Phi]] /. z -> r*Cos[\[Theta]]                 }
       },Element[r > 0 | 0 <= \[Theta] <= Pi | 0 <= \[Phi] <= 2 Pi  , Reals] ];

MatrixForm[JacoSp]



=============================================================================
}



\subsection{Separation of variables {\it Ansatz}}

This can be exploited for a
{\em separation of variable} {\it Ansatz},
which, according to  Schr\"odinger, should be well known
(in German {\em sattsam bekannt})
by now (cf chapter \ref{2011-m-ch-sv}).
We thus write the solution $\psi$ as a product of functions
of separate variables
\begin{equation}
\psi (r, \theta ,\varphi )=R(r)\Theta(\theta)\Phi(\varphi) = R(r)Y_l^m ( \theta ,\varphi )
\label{2011-m-ch-qaesva}
\end{equation}
That the angular part $\Theta(\theta)\Phi(\varphi)$ of this product
will turn out to be
the spherical harmonics $Y_l^m ( \theta ,\varphi )$  introduced earlier
on page  \pageref{2011-m-ch-sfshar}
is nontrivial -- indeed, at this point it is an {\em ad hoc} assumption.
We will come back to its derivation in fuller detail later.

\subsection{Separation of the radial part from the angular one}

For the time being, let us first concentrate on
the radial part $R(r)$.
Let us first separate the variables of
the Schr\"odinger equation (\ref{2011-m-ch-qae-sebf})
in spherical coordinates
\begin{equation}
\begin{split}
\left\{
\frac{1}{r^2} \left[ \frac{\partial}{\partial r}\left( r^2\frac{\partial}{\partial r}\right)
+
\frac{1}{\sin \theta}   \frac{\partial}{\partial \theta }
\sin \theta \frac{\partial}{\partial \theta }
+
\frac{1}{\sin^2 \theta} \frac{\partial^2}{\partial \varphi^2 }
\right]
\right.    \\
+
\left.
\frac{2\mu }{\hslash^2} \left(\frac{e^2}{4\pi \epsilon_0 r} + E \right)\right\}
\psi (r, \theta ,\varphi  )=0
.
\end{split}
\label{2011-m-ch-qa1e}
\end{equation}
Multiplying~(\ref{2011-m-ch-qa1e}) with $r^2$ yields
\begin{equation}
\begin{split}
\left\{  \frac{\partial}{\partial r}\left( r^2\frac{\partial}{\partial r}\right) +
\frac{2\mu r^2}{\hslash^2} \left(\frac{e^2}{4\pi \epsilon_0 r} + E \right) \right.  \\
\qquad
+  \left.
\frac{1}{\sin \theta}   \frac{\partial}{\partial \theta }
\sin \theta \frac{\partial}{\partial \theta }
+
\frac{1}{\sin^2 \theta} \frac{\partial^2}{\partial \varphi^2 }
\right\}
\psi (r, \theta ,\varphi  )=0
.
\end{split}
\label{2011-m-ch-qae2}
\end{equation}
After division by $\psi (r, \theta ,\varphi  )= R(r)\Theta(\theta)\Phi(\varphi)$
and writing separate variables on separate sides of the equation one obtains
\begin{equation}
\begin{split}
\frac{1}{R( r )}
\left[  \frac{\partial}{\partial r}\left( r^2\frac{\partial}{\partial r}\right) +
\frac{2\mu r^2}{\hslash^2} \left(\frac{e^2}{4\pi \epsilon_0 r} + E \right) \right] R(r)
\qquad
\\  =
-\frac{1}{\Theta(\theta)\Phi(\varphi)} \left(
\frac{1}{\sin \theta}   \frac{\partial}{\partial \theta }
\sin \theta \frac{\partial}{\partial \theta }
+
\frac{1}{\sin^2 \theta} \frac{\partial^2}{\partial \varphi^2 }
\right)
\Theta(\theta)\Phi(\varphi)
.
\end{split}
\label{2011-m-ch-qae3}
\end{equation}
Because the left hand side  of this equation is independent of the angular variables
$\theta $ and $\varphi$, and its right hand side  is independent of the radial variable $r$,
both sides have to be independent with respect to variations of $r$, $\theta $ and $\varphi$, and
can thus be equated with a constant;
say, $\lambda$.
Therefore, we obtain two ordinary
differential equations: one for the radial part [after multiplication of (\ref{2011-m-ch-qae3}) with $R(r)$ from the left]
\begin{equation}
\left[  \frac{\partial}{\partial r} r^2\frac{\partial}{\partial r}  +
\frac{2\mu r^2}{\hslash^2} \left(\frac{e^2}{4\pi \epsilon_0 r} + E \right) \right] R(r)
 =  \lambda  R( r )  ,
\label{2011-m-ch-qae4a}
\end{equation}
and another one for the angular part [after multiplication of (\ref{2011-m-ch-qae3}) with $\Theta(\theta)\Phi(\varphi)$ from the left]
\begin{equation}
\left(
\frac{1}{\sin \theta}   \frac{\partial}{\partial \theta }
\sin \theta \frac{\partial}{\partial \theta }
+
\frac{1}{\sin^2 \theta} \frac{\partial^2}{\partial \varphi^2 }
\right)
\Theta(\theta)\Phi(\varphi)   =  -  \lambda  \Theta(\theta)\Phi(\varphi),
\label{2011-m-ch-qae4b}
\end{equation}
respectively.


\subsection{Separation of the polar angle $\theta$ from the azimuthal angle $\varphi $}

As already hinted in Eq. (\ref{2011-m-ch-qaesva})
the angular portion can still be separated into a polar and an azimuthal
part
because, when multiplied by $\sin^2 \theta /[\Theta(\theta)\Phi(\varphi)]$,
Eq.   (\ref{2011-m-ch-qae4b})
can be rewritten as
\begin{equation}
\left(
\frac{\sin \theta}{\Theta(\theta)}
\frac{\partial}{\partial \theta }
\sin \theta \frac{\partial \Theta(\theta)}{\partial \theta }
+  \lambda  \sin^2 \theta   \right)
+
\frac{1}{\Phi(\varphi)} \frac{\partial^2\Phi(\varphi)}{\partial \varphi^2 }
= 0,
\label{2011-m-ch-qae4bc}
\end{equation}
and hence
\begin{equation}
\begin{split}
\frac{\sin \theta}{\Theta(\theta)}
\frac{\partial}{\partial \theta }
\sin \theta \frac{\partial \Theta(\theta)}{\partial \theta }
+  \lambda  \sin^2 \theta
  = -
\frac{1}{\Phi(\varphi)} \frac{\partial^2\Phi(\varphi)}{\partial \varphi^2 }
=  m^2,
\end{split}
\label{2011-m-ch-qae8}
\end{equation}
where $m$ is some constant.

\subsection{Solution of the equation  for the azimuthal angle factor $\Phi(\varphi )$}

The  resulting differential equation  for $\Phi(\varphi )$
\begin{equation}
\frac{   d   ^2\Phi(\varphi )}{   d    \varphi^2 }
=  -m^2\Phi(\varphi),
\label{2011-m-ch-qaephi}
\end{equation}
has the general solution consists of two linear independent parts
\begin{equation}
\Phi(\varphi) = A e^{im\varphi}+B e^{-im\varphi}.
\label{2011-m-ch-qae11}
\end{equation}
Because $\Phi$ must obey the periodic boundary conditions $\Phi(\varphi)=\Phi(\varphi  +2\pi)$,
$m$ must be an integer: let $B=0$; then
 \begin{equation}
\begin{split}
\Phi(\varphi)= Ae^{im\varphi} =\Phi(\varphi  +2\pi) = Ae^{im(\varphi  +2\pi)} = Ae^{im\varphi}e^{2i\pi m}  ,\\
1  =  e^{2i\pi m}  = \cos (2 \pi m) +i \sin (2 \pi m)  ,\\
\end{split}
\label{2018-mm-ch-sf-acin}
\end{equation}
which is only true for $m\in \mathbb{Z}$. A similar calculation yields the same result if $A=0$.


An integration shows that, if we require the system of functions $\{e^{im\varphi}\vert m \in \mathbb{Z}\}$
to be orthonormalized, then the two constants $A,B$ must be equal.
Indeed, if we define
\begin{equation}
\Phi_m(\varphi) = Ae^{im\varphi}
\label{2011-m-ch-qae11def}
\end{equation}
and require that it is normalized, it follows that
\begin{equation}
\begin{split}
\int_0^{2\pi} \overline {\Phi}_m(\varphi) \Phi_m(\varphi)d \varphi \\
\qquad     =
\int_0^{2\pi} \overline {A}e^{-im\varphi}Ae^{im\varphi} d \varphi \\
\qquad    =
\int_0^{2\pi}\vert A\vert^2 d \varphi \\
\qquad  = 2\pi  \vert A\vert^2  \\
\qquad
= 1,
\end{split}
\label{2011-m-ch-qae11normexpl}
\end{equation}
it is consistent to set
%\begin{equation}
$
A= \frac{1} {\sqrt{2\pi } };
$
%\label{2011-m-ch-qae11normexpln}
%\end{equation}
and hence,
\begin{equation}
\Phi_m(\varphi) = \frac{e^{im\varphi}} {\sqrt{2\pi }  }
\label{2011-m-ch-qae11normexplnendg}
\end{equation}
Note that, for different $m\neq n$, because $m-n\in \mathbb{Z}$,
\begin{equation}
\begin{split}
\int_0^{2\pi} \overline {\Phi}_n(\varphi) \Phi_m(\varphi)d \varphi
  =
\int_0^{2\pi} \frac{e^{-in\varphi}} {\sqrt{2\pi }  } \frac{e^{im\varphi}} {\sqrt{2\pi }}  d \varphi \\
  =
\int_0^{2\pi} \frac{e^{i(m-n)\varphi}} { 2\pi  }   d \varphi \
  =
\left. -\frac{i e^{i(m-n)\varphi}} { 2\pi (m-n) } \right|_{\varphi =0}^{\varphi = 2\pi}  = 0
.
\end{split}
\label{2011-m-ch-qae11normexplnoni}
\end{equation}


\subsection{Solution of the equation  for the  polar angle factor $\Theta (\theta )$}

The left-hand side of
Eq. (\ref{2011-m-ch-qae8}) contains only the polar coordinate.
Upon division by $\sin ^2 \theta$ we obtain
\begin{equation}
\begin{split}
\frac{1}{\Theta(\theta)\sin \theta}
\frac{   d   }{   d    \theta }
\sin \theta \frac{   d    \Theta(\theta)}{   d    \theta }
+  \lambda
= \frac{m^2}{\sin^2\theta}\textrm{, or }\\
\frac{1}{\Theta(\theta)\sin \theta}
\frac{   d   }{   d    \theta }
\sin \theta \frac{   d    \Theta(\theta)}{   d    \theta }   -\frac{m^2}{\sin^2\theta }
= -  \lambda    ,\\
\end{split}
\label{2011-m-ch-pcde}
\end{equation}

Now, first, let us consider the case $m=0$.
With the variable substitution
$x = \cos \theta$, and thus
$\frac{dx}{d\theta}= -\sin \theta$ and  $dx= -\sin \theta d\theta$, we obtain from (\ref{2011-m-ch-pcde})
\begin{equation}
\begin{split}
\frac{   d   }{   d    x }
\sin^2 \theta \frac{   d    \Theta(x)}{   d    x }
= -  \lambda  \Theta(x)   ,\\
\frac{   d   }{   d    x }
(1-x^2) \frac{   d    \Theta(x)}{   d    x } +  \lambda  \Theta(x)
=  0 ,\\
\left(x^2-1 \right)\frac{   d   ^2 \Theta(x)}{   d    x^2 } + 2 x \frac{   d    \Theta(x)}{   d    x } =  \lambda  \Theta(x)
,\\
\end{split}
\label{2011-m-ch-pcde1}
\end{equation}
which is of the same form as the {\em Legendre equation}
\index{Legendre equation}
(\ref{2011-m-ch-sfelpede})
mentioned on page \pageref{2011-m-ch-sfelpede}.

Consider the series {\it Ansatz}
\begin{equation}
\Theta(x) = \sum_{k=0}^\infty  a_k x^k
\label{2011-m-ch-pcde12}
\end{equation}
for solving (\ref{2011-m-ch-pcde1}).
\marginnote{This is actually a ``shortcut'' solution of the Fuchsian Equation mentioned earlier.}
Insertion into  (\ref{2011-m-ch-pcde1}) and comparing the coefficients of $x$
for equal degrees
yields the recursion relation
\begin{equation}
\begin{split}
\left(x^2-1 \right)\frac{   d   ^2  }{   d    x^2 }
 \sum_{k=0}^\infty  a_k x^k
   + 2 x \frac{   d    }{   d    x } \sum_{k=0}^\infty  a_k x^k
=  \lambda \sum_{k=0}^\infty  a_k x^k  ,\\
%
 \left(x^2-1 \right) \sum_{k=0}^\infty  k(k-1) a_k x^{k-2}
   + 2 x  \sum_{k=0}^\infty  k a_k x^{k-1}
=  \lambda \sum_{k=0}^\infty  a_k x^k  ,\\
%
\sum_{k=0}^\infty
\left[ \underbrace{k(k-1) + 2k}_{k(k+1)} -\lambda \right] a_k  x^k
-
\underbrace{\sum_{k=2}^\infty k(k-1) a_k x^{k-2}}_{\text{index shift }k-2=m\text{, }k=m+2}
=0
 ,\\
\sum_{k=0}^\infty
\left[ k(k+1) -\lambda \right] a_k  x^k
-
\sum_{m=0}^\infty (m+2)(m+1) a_{m+2} x^m
=0
 ,\\
%
\sum_{k=0}^\infty
\Big\{
\left[ k(k+1) -\lambda \right] a_k
-
 (k+1)(k+2) a_{k+2}
\Big\}
x^k
=0
 ,
\end{split}
\label{2011-m-ch-pcde123}
\end{equation}
and thus, by taking all polynomials of the order of $k$ and proportional to $x^k$,
so that, for $x^k\neq 0$ (and thus excluding the trivial solution),
\begin{equation}
\begin{split}
\left[ k(k+1) -\lambda \right] a_k -
 (k+1)(k+2) a_{k+2} =0,\\
a_{k+2} = a_k \frac{k(k+1) - \lambda}{(k+1)(k+2)}.
\end{split}
\label{2011-m-ch-pcde1236}
\end{equation}

In order to converge also for $x=\pm 1$, and hence for $\theta =0$ and $\theta= \pi$,
the sum in (\ref{2011-m-ch-pcde12})
has to have only {\em a finite number of terms}.
Because  if the sum would be infinite, the terms $a_k$, for large $k$,
would be dominated by $a_{k-2} O( k^2/k^2)= a_{k-2} O(1)$.
As a result  $a_k$
would converge to $a_k \stackrel{ k  \rightarrow \infty}{\longrightarrow} a_\infty$ with constant $a_\infty \neq 0$
Therefore, $\Theta$ would diverge as
$\Theta (1) \stackrel{ k  \rightarrow \infty}{\approx} k a_\infty  \stackrel{ k  \rightarrow \infty}{\longrightarrow}  \infty$.
That means that, in Eq. (\ref{2011-m-ch-pcde1236})
for some $k=l\in {\Bbb N}$, the coefficient  $a_{l+2}=0$ has to vanish; thus
\begin{equation}
\lambda = l(l+1).
\label{2011-m-ch-pcde12361}
\end{equation}
This results in {\it Legendre polynomials} $\Theta (x) \equiv P_l(x)$.
\index{Legendre polynomial}


Let us shortly mention the case $m\neq 0$.
With the same variable substitution  $x = \cos \theta$, and thus
$\frac{dx}{d\theta}= -\sin \theta$ and  $dx= -\sin \theta d\theta$ as before,
the equation for the polar angle dependent factor (\ref{2011-m-ch-pcde})
becomes
\begin{equation}
\begin{split}
\left[
\frac{   d   }{   d    x }
(1-x^2) \frac{   d    }{   d    x }  +  l(l+1)   -\frac{m^2}{ 1-x^2 }
\right]
\Theta(x)  =0,\\
\end{split}
\label{2011-m-ch-pcde9}
\end{equation}
This is exactly the form of the
general Legendre equation (\ref{2011-m-ch-sfgle}), whose solution is a multiple
of the associated Legendre polynomial   $P_l^m(x)$, with $\vert m\vert \le l$.

Note (without proof) that, for equal $m$, the $P_l^m(x)$ satisfy the orthogonality condition
\begin{equation}
\int_{-1}^{1} P_l^m(x) P_{l'}^m(x) dx =
{\frac{2 (l+m)!} {(2l+1)(l-m)!}}  \delta_{l l'}.
\end{equation}
Therefore  we obtain a normalized polar solution by dividing $P_l^m(x)$
by $\left\{\left[2 (l+m)!\right]/\left[(2l+1)(l-m)!\right]\right\}^{1/2}$.

In putting both normalized  polar and azimuthal angle
factors together we arrive at the spherical harmonics (\ref{2014-m-ch-sf-sh}); that is,
\begin{equation}
\Theta(\theta)\Phi(\varphi)
= \sqrt{\frac {(2l+1)(l-m)!}{2 (l+m)!}}  P_l^m(\cos \theta ) \frac {e^{im\varphi }}{\sqrt{2\pi}} =
 Y_l^m (\theta ,\varphi )
\end{equation}
for $-l\le m\le l$, $l\in {\Bbb N}_0$.
Note that the discreteness of these solutions
follows from physical requirements about their finite
existence.

\subsection{Solution of the equation  for radial factor $R(r)$}

% http://www.eng.fsu.edu/~dommelen/quantum/style_a/nt_hydr.html

The solution of the equation   (\ref{2011-m-ch-qae4a})
\begin{equation}
\begin{split}
\left[  \frac{   d   }{   d    r}  r^2\frac{   d   }{   d    r}   +
\frac{2\mu r^2}{\hslash^2} \left(\frac{e^2}{4\pi \epsilon_0r} + E \right) \right] R(r)
 =  l(l+1)  R( r ) \textrm{ , or}\\
-\frac{1}{R(r)} \frac{d}{   d    r}  r^2\frac{   d   }{   d    r} R( r ) +    l(l+1)
-
2
\frac{\mu e^2}{4\pi \epsilon_0\hslash^2} r
 = \frac{2\mu  }{ \hslash^2}  r^2 E
\end{split}
\label{2011-m-ch-qae4a19}
\end{equation}
for the radial factor $R(r)$
turned out to be the most difficult part for Schr\"odinger
\cite{Moore-Schroedinger}.

Note that, since the additive term  $ l(l+1) $ in (\ref{2011-m-ch-qae4a19}) is non-dimensional,
so must be the other terms.
We can make this more explicit by the substitution of variables.

First, consider $y =\frac{r}{a_0}$ obtained by dividing $r$ by the
{\em Bohr radius}
\index{Bohr radius}
\begin{equation}
a_0= \frac{4\pi \epsilon_0 \hslash^2}{m_e e^2}\approx 5\; 10^{-11} m,
\label{2011-m-ch-qaebohrr}
\end{equation}
thereby assuming that the reduced mass is equal to the electron mass $\mu \approx m_e$.
More explicitly,
$r=ya_0= y  (4\pi \epsilon_0 \hslash^2)/(m_e e^2)$,
or $y= r/a_0= r  (m_e e^2)/(4\pi \epsilon_0 \hslash^2)$.
Furthermore, let us define $\varepsilon = E \frac{2\mu a_0^2}{\hslash^2}$.

These substitutions yield
\begin{equation}
\begin{split}
-\frac{1}{R(y)} \frac{d}{   d    y}  y^2\frac{   d   }{   d    y} R( y ) +    l(l+1)
-2 y
 = y^2 \varepsilon \textrm{, or}\\
-y^2 \frac{d^2}{   d    y^2} R( y )  - 2 y \frac{   d   }{   d    y} R( y )
+ \left[   l(l+1) -2 y   -\varepsilon   y^2  \right] R( y )
 = 0.
\end{split}
\label{2011-m-ch-qae4a191}
\end{equation}

Now we introduce a new function $\hat{R}$  {\it via}
\begin{equation}
R(  \xi )=  \xi ^l e^{-\frac{1}{2}  \xi }\hat{R}(  \xi ) ,
\label{2011-m-ch-qae4a19s}
\end{equation}
with $  \xi =\frac{2y}{n}$ and by replacing the energy variable with
$\varepsilon =-\frac{1}{n^2}$.
(It will later be argued that $\varepsilon$ must be discrete;  with $n\in {\Bbb N}-0$.)
This yields
\begin{equation}
\begin{split}
\xi  \frac{d^2}{   d    \xi^2}\hat{R}( \xi )  +[ 2 (l+1)-\xi] \frac{   d   }{   d    \xi } \hat{R}( \xi )
+ ( n-l-1  ) \hat{R}( \xi )
 = 0.
\end{split}
\label{2011-m-ch-qae4a191ssAe}
\end{equation}

The discretization of $n$ can again be motivated by requiring physical properties from
the solution; in particular, convergence.
Consider again a series solution {\it Ansatz}
\begin{equation}
\hat{R}(  \xi ) = \sum_{k=0}^\infty c_k\xi^k ,
\label{2011-m-ch-qae4a19sssA}
\end{equation}
which, when inserted into (\ref{2011-m-ch-qae4a191}),
yields
\begin{equation}
\begin{split}
\left\{
\xi  \frac{d^2}{   d    \xi^2}   + [ 2 (l+1)-\xi] \frac{   d   }{   d    \xi } + ( n-l-1  )
\right\}
\sum_{k=0}^\infty c_k\xi^k
=0,\\
%
\xi  \sum_{k=0}^\infty k(k-1) c_k\xi^{k-2}
+ [ 2 (l+1)-\xi] \sum_{k=0}^\infty k c_k\xi^{k-1}
+ ( n-l-1  )\sum_{k=0}^\infty c_k\xi^k
=0,\\
%
\underbrace{\sum_{k=1}^\infty \left[ \underbrace{k(k-1) + 2k(l+1)}_{=k(k+2l+1)}\right] c_k\xi^{k-1}}_{\text{index shift }k-1=m\text{, }k=m+1}
+
 \sum_{k=0}^\infty (-k + n-l-1  ) c_k\xi^k
=0,\\
%
\sum_{m=0}^\infty \left[ (m+1)(m+2l+2)\right] c_{m+1}\xi^m
+
 \sum_{k=0}^\infty (-k + n-l-1  ) c_k\xi^k
=0,\\
%
\sum_{k=0}^\infty
\Big\{
\left[ (k+1)(k+2l+2)\right] c_{k+1}
+
 (-k + n-l-1  ) c_k
\Big\}
\xi^k
=0,
\end{split}
\label{2011-m-ch-qae4a191ssA}
\end{equation}
so that, by comparing the coefficients of $\xi^k$, we obtain
\begin{equation}
\begin{split}
\left[ (k+1)(k+2l+2)\right] c_{k+1}
= -
 (-k + n-l-1  ) c_k  ,\\
c_{k+1}
=
c_k   \frac{k - n + l+ 1   }{(k+1) (k  + 2l + 2 )}  .\\
\end{split}
\label{2011-m-ch-qae4a191ssA5}
\end{equation}
Because of convergence of $\hat{R}$ and thus of $R$
--
note that, for large $\xi$ and $k$, the $k$'th term in
Eq.~(\ref{2011-m-ch-qae4a19sssA}) determining $\hat{R}(  \xi )$
would behave as $\xi^k/k!$
and thus  $\hat{R}(  \xi )$ would roughly behave as the exponential function $e^\xi$
--
the series solution
(\ref{2011-m-ch-qae4a19sssA})
should terminate at some   $k =   n-l-1$,
or  $ n=  k+l+1$. Since $k$, $l$, and $1$ are all integers,
$n$ must be an integer as well.
And since $k\ge 0$,
and therefore $n-l-1 \ge 0$,
$n$ must at least be $l+1$, or
\begin{equation}
l\le n-1 .
\label{2011-m-ch-qae4a191ssA5hqz}
\end{equation}


Thus,  we end up with an {\em associated Laguerre equation}
\index{associated Laguerre equation} of the form
\begin{equation}
\left\{
  \xi    \frac{d^2}{   d     \xi  ^2 }
+[2(l+1)-  \xi  ]  \frac{d }{   d       \xi  }
+(n-l-1)
\right\}    \hat{R}(   \xi  ) =0
\textrm{, with } n \ge l+1\textrm{, and } n,l\in {\Bbb Z}.
\label{2011-m-ch-qaeale}
\end{equation}
Its   solutions are  the {\it associated Laguerre polynomials}
$L^{2l+1}_{n+l}$
which are the $(2l+1)$-th derivatives of the
Laguerre's polynomials  $L_{n+l}$; \index{Laguerre polynomial}
that is,
\begin{equation}
\begin{split}
 L_n(x)=e^x   \frac{d^n }{   d     x^n }  \left (x^ne^{-x}\right),\\
 L_n^m(x)=   \frac{d^m }{   d     x^m }  L_n(x).
\end{split}
\label{2011-m-ch-qaelp}
\end{equation}
This yields a normalized wave function
\begin{equation}
\begin{split}
R_n(r) =  {\cal N}
\left( \frac{2 {r}}{n a_0 }\right)^l e^{-\frac{r}{a_0 n}}
L^{2l+1}_{n+l} \left( \frac{2 {r}}{n a_0 }\right)\textrm{, with }\\
{\cal N}   =-\frac{2}{n^2}\sqrt{\frac{(n-l-1)!}{[(n+l)!a_0]^3}} ,
\end{split}
\label{2011-m-ch-qaecsrp}
\end{equation}
where
${\cal N}$ stands for the normalization factor.

\subsection{Composition of the general solution of the Schr\"odinger Equation}

Now we shall coagulate
\marginnote{Always remember the alchemic  principle of {\it solve et coagula}!}
and combine the factorized solutions (\ref{2011-m-ch-qaesva})
into a complete solution of the Schr\"odinger equation
for $n+1,\, l,\, \vert m \vert  \in {\Bbb N}_0$, $0\le l\le n-1$, and $\vert m \vert \le l$,
\begin{equation}
\begin{split}
\psi_{n,l,m} (r, \theta ,\varphi )
 = R_n(r)Y_l^m ( \theta ,\varphi )\\
 = -\frac{2}{n^2}\sqrt{\frac{(n-l-1)!}{[(n+l)!a_0]^3}}
\left( \frac{2 {r}}{n a_0 }\right)^l e^{-\frac{r}{a_0 n}}
L^{2l+1}_{n+l} \left( \frac{2 {r}}{n a_0 }\right) \times \qquad \qquad  \\
\times \sqrt{\frac {(2l+1)(l-m)!}{2 (l+m)!}}  P_l^m(\cos \theta )
\frac {e^{im\varphi }}{\sqrt{2\pi}} .
\end{split}
\label{2011-m-ch-qaesva13444}
\end{equation}



\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright
\decofourleft
%\aldine X \decoone c
%\floweroneright
% \aldineleft ]
% \decosix
%\leafleft
% \aldineright  w  \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
%\decofourright
% \floweroneleft
}
\end{center}

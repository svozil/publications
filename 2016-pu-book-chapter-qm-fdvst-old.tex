%2016-pu-book-chapter-qm-fdvst



\chapter{Hilbert space theory}
\label{2016-pu-book-chapter-hst} % Always give a unique label

This chapter is a review of finite-dimensional vector space theory.
Readers familiar with it can skip it entirely.


Two fine books, in particular, inspired the writing of the following chapters:
One is Halmos' {\em Finite-dimensional Vector Spaces}~\cite{halmos-vs}
which is a soft and compact yet considerate introduction to this topic.
From the many books I have come across this one deserves to be called a positive antithesis to
Pascal's statement
{\it ``The present letter is a very long one, simply because I had no leisure to make it shorter''}
(in {\it Provincial Letters: Letter XVI}) it is this one.
%http://www.arcticbeacon.com/docs/Pascal/Provincial_Letters_of_Blaise_Pascal.pdf

The second book is Mermin's {\em Quantum Computer Science}~\cite{mermin-07},
also available as his {\em Lecture Notes on Quantum Computation}~\cite{mermin-04}.
These introductions are mainly directed toward non-(quantum-)physicists.
They cope with two interconnected difficulties, or rather obstacles, encountered by nonspecialists of quantum mechanics in a very original way.
One issue is notational: it is the bra-ket notation often used in quantum mechanics.
To put it metaphorically, it appears that Dirac, by introducing this notation for Hilbert space entities in quantum mechanics~\cite{dirac},
has been fighting fire with fire, or cast out devils by Beelzebub; in other words:  has replace one evil with another one.
My experience (and I have accumulated  a lot of teaching experience) is that if you confront an (already suspicious) audience with a ket, you are immediately
loosing most of them.
Most students will take this as an immediate excuse to stop thinking (or at least listening); and to dose off.
Mermin copes with this reaction by not talking about quantum issues at first;
and rather softly introducing Hilbert space, as well as bras and kets in the context of classical reversible computation.

The other obstacle for comprehension of quantum mechanics is the theory of linear vector spaces, as well as functional analysis.
Although most researcher have heard a thing or two about Hilbert spaces -- that is, vector spaces equipped with a scalar product
-- they have forgotten about that; thereby hoping that they will never need this subject again.
For them, a glance at Halmos' {\em Finite-dimensional Vector Spaces}~\cite{halmos-vs} might come to their rescue.


Most propositions will be stated without proofs; for proofs and more comprehensive treatment the reader is referred to
Halmos' aforementioned {\em Finite-dimensional Vector Spaces}~\cite{halmos-vs}.

\section{Some conventions and notations}

In what follows we shall be mainly concerned with vector spaces of finite dimensions.


Let $\Re z$ and $\Im z$ stand for the real and imaginary parts of $z$, respectively.
The overline sign stands for complex conjugation; that is,
if
$z= \Re z + i \Im z $ is a complex number, then
$\overline{z}= \Re z - i \Im z$.

A supercript ``$^\intercal$'' stands for transposition.



For an $n \times m$ matrix $\textsf{\textbf{A}}$ we shall use the following {\em index notation}:
suppose the (column) index $i$  indicating their column number in a matrix-like object $\textsf{\textbf{A}}$ ``runs {\em horizontally},''
and
the (row) index $j$  indicating their row number in a matrix-like object  $\textsf{\textbf{A}}$ ``runs {\em vertically},''
so that, with
$1 \le i \le n$ and
$1 \le j \le m$,
\index{index notation}
\begin{equation}
\textsf{\textbf{A}}
\equiv
\begin{pmatrix}
a_{11}&a_{12}& \cdots & a_{1m}\\
a_{21}&a_{22}& \cdots & a_{2m}\\
\vdots &\vdots & \ddots & \vdots \\
a_{n1}&a_{n2}& \cdots & a_{nm}
\end{pmatrix}
\equiv a_{ij}
.
\label{2016-m-ch-fdvs-matrixind}
\end{equation}
Stated differently, $a_{ij}$ is the element  of the table representing $\textsf{\textbf{A}}$ which is in the $i$th row and in the $j$th column.

A {\em matrix multiplication}
\index{matrix multiplication} (written with or without dot)
$\textsf{\textbf{A}} \cdot \textsf{\textbf{B}} = \textsf{\textbf{A}}  \textsf{\textbf{B}}$
of an $n \times m$ matrix $\textsf{\textbf{A}}\equiv a_{ij}$
with an $m \times l$ matrix $\textsf{\textbf{B}}\equiv b_{pq}$
can then be written as an $n \times l$ matrix
$\textsf{\textbf{A}} \cdot \textsf{\textbf{B}} \equiv a_{ij}b_{jk}$,
$1\le i\le n$,
$1\le j\le m$,
$1\le k\le l$.
Here the {\em Einstein summation convention}
\index{Einstein summation convention}
$a_{ij}b_{jk} = \sum_j a_{ij}b_{jk}$
has been used,
which requires that, when an index variable appears twice in a single term, one has to
sum over all of the possible index values.
Stated differently, if $\textsf{\textbf{A}}$ is an $n \times m$ matrix and $\textsf{\textbf{B}}$ is an $m \times l$ matrix,
their matrix product $\textsf{\textbf{A}}\textsf{\textbf{B}}$ is an $n \times l$ matrix, in which the $m$
entries across the rows of $\textsf{\textbf{A}}$ are multiplied with the $m$ entries down the columns of $\textsf{\textbf{B}}$.


\section{Bra and ket vectors}
\label{2016-pu-book-chapter-bkv} % Always give a unique label

We shall use Dirac's~\cite{dirac} bra and ket notation for dual (for a definition see later, Section \ref{2011-m-dvs} on page \pageref{2011-m-dvs})
vectors and vectors, respectively.
Thereby,
the ``ket vector'' $\vert {\bf x}\rangle$
\index{ket vector}
will be represented by column vectors, that is, by vertically arranged tuples of scalars,
or, equivalently, as $n \times 1$ matrices; that is,
\begin{equation}
\vert {\bf x}\rangle
\equiv
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
.
\end{equation}


A vector  from the dual space
will be identified with the ``bra vector'' $\langle {\bf x}\vert$.
\index{bra vector}
Bra vectors will be represented by row vectors, that is, by horizontally arranged tuples of scalars,
or, equivalently, as $1 \times n$ matrices; that is,
\begin{equation}
\langle {\bf x}   \vert
\equiv
\begin{pmatrix}
x_1,
x_2,
\ldots ,
x_n
\end{pmatrix}
.
\end{equation}



Dot (scalar or inner) products between two vectors $\vert {\bf x}\rangle $ and $\vert {\bf y}\rangle $   in Euclidean space are
\index{Euclidean space}
denoted by ``$\langle \textrm{bra} \vert  (\textrm{c}) \vert \textrm{ket}  \rangle$''  form;
that is, by $\langle {\bf x} \vert  {\bf y}  \rangle$.










\section{Fields of real and complex numbers}

In physics, scalars occur either as real or complex numbers.
Thus we shall restrict our attention to these cases.

A (real or complex) scalar {\em field}  $\langle  {\Bbb F} , +, \cdot , -, ^{-1}, 0, 1\rangle$
\index{field}
is a set together with two operations,
usually called {\em addition} and {\em multiplication},   denoted by ``$+$'' and ``$\cdot$''
(often  ``$a\cdot b$'' is identified with the expression ``$ab$'' without the center dot)
respectively, such that the following  axioms hold:
 (i)
closure of ${\Bbb F}$ with respect to addition and multiplication:
for all $a, b \in {\Bbb F}$, both $a + b$ and $a   b$ are in ${\Bbb F}$;
 (ii)
associativity of addition and multiplication:
for all $a$, $b$, and $c$ in ${\Bbb F}$,
the following equalities hold: $a + (b + c) = (a + b) + c$,
and
$a (b c) = (a  b) c$;
 (iii)
commutativity of addition and multiplication:
for all $a$ and $b$ in ${\Bbb F}$, the following equalities hold: $a + b = b + a$ and $a b = b  a$;
 (iv)
additive and multiplicative identity:
there exists an element of ${\Bbb F}$,
called the additive identity element and denoted by $0$, such that for all $a$ in ${\Bbb F}$,
$a + 0 = a$.
Likewise, there is an element, called the multiplicative identity element and denoted by $1$,
such that for all $a$ in ${\Bbb F}$, $1 \cdot a  = a$.
(To exclude the trivial ring, the additive identity and the multiplicative
identity are required to be distinct.)
 (v)
additive and multiplicative inverses:
for every $a$ in ${\Bbb F}$, there exists an element $-a$ in ${\Bbb F}$, such that $a + (-a) = 0$.
Similarly, for any $a$ in ${\Bbb F}$ other than $0$, there exists an element $a^{-1}$ in ${\Bbb F}$,
such that $a \cdot a^{-1} = 1$.
(The elements $+ (-a)$ and  $a^{-1}$
are also denoted $-a$ and $\frac{1}{a}$, respectively.)
Stated differently: subtraction and division operations exist.
 (vi)
Distributivity of multiplication over addition:
For all $a$, $b$ and $c$ in ${\Bbb F}$, the following equality holds:
$a (b + c) = (a  b) + (a  c)$.


\section{Vector space}

Vector spaces are structures allowing the
sum (addition) -- in physics called the {\em coherent superposition} -- of objects called ``vectors,'' and multiplication
of vectors by scalars; thereby remaining in this structure.
That is, for instance, the coherent superposition  $\vert {\bf x}  \rangle + \vert {\bf y} \rangle $
\index{superposition}
\index{coherent superposition}
of two vectors $\vert {\bf x} \rangle $ and $ \vert {\bf y} \rangle $ can be guaranteed to
be a vector.

Algebraically, ``vectors'' are elements of vector spaces.
Geometrically a vector may be interpreted as ``a quantity which is usefully represented by an arrow'' \cite{Weinreich}.

More precisely, a {\em linear vector space}      $\langle  \mathfrak V , +, \cdot , -,  0, 1\rangle$
\index{linear vector space}
is a set $\mathfrak V$ of elements called {\em vectors},
\index{vector}
here denoted by  bold face  symbols such as
$
\vert {\bf a}\rangle,
\vert {\bf x}\rangle,
\vert {\bf v}\rangle,
\vert {\bf w}\rangle,
\ldots
$,
satisfying certain   axioms:
With respect to vector addition,
(i)
commutativity, that is, $\vert {\bf x}\rangle + \vert {\bf y}\rangle   = \vert {\bf y}\rangle + \vert {\bf x}\rangle$;
(ii)
associativity, that is, $(\vert {\bf x}\rangle + \vert {\bf y}\rangle ) +  \vert {\bf z}\rangle =\vert {\bf x}\rangle + (\vert {\bf y}\rangle +  \vert {\bf z}\rangle )$;
(iii)
the uniqueness of the origin or null vector $0$;
as well as
(iv)
the uniqueness of  the negative vector.

With respect to scalar multiplication of vectors the following axioms are satisfied:
(v)
the existence of a unit factor $1$; and
(vi)
distributivity with respect to scalar and vector additions; that is,
$
(\alpha +\beta )\vert {\bf x} \rangle = \alpha \vert  {\bf x} \rangle + \beta  \vert  {\bf x} \rangle$,
as well as
$
\alpha (\vert {\bf x} \rangle +\vert {\bf y} \rangle ) = \alpha\vert  {\bf x} \rangle +\alpha\vert  {\bf y} \rangle$
for all
$\vert {\bf x} \rangle , \vert {\bf y} \rangle  \in \mathfrak V$ and scalars $\alpha ,\beta \in  {\Bbb F}$,
respectively.



\section{Scalar or inner product}
\index{scalar product}
\index{inner product}
\label{2011-m-scalarproduct}

A {\em scalar} or {\em inner} product presents some form of measure of ``distance'' or ``apartness''
of two vectors in a linear vector space.
An inner product space is a vector space $\mathfrak V$,
together with an inner product; that is, with a map
 $\langle \cdot \vert \cdot \rangle :  \mathfrak V  \times  \mathfrak V  \longrightarrow {\Bbb F}$
 (usually ${\Bbb F}={\Bbb C}$ or ${\Bbb F}={\Bbb R}$)
which satisfies the following three conditions (or, stated differently, axioms) for all vectors  and all scalars:
(i)
Conjugate (Hermitian) symmetry:
\index{conjugate symmetry}
\index{Hermitian symmetry}
$
\langle {\bf x}\vert {\bf y}\rangle
=
\overline{\langle {\bf y}\vert {\bf x}\rangle }$;
(ii)
linearity in the first argument:
$
\langle  \alpha{\bf x} + \beta {\bf y}  \vert {\bf z}\rangle
=
\alpha {\langle {\bf x}\vert {\bf z}\rangle}
+
\beta {\langle {\bf y}\vert {\bf z}\rangle}
$;
(iii)
positive-definiteness:
$
\langle {\bf x}\vert {\bf x}\rangle
\ge
0$;  with equality if and only if ${\bf x} = 0$.


Note that from the first two properties, it follows that the inner product is
{\em antilinear}, or synonymously,
{\em conjugate-linear}, in its second argument (note that $\overline{(uv)}=(\overline{u}) (\overline{v})$ for all $u,v\in {\Bbb C}$):
\begin{equation}
\begin{split}
 \langle  {\bf z} \vert \alpha {\bf x} + \beta {\bf y} \rangle
 =
 \overline{\langle \alpha {\bf x} + \beta {\bf y} \vert  {\bf z} \rangle }
 =
 \overline{ \alpha}\overline{\langle {\bf x} \vert  {\bf z}\rangle}
 + \overline{\beta }\overline{\langle {\bf y} \vert  {\bf z}\rangle}
 =
 \overline{\alpha} {\langle {\bf z}\vert {\bf x}\rangle}
 +\overline{\beta} {\langle {\bf z}\vert {\bf y}\rangle}
.
\end{split}
\end{equation}


\section{Norm}

 The {\em norm} of a vector ${\bf x}$
\index{norm}
is defined {\it via} the scalar product by
\begin{equation}
\|
{\bf x}
\|
=\sqrt{\langle {\bf x}\vert {\bf x}\rangle }
.
\end{equation}

Conversely, the {\em polarization identity}
\index{polarization identity}
expresses the inner product of two vectors in terms of the norm of their differences; that is,

\begin{equation}
\langle {\bf x}\vert {\bf y}\rangle
=
\frac{1}{4}\left[
\| \vert  {\bf x}\rangle +\vert  {\bf y}\rangle  \|^2
-
\|\vert  {\bf x}\rangle - \vert {\bf y}\rangle  \|^2
+ i
\left(
\| \vert  {\bf x}\rangle + i\vert {\bf y}\rangle  \|^2
-
\| \vert  {\bf x}\rangle - i\vert {\bf y} \rangle \|^2
\right)
\right]
.
\label{2015-m-ch-polidc}
\end{equation}


\section{Hilbert space}


A (quantum mechanical) {\em Hilbert space}  is a linear
\index{Hilbert space}
vector space ${\mathfrak V}$ over the field ${\Bbb C}$ of complex numbers
(sometimes only ${\Bbb R}$ is used)
equipped with vector addition, scalar multiplication, and some inner (scalar) product.

Furthermore, {\em closure} is an additional requirement, but nobody has made operational sense of that so far.
We shall neither define nor discuss closure here.




\section{Basis}
\label{2012-m-ch-fdvs-Basis}

We shall use bases of vector spaces to formally represent vectors (elements) therein.
But first we need to define {linear independence}.

A set ${\mathfrak S}=\{
\vert {\bf x}_1\rangle ,
\vert {\bf x}_2\rangle ,
\ldots ,
\vert {\bf x}_k\rangle \} \subset {\mathfrak V}$
of vectors ${\bf x}_i$ in a linear vector space
is {\em linearly independent}
\index{linear independence}
if $\vert {\bf x}_i\rangle \neq 0$,
and additionally, if either $k=1$,
or if no vector in $\mathfrak S$ can be written as a linear combination of other vectors in this set $\mathfrak S$;
that is, there are no scalars $\alpha_j$ satisfying
 $\vert {\bf x}_i\rangle =\sum_{1\le j \le k, \; j\neq i}\alpha_j \vert {\bf x}_j\rangle$.



A {\em basis}
\index{basis}
[or a {\em coordinate system}, or a {\em frame (of reference)}]
\index{coordinate system}
\index{frame}
is a set    $\mathfrak B$
of linearly independent vectors
such that every vector
in $\mathfrak V$ is a linear combination of the vectors in the basis; hence
$\mathfrak B$ spans $\mathfrak V$.

What particular basis should one choose?
{\it A priori} no basis is privileged over the other.
Yet, in view of certain (mutual) properties of elements of some bases (such as orthogonality or orthonormality)
we shall prefer (s)ome over others.



All bases $\mathfrak B$ of $\mathfrak V$ contain the same number of elements.

A vector space is finite dimensional if its bases are finite; that is, its bases
contain a finite number of elements.




The Cartesian standard basis in $n$-dimensional complex space ${\Bbb C}^n$
\index{Cartesian standard basis}
is the set of (usually ``straight'')
vectors $x_i, i=1, \ldots , n$, of ``unit length''
represented by $n$-tuples,
defined by the condition that the $i$'th coordinate of the $j$'th basis vector
${\bf e}_j$ is given by $\delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta function
\index{Kronecker delta function}
\begin{equation}
\delta_{ij} =\begin{cases}
0  &\text{ for }i\neq j , \\
1  &\text{ for }i = j.
\end{cases}
\end{equation}
Thus we can represent the basis vectors by
\begin{equation}
\begin{split}
\vert {\bf e}_1 \rangle = \begin{pmatrix}1\\ 0\\ \vdots\\ 0\end{pmatrix},\quad
\vert {\bf e}_2 \rangle = \begin{pmatrix}0\\ 1\\ \vdots\\ 0\end{pmatrix},\quad
\ldots\quad
\vert {\bf e}_n \rangle = \begin{pmatrix}0\\ 0\\ \vdots\\ 1\end{pmatrix}.
\end{split}
\label{2016-m-fdvs-csb}
\end{equation}





The {\em dimension}
\index{dimension}
of $\mathfrak V$ is the number of elements in $\mathfrak B$.

In quantum physics, the dimension of a quantized system is associated with
the {\em number of mutually exclusive measurement outcomes}.
For a spin state measurement of an electron
along a particular direction,
as well as for a measurement of the linear polarization
of a photon in a particular direction,
the dimension is two, since both measurements
may yield two distinct outcomes
which we can
interpret as vectors in two-dimensional Hilbert space,
which, in Dirac's bra-ket notation, can be written as
$
\vert \uparrow \rangle$ and $\vert \downarrow \rangle$,
or $\vert + \rangle$ and $
\vert - \rangle
$,
or
$
\vert H \rangle $ and $
\vert V \rangle
$,
or
$
\vert 0 \rangle $ and $
\vert 1 \rangle
$.




\section{Vector coordinates or components}
The coordinates or components of a vector with respect to some basis
represent the coding of that vector in that particular basis.
It is important to realize that, as bases change, so do coordinates.
Indeed, the changes in coordinates have to ``compensate'' for the bases change,
because the same coordinates in a different basis would render an altogether different
vector.
Thus it is often said that, in order to represent one and the same vector,
if the base vectors {\em vary}, the corresponding components or coordinates have to {\em contra-vary.}


Every vector $\vert {\bf x} \rangle $
can be written as a linear combination -- in quantum physics called
{coherent superposition}
\index{superposition}
\index{coherent superposition}
-- of  base vectors
\begin{equation}
\vert {\bf x} \rangle =  \sum_{i=1}^n x_i \vert {\bf e}_i \rangle
\equiv
\begin{pmatrix}x_1\\x_2\\ \vdots \\ x_n\end{pmatrix}.
\label{2016-m-fdvs-rv0}
\end{equation}
With the notation
defined by
\begin{equation}
\begin{split} X = \begin{pmatrix}
x_1, x_2, \ldots , x_n
\end{pmatrix}^\intercal
\textrm{, and }
\\
\textsf{\textbf{U}} =
\begin{pmatrix} \vert {\bf e}_1\rangle ,\vert {\bf e}_2 \rangle ,  \ldots , \vert {\bf e}_n\rangle \end{pmatrix}
,
\label{2016-m-fdvs-not}
\end{split}
\end{equation}
such that  $u_{ij} = e_{i,j}$ is the $j$th component of the $i$th vector,
Eq.~(\ref{2016-m-fdvs-rv0}) can be written
in ``Euclidean dot product notation,''
that is,
``column times row''
and
``row times column'' (the dot is usually omitted)
\begin{equation}
\begin{split}
\vert {\bf x} \rangle =
\begin{pmatrix} \vert {\bf e}_1\rangle ,\vert {\bf e}_2 \rangle ,  \ldots , \vert {\bf e}_n\rangle \end{pmatrix}
\begin{pmatrix} x_1\\x_2\\ \vdots \\ x_n \end{pmatrix} \equiv
\\
\equiv
\begin{pmatrix}
{\bf e}_{1,1} &   {\bf e}_{2,1}  &  \cdots &  {\bf e}_{n,1}\\
{\bf e}_{1,2} &   {\bf e}_{2,2}  &  \cdots &  {\bf e}_{n,2}\\
\cdots  & \cdots  &  \ddots &  \cdots \\
{\bf e}_{1,n} &   {\bf e}_{2,n}  &  \cdots &  {\bf e}_{n,n}\\
\end{pmatrix}
\begin{pmatrix} x_1\\x_2\\ \vdots \\ x_n \end{pmatrix}
\equiv
\textsf{\textbf{U}} X
 .
\label{2016-m-fdvs-rv}
\end{split}
\end{equation}
Of course, with the Cartesian standard basis (\ref{2016-m-fdvs-csb}), $\textsf{\textbf{U}} = {\Bbb I}_n$, but
(\ref{2016-m-fdvs-rv}) remains valid for general bases.


In (\ref{2016-m-fdvs-rv}) the identification of the tuple
$ X = \begin{pmatrix}
x_1, x_2, \ldots , x_n
\end{pmatrix}^\intercal
$
containing the vector components $x_i$
with the vector $\vert {\bf x} \rangle $
really means
``coded {\em with respect}, or {\em relative},  to the basis ${\mathfrak B}=\{\vert {\bf e}_1 \rangle ,\vert {\bf e}_2 \rangle , \ldots , \vert {\bf e}_n \rangle \}$.''
Thus in what follows, we shall often identify the column vector
$
\begin{pmatrix}
x_1, x_2, \ldots , x_n
\end{pmatrix}^\intercal
$
containing the coordinates of the vector
with the vector $\vert {\bf x} \rangle$, but we always need to keep in mind that
the tuples of coordinates are defined only with respect to a particular basis
$\{\vert {\bf e}_1 \rangle ,\vert {\bf e}_2 \rangle , \ldots , \vert {\bf e}_n \rangle \}$;
otherwise these numbers lack any meaning whatsoever.



\section{Dual space}
\label{2011-m-dvs}

Every vector space ${\frak V}$
has a corresponding {\em dual vector space}
\index{dual vector space}
\index{dual space}
(or just {\em dual space})  ${\frak V}^\ast$
consisting of all linear functionals on ${\frak V}$.

A {\em linear functional}
\index{linear functional}
on a vector space ${\frak V}$ is a scalar-valued linear function $\langle {\bf y} \vert$
defined for every vector   $\vert {\bf x} \rangle \in {\frak V}$, with the linear property that
\begin{equation}
\langle {\bf y} \vert (\alpha_1 \vert {\bf x}_1 \rangle +\alpha_2\vert  {\bf x}_2 \rangle)
=
\alpha_1 \langle {\bf y} \vert (\vert {\bf x}_1 \rangle) +\alpha_2 \langle {\bf y} \vert (\vert {\bf x}_2 \rangle) .
\end{equation}
The object $\langle {\bf y} \vert$ is called a {\em bra vector} in the dual space



The following very illustrative supermarket example has been
communicated to me by Hans Havlicek~\cite{havlicek-priv3}:
suppose you visit a supermarket, with a variety of products therein.
Suppose further that you select some items and collect them in a cart or trolley.
Suppose further that, in order to complete your purchase, you finally go to the cash desk,
where the sum total of your purchase is computed from the price-per-product information stored
in the memory of the cash register.

In this example, the vector space can be identified with all conceivable configurations of products in a cart or trolley.
Its dimension is determined by the number of different, mutually distinct products in the supermarket.
Its ``base vectors'' can be identified with the mutually distinct products in the supermarket.
The respective functional is the computation of the price of any such purchase.
It is based on a particular price information.
Every such price information contains one price per item for all mutually distinct products.
The dual space consists of all conceivable price informations.

Suppose
${\frak B} = \{\vert {\bf f}_1 \rangle ,\ldots , \vert {\bf f}_n \rangle \}$
is a basis of an $n$-dimensional vector space  ${\frak V}$.
A unique {\em dual basis}
\index{dual basis}
${\frak B}^\ast
=\{\langle {\bf f}_1 \vert  ,\ldots ,\langle {\bf f}_n\vert  \}$ of the dual vector space ${\frak V}^\ast $
can be defined by
\begin{equation}
\langle {\bf f}_j\vert  ({\bf f}_i) =  [\vert {\bf f}_i \rangle ,  \langle {\bf f}_j\vert  ]=\delta_{ij},
\label{2011-m-Dualbasis-e1}
\end{equation}
where  $\delta_{ij}$
is the Kronecker delta function.
The dual space  ${\frak V}^\ast $ spanned by the dual basis ${\frak B}^\ast $ is $n$-dimensional.


Rather than elaborating on more general cases involving nonorthonormal bases and noneuclidean inner products we shall
restrict our attention to Cartesian orthonormal bases of Euclidean vector spaces.
\index{Euclidean space}
In these cases
{\em Riesz representation theorem}
\index{Riesz representation theorem}
\index{Fr\'echet-Riesz representation theorem}
(sometimes also called the {\em Fr\'echet-Riesz theorem})
connects   functionals
with inner products in a straightforward manner:
To any linear functional $\langle {\bf y} \vert$
on a finite-dimensional inner product space ${\frak V}$
there corresponds a unique vector   $ \vert {\bf y} \rangle \in {\frak V}$,
such that
\begin{equation}
\langle {\bf y} \vert (\vert {\bf x} \rangle ) = \langle {\bf x}\vert {\bf y}\rangle
\label{2015-m-ch-fdlvs-e1}
\end{equation}
for all $\vert {\bf x} \rangle\in {\frak V}$.

Note that on the right hand side of~(\ref{2015-m-ch-fdlvs-e1})
there is a scalar product between two vectors $ \vert {\bf x}\rangle $ and $\vert {\bf y} \rangle$ in  ${\frak V}$,
whereas the dual vector $\langle {\bf y} \vert$ on the left hand side is the the dual space ${\frak V}^\ast$.




\section{Linear transformation}

\subsection{Definition}
A {\em linear transformation}, or, used synonymously, a {\em linear operator},
\index{linear transformation}
\index{linear operator}
$\textsf{\textbf{A}} $ on a vector space ${\frak V}$ is a correspondence that assigns every vector
$\vert {\bf x} \rangle \in {\frak V}$ a vector $\textsf{\textbf{A}}\vert {\bf x} \rangle \in {\frak V}$,
in a linear way; such  that
\begin{equation}
\textsf{\textbf{A}}  (\alpha\vert {\bf x} \rangle + \beta \vert {\bf y} \rangle ) = \alpha \textsf{\textbf{A}}(\vert {\bf x} \rangle )
+  \beta \textsf{\textbf{A}} (\vert {\bf y} \rangle ) = \alpha \textsf{\textbf{A}}\vert {\bf x} \rangle
+  \beta \textsf{\textbf{A}} \vert {\bf y} \rangle ,
\end{equation}
identically for all vectors $\vert {\bf x} \rangle ,\vert {\bf y} \rangle \in {\frak V}$ and all scalars $\alpha , \beta$.


\subsection{Operations and commutator}
The {\em sum}
\index{sum of transformations}
$\textsf{\textbf{S}} =\textsf{\textbf{A}} +\textsf{\textbf{B}} $
of two linear transformations $\textsf{\textbf{A}}$ and $\textsf{\textbf{B}} $
is defined by
$\textsf{\textbf{S}} \vert {\bf x} \rangle =\textsf{\textbf{A}}\vert {\bf x} \rangle  +\textsf{\textbf{B}} \vert {\bf x} \rangle $
for every $\vert {\bf x} \rangle \in {\frak V}$.

The {\em product}
\index{product of transformations}
$\textsf{\textbf{P}} =\textsf{\textbf{A}} \textsf{\textbf{B}} $
of two linear transformations $\textsf{\textbf{A}}$ and $\textsf{\textbf{B}} $
is defined by
$\textsf{\textbf{P}} \vert {\bf x} \rangle =\textsf{\textbf{A}}(\textsf{\textbf{B}} \vert {\bf x} \rangle )$
for every $\vert {\bf x} \rangle \in {\frak V}$.

The notation
$\textsf{\textbf{A}}^n\textsf{\textbf{A}}^m=\textsf{\textbf{A}}^{n+m}$
and $(\textsf{\textbf{A}}^n)^m= \textsf{\textbf{A}}^{nm}$,
with $\textsf{\textbf{A}}^1=\textsf{\textbf{A}}$ and
$\textsf{\textbf{A}}^0 =\textsf{\textbf{1}}$ turns out to be useful.

With the exception of commutativity, all formal algebraic properties
of numerical addition and multiplication,
are valid for transformations; that is
$
\textsf{\textbf{A}}\textsf{\textbf{0}}=
\textsf{\textbf{0}}\textsf{\textbf{A}} =\textsf{\textbf{0}}
$,
$
\textsf{\textbf{A}}\textsf{\textbf{1}}=
\textsf{\textbf{1}}\textsf{\textbf{A}} =\textsf{\textbf{A}}
$,
$
\textsf{\textbf{A}} (\textsf{\textbf{B}}+\textsf{\textbf{C}})=
\textsf{\textbf{A}} \textsf{\textbf{B}}
+
\textsf{\textbf{A}} \textsf{\textbf{C}}
$,
$
(\textsf{\textbf{A}}+ \textsf{\textbf{B}})\textsf{\textbf{C}}=
\textsf{\textbf{A}} \textsf{\textbf{C}}
+
\textsf{\textbf{B}} \textsf{\textbf{C}}
$,  and
$
\textsf{\textbf{A}} (\textsf{\textbf{B}}\textsf{\textbf{C}})=
(\textsf{\textbf{A}} \textsf{\textbf{B}})
 \textsf{\textbf{C}}
$.
In {\em matrix notation},  $\textsf{\textbf{1}} ={\Bbb{1}}$, and the entries of $\textsf{\textbf{0}}$
are $0$ everywhere.

The {\em inverse operator}
\index{inverse operator}
$\textsf{\textbf{A}}^{-1}$
of $\textsf{\textbf{A}}$
is defined by
$\textsf{\textbf{A}}\textsf{\textbf{A}}^{-1}=\textsf{\textbf{A}}^{-1}\textsf{\textbf{A}}=
{\Bbb I}$.


The {\em commutator}
\index{commutator}
of two matrices $\textsf{\textbf{A}}$  and $\textsf{\textbf{B}}$ is defined by
\begin{equation}
[\textsf{\textbf{A}}, \textsf{\textbf{B}} ]
=
\textsf{\textbf{A}} \textsf{\textbf{B}}
-
 \textsf{\textbf{B}}      \textsf{\textbf{A}}.
\end{equation}

The {\em polynomial}
\index{polynomial}
can be directly adopted from ordinary arithmetic; that is,
any finite polynomial $p$ of degree $n$
of an operator (transformation) $\textsf{\textbf{A}}$ can be written as
\begin{equation}
p(\textsf{\textbf{A}})= \alpha_0   \textsf{\textbf{1}}
+ \alpha_1   \textsf{\textbf{A}}^1
+ \alpha_2   \textsf{\textbf{A}}^2+
\cdots
+
\alpha_n   \textsf{\textbf{A}}^n
=\sum_{i=0}^n \alpha_i \textsf{\textbf{A}}^i
.
\end{equation}

More general functions of normal operators will be defined in Section~\ref{2016-pu-book-chapter-qm-fono},
page~\pageref{2016-pu-book-chapter-qm-fono}.


\subsection{Linear transformations as matrices}

\index{matrix}
\index{transformation matrix}


Let ${\frak V}$ be an $n$-dimensional vector space;
let
${\frak B}=\{\vert {\bf f}_1 \rangle ,\vert {\bf f}_2 \rangle,\ldots ,\vert {\bf f}_n \rangle\}$ be any basis of ${\frak V}$,
and let  $\textsf{\textbf{A}}$ be a linear transformation on ${\frak V}$.

Because every vector is a linear combination of the basis vectors
$\vert {\bf f}_i \rangle$,
every linear transformation can be defined by
``its performance on the basis vectors;'' that is,
by the particular mapping of
all $n$ basis vectors into the transformed vectors, which in turn can be represented as linear combination of the $n$ basis vectors.

Therefore it is possible to define some $n \times n$ matrix with $n^2$ coefficients or coordinates
$\alpha_{ij}$ such that~\cite[\S~37]{halmos-vs}
\begin{equation}
\textsf{\textbf{A}}\vert  {\bf f}_j \rangle = \sum_i \alpha_{ij}\vert {\bf f}_i  \rangle
\label{2015-m-ch-fdlvs-dtlt}
\end{equation}
for all $j=1,\ldots ,n$.
Again, note that this definition of a {\em transformation matrix}
is ``tied to'' a basis.

The ``reverse order'' of indices in (\ref{2015-m-ch-fdlvs-dtlt}) has been chosen
in order for the vector coordinates to transform in the ``right order:''
with~(\ref{2016-m-fdvs-rv0}) on page~\pageref{2016-m-fdvs-rv0},
\begin{equation}
\begin{split}
\textsf{\textbf{A}}\vert  {\bf x} \rangle =
\textsf{\textbf{A}} \sum_i x_i \vert  {\bf f}_i \rangle =
\sum_i \textsf{\textbf{A}} x_i \vert  {\bf f}_i \rangle =
\sum_i x_i \textsf{\textbf{A}} \vert  {\bf f}_i \rangle =
\sum_{i,j} x_i \alpha_{ji} \vert  {\bf f}_j \rangle =  \\
\sum_{i,j} \alpha_{ji}  x_i \vert  {\bf f}_j \rangle =
[i\leftrightarrow j] =
\sum_{i,j} \alpha_{ij}  x_j \vert  {\bf f}_i \rangle ,
\end{split}
\label{2016-m-ch-fdlvs-dtltk}
\end{equation}
and, due to the linear independence of the basis vectors and by comparison of the coefficients of $\vert  {\bf f}_i \rangle$,
with respect to  the basis
${\frak B}=\{\vert {\bf f}_1 \rangle ,\vert {\bf f}_2 \rangle,\ldots ,\vert {\bf f}_n \rangle\}$,
\begin{equation}
\textsf{\textbf{A}} x_i = \sum_{j} \alpha_{ij}  x_j  .
\label{2016-m-ch-fdlvs-dtltk1}
\end{equation}

For {\em orthonormal bases}
there is an even closer connection -- representable as scalar product -- between a matrix
defined by an $n$-by-$n$ square array and the representation in terms of the elements of the bases; that is,
 by inserting
two resolutions of the identity (see Section~\ref{2016-m-ch-fdvsrotio} on page~\pageref{2016-m-ch-fdvsrotio}) before and after the
linear transformation $\textsf{\textbf{A}}$,
\index{resolution of the identity}
\begin{equation}
\begin{split}
\textsf{\textbf{A}}  =
{\Bbb I}_n \textsf{\textbf{A}} {\Bbb I}_n  =
\sum_{i,j=1}^n
\vert {\bf f}_i\rangle \langle {\bf f}_i\vert A\vert {\bf f}_j\rangle \langle {\bf f}_j\vert  =
\sum_{i,j=1}^n \alpha_{ij}
\vert {\bf f}_i\rangle  \langle {\bf f}_j\vert,
\end{split}
\end{equation}
whereby
\begin{equation}
\begin{split}
 \langle {\bf f}_i \vert A \vert {\bf f}_j\rangle
= \langle {\bf f}_i \vert A   {\bf f}_j\rangle \\
= \langle {\bf f}_i \vert \sum_l \alpha_{lj}{\bf f}_l \rangle
=  \sum_l \alpha_{lj} \langle {\bf f}_i \vert {\bf f}_l \rangle
=  \sum_l \alpha_{lj} \delta_{il}  = \alpha_{ij}
.
\end{split}
\end{equation}






\section{Projection or projection operator}
\index{projection}

\subsection{Definition}
If ${\frak V}$ is the direct sum of some subspaces
${\frak M}$
and
${\frak N}$
so that every $\vert {\bf z} \rangle  \in {\frak V}$ can be uniquely written in the form
$
\vert {\bf z} \rangle
=
\vert {\bf x} \rangle
+
\vert {\bf y} \rangle
$, with
$\vert {\bf x} \rangle  \in {\frak M}$
and with
$\vert {\bf x} \rangle  \in {\frak N}$,
then
the {\em projection}, or, used synonymously,
{\em projection}
on ${\frak M}$
along ${\frak N}$ is the transformation $\textsf{\textbf{E}}$
defined by $\textsf{\textbf{E}}\vert {\bf z} \rangle =\vert {\bf x} \rangle $.
Conversely,
 $\textsf{\textbf{F}}\vert {\bf z} \rangle =\vert {\bf y} \rangle $  is the projection
on ${\frak N}$
along ${\frak M}$.

A (nonzero) linear transformation
$\textsf{\textbf{E}}$ is a projection if and only if
it is idempotent~\cite[\S~41]{halmos-vs} ; that is,
\index{idempotence}
$\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}\neq 0$.






\subsection{Construction of projections from unit vectors}

How can we construct projections from unit vectors, or systems of orthogonal projections from some vector in some orthonormal basis
with the standard dot product?

Let $\vert {\bf x} \rangle $ be the coordinates of a unit vector;
that is $\| \vert {\bf x} \rangle  \| =1$.

In complex vector space the {\em conjugate transpose} (also denoted as
{\em Hermitian conjugate} or {\em Hermitian adjoint}),
\index{conjugate transpose}
\index{Hermitian conjugate}
\index{Hermitian adjoint}
``$\dagger$,'' standing for transposition and complex conjugation of the (vector) coordinates, will be used.
More explicitly,
\begin{equation}
\begin{split}
\begin{pmatrix}x_1,\ldots, x_n\end{pmatrix}^\dagger =
%\left(
\begin{pmatrix}
\overline{x_1}\\ \vdots\\ \overline{x_n}
\end{pmatrix}
,  \textrm{ and }
%\left(
\begin{pmatrix}
x_1\\ \vdots\\ x_n
\end{pmatrix}^\dagger
= (\overline{x_1},\ldots, \overline{x_n})
.
\end{split}
\end{equation}
Note that, just as for real vector spaces,

$\left(\vert {\bf x}\rangle^\intercal \right)^\intercal =\vert {\bf x}\rangle$,
 so is
$\left(\vert  {\bf x}\rangle ^\dagger \right)^\dagger= \vert{\bf x} \rangle$ for complex vector spaces.

For orthonormal bases of complex Hilbert space
we can express the dual vector in terms of the original vector by
taking the conjugate transpose, and {\em vice versa}; that is,
\begin{equation}
\begin{split}
\langle {\bf x} \vert = \left(\vert {\bf x}\rangle \right)^\dagger,
\textrm{ and }
\vert {\bf x}\rangle  = \left(\langle {\bf x}\vert\right)^\dagger.
\end{split}
\end{equation}

In real vector space, the {\em dyadic product}, or {\em tensor product}, or {\em outer product}
\index{outer product}
\index{dyadic product}
\index{tensor product}
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_{\bf x} =   \vert{\bf x}\rangle \langle {\bf x}\vert
\equiv
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
\begin{pmatrix}x_1,x_2,\ldots ,x_n\end{pmatrix}\\
\\
=
\begin{pmatrix}
x_1\begin{pmatrix}x_1,x_2,\ldots ,x_n\end{pmatrix}\\
x_2\begin{pmatrix}x_1,x_2,\ldots ,x_n\end{pmatrix}\\
\vdots  \\
x_n\begin{pmatrix}x_1,x_2,\ldots ,x_n \end{pmatrix}
\end{pmatrix}
=
\begin{pmatrix}
x_1x_1&x_1x_2& \cdots&x_1x_n\\
x_2x_1&x_2x_2& \cdots&x_2x_n\\
\vdots & \vdots & \vdots &\vdots \\
x_nx_1&x_nx_2& \cdots&x_nx_n
\end{pmatrix}
\end{split}
\label{2016-pu-book-chapter-qm.tex-dp}
\end{equation}
is the projection
associated with ${\bf x}$.

If the vector ${\bf x}$ is not normalized,
then the associated projection is
\begin{equation}
\textsf{\textbf{E}}_{\bf x} = \frac{\vert{\bf x}\rangle \langle {\bf x}\vert}{\langle {\bf x}\vert {\bf x}\rangle}
\end{equation}



In complex vector space, transposition has to be substituted by the conjugate transposition;
\index{conjugate transpose}
\index{Hermitian conjugate}
\index{Hermitian adjoint}
that is
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_{\bf x} = \vert{\bf x}\rangle \langle {\bf x}\vert
\end{split}
\end{equation}




Note also that
\begin{equation}
\textsf{\textbf{E}}_{\bf x} \vert {\bf y}\rangle
\equiv
\textsf{\textbf{E}}_{\bf x} {\bf y}
=
\langle {\bf x}\vert  {\bf y} \rangle {\bf x},
\equiv
\langle {\bf x}\vert  {\bf y} \rangle \vert {\bf x}\rangle ,
\end{equation}
which can be directly proven by insertion.


%\subsection{Combination of projections}





\section{Change of basis}
\label{2012-m-ch-fdlvs-changeofbasis}
\index{change of basis}
\index{basis change}
\index{change of basis}

Let ${\frak V}$ be an $n$-dimensional vector space and let
${\frak X}
=
\{
\vert {\bf e}_1 \rangle ,
\ldots ,
\vert {\bf e}_n \rangle
\}$
and
${\frak Y}
=  \{
\vert {\bf f}_1 \rangle ,
\ldots ,
\vert {\bf f}_n \rangle
\}$ be two bases of ${\frak V}$.

Take an arbitrary vector $\vert {\bf z} \rangle \in {\frak V}$.
In terms of the two bases
${\frak X}$ and
${\frak Y}$,
${\bf z}$ can be written as
\begin{equation}
\vert {\bf z}  \rangle =
\sum_{i=1}^n x_i\vert {\bf e}_i \rangle
=
\sum_{i=1}^n  y_i\vert {\bf f}_i \rangle ,
\label{2011-m-btbexy}
\end{equation}
where $x_i$ and $y_i$ stand for the coordinates of the vector  $\vert {\bf z}  \rangle $
with respect to the bases ${\frak X}$ and
${\frak Y}$,
respectively.

The following questions arise:
(i)
What is the relation between the ``corresponding'' basis vectors $\vert {\bf e}_i \rangle $ and $\vert {\bf f}_j \rangle $?
(ii)
What is the relation between the coordinates $x_i$ (with respect to the basis  ${\frak X}$) and $y_j$ (with respect to the basis  ${\frak Y}$)
of the vector $\vert {\bf z}  \rangle $ in Eq.~(\ref{2011-m-btbexy})?


\subsection{Settlement of change of basis vectors by definition}

As an {\it Ansatz} for answering question (i), recall that, just like any other vector in ${\frak V}$,
the new basis vectors ${\bf f}_i$ contained in the new basis ${\frak Y}$
can be (uniquely) written as a {\em linear combination}
(in quantum physics called {\em coherent superposition})
\index{coherent superposition}
\index{superposition}
\index{linear combination}
of the basis vectors
${\bf e}_i$ contained in the old  basis ${\frak X}$.
This can be defined {\it via}
a linear transformation $\textsf{\textbf{A}}$ between the corresponding vectors of the bases
 ${\frak X}$ and
${\frak Y}$ by~\cite[\S~46]{halmos-vs}
\begin{equation}
\vert {\bf f}_i \rangle = \vert ({\bf e}\textsf{\textbf{A}})_i \rangle
,
\label{2011-m-btbe}
\end{equation}
for all $i=1, \ldots , n$.
%
More specifically, let ${a}_{ji}$ be the matrix of the linear transformation $\textsf{\textbf{A}}$
in the basis
${\frak X}
=
\{
\vert {\bf e}_1 \rangle ,
\ldots ,
\vert {\bf e}_n \rangle
\}$,
and let us rewrite (\ref{2011-m-btbe}) as a matrix equation
\begin{equation}
\vert {\bf f}_i\rangle = \sum_{j=1}^n a_{ji}\vert  {\bf e}_j \rangle  = \sum_{j=1}^n (a^\intercal )_{ij} \vert {\bf e}_j   \rangle
.
\label{2011-m-btbe-r}
\end{equation}


This {\it Ansatz} includes a convention; namely  the {\em order of the indices} of the transformation matrix.
Why have we taken the inconvenience of defining
$\vert {\bf f}_i\rangle $ by $\sum_{j=1}^n a_{ji} \vert {\bf e}_j\rangle $ rather than by $\sum_{j=1}^n a_{ij}\vert  {\bf e}_j\rangle $?
That is, in  Eq.~(\ref{2011-m-btbe-r}), why not exchange $a_{ji}$ by $a_{ij}$,
so that the summation index $j$ is ``next to'' $\vert {\bf e}_j$?

This is because, just as for the case of linear transformations in general discussed earlier,
we want to transform the coordinates according to this ``more intuitive'' rule, and we cannot have both
at the same time.
More explicitly, suppose that we want to have
\begin{equation}
y_i =\sum_{j=1}^n b_{ij} x_j
,
\label{2016-m-btbe-2-implied}
\end{equation}
or, in operator notation and the coordinates as $n$-tuples,
\begin{equation}
y = \textsf{\textbf{B}} x
.
\label{2016-m-btbe-2-impliedm}
\end{equation}
Then, by insertion of  Eqs.~(\ref{2011-m-btbe-r}) and (\ref{2016-m-btbe-2-implied}) into (\ref{2011-m-btbexy})
we obtain
\begin{equation}
\vert {\bf z} \rangle =
\sum_{i=1}^n x_i \vert  {\bf e}_i  \rangle
=
\sum_{i=1}^n  y_i \vert {\bf f}_i \rangle
= \sum_{i=1}^n \left( \sum_{j=1}^n b_{ij} x_j \right)   \left( \sum_{k=1}^n a_{ki} \vert {\bf e}_k \rangle \right)
= \sum_{i,j,k=1}^n  a_{ki} b_{ij}  x_j \vert {\bf e}_k  \rangle
,
\label{2016-m-btbexy}
\end{equation}
which, by comparison, can only be satisfied if  $\sum_{i=1}^n  a_{ki} b_{ij} = \delta_{kj}$.
Therefore, $\textsf{\textbf{A}}\textsf{\textbf{B}} = {\Bbb I}_n$ and
$\textsf{\textbf{B}}$   is the inverse of
$\textsf{\textbf{A}}$.
This is quite plausible, since any scale basis change needs to be compensated by a reciprokal or inversely proportional
scale change of the coordinates.




Note that, if  ${\frak X}$ is an orthonormal basis,
then the basis transformation has a diagonal form
\begin{equation}
\sum_{i=1}^n \vert {\bf f}_i \rangle \langle {\bf e}_i \vert
\label{2013-m-ch-fdvs-dftm}
\end{equation}
because all off-diagonal components $a_{ij}$, $i\neq j$ of $\textsf{\textbf{A}}$
vanish.
This can be easily checked by applying $\textsf{\textbf{A}}$ to the elements ${\bf e}_i $ of the basis ${\frak X}$.
See also Section
\ref{2012-m-ch-citoob} on page \pageref{2012-m-ch-citoob}
for a representation of unitary transformations in terms of basis changes.
In quantum mechanics, the temporal evolution is represented by nothing but a change of orthonormal bases in Hilbert space.



\subsection{Scale change of vector components by contra-variation}



Having settled question (i) by the {\it Ansatz}
(\ref{2011-m-btbe}),
we turn to question (ii) next.
Since
\begin{equation}
\vert {\bf z} \rangle  =
 \sum_{j=1}^n y_j  \vert {\bf f}_j \rangle =
 \sum_{j=1}^n  y_j\vert  ({\bf e}\textsf{\textbf{A}})_j \rangle =
 \sum_{j=1}^n  y_j  \sum_{i=1}^n a_{ij} \vert {\bf e}_i \rangle =
  \sum_{i=1}^n \left(\sum_{j=1}^n  a_{ij} y^j \right)  \vert  {\bf e}_i \rangle ;
\end{equation}
we obtain by comparison of the coefficients in Eq. (\ref{2011-m-btbexy}),
\begin{equation}
x_i= \sum_{j=1}^n a_{ij} y_j.
\label{2012-m-ch-e-tl1}
\end{equation}
That is, in terms of the ``old'' coordinates $x^i$,
the ``new'' coordinates are
\begin{equation}
\begin{split}
\sum_{i=1}^n (a^{-1})_{ji} x_i= \sum_{i=1}^n (a^{-1})_{ji}  \sum_{k=1}^n  a_{ik} y_k \\
=  \sum_{k=1}^n \left[ \sum_{i=1}^n (a^{-1})_{ji}  a_{ik} \right] y_k
=   \sum_{k=1}^n \delta^j_k y_k
=  y_j
.
\label{2012-m-ch-e-tl2}
\end{split}
\end{equation}

If we prefer to represent the vector coordinates of
$\vert {\bf x}\rangle $ and $\vert {\bf y}\rangle $ as $n$-tuples,
then Eqs.~(\ref{2012-m-ch-e-tl1})  and (\ref{2012-m-ch-e-tl2})
have an interpretation as matrix multiplication; that is,
\begin{equation}
\vert {\bf x}\rangle  =  \textsf{\textbf{A}}\vert {\bf y}\rangle , {\text{ and }}
\vert {\bf y}\rangle  =  (\textsf{\textbf{A}}^{-1})\vert {\bf x} \rangle
.
\label{2012-m-ch-e-tl3}
\end{equation}



\section{Mutually unbiased bases}
\index{mutually unbiased bases}

Two  orthonormal bases
${\frak B} =\{
\vert {\bf e}_1 \rangle ,
\ldots ,
\vert {\bf e}_n \rangle
\}$
and
${\frak B}'=\{
\vert {\bf f}_1 \rangle ,
\ldots ,
\vert {\bf f}_n  \rangle
\}$
are said to be {\em mutually unbiased}
if
their scalar or inner products are
\begin{equation}
\vert \langle {\bf e}_i\vert {\bf f}_j  \rangle \vert^2
=
\frac{1}{n}
\end{equation}
for all $1\le i,j\le n$.
Note without proof -- that is, you do not have to be concerned
that you need to understand  this from what has been said so far --
that ``the elements of two or more mutually unbiased bases are mutually maximally apart.''





Schwinger presented an algorithm (see \cite{Schwinger.60} for a proof)
to construct a new mutually unbiased basis ${\frak B}$   from an existing orthogonal one.
The proof idea
is to create a new basis ``in-between'' the old basis vectors.
by the following construction steps:
\begin{itemize}
\item[(i)]
take the existing orthogonal basis and permute all of its elements by ``shift-permuting'' its elements; that is, by
changing
the basis vectors according to their enumeration $i \rightarrow i+1$ for $i=1,\ldots , n-1$, and $n \rightarrow 1$;
or any other nontrivial (i.e., do not consider identity for any basis element) permutation;
\item[(ii)]
consider the {\em (unitary) transformation} (cf. Sections \ref{2012-m-ch-fdlvs-changeofbasis} and \ref{2012-m-ch-citoob})
corresponding to the basis change from the old basis to the new, ``permutated'' basis;
\item[(iii)]
finally, consider the (orthonormal) {\em eigenvectors} \index{eigenvector}
of this (unitary; cf. page
\pageref{2014-m-ch-fdvs-unitary}) transformation associated with the basis change.
These eigenvectors are the elements of a new bases  ${\frak B}'$.
Together with ${\frak B}$ these two bases
-- that is, ${\frak B}$ and ${\frak B}'$ --  are mutually unbiased.
\end{itemize}


So far, nobody  has discovered a systematic way to derive and construct a {\em complete} or {\em maximal}
set of mutually unbiased bases in arbitrary dimensions; in particular,
{\em how many} bases are there in such sets.


\section{Completeness or resolution of the identity operator in terms of base vectors}
\label{2016-m-ch-fdvsrotio}
\index{resolution of the identity}
\index{completeness}

The identity ${\Bbb I}_n$ in an $n$-dimensional vector space ${\frak V}$ can be represented in terms of the sum
over all outer (by another naming tensor or dyadic) products
of all vectors of an arbitrary orthonormal basis
\index{outer product}
\index{dyadic product}
\index{tensor product}
${\frak B} =
\{
\vert {\bf e}_1 \rangle ,
\ldots ,
\vert{\bf e}_n \rangle
\}
$; that is,
\begin{equation}
 {\Bbb I}_n = \sum_{i=1}^n \vert {\bf e}_i \rangle \langle {\bf e}_i \vert
 .
\label{2016-m-ch-fdlws-roi}
\end{equation}
This is sometimes also referred to as {\em completeness}.
\index{resolution of the identity}
\index{completeness}

{%%\color{OliveGreen}
 % %\bproof

For a proof, consider an arbitrary vector $\vert {\bf x} \rangle  \in {\frak V}$.
Then,
\begin{equation}
 {\Bbb I}_n \vert {\bf x} \rangle
 =
\left(\sum_{i=1}^n \vert {\bf e}_i \rangle \langle {\bf e}_i \vert \right)
\vert {\bf x} \rangle
=
\sum_{i=1}^n \vert {\bf e}_i \rangle \langle {\bf e}_i \vert {\bf x} \rangle
=
\sum_{i=1}^n \vert {\bf e}_i \rangle  x_i
=  \vert {\bf x} \rangle
.
\end{equation}
 % %\eproof
}


{%\color{blue}
 % %\bexample
Consider, for example, the basis
${\frak B}=\{ \vert {\bf e}_1 \rangle , \vert {\bf e}_2 \rangle \} \equiv \{(1,0)^\intercal ,(0,1)^\intercal \}$.
Then the two-dimensional resolution of the identity operator ${\Bbb I}_2$
can be written as
\begin{equation}
\begin{split}
{\Bbb I}_2 =   \vert {\bf e}_1 \rangle \langle  {\bf e}_1 \vert   +     \vert {\bf e}_2 \rangle  \langle  {\bf e}_2 \vert \\
=   (1,0)^\intercal  (1,0) +   (0,1)^\intercal  (0,1)
=
\begin{pmatrix}
1 (1,0) \\  0 (1,0)
\end{pmatrix}
 +
\begin{pmatrix}
0 (0,1)\\
1 (0,1)
\end{pmatrix} \\
 =
\begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix}
+
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
.
\end{split}
\end{equation}

Consider, for another example, the basis
${\frak B}' \equiv \{\frac{1}{\sqrt{2}}(-1,1)^\intercal ,\frac{1}{\sqrt{2}}(1,1)^\intercal \}$.
Then the two-dimensional resolution of the identity operator ${\Bbb I}_2$
can be written as
\begin{equation}
\begin{split}
{\Bbb I}_2 =  \frac{1}{\sqrt{2}}   (-1,1)^\intercal   \frac{1}{\sqrt{2}}(-1,1) +   \frac{1}{\sqrt{2}}(1,1)^\intercal  \frac{1}{\sqrt{2}}(1,1)
\\
=
\frac{1}{{2}}
\begin{pmatrix}
-1 (-1,1) \\  1 (-1,1)
\end{pmatrix}
 +
\frac{1}{{2}}
\begin{pmatrix}
1 (1,1)\\
1 (1,1)
\end{pmatrix}
 =
\frac{1}{{2}}
\begin{pmatrix}
1 & -1 \\
-1 & 1
\end{pmatrix}
+
\frac{1}{{2}}
\begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
.
\end{split}
\end{equation}


 % %\eexample
}



\section{Determinant}
\index{determinant}


In what follows, the {\em determinant} of a matrix $A$ will be denoted by $\textrm{det} A$ or,
equivalently, by $\vert A \vert$.

Suppose $A=a_{ij}$ is the  $n$-by-$n$ square matrix representation of
a linear transformation $\textsf{\textbf{A}}$
in an $n$-dimensional vector space ${\frak V}$.
We shall define its {\em determinant}
in two equivalent ways.


The
{\em Leibniz formula}
\index{Leibniz formula} defines the determinant of the $n$-by-$n$ square matrix  $A=a_{ij}$ by
\begin{equation}
\textrm{det}A
=\sum_{\sigma \in S_n} \textrm{sgn}(\sigma) \prod_{i=1}^n a_{\sigma(i),j} ,
\end{equation}
where ``sgn'' represents the {\em sign function}
\index{sign function}
of permutations $\sigma$ in the permutation group $S_n$
on $n$ elements $\{1,2, \ldots , n\}$,
which returns $-1$ and $+1$ for odd and even permutations,
respectively.
$\sigma (i)$ stands for the element in position $i$ of $\{1,2, \ldots , n\}$ {\em after} permutation $\sigma$.

An equivalent (no proof is given here) definition
\begin{equation}
\textrm{det}A
=\varepsilon_{i_1 i_2\cdots i_n} a_{1i_1}a_{2i_2} \cdots a_{ni_n},
\end{equation}
makes use of the  totally antisymmetric Levi-Civita symbol $\varepsilon$,
\index{Levi-Civita symbol}
\index{antisymmetric tensor} and  of the
 Einstein summation convention.
\index{Einstein summation convention}


The second,
{\em Laplace formula}
\index{Laplace formula}
definition of the determinant
is recursive and expands the determinant in cofactors.
It is also called
{\em Laplace expansion},
\index{Laplace expansion}
or
{\em cofactor expansion}
\index{cofactor expansion}.
First,
a {\em minor}
\index{minor}
$M_{ij}$ of an  $n$-by-$n$ square matrix  $A$ is
defined to be the determinant of the
$(n-1)\times (n-1)$ submatrix
that remains after the entire $i$th row and $j$th column have been deleted from $A$.

A {\em cofactor}
\index{cofactor}
$A_{ij}$
of an $n$-by-$n$ square matrix  $A$
is defined in terms of its associated minor by
\begin{equation}
A_{ij}=(-1)^{i+j}M_{ij}.
\end{equation}

The {\em determinant} of a square matrix $A$, denoted by
$\textrm{det} A$ or $\vert A\vert$, is a scalar rekursively defined by
\begin{equation}
\textrm{det}A
=\sum_{j=1}^n a_{ij}A_{ij}
=\sum_{i=1}^n a_{ij}A_{ij}
\end{equation}
for any $i$ (row expansion) or $j$ (column expansion), with $i,j=1,\ldots ,n$.
For $1\times 1$ matrices (i.e., scalars), $\textrm{det}A =a_{11}$.


The determinant $\textrm{det}A $ of a matrix $A$ is nonzero if and only if $A$ is invertible.
In particular, if $A$ is not invertible, $\textrm{det}A =0$.
If $A$ has an inverse matrix $A^{-1}$, then $\textrm{det}(A^{-1}) = (\textrm{det}A)^{-1} $.




\section{Trace}
\label{2013-ch-fdvs-trace}
\index{trace}

\subsection{Definition}
The {\em trace} of an $n$-by-$n$ square matrix $A=a_{ij}$, denoted by
$\textrm{Tr} A$,  is a scalar
defined to be the sum of the elements on the main diagonal
 (the diagonal from the upper left to the lower right) of A; that is  (also in Dirac's bra and ket notation),
\begin{equation}
\textrm{Tr}\,A
= a_{11} +a_{22}+ \cdots +a_{nn}
=\sum_{i=1}^n a_{ii}=\sum_{i=1}^n \langle i \vert A\vert i \rangle.
\end{equation}

Traces are noninvertible (irreversible) almost by definition: for $n\ge 2$ and for arbitrary values $a_{ii} \in {\Bbb R}, {\Bbb C}$, there are
``many''  ways to obtain the same value of $ \sum_{i=1}^n a_{ii} $.

Traces are linear functionals, because, for two arbitrary matrices $A,B$
and two arbitrary scalars $\alpha, \beta$,
\begin{equation}
\textrm{Tr}\,(\alpha A + \beta B)
=\sum_{i=1}^n (\alpha a_{ii} + \beta b_{ii})
= \alpha \sum_{i=1}^n a_{ii} + \beta \sum_{i=1}^n  b_{ii}
=
\alpha \textrm{Tr}\,(A)+ \beta \textrm{Tr}\,(B)
.
\end{equation}

Traces can be realized {\it via} some arbitrary orthonormal basis ${\frak B} =\{
 \langle {\bf e}_1\rangle ,
\ldots ,
 \langle {\bf e}_n\rangle
\}$
by ``sandwiching'' an operator $\textsf{\textbf{A}}$ between all basis elements -- thereby effectively taking the diagonal components
of    $\textsf{\textbf{A}}$ with respect to the basis ${\frak B}$ --
and summing over all these scalar components; that is, with definition (\ref{2015-m-ch-fdlvs-dtlt}),
\begin{equation}
\begin{split}
\textrm{Tr}\;\textsf{\textbf{A}}
=\sum_{i=1}^n   \langle {\bf e}_i \vert \textsf{\textbf{A}} \vert {\bf e}_i \rangle
=\sum_{i=1}^n   \langle {\bf e}_i \vert \textsf{\textbf{A}}  {\bf e}_i \rangle  \\
=\sum_{i=1}^n  \sum_{l=1}^n  \langle {\bf e}_i \vert \alpha_{li}  {\bf e}_l \rangle
=\sum_{i=1}^n  \sum_{l=1}^n  \alpha_{li} \langle {\bf e}_i \vert  {\bf e}_l \rangle \\
=\sum_{i=1}^n  \sum_{l=1}^n  \alpha_{li} \delta_{il}
=\sum_{i=1}^n   \alpha_{ii}
.
\end{split}
\end{equation}
This representation is particularly useful in quantum mechanics.


Suppose an operator is defined {\em via} two arbitrary vectors
$\vert {\bf u} \rangle$
and
$\vert {\bf v} \rangle$
by
$\textsf{\textbf{A}} =  \vert {\bf u} \rangle \langle {\bf v} \rangle
$.
Then its trace can be rewritten as the scalar product of the two vectors (in exchanged order); that is,
for  some arbitrary orthonormal basis ${\frak B} =\{
 \langle {\bf e}_1\rangle ,
\ldots ,
 \langle {\bf e}_n\rangle
\}$
\begin{equation}
\begin{split}
\textrm{Tr}\;\textsf{\textbf{A}}
=\sum_{i=1}^n   \langle {\bf e}_i \vert \textsf{\textbf{A}} \vert {\bf e}_i \rangle
=\sum_{i=1}^n   \langle {\bf e}_i \vert {\bf u} \rangle \langle {\bf v} \vert   {\bf e}_i \rangle  \\
=\sum_{i=1}^n   \langle {\bf v} \vert   {\bf e}_i \rangle \langle {\bf e}_i \vert {\bf u} \rangle
=   \langle {\bf v} \vert   {\Bbb I}_n \vert {\bf u} \rangle
=   \langle {\bf v} \vert  {\bf u} \rangle
.
\end{split}
\end{equation}

In general traces represent noninvertible (irreversible) many-to-one functionals, since the same trace value can be obtained from different inputs.
More explicitly, consider two nonidentical vectors  $\vert {\bf u} \rangle \neq \vert {\bf v} \rangle$ in real Hilbert space.
In this case,
\begin{equation}
\textrm{Tr}\;\textsf{\textbf{A}} =
\textrm{Tr}\;\vert {\bf u} \rangle \langle {\bf v} \rangle =
\langle {\bf v} \vert  {\bf u} \rangle
=
\langle {\bf u} \vert  {\bf v} \rangle =
\textrm{Tr}\;\vert {\bf v} \rangle \langle {\bf u} \rangle =
\textrm{Tr}\;\textsf{\textbf{A}}^\intercal
\end{equation}
This example shows that the traces of two matrices such as $\textrm{Tr}\;\textsf{\textbf{A}}$ and $\textrm{Tr}\;\textsf{\textbf{A}}^\intercal  $ can be identical although
the argument matrices  $\textsf{\textbf{A}}=\vert {\bf u} \rangle \langle {\bf v} \rangle
$ and $\textsf{\textbf{A}}^\intercal   = \vert {\bf v} \rangle \langle {\bf u} \rangle$ need not be.




A {\em trace class} operator is a compact operator for which a trace is finite and independent of the choice of basis.
\index{trace class}

\subsection{Partial trace}
\index{partial trace}
\label{2015-partialtrace}

The quantum mechanics of multi-particle (multipartite) systems allows for configurations -- actually rather processes --
that can be informally described as ``beam dump experiments;'' in which we start out with entangled states
 which carry information
about {\em joint properties of the constituent quanta}
and {\em choose to disregard} one quantum state entirely; that is, we pretend
not to care of the (possible) outcomes of a measurement on this particle.
In this case, we have to {\em trace out} that particle; and as a result we obtain a {\em reduced state} without this particle we do
not care about.

Formally the partial trace with respect to the first particle
maps the general density matrix
${ \rho}_{12} = \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}   \vert i_1\rangle \langle j_1\vert \otimes \vert i_2\rangle \langle j_2 \vert$
on a composite Hilbert space
$
{\frak H}_1
\otimes
{\frak H}_2
$
to  a density matrix on the Hilbert space
${\frak H}_2
$ of the second particle by
\begin{equation}
\begin{split}
\textrm{Tr}_1
{ \rho}_{12}
= \textrm{Tr}_1  \left(   \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}   \vert i_1\rangle \langle j_1\vert \otimes \vert i_2 \rangle \langle j_2 \vert \right)
=\\
= \sum_{k_1}   \left\langle {\bf e}_{k_1}  \left\vert \left(   \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}
 \vert i_1\rangle \langle j_1\vert \otimes \vert i_2 \rangle \langle j_2 \vert  \right)  \right\vert {\bf e}_{k_1} \right\rangle
=\\
=  \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}  \left( \sum_{k_1}
 \langle {\bf e}_{k_1}  \vert i_1\rangle \langle j_1\vert    {\bf e}_{k_1}  \rangle  \right)  \vert i_2 \rangle \langle j_2 \vert
=\\
=  \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}  \left( \sum_{k_1}
 \langle j_1\vert    {\bf e}_{k_1}  \rangle  \langle {\bf e}_{k_1}  \vert i_1\rangle \right)  \vert i_2 \rangle \langle j_2 \vert
=\\
=  \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}
 \langle j_1\vert   \; {\Bbb I} \; \vert i_1\rangle   \vert i_2 \rangle \langle j_2 \vert
=  \sum_{i_1 j_1 i_2 j_2} \rho_{i_1 j_1 i_2 j_2}
 \langle j_1 \vert  i_1\rangle    \vert i_2 \rangle \langle j_2 \vert
.
\end{split}
\label{2016-m-ch-fdvs-pt}
\end{equation}
Suppose further that the vectors
$\vert i_1 \rangle$
and
$\vert j_1 \rangle$
by   associated with the first particle
belong to an orthonormal basis.
Then $\langle j_1 \vert  i_1\rangle =\delta_{i_1 j_1}$ and~(\ref{2016-m-ch-fdvs-pt})
reduces to
\begin{equation}
\begin{split}
\textrm{Tr}_1
{ \rho}_{12}
=  \sum_{i_1 i_2 j_2} \rho_{i_1 i_1 i_2 j_2}
  \vert i_2 \rangle \langle j_2 \vert
.
\end{split}
\label{2016-m-ch-fdvs-pt2}
\end{equation}

The partial trace in general corresponds to a noninvertible map corresponding to an irreversible process; that is,
it is an $m$-to-$n$ with $m>n$, or a many-to-one mapping:
for instance, in two-dimensional Hilbert space, both cases
$\rho_{1 1 i_2 j_2} = 1$, $\rho_{1 2 i_2 j_2} =   \rho_{2 1 i_2 j_2} =  \rho_{2 2 i_2 j_2} = 0$
and
$\rho_{2 2 i_2 j_2} = 1$, $\rho_{1 2 i_2 j_2} =   \rho_{2 1 i_2 j_2} =  \rho_{1 1 i_2 j_2} = 0$
are mapped into the same  $ \sum_{i_1} \rho_{i_1 i_1 i_2 j_2}$.
This can be expected, as information about the first particle is ``erased.''

{%\color{blue}
 % %\bexample
\label{bellstate}
For an explicit example's sake, consider the Bell state  \index{Bell state}
$\vert \Psi^- \rangle= \frac{1}{\sqrt{2}}\left(\vert 0 \rangle \vert 1 \rangle - \vert 1 \rangle \vert 0 \rangle  \right)$.
Suppose we do not care about the state of the first particle, then we may ask what kind of reduced state results from this
pretension.
Then the partial trace is just the trace over the first particle; that is, with subscripts referring to the particle number,
\begin{equation}
\begin{split}
\textrm{Tr}_1\, \vert \Psi^- \rangle \langle  \Psi^-   \vert  \\
=\sum_{i_1=0}^1 \langle i_1 \vert \Psi^- \rangle \langle  \Psi^-  \vert i_1 \rangle \\
=\langle 0_1 \vert \Psi^- \rangle \langle  \Psi^-  \vert 0_1 \rangle
+
\langle 1_1  \vert \Psi^- \rangle \langle  \Psi^-  \vert 1_1 \rangle  \\
=\langle 0_1 \vert  \frac{1}{\sqrt{2}}\left(\vert 0_1   1_2 \rangle - \vert 1_1   0_2 \rangle  \right)  \frac{1}{\sqrt{2}}\left(\langle 0_1   1_2 \vert  - \langle 1_1   0_2 \vert   \right)  \vert 0_1 \rangle\\
\qquad
+
\langle 1_1 \vert  \frac{1}{\sqrt{2}}\left(\vert 0_1   1_2 \rangle - \vert 1_1   0_2 \rangle  \right)  \frac{1}{\sqrt{2}}\left(\langle 0_1   1_2 \vert  - \langle 1_1   0_2 \vert   \right)  \vert 1_1 \rangle  \\
= \frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
.
\end{split}
\end{equation}

The resulting state is a
{\em mixed state}
\index{mixed state}
defined by the property that its trace is equal to one,
but the trace of its square is smaller than one; in this case the trace is $\frac{1}{2}$, because
\begin{equation}
\begin{split}
\textrm{Tr}_2\,
\frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right) \\
= \frac{1}{2}  \langle 0_2 \vert
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\vert 0_2 \rangle
+  \frac{1}{2}
\langle 1_2  \vert
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\vert 1_2 \rangle  \\
=  \frac{1}{2} + \frac{1}{2} =1;
\end{split}
\end{equation}
but
\begin{equation}
\begin{split}
\textrm{Tr}_2\,
\left[
\frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\right]
\\ =
\textrm{Tr}_2\,
\frac{1}{4}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
= \frac{1}{2}.
\end{split}
\end{equation}
This {\em mixed state} is a 50:50 mixture of the pure particle states  $\vert 0_2 \rangle$ and $\vert 1_2 \rangle$, respectively.
Note that this is different from
a coherent superposition
\index{coherent superposition}
\index{superposition}
$\vert 0_2 \rangle + \vert 1_2 \rangle$
 of the pure particle states  $\vert 0_2 \rangle$ and $\vert 1_2 \rangle$, respectively --
also formalizing a 50:50 mixture with respect to measurements of property $0$ {\it versus} $1$, respectively.

 % %\eexample
}


\subsection{Purification}
\index{purification}
\label{2015-m-ch-fdvs-purification}


In general, quantum states ${\boldsymbol{\rho}}$ satisfy two criteria \cite{ba-89}:  they are
(i) of trace class one:
$\textrm{Tr}({\boldsymbol{\rho}}) =1$;
and
(ii) positive (or, by another term nonnegative):
$
\langle {\bf x} \vert {\boldsymbol{\rho}} \vert {\bf x} \rangle
=\langle {\bf x} \vert {\boldsymbol{\rho}}  {\bf x} \rangle \ge  0
$ for all vectors ${\bf x}$ of the  Hilbert space.

With finite dimension $n$ it follows immediately from (ii)
that $\boldsymbol{\rho}$ is self-adjoint; that is,
${\boldsymbol{\rho}}^\dagger ={\boldsymbol{\rho}}$), and normal, and thus has a spectral decomposition
\begin{equation}
\boldsymbol{\rho} =\sum_{i=1}^n \rho_i \vert \psi_i\rangle \langle \psi_i \vert
\label{2015-sfgs}
\end{equation}
into orthogonal projections $\vert \psi_i\rangle \langle \psi_i \vert$,
with
(i) yielding $\sum_{i=1}^n\rho_i =1$
(hint: take a trace with the orthonormal basis corresponding to all the $\vert \psi_i\rangle$);
(ii) yielding  $\overline{\rho_i}=\rho_i$;
and (iii) implying $\rho_i \ge 0$, and hence [with (i)] $0 \le \rho_i \le 1$
for all $1\le i \le n$.


As has been pointed out earlier, quantum mechanics differentiates between ``two sorts of states,'' namely
pure states and mixed ones:
 (i)
Pure states ${\boldsymbol{\rho}}_p$  are
represented by one-dimensional orthogonal projections; or, equivalently as one-dimensional linear subspaces by some (unit) vector.
They can be written as ${\boldsymbol{\rho}}_p =  \vert \psi \rangle \langle \psi  \vert$ for some unit vector $\vert \psi \rangle$,
and satisfy $({\boldsymbol{\rho}}_p)^2={\boldsymbol{\rho}}_p$.
 (ii)
General, mixed states ${\boldsymbol{\rho}}_m$, are ones that are no projections and therefore
satisfy $({\boldsymbol{\rho}}_m)^2 \neq {\boldsymbol{\rho}}_m$.
They can be composed from projections by their spectral form (\ref{2015-sfgs}).



The question arises: is it possible to ``purify'' any mixed state by (maybe somewhat superficially) ``enlarging'' its
Hilbert space, such that the resulting state ``living in a larger Hilbert space'' is pure?
This can indeed be achieved by a rather simple procedure:
By considering the spectral form (\ref{2015-sfgs}) of a general mixed state ${\boldsymbol{\rho}}$,
define a new, ``enlarged,'' pure state  $\vert \Psi\rangle \langle \Psi \vert$, with
\begin{equation}
\vert \Psi\rangle = \sum_{i=1}^n \sqrt{\rho_i}  \vert \psi_i\rangle  \vert \psi_i\rangle
.
\label{2015-puran}
\end{equation}

{%%\color{OliveGreen} % %\bproof
That $\vert \Psi\rangle \langle \Psi \vert$ is pure can be tediously verified by proving that it is idempotent:
\index{idempotence}
\begin{equation}
\begin{split}
(\vert \Psi\rangle \langle \Psi \vert )^2
\\=
\left\{
\left[\sum_{i=1}^n \sqrt{\rho_i}  \vert \psi_i\rangle  \vert \psi_i\rangle \right]
\left[\sum_{j=1}^n \sqrt{\rho_j}  \langle  \psi_j\vert \langle \psi_j\vert \right]
\right\}^2
\\=
\left[\sum_{i_1=1}^n \sqrt{\rho_{i_1}}  \vert \psi_{i_1}\rangle  \vert \psi_{i_1}\rangle \right]
\left[\sum_{j_1=1}^n \sqrt{\rho_{j_1}}  \langle  \psi_{j_1}\vert \langle \psi_{j_1}\vert \right]\times \\
\left[\sum_{i_2=1}^n \sqrt{\rho_{i_2}}  \vert \psi_{i_2}\rangle  \vert \psi_{i_2}\rangle \right]
\left[\sum_{j_2=1}^n \sqrt{\rho_{j_2}}  \langle  \psi_{j_2}\vert \langle \psi_{j_2}\vert \right]
\qquad
\\=
\left[\sum_{i_1=1}^n \sqrt{\rho_{i_1}}  \vert \psi_{i_1}\rangle  \vert \psi_{i_1}\rangle \right]
\underbrace{
\left[\sum_{j_1=1}^n \sum_{i_2=1}^n \sqrt{\rho_{j_1}}\sqrt{\rho_{i_2}}  (\delta_{i_2 j_1})^2 \right]
}_
{
 \sum_{j_1=1}^n   \rho_{j_1} = 1
}
\left[\sum_{j_2=1}^n \sqrt{\rho_{j_2}}  \langle  \psi_{j_2}\vert \langle \psi_{j_2}\vert \right]
\\=
\left[\sum_{i_1=1}^n \sqrt{\rho_{i_1}}  \vert \psi_{i_1}\rangle  \vert \psi_{i_1}\rangle \right]
\left[\sum_{j_2=1}^n \sqrt{\rho_{j_2}}  \langle  \psi_{j_2}\vert \langle \psi_{j_2}\vert \right]
\\=  \vert \Psi\rangle \langle \Psi \vert
.
\label{2015-puranproof}
\end{split}
\end{equation}
}

Note that this construction is not unique -- any construction
$\vert \Psi' \rangle = \sum_{i=1}^n \sqrt{\rho_i}  \vert \psi_i\rangle  \vert \phi_i\rangle$
involving auxiliary components
$\vert \phi_i\rangle$
representing the elements of some orthonormal basis $\{\vert \phi_1\rangle , \ldots , \vert \phi_n\rangle \}$
would suffice.

The original mixed state ${\boldsymbol{\rho}}$ is obtained from the pure state (\ref{2015-puran})
corresponding to the unit vector $\vert \Psi\rangle = \vert \psi \rangle \vert \psi^a \rangle  = \vert \psi \psi^a \rangle$
-- we might say that ``the superscript $a$ stands for auxiliary'' --
by a partial trace (cf. Sec.~\ref{2015-partialtrace}) over one of its components, say  $\vert \psi^a\rangle$.
\index{partial trace}

{%%\color{OliveGreen} % %\bproof
For the sake of a proof let us ``trace out of the auxiliary components $\vert \psi^a\rangle$,'' that is,
take the trace
\begin{equation}
\textrm{Tr}_{a} (\vert \Psi\rangle \langle \Psi \vert )
=
\sum_{k=1}^n  \langle \psi^a_{k} \vert  (\vert \Psi\rangle \langle \Psi \vert ) \vert \psi^a_{k}\rangle
\end{equation}
of
$\vert \Psi\rangle \langle \Psi \vert$
with respect to one of its components $\vert \psi^a\rangle$:
\begin{equation}
\begin{split}
\textrm{Tr}_{a}\left(
\vert \Psi\rangle \langle \Psi \vert
\right)
\\=
\textrm{Tr}_{a}\left(
\left[\sum_{i=1}^n \sqrt{\rho_{i}}  \vert \psi_{i}\rangle  \vert \psi^a_{i}\rangle \right]
\left[\sum_{j=1}^n \sqrt{\rho_{j}}  \langle  \psi^a_{j}\vert \langle \psi_{j}\vert \right]
\right)
\\=
\sum_{k=1}^n  \left\langle \psi^a_{k} \left\vert
\left[\sum_{i=1}^n \sqrt{\rho_{i}}  \vert \psi_{i}\rangle  \vert \psi^a_{i}\rangle \right]
\left[\sum_{j=1}^n \sqrt{\rho_{j}}  \langle  \psi^a_{j}\vert \langle \psi_{j}\vert \right]
\right\vert \psi^a_{k}\right\rangle
\\=
\sum_{k=1}^n \sum_{i=1}^n \sum_{j=1}^n  \delta_{ki} \delta_{kj}
\sqrt{\rho_{i}} \sqrt{\rho_{j}}
\vert \psi_{i}\rangle
\langle \psi_{j}\vert
\\=
\sum_{k=1}^n
\rho_{k}
\vert \psi_{k}\rangle
\langle \psi_{k}\vert
= {\boldsymbol{\rho}}
.
\label{2015-puranproof1}
\end{split}
\end{equation}
}





\section{Adjoint or dual transformation}
\label{2014-m-fdvs-adjoint}
\index{adjoints}
\index{adjoint operator}
\index{dual operator}

\subsection{Definition}

Let $\vert {\bf x}\rangle $ be some vector in a vector space ${\frak V}$, let $\langle {\bf y}\vert $
be any vector of its dual space ${\frak V}^\ast$, and let  $\textsf{\textbf{A}}$
be some linear transformation.
The {\em adjoint} (or {\em dual}) transformation $\textsf{\textbf{A}}^\ast$ can be defined by
\begin{equation}
\langle \textsf{\textbf{A}}^\ast  {\bf y} \vert (\vert{\bf x}\rangle )
\equiv
[\vert{\bf x}\rangle ,\langle \textsf{\textbf{A}}^\ast{\bf y}\vert ]
=
[\vert \textsf{\textbf{A}}{\bf x}\langle ,\langle {\bf y}\vert]
\equiv
\langle {\bf y}\vert (\vert \textsf{\textbf{A}}{\bf x}\langle ).
\label{2016-m-ch-fdlvs-adjointop}
\end{equation}




\subsection{Adjoint matrix notation}

As mentioned earlier, for orthonormal bases and in Euclidean space,  the
{Riesz representation theorem}
\index{Riesz representation theorem}
\index{Fr\'echet-Riesz representation theorem}
connects   functionals
with inner products.
Therefore,  for all ${\bf x}\in {\frak V}$,
there exist a unique $\vert {\bf y} \rangle \in {\frak V}$ with
\begin{equation}
\begin{split}
\langle \textsf{\textbf{A}} {\bf x}\vert {\bf y}\rangle
=\overline{ \langle{\bf y}\vert \textsf{\textbf{A}} {\bf x}\rangle }
= \\
= \overline{ \left[y_i   \overline{ \left( A_{ij}  x_j  \right) }  \right]}
= \overline{ \left[y_i    \overline{ A}_{ij} \overline{ x_j}     \right]}
= \overline{ y_i }    \overline{\overline{ A_{ij} }}  \overline{ \overline{ x_j } }
= \\
= \overline{y_i} A_{ij}  x_j
= x_j A_{ij}  \overline{y_i}
= x_j (A^\intercal )_{ji}  \overline{y_i}
= x  \textsf{\textbf{A}}^\intercal  \overline{ y },
\end{split}
\label{2016-m-ch-fdvs-adjdef}
\end{equation}
and another unique vector $\vert {\bf y}' \rangle $ obtained from $\vert {\bf y} \rangle $ by
some linear operator $\textsf{\textbf{A}}^\ast$
such that $\vert {\bf y}' \rangle =\textsf{\textbf{A}}^\ast \vert {\bf y} \rangle $ with
\begin{equation}
\begin{split}
 \langle {\bf x}\vert {\bf y}'\rangle
= \langle {\bf x}\vert \textsf{\textbf{A}}^\ast {\bf y}\rangle
=\\
= x_i \overline{ \left(A_{ij}^\ast y_j \right) }
=\\
[ i \leftrightarrow j]
= x_j \overline{  A_{ji}^\ast} \overline{  y_i  }
=    x \overline{\textsf{\textbf{A}}^\ast} \overline{y}.
\end{split}
\label{2016-m-ch-fdvs-adjdef2}
\end{equation}
Therefore, by comparing Es.~(\ref{2016-m-ch-fdvs-adjdef2}) and /\ref{2016-m-ch-fdvs-adjdef}),  we obtain $\overline{ \textsf{\textbf{A}}^\ast}=\textsf{\textbf{A}}^\intercal $, so that
\begin{equation}
\textsf{\textbf{A}}^\ast =\overline{\textsf{\textbf{A}}^\intercal } = \overline{\textsf{\textbf{A}}}^\intercal .
\end{equation}
That is, in matrix notation, the adjoint transformation is just the
transpose of the complex conjugate of the original matrix.



We mention without proof that the adjoint operator is a linear operator.
Furthermore,
$\textsf{\textbf{0}}^\ast = \textsf{\textbf{0}}$,
$\textsf{\textbf{1}}^\ast = \textsf{\textbf{1}}$,
$(\textsf{\textbf{A}}+\textsf{\textbf{B}})^\ast = \textsf{\textbf{A}}^\ast+\textsf{\textbf{B}}^\ast$,
$(\alpha \textsf{\textbf{A}})^\ast = \alpha \textsf{\textbf{A}}^\ast$,
$( \textsf{\textbf{A}}\textsf{\textbf{B}})^\ast =   \textsf{\textbf{B}}^\ast
 \textsf{\textbf{A}}^\ast$,
and
$( \textsf{\textbf{A}}^{-1})^\ast
=
( \textsf{\textbf{A}}^\ast )^{-1}
$.


\section{Self-adjoint transformation}
\index{self-adjoint transformation}
\label{2015-m-ch-fdlvs-self-adjoint}



The following definition yields some analogy to real numbers as compared to complex numbers
(``a complex number $z$ is real if $\overline{z}=z$''),
expressed in terms of operators on a complex vector space.


An operator    $\textsf{\textbf{A}}$   on a linear vector space   ${\frak V}$
is called {\em self-adjoint}, if
\begin{equation}
\textsf{\textbf{A}}^{\ast}=
\textsf{\textbf{A}}
\end{equation}
and if the domains of $\textsf{\textbf{A}}$ and $\textsf{\textbf{A}}^{\ast}$
-- that is, the set of vectors on which they are well defined -- coincide.

In finite dimensional {\em real} inner product spaces,
self-adjoint operators are called {\em symmetric,}
since they are symmetric with respect to transpositions; that is,
\index{symmetric operator}
\begin{equation}
\textsf{\textbf{A}}^{\ast}= \textsf{\textbf{A}}^{T}=
\textsf{\textbf{A}}.
\end{equation}

In finite dimensional
{\em complex} inner product spaces,
self-adjoint operators are called {\em Hermitian,}
since they are identical with respect to Hermitian conjugation (transposition of the matrix and complex conjugation of its
entries); that is,
\index{Hermitian operator}
\begin{equation}
\textsf{\textbf{A}}^{\ast}= \textsf{\textbf{A}}^{\dagger}=
\textsf{\textbf{A}}.
\end{equation}

In what follows, we shall consider only the latter case and identify self-adjoint operators with Hermitian ones.
In terms of matrices, a matrix $A$ corresponding to an operator $\textsf{\textbf{A}}$ in
some fixed basis is self-adjoint
if
\begin{equation}
A^{\dagger}\equiv (\overline{A_{ij}})^\intercal =  \overline{A_{ji}} =A_{ij} \equiv A.
\end{equation}
That is, suppose $A_{ij}$ is the matrix representation
corresponding to a linear transformation $\textsf{\textbf{A}}$  in some basis ${\frak B}$,
then the {\em Hermitian} matrix $\textsf{\textbf{A}}^\ast = \textsf{\textbf{A}}^\dagger$
to the dual basis
${\frak B}^\ast $
is
$\overline{(A_{ij}})^\intercal $.





Note that the coherent real-valued superposition
\index{coherent superposition}
\index{superposition}
of a self-adjoint transformations
(such as the sum or difference of correlations in
the Clauser-Horne-Shimony-Holt expression~\cite{filipp-svo-04-qpoly-prl})
is a self-adjoint transformation.

{%%\color{OliveGreen} % %\bproof
For a direct proof,
suppose that $\alpha_i \in {\Bbb R}$ for all $1\le i \le n$ are $n$ real-valued coefficients and
$\textsf{\textbf{A}}_1, \ldots \textsf{\textbf{A}}_n$ are $n$ self-adjoint operators.
Then
$\textsf{\textbf{B}} = \sum_{i=1}^n \alpha_i \textsf{\textbf{A}}_i$
is self-adjoint, since
\begin{equation}
\textsf{\textbf{B}}^\ast  = \sum_{i=1}^n \overline{\alpha_i} \textsf{\textbf{A}}_i^\ast  = \sum_{i=1}^n  \alpha_i  \textsf{\textbf{A}}_i
=\textsf{\textbf{B}}
.
\end{equation}
 % %\eproof
}

\section{Positive transformation}
\index{positive transformation}
\index{nonnegative transformation}
\label{2015-m-ch-fdlvs-positive}

A linear transformation  $\textsf{\textbf{A}}$ on an inner product space ${\frak V}$ is {\em positive} (or, used synonymously, {\em nonnegative}),
that is, in symbols $\textsf{\textbf{A}}\ge 0$,
if $\langle \textsf{\textbf{A}}{\bf x}\vert {\bf x}\rangle  \ge 0$ for all ${\bf x}\in {\frak V}$.
If  $\langle \textsf{\textbf{A}}{\bf x}\vert {\bf x}\rangle = 0$ implies
${\bf x}=0$, $\textsf{\textbf{A}}$ is called {\em strictly positive}.


Positive transformations -- indeed, transformations with real inner products such that
$
\langle \textsf{\textbf{A}} {\bf x}\vert {\bf x}\rangle
= \overline{\langle {\bf x}\vert \textsf{\textbf{A}}{\bf x}\rangle }
=
\langle {\bf x}\vert \textsf{\textbf{A}}{\bf x}\rangle$
for all vectors  ${\bf x}$
of a complex inner product space ${\frak V}$ --
are self-adjoint.

{%%\color{OliveGreen} % %\bproof
For a direct proof,
recall the
polarization identity
\index{polarization identity}
(\ref{2015-m-ch-polidc})  in a slightly different form, with the first argument (vector) transformed by $\textsf{\textbf{A}}$,
as well as the definition of the adjoint operator (\ref{2016-m-ch-fdlvs-adjointop})
on page~\pageref{2016-m-ch-fdlvs-adjointop},
and write
\begin{equation}
\begin{split}
\langle {\bf x}\vert \textsf{\textbf{A}}^\ast {\bf y}\rangle
=
\langle \textsf{\textbf{A}} {\bf x}\vert {\bf y}\rangle
=
\frac{1}{4}\left[
\langle \textsf{\textbf{A}} ({\bf x}+ {\bf y}) \vert {\bf x}+ {\bf y}\rangle
-
\langle \textsf{\textbf{A}}({\bf x}- {\bf y}) \vert {\bf x}- {\bf y}\rangle \right.  \\
\left.
+ i
\langle \textsf{\textbf{A}} ({\bf x}+ i{\bf y}) \vert {\bf x}+ i{\bf y}\rangle
- i
\langle \textsf{\textbf{A}} ({\bf x}- i{\bf y}) \vert {\bf x} -i {\bf y}\rangle
\right]
\\
=
\frac{1}{4}\left[
\langle {\bf x}+ {\bf y} \vert \textsf{\textbf{A}} ({\bf x}+ {\bf y})\rangle
-
\langle {\bf x}- {\bf y} \vert \textsf{\textbf{A}}({\bf x}- {\bf y})\rangle \right.  \\
\left.
+ i
\langle {\bf x}+ i{\bf y} \vert \textsf{\textbf{A}} ({\bf x}+ i{\bf y})\rangle
- i
\langle {\bf x}- i{\bf y} \vert \textsf{\textbf{A}} ({\bf x} -i {\bf y})\rangle
\right]
=
\langle {\bf x}\vert \textsf{\textbf{A}}{\bf y}\rangle
.
\end{split}
\label{2015-m-ch-polidrv}
\end{equation}
 % %\eproof
}




\section{Unitary transformations and isometries}
\index{unitary transformation}
\label{2014-m-ch-fdvs-unitary}


\subsection {Equivalent definitions}
Note that a complex number $z$ has absolute value one if $z\overline{z}=1$, or $\overline{z}=1/z$.
In analogy to this ``modulus one'' behaviour,
consider {\em unitary transformations}, or, used synonymously, {\em (one-to-one) isometries}
$\textsf{\textbf{U}}$ for which
\begin{equation}
\textsf{\textbf{U}}^\ast = \textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^{-1},
\textrm{ or } \textsf{\textbf{U}}\textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^\dagger \textsf{\textbf{U}}={\Bbb I}.
\end{equation}
The following conditions are equivalent:
(i)
$\textsf{\textbf{U}}^\ast = \textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^{-1}$,
or $\textsf{\textbf{U}}\textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^\dagger \textsf{\textbf{U}}={\Bbb I}$.
(ii)
$\langle \textsf{\textbf{U}}{\bf x}\vert \textsf{\textbf{U}}{\bf y} \rangle
=
\langle {\bf x}\vert {\bf y} \rangle$ for all ${\bf x} ,{\bf y} \in {\frak V}$;
(iii)
$\textsf{\textbf{U}}$ is an {\em isometry};
\index{isometry}
that is, preserving the norm
$\| \textsf{\textbf{U}}{\bf x}\|
=
\|{\bf x}\|$ for all ${\bf x}  \in {\frak V}$.
(iv)
$\textsf{\textbf{U}}$ represents a change of orthonormal basis~\cite[\S~74]{halmos-vs}
(see also Schwinder~\cite{Schwinger.60}):
Let ${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
be an orthonormal basis.
Then
$\textsf{\textbf{U}}{\frak B}={\frak B}'=\{\textsf{\textbf{U}}{\bf f}_1, \textsf{\textbf{U}} {\bf f}_2,
\ldots ,\textsf{\textbf{U}} {\bf f}_n\}$
is also an orthonormal basis of  ${\frak V}$.
Conversely, two arbitrary orthonormal bases
${\frak B}$
and
${\frak B}'$
are connected by a unitary transformation $\textsf{\textbf{U}}$ {\it via} the pairs  ${\bf f}_i$ and $ \textsf{\textbf{U}}{\bf f}_i$
for all $1\le i \le n$, respectively.
More explicitly, denote  $\textsf{\textbf{U}} {\bf f}_i   = {\bf e}_i$; then
(recall ${\bf f}_i$ and ${\bf e}_i$ are elements of the orthonormal bases
$  {\frak B} $  and $ \textsf{\textbf{U}} {\frak B} $, respectively)
$\textsf{\textbf{U}}_{ef}   = \sum_{i=1}^n{\bf e}_i  {\bf f}_i^\dagger$.



Note that $\textsf{\textbf{U}}$ preserves length or distances and thus is an {\em isometry}, as for all ${\bf x}, {\bf y}$,
\begin{equation}
\begin{split}
\| \textsf{\textbf{U}}{\bf x} - \textsf{\textbf{U}}{\bf y} \| =
\| \textsf{\textbf{U}}\left( {\bf x} - {\bf y} \right) \| =
\|{\bf x}- {\bf y}\|
.
\end{split}
\label{2016-m-ch-uniteqpl}
\end{equation}

Note also that $\textsf{\textbf{U}}$ preserves the angle $\theta$ between two nonzero vectors $ \vert x \rangle $ and $ \vert y \rangle $  defined by
\begin{equation}
\begin{split}
\cos \theta = \frac{ \langle {\bf x} \vert {\bf y} \rangle } {\|{\bf x}\| \| {\bf y}\| }
\end{split}
\label{2016-m-ch-unitanglepres}
\end{equation}
as it preserves the inner product and the norm.

{%\color{blue}
 % %\bexample
Since unitary transformations can also be defined via {\em one-to-one transformations preserving the scalar product,}
functions such as
$f: \vert x \rangle \mapsto  \vert x '\rangle  =\alpha  \vert x \rangle $ with $\alpha \neq e^{i\varphi}$, $\varphi \in {\Bbb R}$,
do not correspond to a  unitary transformation in a one-dimensional Hilbert space, as
the scalar product $f:
\langle x \vert y \rangle
\mapsto
\langle x'\vert y'\rangle = \vert \alpha \vert^2 \langle x\vert y\rangle$
is not preserved; whereas if $\alpha$ is a modulus of one; that is,
with $\alpha = e^{i\varphi}$, $\varphi \in {\Bbb R}$,
$\vert \alpha \vert^2=1$, and the scalar product is preseved.
Thus, $u: x \mapsto x' =e^{i\varphi} x$, $\varphi \in {\Bbb R}$,
represents a unitary transformation.
 % %\eexample
}

%\subsection {Characterization of change of orthonormal basis}



\subsection {Characterization in terms of orthonormal basis}
\label{2012-m-ch-citoob}


A complex matrix $\textsf{\textbf{U}}$ is unitary if and only if its row (or column) vectors form
an orthonormal basis.

This can be readily verified \cite{Schwinger.60} by writing $\textsf{\textbf{U}}$
in terms of two orthonormal bases
${\frak B}=\{\vert {\bf e}_1\rangle , \vert  {\bf e}_2\rangle , \ldots , \vert {\bf e}_n\rangle \}$
${\frak B}'= \{\vert {\bf f}_1\rangle , \vert  {\bf f}_2\rangle , \ldots , \vert {\bf f}_n\rangle \}$ as
\begin{equation}
\textsf{\textbf{U}}_{ef}
= \sum_{i=1}^n  \vert {\bf e}_i\rangle \langle {\bf f}_i \vert
.
\label{2016-m-ch-citoob-sinobv}
\end{equation}
Together with $\textsf{\textbf{U}}_{fe}=   \sum_{i=1}^n  \vert {\bf f}_i\rangle \langle {\bf e}_i \vert $
we form
\begin{equation}
\begin{split}
\langle {\bf e}_k \vert \textsf{\textbf{U}}_{ef}  = \langle {\bf e}_k \vert \sum_{i=1}^n  \vert {\bf e}_i \rangle \langle {\bf f}_i \vert
= \sum_{i=1}^n  \langle {\bf e}_k \vert {\bf e}_i\rangle  \langle {\bf f}_i \vert
= \sum_{i=1}^n  \delta_{ki} \langle {\bf f}_i \vert  =  \langle {\bf f}_k \vert
.
\end{split}
\end{equation}
In a similar way we find that
\begin{equation}
\begin{split}
\textsf{\textbf{U}}_{ef} \vert {\bf f}_k \rangle = \vert {\bf e}_k \rangle,
\langle {\bf f}_k \vert \textsf{\textbf{U}}_{fe}   =  \langle {\bf e}_k\vert ,
\textsf{\textbf{U}}_{fe}\vert {\bf e}_k \rangle  = \vert {\bf f}_k \rangle .
\end{split}
\end{equation}
Moreover,
\begin{equation}
\begin{split}
\textsf{\textbf{U}}_{ef}\textsf{\textbf{U}}_{fe}
=
 \sum_{i=1}^n  \sum_{j=1}^n
(\vert {\bf e}_i\rangle \langle {\bf f}_i \vert )
(\vert {\bf f}_j\rangle \langle {\bf e}_j \vert )
=
 \sum_{i=1}^n  \sum_{j=1}^n
\vert {\bf e}_i\rangle \delta_{ij} \langle {\bf e}_j \vert
=
 \sum_{i=1}^n
\vert {\bf e}_i\rangle   \langle {\bf e}_i \vert
=
{\Bbb I}
.
\end{split}
\end{equation}
In a similar way we obtain
$\textsf{\textbf{U}}_{fe}\textsf{\textbf{U}}_{ef}=
{\Bbb I}$.
Since
\begin{equation}
\textsf{\textbf{U}}_{ef}^\dagger = \sum_{i=1}^n  ( {\bf f}_i^\dagger )^\dagger {\bf e}_i^\dagger
= \sum_{i=1}^n  {\bf f}_i {\bf e}_i^\dagger
= \textsf{\textbf{U}}_{fe},
\end{equation}
we obtain that $\textsf{\textbf{U}}_{ef}^\dagger = (\textsf{\textbf{U}}_{ef})^{-1}$
and $\textsf{\textbf{U}}_{fe}^\dagger = (\textsf{\textbf{U}}_{fe})^{-1}$.

Note also that the {\em composition} holds; that is, $\textsf{\textbf{U}}_{ef} \textsf{\textbf{U}}_{fg}=  \textsf{\textbf{U}}_{eg}$.



If we
identify one of the bases  ${\frak B}$ and ${\frak B}'$ by the Cartesian standard basis,
it becomes clear that, for instance,
every unitary operator  $\textsf{\textbf{U}}$  can be written in terms of an orthonormal basis
of the dual space
${\frak B}^\ast=\{ \langle {\bf f}_1  \vert     ,   \langle {\bf f}_2  \vert      \ldots ,  \langle {\bf f}_n  \vert     \}$
by ``stacking'' the conjugate transpose vectors of that orthonormal basis ``on top of each other;''
\index{conjugate transpose}
\index{Hermitian conjugate}
\index{Hermitian adjoint}
that is,
\begin{equation}
\textsf{\textbf{U}}
=
\begin{pmatrix}
1\\
0\\
\vdots\\
0
\end{pmatrix} \langle {\bf f}_1  \vert
+
\begin{pmatrix}
0\\
1\\
\vdots\\
0
\end{pmatrix}  \langle {\bf f}_2  \vert
+
\cdots +
\begin{pmatrix}
0\\
0\\
\vdots\\
n
\end{pmatrix}  \langle {\bf f}_n  \vert
=
\begin{pmatrix}
\langle {\bf f}_1\vert \\
\langle {\bf f}_2\vert\\
\vdots\\
\langle {\bf f}_n\vert
\end{pmatrix}
.
\end{equation}
Thereby the conjugate transpose vectors of the orthonormal basis  ${\frak B}$ serve as the
\index{conjugate transpose}
\index{Hermitian conjugate}
\index{Hermitian adjoint}
rows of $\textsf{\textbf{U}}$.

In a similar manner, every unitary operator  $\textsf{\textbf{U}}$  can be written in terms of an orthonormal basis
${\frak B}=\{\vert {\bf f}_1 \rangle , \vert {\bf f}_2 \rangle, \ldots , \vert {\bf f}_n \rangle\}$
by ``pasting'' the  vectors of that orthonormal basis ``one after another;''
that is
\begin{equation}
\begin{split}
\textsf{\textbf{U}}
=
\vert {\bf f}_1 \rangle \begin{pmatrix} 1,0,\ldots ,0\end{pmatrix} +
\vert {\bf f}_2 \rangle \begin{pmatrix} 0,1,\ldots ,0\end{pmatrix} +
\cdots +
\vert {\bf f}_n \rangle \begin{pmatrix} 0,0,\ldots ,1\end{pmatrix}   \\
=
\begin{pmatrix}
\vert {\bf f}_1 \rangle ,
\vert {\bf f}_2 \rangle ,
\cdots,
\vert {\bf f}_n \rangle
\end{pmatrix}
.
\end{split}
\label{2015-m-ch-fdlvs-uniascolv}
\end{equation}
Thereby the vectors of the orthonormal basis  ${\frak B}$ serve as the
columns of $\textsf{\textbf{U}}$.

Note also that any permutation of vectors in ${\frak B}$ would also yield unitary matrices.


\section{Orthonormal (orthogonal) transformations}
\index{orthonormal transformation}
\index{orthogonal transformation}
\label{2015-m-ch-fdlvs-orthproj}

Orthonormal (orthogonal) transformations are special cases of unitary transformations restricted to {\em real} Hilbert space.

An {\em orthonormal} or {\em orthogonal transformation} $\textsf{\textbf{R}}$ is a linear transformation
whose corresponding square matrix $R$ has real-valued entries
and mutually orthogonal, normalized row (or, equivalently, column) vectors.
As a consequence (see the equivalence of definitions of unitary definitions and the proofs mentioned earlier),
\begin{equation}
RR^\intercal = R^\intercal R= {\Bbb I}, \textrm{ or } R^{-1}=R^\intercal  .
\end{equation}
As all unitary transformations, orthonormal transformations $\textsf{\textbf{R}}$
preserve a symmetric inner product as well as the norm.

If $\textrm{det} R=1$, $\textsf{\textbf{R}}$ corresponds to a {\em rotation.}
\index{rotation}
If $\textrm{det} R=-1$, $\textsf{\textbf{R}}$ corresponds to a rotation and a {\em reflection.}
\index{reflection}
A reflection is an isometry (a distance preserving map) with a hyperplane as set of fixed points.




\section{Permutations}
\label{2016-pu-ch-qm-perm}
\index{permutation}

Permutations are ``discrete'' orthogonal transformations in the sense that
they merely allow the entries ``$0$'' and ``$1$'' in the respective matrix representations.
With regards to classical and quantum bits \cite{mermin-04,mermin-07}
they serve as a sort of ``reversible classical analogue'' for classical reversible computation,
as compared to the more general, continuous unitary transformations of quantum bits introduced earlier.

Permutation matrices are defined by the requirement that they only contain a single nonvanishing entry ``$1$'' per row and column;
all the other row and column entries vanish; that is, the respective matrix entry is ``$0$.''


Note that from the definition and from matrix multiplication follows that,
if $\textsf{\textbf{P}}$ is a permutation matrix, then $\textsf{\textbf{P}} \textsf{\textbf{P}}^\intercal =\textsf{\textbf{P}}^\intercal  \textsf{\textbf{P}}={\Bbb I}_n$.
That is, $\textsf{\textbf{P}}^\intercal $ represents the inverse element of $\textsf{\textbf{P}}$.
As $\textsf{\textbf{P}}$ is real-valued, it is a {\em normal operator} (cf. page \pageref{2014-m-fdvs-normality}).
\index{normal operator}
\index{normal transformation}


Note further that, as all unitary matrices, any permutation matrix can be interpreted in terms of row and column vectors:
The set of all these row and column vectors constitute the Cartesian standard basis of $n$-dimensional vector space,
with permuted elements.

Note also that, if $\textsf{\textbf{P}}$ and $\textsf{\textbf{Q}}$ are permutation matrices, so is $\textsf{\textbf{P}}\textsf{\textbf{Q}}$
and $\textsf{\textbf{Q}}\textsf{\textbf{P}}$.
The set of all $n!$
permutation $(n\times n)-$matrices corresponding to permutations of $n$ elements of $\{ 1,2,\ldots ,n\}$ form the
{\em symmetric group $S_n$}, with ${\Bbb I}_n$ being the identity element.
\index{symmetric group}


% http://math.stackexchange.com/questions/1970702/what-are-the-properties-of-eigenvalues-of-permutation-matrices

% A permutation may be written as a unique product of primitive cycles p=(c1)?(ck)p=(c1)?(ck). This corresponds to writing the matrix in block form with each cycle representing a block. Each cycle of length |ci||ci| has precisely the |ci||ci|'th roots of unity as eigenvalues. This tells you at least precisely when a collection of eigenvalues (with multiplicity) may correspond to a permutation matrix.


\section{Orthogonal (perpendicular) projections}
\index{orthogonal projection}
\index{perpendicular projection}

{\em Orthogonal,} or, used synonymously,
{\em perpendicular} projections
are associated with a {\em direct sum decomposition} of the vector space ${\frak V}$;
that is,
\begin{equation}
 {\frak M}\oplus {\frak M}^\perp ={\frak V},
\label{2012-m-ch-fdvs-perp}
\end{equation}
whereby $ {\frak M}= P_{\frak M}({\frak V})$
is the image of some projector $\textsf{\textbf{E}}=P_{\frak M}$
along ${\frak M}^\perp$, and  ${\frak M}^\perp$ is
the kernel of $P_{\frak M}$.
That is, ${\frak M}^\perp = \left\{ \vert {\bf x} \rangle \in {\frak V} \mid P_{\frak M}(\vert {\bf x} \rangle ) = {\bf 0}\right\}$
\index{kernel}
is the subspace of ${\frak V}$
whose elements are mapped to the zero vector ${\bf 0}$ by $P_{\frak M}$.


Let us, for the sake of concreteness,
suppose that, in $n$-dimensional complex Hilbert space
${\Bbb C}^n$, we are given a $k$-dimensional subspace
\begin{equation}
{\frak M} = \textrm{span}\left(\vert {\bf x}_1\rangle ,\ldots ,\vert {\bf x}_k \right\rangle )
\end{equation}
spanned
\index{linear span}
\index{span}
by  $k \le n$  linear independent base vectors ${\bf x}_1,\ldots ,{\bf x}_k$.
In addition, we are given another (arbitrary) vector ${\bf y} \in {\Bbb C}^n$.

Now consider the following question:
how can we project $\vert {\bf y} \rangle $ onto ${\frak M}$ orthogonally (perpendicularly)?
That is, can we find a vector $\vert {\bf y}' \rangle  \in {\frak M}$ so that
$\vert {\bf y}^\perp \rangle  = \vert {\bf y} \rangle  - \vert {\bf y}' \rangle $
is orthogonal (perpendicular) to all of ${\frak M}$?

The orthogonality of $\vert {\bf y}^\perp \rangle $ on the entire ${\frak M}$ can be rephrased in terms
of all the vectors $\vert {\bf x}_1 \rangle ,\ldots ,\vert {\bf x}_k \rangle $ spanning ${\frak M}$; that is,
for all $\vert {\bf x}_i \rangle  \in {\frak M}$, $1\le i\le k$
we must have
$\langle {\bf x}_i \vert {\bf y}^\perp \rangle = 0$.
This can be transformed into matrix algebra by considering the $n \times k$ matrix
[note that ${\bf x}_i$ are column vectors,
and recall the construction in Eq.~(\ref{2015-m-ch-fdlvs-uniascolv})]
\begin{equation}
\textsf{\textbf{A}} =
\begin{pmatrix}
\vert {\bf x}_1\rangle , \ldots , \vert {\bf x}_k \rangle
\end{pmatrix},
\end{equation}
and by requiring
\begin{equation}
\textsf{\textbf{A}}^\dagger \vert {\bf y}^\perp \rangle    =
 \textsf{\textbf{A}}^\dagger  \left(\vert {\bf y} \rangle - \vert {\bf y}' \rangle \right) =
 \textsf{\textbf{A}}^\dagger \vert {\bf y} \rangle   - \textsf{\textbf{A}}^\dagger \vert {\bf y}' \rangle    =
0,
\end{equation}
yielding
\begin{equation}
 \textsf{\textbf{A}}^\dagger  \vert  {\bf y}  \rangle
=
\textsf{\textbf{A}}^\dagger \vert {\bf y}' \rangle .
\label{2015-m-ch-cfvs-opab1}
\end{equation}

On the other hand, $\vert {\bf y}' \rangle $ must be a linear combination of
$\vert {\bf x}_1 \rangle ,\ldots ,\vert {\bf x}_k \rangle $ with the $k$-tuple of coefficients ${\bf c}$ defined by
\begin{equation}
\vert {\bf y}' \rangle
=
c_1 \vert {\bf x}_1 \rangle + \cdots + c_k \vert {\bf x}_k \rangle
=
\begin{pmatrix}\vert {\bf x}_1 \rangle, \ldots ,\vert {\bf x}_k \rangle
\end{pmatrix}
\begin{pmatrix} c_1\\ \vdots \\ c_k
\end{pmatrix}
=
\textsf{\textbf{A}}  {\bf c}
.
\label{2015-m-ch-cfvs-opab22}
\end{equation}
Insertion into (\ref{2015-m-ch-cfvs-opab1}) yields
\begin{equation}
\textsf{\textbf{A}}^\dagger  \vert {\bf y} \rangle
=
\textsf{\textbf{A}}^\dagger  \textsf{\textbf{A}} {\bf c}
.
\label{2015-m-ch-cfvs-opab2}
\end{equation}
Taking the inverse of $\textsf{\textbf{A}}^\dagger  \textsf{\textbf{A}} $
(this is a $k \times k$ diagonal matrix which is invertible, since
the $k$ vectors defining $\textsf{\textbf{A}}$ are linear independent),
and multiplying (\ref{2015-m-ch-cfvs-opab2}) from the left yields
\begin{equation}
{\bf c}
=
\left(\textsf{\textbf{A}}^\dagger  \textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}^\dagger\vert {\bf y} \rangle
.
\label{2015-m-ch-cfvs-opab3}
\end{equation}
With (\ref{2015-m-ch-cfvs-opab22}) and (\ref{2015-m-ch-cfvs-opab3})
we find ${\bf y}'$ to be
\begin{equation}
\vert {\bf y}' \rangle
=
\textsf{\textbf{A}}  {\bf c} =
\textsf{\textbf{A}}  \left(\textsf{\textbf{A}}^\dagger  \textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}^\dagger \vert {\bf y} \rangle
.
\label{2015-m-ch-cfvs-opab4}
\end{equation}
We can define
\begin{equation}
\textsf{\textbf{E}}_{{\frak M}}=
\textsf{\textbf{A}}  \left(\textsf{\textbf{A}}^\dagger  \textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}^\dagger
\label{2015-m-ch-cfvs-opab5}
\end{equation}
to be the {\em projection matrix for the subspace ${\frak M}$}.
Note that
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_{{\frak M}}^\dagger
=
\left[
\textsf{\textbf{A}}  \left(\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}^\dagger
\right]^\dagger
=
\textsf{\textbf{A}} \left[  \left(\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}} \right)^{-1}\right]^\dagger \textsf{\textbf{A}}^\dagger
=
\textsf{\textbf{A}} \left[ \textsf{\textbf{A}}^{-1}
\left(\textsf{\textbf{A}}^\dagger\right)^{-1} \right]^\dagger \textsf{\textbf{A}}^\dagger \\
=
\textsf{\textbf{A}} \textsf{\textbf{A}}^{-1}
\left(\textsf{\textbf{A}}^{-1}\right)^\dagger  \textsf{\textbf{A}}^\dagger
= \textsf{\textbf{A}} \textsf{\textbf{A}}^{-1}
\left(\textsf{\textbf{A}}^\dagger\right)^{-1}  \textsf{\textbf{A}}^\dagger
= \textsf{\textbf{A}} \left(\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}}\right)^{-1}  \textsf{\textbf{A}}^\dagger
=  \textsf{\textbf{E}}_{{\frak M}},
\end{split}
\label{2015-m-ch-cfvs-opab6}
\end{equation}
that is, $\textsf{\textbf{E}}_{{\frak M}}$ is self-adjoint and thus normal, as well as idempotent:
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_{{\frak M}}^2
=
\left(
\textsf{\textbf{A}}  \left(\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}^\dagger
\right)
\left(
\textsf{\textbf{A}}  \left(\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}^\dagger
\right) \\
=
\textsf{\textbf{A}}^\dagger  \left(\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}} \right)^{-1} \left( \textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}}  \right) \left(\textsf{\textbf{A}} ^\dagger
\textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}
=
\textsf{\textbf{A}}^\dagger   \left(\textsf{\textbf{A}} ^\dagger
\textsf{\textbf{A}} \right)^{-1} \textsf{\textbf{A}}
=
\textsf{\textbf{E}}_{{\frak M}}.
\end{split}
\label{2015-m-ch-cfvs-opab7}
\end{equation}

Conversely, every normal projection operator has a ``trivial'' spectral decomposition
(cf. later Sec.~\ref{2012-m-ch-Spectraltheorem} on page \pageref{2012-m-ch-Spectraltheorem})
$\textsf{\textbf{E}}_{{\frak M}}
=
1 \cdot \textsf{\textbf{E}}_{{\frak M}} + 0 \cdot \textsf{\textbf{E}}_{{\frak M}^\perp}
=
1 \cdot \textsf{\textbf{E}}_{{\frak M}} + 0 \cdot \left(\textbf{1} - \textsf{\textbf{E}}_{{\frak M}}\right)$
associated with the two eigenvalues $0$ and $1$, and thus must be orthogonal.

If the basis ${\frak B}= \left\{
\vert {\bf x}_1 \rangle ,\ldots ,\vert {\bf x}_k \rangle  \right\}$ of ${\frak M}$
is orthonormal, then
\begin{equation}
\begin{split}
\textsf{\textbf{A}}^\dagger
\textsf{\textbf{A}}
\equiv
\begin{pmatrix}
\langle {\bf x}_1 \vert \\ \vdots \\ \langle {\bf x}_k \vert
\end{pmatrix}
\begin{pmatrix}
\vert {\bf x}_1\rangle , \ldots , \vert {\bf x}_k \rangle
\end{pmatrix}
=
\begin{pmatrix}
\langle {\bf x}_1 \vert {\bf x}_1\rangle   & \ldots &    \langle {\bf x}_1 \vert {\bf x}_k\rangle \\
\vdots &  \vdots &   \vdots \\
\langle {\bf x}_k \vert {\bf x}_1\rangle   & \ldots &    \langle {\bf x}_k \vert {\bf x}_k\rangle \\
\end{pmatrix}
\equiv
{\Bbb I}_k
\end{split}
\end{equation}
represents a $k$-dimensional resolution of the identity operator.
\index{resolution of the identity}
Thus, $\left(\textsf{\textbf{A}}^\dagger  \textsf{\textbf{A}} \right)^{-1}\equiv \left({\Bbb I}_k\right)^{-1}$
is also a $k$-dimensional resolution of the identity operator,
and the orthogonal projector $\textsf{\textbf{E}}_{{\frak M}}$
in Eq.~(\ref{2015-m-ch-cfvs-opab5})
reduces to
\begin{equation}
\textsf{\textbf{E}}_{{\frak M}}=
\textsf{\textbf{A}}   \textsf{\textbf{A}}^\dagger
\equiv
\sum_{i=1}^k \vert {\bf x}_i \rangle \langle {\bf x}_i \vert
.
\end{equation}





In general, the orthonormal projection corresponding to some arbitrary subspace of some Hilbert space can be (non-uniquely)
constructed by
(i) finding an orthonormal basis spanning that subsystem (this is nonunique),
if necessary by a Gram-Schmidt process;
\index{Gram-Schmidt process}
(ii) forming the projection operators corresponding to the dyadic or outer product
\index{dyadic product}
\index{outer product}
of all these vectors; and
(iii) summing up all these orthogonal operators.


The following propositions are stated mostly without proof.
A  linear transformation $\textsf{\textbf{E}}$ is an orthogonal (perpendicular) projection
if and only if is self-adjoint; that is,
$\textsf{\textbf{E}} = \textsf{\textbf{E}}^2=\textsf{\textbf{E}}^\ast $.

Perpendicular projections are {\em positive} linear transformations,
with
$\left\| \textsf{\textbf{E}}{\bf x} \right\| \le \| {\bf x} \|$
for all
${\bf x} \in {\frak V}$.
Conversely,
if a linear transformation $\textsf{\textbf{E}}$
is idempotent; that is,
$\textsf{\textbf{E}}^2=\textsf{\textbf{E}}$,
and  $\left\| \textsf{\textbf{E}}{\bf x} \right\| \le \| {\bf x} \|$
for all
${\bf x} \in {\frak V}$,
then  is self-adjoint; that is,
$\textsf{\textbf{E}}=\textsf{\textbf{E}}^\ast$.

Recall that
for {\em real} inner product spaces, the self-adjoint operator can be identified with a {\em symmetric} operator
$\textsf{\textbf{E}}=\textsf{\textbf{E}}^\intercal $,
\index{symmetric operator}
whereas
for {\em complex} inner product spaces, the self-adjoint operator can be identified with a {\em Hermitian} operator
$\textsf{\textbf{E}}=\textsf{\textbf{E}}^\dagger$.
\index{Hermitian operator}


If $\textsf{\textbf{E}}_1,\textsf{\textbf{E}}_2, \ldots , \textsf{\textbf{E}}_n$ are (perpendicular)
projections,
then a necessary and sufficient condition that
$\textsf{\textbf{E}} =\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2+\cdots +\textsf{\textbf{E}}_n$
be a (perpendicular) projection is that
 $\textsf{\textbf{E}}_i \textsf{\textbf{E}}_j =\delta_{ij}\textsf{\textbf{E}}_i =\delta_{ij}\textsf{\textbf{E}}_j$;
and, in particular,
$\textsf{\textbf{E}}_i \textsf{\textbf{E}}_j =0$
whenever $i\neq j$; that is, that all $E_i$ are pairwise orthogonal.



{%\color{blue}
 % %\bexample
Examples for projections which are not orthogonal are
$$\begin{pmatrix}
1&\alpha \\
0&0
\end{pmatrix}
\text{,  or }
\begin{pmatrix}
1&0&\alpha \\
0&1&\beta \\
0&0&0
\end{pmatrix},$$
with $\alpha \neq 0$.
Such projectors are sometimes called
{\em oblique} projections.
\index{oblique projections}
 % %\eexample
}


{%\color{blue}
 % %\bexample
Indeed, for two-dimensional Hilbert space, the solution
of idempotence
%Reduce[{{a, b}, {c, d}}.{{a, b}, {c, d}} == {{a, b}, {c, d}}, {a, b,  c, d}]
$$
\begin{pmatrix}
a&b \\
c&d
\end{pmatrix}
\begin{pmatrix}
a&b \\
c&d
\end{pmatrix}
=
\begin{pmatrix}
a&b \\
c&d
\end{pmatrix}
$$
yields the three orthogonal projections
$$
\begin{pmatrix}
1&0 \\
0&0
\end{pmatrix},
\begin{pmatrix}
0&0 \\
0&1
\end{pmatrix},\text{ and }
\begin{pmatrix}
1&0 \\
0&1
\end{pmatrix},
$$
as well as a continuum of oblique projections
$$
\begin{pmatrix}
0&0 \\
c&1
\end{pmatrix},
\begin{pmatrix}
1&0 \\
c&0
\end{pmatrix},\text{ and }
\begin{pmatrix}
a&b \\
\frac{a(1-a)}{b}&1-a
\end{pmatrix},
$$
with $a,b,c \neq 0$.
 % %\eexample
}


\section{Proper value or eigenvalue}
\index{proper value}
\index{proper vector}
\index{eigenvalue}
\index{eigenvector}
\index{eigensystem}

\subsection{Definition}

A scalar $\lambda$ is a {\em proper value} or {\em eigenvalue},
and a nonzero vector ${\bf x}$ is a {\em proper vector} or {\em eigenvector}
of a linear transformation $\textsf{\textbf{A}}$
if
\begin{equation}
\textsf{\textbf{A}}\vert {\bf x}  \rangle =   \lambda {\bf x} =   \lambda {\Bbb I} \vert {\bf x}  \rangle .
\end{equation}
In an $n$-dimensional
vector space $\frak V$
The set of the set of eigenvalues and the set of the associated eigenvectors
$\{\{\lambda_1,\ldots ,\lambda_k\},\{{\bf x}_1,\ldots ,{\bf x}_n\}\}$
of a linear transformation $\textsf{\textbf{A}}$ form an {\em eigensystem} of $\textsf{\textbf{A}}$.

\subsection{Determination}
\index{characteristic equation}
\index{secular determinant}
\index{secular equation}


% http://vergil.chemistry.gatech.edu/notes/linear_algebra/node5.html

Since the eigenvalues and eigenvectors are those scalars $\lambda$  vectors ${\bf x}$ for which
$\textsf{\textbf{A}}\vert {\bf x}  \rangle=   \lambda \vert {\bf x}  \rangle$,
this equation can be rewritten with a zero vector on the right side of the equation; that is (${\Bbb I}=\textrm{diag}(1,\ldots ,1)$ stands for the identity matrix),
\begin{equation}
(\textsf{\textbf{A}} - \lambda {\Bbb I})\vert {\bf x}  \rangle= {\bf 0}.
\label{2011-m-eve}
\end{equation}
Suppose that $\textsf{\textbf{A}} - \lambda {\Bbb I}$ is invertible. Then we could formally write
$\vert {\bf x}  \rangle = (\textsf{\textbf{A}} - \lambda {\Bbb I})^{-1}{\bf 0}$; hence $\vert {\bf x}  \rangle$ must be the zero vector.

We are not interested in this trivial solution of Eq. (\ref{2011-m-eve}).
Therefore, suppose that, contrary to the previous assumption,
$\textsf{\textbf{A}} - \lambda {\Bbb I}$ is {\em not} invertible.
We have mentioned earlier (without proof) that this implies that its determinant vanishes; that is,
\begin{equation}
\textrm{det} (\textsf{\textbf{A}} - \lambda {\Bbb I}) = \vert \textsf{\textbf{A}} - \lambda {\Bbb I}\vert =0.
\label{2014-m-eve-ce}
\end{equation}
This determinant is often called the {\em secular determinant};
\index{secular determinant}
\index{secular equation}
and the corresponding equation after expansion of the determinant is called the
{\em secular equation}
or {\em characteristic equation}.
Once the eigenvalues, that is, the roots (i.e., the solutions) of this equation are determined,
the eigenvectors can be obtained one-by-one by inserting these eigenvalues one-by-one into Eq. (\ref{2011-m-eve}).



If the eigenvalues obtained are not distinct und thus some eigenvalues are {\em degenerate},
\index{degenerate eigenvalues}
the associated eigenvectors traditionally -- that is, by convention and not necessity -- are chosen to be
{\em mutually orthogonal.}
A more formal motivation will come from the spectral theorem.
\index{spectral theorem}

The following theorems are enumerated without proofs:
If $\textsf{\textbf{A}}$
is a self-adjoint transformation on an inner product space, then every proper value (eigenvalue)  of $\textsf{\textbf{A}}$
is real.
If $\textsf{\textbf{A}}$ is positive, or strictly positive,
then every proper value of  $\textsf{\textbf{A}}$ is positive, or strictly positive, respectively

Due to their idempotence $\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}$,
projections have eigenvalues $0$ or $1$.

Every eigenvalue of an isometry has absolute value one.

If  $\textsf{\textbf{A}}$  is either a self-adjoint transformation or an isometry,
then proper vectors of $ \textsf{\textbf{A}}$
belonging to distinct proper values are orthogonal.


\section{Normal transformation}
\index{normal transformation}
\index{normal operator}
\label{2014-m-fdvs-normality}

A transformation $\textsf{\textbf{A}}$ is called {\em normal}
if it commutes with its adjoint; that is,
\begin{equation}
[\textsf{\textbf{A}},\textsf{\textbf{A}}^\ast ]= \textsf{\textbf{A}}\textsf{\textbf{A}}^\ast  -
\textsf{\textbf{A}}^\ast  \textsf{\textbf{A}} =0.
\end{equation}


It follows from their definition that Hermitian and unitary transformations are normal. That is,
$\textsf{\textbf{A}}^\ast =\textsf{\textbf{A}}^\dagger$,
and for Hermitian operators,
$\textsf{\textbf{A}}=\textsf{\textbf{A}}^\dagger$,
and thus
$[\textsf{\textbf{A}},\textsf{\textbf{A}}^\dagger]= \textsf{\textbf{A}}\textsf{\textbf{A}} -
\textsf{\textbf{A}} \textsf{\textbf{A}} =(\textsf{\textbf{A}})^2 -(\textsf{\textbf{A}})^2=0$.
For unitary operators,
$\textsf{\textbf{A}}^\dagger =\textsf{\textbf{A}}^{-1}$,
and thus
$[\textsf{\textbf{A}},\textsf{\textbf{A}}^\dagger]= \textsf{\textbf{A}}\textsf{\textbf{A}}^{-1} -
\textsf{\textbf{A}}^{-1} \textsf{\textbf{A}} ={\Bbb I} -{\Bbb I} =0$.


We mention without proof that
a normal transformation on a finite-dimensional unitary space is
(i) Hermitian,
(ii) positive,
(iii) strictly positive,
(iv) unitary,
(v) invertible,
(vi) idempotent
\index{idempotence}
if and only if all its proper values are
(i) real,
(ii) positive,
(iii) strictly positive,
(iv) of absolute value one,
(v) different from zero,
(vi) equal to zero or one.

\section{Spectrum}
\index{spectrum}

\subsection{Spectral theorem}
\label{2012-m-ch-Spectraltheorem}

\index{spectral theorem}

Let $\frak V$ be an $n$-dimensional linear vector space.
The {\em spectral theorem} states
\index{spectral theorem}
that to every self-adjoint (more general, normal) transformation $ \textsf{\textbf{A}}$
on an $n$-dimensional inner product space there correspond real numbers, the {\em spectrum}
$
\lambda_1,
\lambda_2, \ldots ,
\lambda_k
$
of all the eigenvalues of   $ \textsf{\textbf{A}}$,
and their associated  orthogonal projections
$
\textsf{\textbf{E}}_1,
\textsf{\textbf{E}}_2, \ldots ,
\textsf{\textbf{E}}_k
$
where $0<k\le n$ is a strictly positive integer so that
\begin{itemize}
\item[(i)]
the $\lambda_i$ are pairwise distinct;
\item[(ii)]
the $\textsf{\textbf{E}}_i$ are pairwise orthogonal and different from $\textsf{\textbf{0}}$;
\item[(iii)]
the set of projectors is complete in the sense that their
sum $\sum_{i=1}^k \textsf{\textbf{E}}_i={\Bbb I}_n$
is a resolution of the identity operator; and
\index{resolution of the identity}
\index{completeness}
\item[(iv)]
$
\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i
$
is the {\em spectral form} of $\textsf{\textbf{A}}$.
\index{spectral form}
\end{itemize}



\subsection{Composition of the spectral form}

If the spectrum of a  Hermitian (or, more general, normal) operator $\textsf{\textbf{A}}$ is nondegenerate, that is, $k=n$, then the
$i$th projection
can be written as the outer (dyadic or tensor) product
\index{outer product}
\index{dyadic product}
\index{tensor product}
$
\textsf{\textbf{E}}_i= \vert {\bf x}_i \rangle \langle {\bf x}_i\vert$
of the $i$th normalized eigenvector ${\bf x}_i $ of $\textsf{\textbf{A}}$.
In this case, the set of all normalized eigenvectors $\{{\bf x}_1, \ldots ,{\bf x}_n\}$ is an orthonormal basis of the vector space $\frak V$.
If the spectrum of $\textsf{\textbf{A}}$ is degenerate, then the projection can be chosen to be the orthogonal sum of projections
corresponding to orthogonal eigenvectors, associated with the same  eigenvalues.

Furthermore, for a  Hermitian (or, more general, normal) operator $\textsf{\textbf{A}}$,
if $1\le i \le k$,
then there exist polynomials with real coefficients, such as,  for instance,
\begin{equation}
p_i  (t)
=
\prod_{
\begin{array}{c}
1\le j\le k\\
j\neq i
\end{array}
}
\frac{t-\lambda_j}{\lambda_i -\lambda_j}
\label{2011-m-epsf}
\end{equation}
so that
$p_i(\lambda_j) =\delta_{ij}$;
moreover, for every such polynomial,
$p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$.



With the help of the polynomial $p_i(t)$ defined in Eq. (\ref{2011-m-epsf}),
which requires knowledge of the eigenvalues,
the spectral form of a Hermitian (or, more general, normal) operator  $\textsf{\textbf{A}}$ can thus be rewritten as
\begin{equation}
\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i p_i(\textsf{\textbf{A}})=  \sum_{i=1}^k \lambda_i \prod_{
1\le j\le k,\;
j\neq i
}\frac{\textsf{\textbf{A}} - \lambda_j{\Bbb I}_n}{\lambda_i -\lambda_j}.
\end{equation}
That is, knowledge of all the eigenvalues entails construction
of all the projections in the spectral decomposition
of a normal transformation.




\section{Functions of normal transformations}
\label{2016-pu-book-chapter-qm-fono}
\index{Functions of normal transformation}

Suppose $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i  \textsf{\textbf{E}}_i $ is a normal transformation
in its spectral form.
If $f$ is an arbitrary complex-valued function defined at least at the eigenvalues of $\textsf{\textbf{A}}$,
then a linear transformation  $f(\textsf{\textbf{A}})$ can be defined by
\begin{equation}
f(\textsf{\textbf{A}})=
f\left(\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i\right)
=\sum_{i=1}^k f(\lambda_i)  \textsf{\textbf{E}}_i
 .
\end{equation}
Note that, if $f$ has a polynomial expansion such as analytic functions, then orthogonality and idempotence
\index{idempotence}
of the projections $\textsf{\textbf{E}}_i $ in the spectral form guarantees this kind of ``linearization.''

{
%\color{blue}
 % %\bexample
For the definition of the ``square root''
for every positive operator $\textsf{\textbf{A}})$, consider
\begin{equation}
\sqrt{\textsf{\textbf{A}}}=\sum_{i=1}^k \sqrt{\lambda_i}  \textsf{\textbf{E}}_i.
\end{equation}
With this definition,
$\left(\sqrt{\textsf{\textbf{A}}}\right)^2=
\sqrt{\textsf{\textbf{A}}}\sqrt{\textsf{\textbf{A}}}= {\textsf{\textbf{A}}}$.

Consider, for instance, the ``square root''  of the $\textsf{\textbf{not}}$ operator
\index{not operator}
\index{square root of not operator}
\begin{equation}
\textsf{\textbf{not}}
=
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}.
\end{equation}
To enumerate $\sqrt{\textsf{\textbf{not}}}$  we need to find the {\em spectral form} of $\textsf{\textbf{not}}$ first.
\index{spectral form}
The eigenvalues of  $\textsf{\textbf{not}}$ can be obtained by solving the
secular equation
\index{secular equation}
\begin{equation}
\text{det}
\left(
\textsf{\textbf{not}} - \lambda {\Bbb I}_2
\right)
=
\text{det}
\left(
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}
-
\lambda
\begin{pmatrix}
1&0\\  0&1
\end{pmatrix}
\right)
=
\text{det}
\begin{pmatrix}
 -\lambda&1\\
1&-\lambda
\end{pmatrix}
   =\lambda^2-1=0.
\end{equation}
$\lambda^2=1$ yields the two eigenvalues
$\lambda_1=1$
and
$\lambda_1=-1$.
The associated eigenvectors
${\bf x}_1$
and
${\bf x}_2$
can be derived from either the equations
$\textsf{\textbf{not}}\,{\bf x}_1={\bf x}_1$
and
$\textsf{\textbf{not}}\,{\bf x}_2=-{\bf x}_2$,
or inserting the eigenvalues into the polynomial~(\ref{2011-m-epsf}).

We choose the former method.
Thus, for $\lambda_1=1$,
\begin{equation}
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}
\begin{pmatrix}
x_{1,1}\\x_{1,2}
\end{pmatrix}
=\begin{pmatrix}
x_{1,1}\\x_{1,2}
\end{pmatrix}
,
\end{equation}
which yields  $x_{1,1}=x_{1,2}$, and thus, by normalizing the eigenvector,
${\bf x}_1=(1/\sqrt{2})(1,1)^\intercal $.
The associated projection is
\begin{equation}
\textsf{\textbf{E}}_1={\bf x}_1{\bf x}_1^\intercal =\frac{1}{2}
\begin{pmatrix}
 1&1\\  1&1
\end{pmatrix}
.
\end{equation}


Likewise, for $\lambda_2=-1$,
\begin{equation}
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}
\begin{pmatrix}
x_{2,1}\\x_{2,2}
\end{pmatrix}
=-\begin{pmatrix}
x_{2,1}\\x_{2,2}
\end{pmatrix}
,
\end{equation}
which yields  $x_{2,1}=-x_{2,2}$, and thus, by normalizing the eigenvector,
${\bf x}_2=(1/\sqrt{2})(1,-1)^\intercal $.
The associated projection is
\begin{equation}
\textsf{\textbf{E}}_2={\bf x}_2{\bf x}_2^\intercal =\frac{1}{2}
\begin{pmatrix}
 1&-1\\  -1&1
\end{pmatrix}
.
\end{equation}

Thus we are finally able to calculate
$\sqrt{\textsf{\textbf{not}}}$
through its spectral form
\begin{equation}
\begin{split}
\sqrt{\textsf{\textbf{not}}}=
\sqrt{\lambda_1}\textsf{\textbf{E}}_1 +
\sqrt{\lambda_2}\textsf{\textbf{E}}_2
=
\\=  \sqrt{1}
\frac{1}{2} \begin{pmatrix}
 1&1\\  1&1
\end{pmatrix}
+  \sqrt{-1}
\frac{1}{2} \begin{pmatrix}
 1&-1\\  -1&1
\end{pmatrix}
=
\\
=
\frac{1}{2}
\begin{pmatrix}
 1+i&1-i\\  1-i&1+i
\end{pmatrix}
=
\frac{1}{1-i}
\begin{pmatrix}
 1&-i\\  -i&1
\end{pmatrix}
.
\end{split}
\end{equation}
It can be readily verified that  $\sqrt{\textsf{\textbf{not}}}\sqrt{\textsf{\textbf{not}}}=\textsf{\textbf{not}}$.
Note that
$\pm_1 \sqrt{\lambda_1}\textsf{\textbf{E}}_1 +
\pm_2 \sqrt{\lambda_2}\textsf{\textbf{E}}_2$, where $\pm_1$ and $\pm_2$ represent separate cases,
yield alternative expressions of $\sqrt{\textsf{\textbf{not}}}$.

% sqrnot[apm_,    bpm_] := (1/2) (apm {{1, 1}, {1, 1}} + I*bpm {{1, -1}, {-1, 1}});

% sqrnot[1, 1].sqrnot[1, 1] == sqrnot[1, -1].sqrnot[1, -1] ==   sqrnot[-1, 1].sqrnot[-1, 1] == sqrnot[-1, -1].sqrnot[-1, -1]

 % %\eexample
}

\section{Decomposition of operators}
\index{decomposition}

\subsection{Standard decomposition}

In analogy to the decomposition of every imaginary number $z= \Re z +i \Im z$ with $\Re z,\Im z\in {\Bbb R}$,
every arbitrary transformation $\textsf{\textbf{A}}$ on a finite-dimensional vector space can be decomposed into two Hermitian operators
$\textsf{\textbf{B}}$
and
$\textsf{\textbf{C}}$
such that
\begin{equation}
\begin{split}
\textsf{\textbf{A}}=\textsf{\textbf{B}} + i \textsf{\textbf{C}}; \textrm{ with }  \\
\textsf{\textbf{B}}=\frac{1}{2}(\textsf{\textbf{A}} +   \textsf{\textbf{A}}^\dagger ), \\
\textsf{\textbf{C}}=\frac{1}{2i}(\textsf{\textbf{A}} -   \textsf{\textbf{A}}^\dagger ).
\end{split}
\end{equation}




\subsection{Polar representation}

In analogy to the polar representation of every imaginary number $z= R e^{i\varphi}$ with $R,\varphi \in {\Bbb R}$, $R>0$,
$0\le \varphi < 2\pi$,
every arbitrary transformation $\textsf{\textbf{A}}$ on a finite-dimensional inner product space can be decomposed into
a unique positive transform
$\textsf{\textbf{P}}$ and an isometry
$\textsf{\textbf{U}}$, such that $\textsf{\textbf{A}}= \textsf{\textbf{U}} \textsf{\textbf{P}}$.
If $\textsf{\textbf{A}}$ is invertible, then $\textsf{\textbf{U}}$  is uniquely determined by
$\textsf{\textbf{A}}$.
A necessary and sufficient condition that $\textsf{\textbf{A}}$ is normal is that
$\textsf{\textbf{U}} \textsf{\textbf{P}}=\textsf{\textbf{P}} \textsf{\textbf{U}} $.

\subsection{Decomposition of isometries}

Any unitary or orthogonal transformation   in finite-dimensional inner product space
\index{decomposition}
can be composed from a succession of two-parameter unitary transformations in
two-dimensional subspaces,
and a multiplication of a single diagonal matrix with elements of modulus one
in an algorithmic, constructive and tractable manner.
The method is similar to Gaussian elimination and facilitates the parameterization of elements
of the unitary group  in arbitrary dimensions (e.g., Ref. \cite{murnaghan}, Chapter 2).

{}     %\color{Purple}
It has been suggested to implement
these group theoretic results by realizing interferometric analogues
of any discrete unitary and Hermitian operator
in a unified and experimentally feasible way by ``generalized beam splitters''   \cite{rzbb,reck-94}.









\section{Commutativity}
\index{commutativity}

If $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
is the spectral form of a self-adjoint transformation  $\textsf{\textbf{A}}$
on a finite-dimensional inner product space,
then a necessary and sufficient condition (``if and only if $=$ iff'')
that a linear transformation
 $\textsf{\textbf{B}}$ commutes with
 $\textsf{\textbf{A}}$
is that it commutes with each
$\textsf{\textbf{E}}_i$, $1\le i\le k$.

{%%\color{OliveGreen} % %\bproof
Sufficiency is derived easily: whenever   $\textsf{\textbf{B}}$
commutes with all the procectors $\textsf{\textbf{E}}_i$, $1\le i\le k$
in the spectral decomposition
$\textsf{\textbf{A}}
=
\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i
$
of   $\textsf{\textbf{A}}$,
then it commutes with $\textsf{\textbf{A}}$; that is,
\begin{equation}
\begin{split}
 \textsf{\textbf{B}}\textsf{\textbf{A}}
=
\textsf{\textbf{B}}\left(\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i\right)
=
\sum_{i=1}^k \lambda_i \textsf{\textbf{B}}\textsf{\textbf{E}}_i
=
\\
=
\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i  \textsf{\textbf{B}}
=
\left(\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i \right)  \textsf{\textbf{B}}
=\textsf{\textbf{A}} \textsf{\textbf{B}}
.
\end{split}
\end{equation}

Necessity follows from the fact that, if  $\textsf{\textbf{B}}$
commutes with  $\textsf{\textbf{A}}$
then it also commutes with every polynomial of  $\textsf{\textbf{A}}$,
since in this case $\textsf{\textbf{A}}\textsf{\textbf{B}} = \textsf{\textbf{B}} \textsf{\textbf{A}}$, and thus
$
\textsf{\textbf{A}}^m \textsf{\textbf{B}}
=
\textsf{\textbf{A}}^{m-1} \textsf{\textbf{A}} \textsf{\textbf{B}}
=
\textsf{\textbf{A}}^{m-1}  \textsf{\textbf{B}} \textsf{\textbf{A}} =
\ldots = \textsf{\textbf{B}} \textsf{\textbf{A}}^m$.
In particular, it commutes with the polynomial $p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$
defined by Eq.~(\ref{2011-m-epsf}).
 % %\eproof
}

If $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
and
$\textsf{\textbf{B}}=\sum_{j=1}^l \mu_j\textsf{\textbf{F}}_j$
are the spectral forms of a self-adjoint transformations
$\textsf{\textbf{A}}$ and $\textsf{\textbf{B}}$
on a finite-dimensional inner product space,
then a necessary and sufficient condition (``if and only if $=$ iff'')
that  $\textsf{\textbf{A}}$ and
 $\textsf{\textbf{B}}$ commute
is that the projections
$\textsf{\textbf{E}}_i$, $1\le i\le k$
and
$\textsf{\textbf{F}}_j$, $1\le j\le l$
commute with each other; i.e.,
$\left[\textsf{\textbf{E}}_i,\textsf{\textbf{F}}_j\right] =
\textsf{\textbf{E}}_i \textsf{\textbf{F}}_j-
\textsf{\textbf{F}}_j\textsf{\textbf{E}}_i =0 $.

{%%\color{OliveGreen} % %\bproof
Again, sufficiency can be derived as follows: suppose all projection operators $\textsf{\textbf{F}}_j$, $1\le j\le l$
occurring in the spectral decomposition of $\textsf{\textbf{B}}$
commute with all projection operators $\textsf{\textbf{E}}_i$, $1\le i\le k$
in the spectral composition of   $\textsf{\textbf{A}}$,
then
\begin{equation}
\begin{split}
 \textsf{\textbf{B}}\textsf{\textbf{A}}
=
\left(\sum_{j=1}^l \mu_j\textsf{\textbf{F}}_j\right) \left(\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i\right)
=
\sum_{i=1}^k \sum_{j=1}^l \lambda_i \mu_j \textsf{\textbf{F}}_j\textsf{\textbf{E}}_i
=
\\
=
\sum_{j=1}^l \sum_{i=1}^k \mu_j \lambda_i \textsf{\textbf{E}}_i \textsf{\textbf{F}}_j
=
\left(\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i \right)  \left(\sum_{j=1}^l \mu_j \textsf{\textbf{F}}_j\right)
=\textsf{\textbf{A}} \textsf{\textbf{B}}
.
\end{split}
\end{equation}


Necessity follows from the fact that, if $\textsf{\textbf{F}}_j$, $1\le j\le l$
commutes with  $\textsf{\textbf{A}}$
then, by the same argument as mentioned earlier,
it also commutes with every polynomial of  $\textsf{\textbf{A}}$;
and
hence also with $p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$ defined by Eq.~(\ref{2011-m-epsf}).
Conversely,
if $\textsf{\textbf{E}}_i$, $1\le i\le k$
commutes with  $\textsf{\textbf{B}}$
then it also commutes with every polynomial of  $\textsf{\textbf{B}}$;
and
hence also with the associated polynomial
$q_j(\textsf{\textbf{A}})=\textsf{\textbf{E}}_j$ defined by Eq.~(\ref{2011-m-epsf}).
 % %\eproof
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If
$\textsf{\textbf{E}}_{\bf x} = \vert {\bf x} \rangle \langle {\bf x} \vert$
and
$\textsf{\textbf{E}}_{\bf y} = \vert {\bf y} \rangle \langle {\bf y} \vert$
are two commuting projections (into one-dimensional subspaces of ${\frak V}$)
corresponding to the normalized vectors ${\bf x}$  and ${\bf y}$,
respectively; that is, if
$\left[
\textsf{\textbf{E}}_{\bf x}
,
\textsf{\textbf{E}}_{\bf y}
\right]=
\textsf{\textbf{E}}_{\bf x}
\textsf{\textbf{E}}_{\bf y}
-
\textsf{\textbf{E}}_{\bf y}
\textsf{\textbf{E}}_{\bf x}
=0$,
then they are either identical (the vectors are collinear) or orthogonal (the vectors ${\bf x}$ is orthogonal to ${\bf y}$).


{%%\color{OliveGreen} % %\bproof
For a proof,
note that if $\textsf{\textbf{E}}_{\bf x}$ and $\textsf{\textbf{E}}_{\bf y}$ commute, then
$\textsf{\textbf{E}}_{\bf x}\textsf{\textbf{E}}_{\bf y} =  \textsf{\textbf{E}}_{\bf y}    \textsf{\textbf{E}}_{\bf x}$; and
hence $
\vert {\bf x} \rangle \langle {\bf x} \vert {\bf y} \rangle \langle {\bf y} \vert
=
\vert {\bf y} \rangle \langle {\bf y} \vert {\bf x} \rangle \langle {\bf x} \vert $. Thus,
$
(
\langle {\bf x} \vert {\bf y} \rangle )\vert {\bf x} \rangle  \langle {\bf y}  \vert
=
(\overline{\langle {\bf x} \vert {\bf y} \rangle})
\vert {\bf y} \rangle  \langle {\bf x} \vert  ,
$
which, applied to arbitrary vectors $ \vert   v\rangle \in {\frak V}$, is only true if either ${\bf x} = \pm {\bf y}$,
or if ${\bf x} \perp {\bf y}$ (and thus $\langle {\bf x} \vert {\bf y} \rangle =0$).
 % %\eproof
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Suppose some set $\textsf{\textbf{M}}
=
\{
\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\}
$
of  self-adjoint transformations on a finite-dimensional inner product space.
These  transformations  $\textsf{\textbf{A}}_i \in \textsf{\textbf{M}}$, $1\le i \le k$
are mutually commuting -- that is, $[\textsf{\textbf{A}}_i,\textsf{\textbf{A}}_j] =0$ for all $1\le i,j \le k$ -- if and only if there exists
a {\em maximal} (with respect to the set $\textsf{\textbf{M}}$) self-adjoint transformation  $\textsf{\textbf{R}}$ and
a set of real-valued functions
$F
=
\{
f_1,
f_2,
\ldots ,
f_k
\}
$ of a real variable so that
$
\textsf{\textbf{A}}_1=f_1 (\textsf{\textbf{R}})
$,
$
\textsf{\textbf{A}}_2=f_2 (\textsf{\textbf{R}})
$,
$\ldots $,
$\textsf{\textbf{A}}_k=f_k (\textsf{\textbf{R}})$.
If such a {\em maximal operator} $\textsf{\textbf{R}}$ exists, then
it can be written as a function of all transformations in the set $\textsf{\textbf{M}}$; that is,
\begin{equation}
\textsf{\textbf{R}}
=
G \left( \textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\right),
\label{2016-pu-book-chapter-qm-fdvst-mo}
\end{equation}
where $G$ is a suitable real-valued function of $n$ variables
(cf. Ref.~\cite{v-neumann-31}, Satz 8, or Ref.~\cite[\S~84]{halmos-vs}).
\index{maximal operator}
\index{maximal transformation}

{%%\color{OliveGreen} % %\bproof
For a proof involving two operators $\textsf{\textbf{A}}_1$ and $\textsf{\textbf{A}}_2$ we note that
sufficiency  can be derived from   commutativity, which follows from
$
\textsf{\textbf{A}}_1\textsf{\textbf{A}}_2=f_1 (\textsf{\textbf{R}}) f_2 (\textsf{\textbf{R}}) =
f_1 (\textsf{\textbf{R}}) f_2 (\textsf{\textbf{R}})= \textsf{\textbf{A}}_2\textsf{\textbf{A}}_1
$.

Necessity follows by first noticing that, as derived earlier, the projection operators
$\textsf{\textbf{E}}_i$ and $\textsf{\textbf{F}}_j$
in the spectral forms of $\textsf{\textbf{A}}_1=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
and
$\textsf{\textbf{A}}_2=\sum_{j=1}^l \mu_j\textsf{\textbf{F}}_j$ mutually commute; that is,
$
\textsf{\textbf{E}}_i \textsf{\textbf{F}}_j
=
\textsf{\textbf{F}}_j \textsf{\textbf{E}}_i
$.

For the sake of construction, design $g(x,y)\in {\Bbb R}$ to be any real-valued function (which can be a polynomial) of two real variables $x,y \in {\Bbb R}$ with the property
that all the coefficients  $c_{ij} = g( \lambda_i,\mu_j )$ are distinct.
Next, define  the maximal operator $\textsf{\textbf{R}}$ by
\begin{equation}
\textsf{\textbf{R}} = g(\textsf{\textbf{A}}_1, \textsf{\textbf{A}}_2) =  \sum_{i=1}^k \sum_{j=1}^l c_{ij}  \textsf{\textbf{E}}_i\textsf{\textbf{F}}_j,
\end{equation}
and the two functions $f_1$ and $f_2$ such that
$f_1(c_{ij}) = \lambda_i$, as well as
$f_2(c_{ij}) = \mu_j$, which result in
\begin{equation}
\begin{split}
f_1(\textsf{\textbf{R}}) = \sum_{i=1}^k \sum_{j=1}^j f_1(c_{ij})  \textsf{\textbf{E}}_i\textsf{\textbf{F}}_j = \sum_{i=1}^k \sum_{j=1}^j \lambda_i  \textsf{\textbf{E}}_i\textsf{\textbf{F}}_j
= \left( \sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i\right) \underbrace{\left(\sum_{j=1}^l \textsf{\textbf{F}}_j\right)}_{\Bbb I} = \textsf{\textbf{A}}_1
\\
f_2(\textsf{\textbf{R}}) = \sum_{i=1}^k \sum_{j=1}^j f_2(c_{ij})  \textsf{\textbf{E}}_i\textsf{\textbf{F}}_j = \sum_{i=1}^k \sum_{j=1}^j \mu_j  \textsf{\textbf{E}}_i\textsf{\textbf{F}}_j
= \underbrace{\left(\sum_{i=1}^k \textsf{\textbf{E}}_i\right)}_{\Bbb I}\left( \sum_{j=1}^l \mu_j \textsf{\textbf{F}}_j\right)  = \textsf{\textbf{A}}_2
.
\end{split}
\end{equation}
 % %\eproof
}

The  maximal operator $\textsf{\textbf{R}}$ can be interpreted as
encoding or containing all the information of a collection of commuting operators at once.
Stated pointedly, rather than to enumerate all the $k$ operators in $\textsf{\textbf{M}}$
separately,
a single maximal operator  $\textsf{\textbf{R}}$  represents  $\textsf{\textbf{M}}$;
in this sense, the operators  $\textsf{\textbf{A}}_i \in \textsf{\textbf{M}}$
are all just (most likely incomplete) {\em aspects}  of --
or individual, ``lossy'' (i.e., many-to-one) functional views on -- the  maximal operator $\textsf{\textbf{R}}$.

\newcommand{\comment}[1]{}
\comment{


a = {{0, 1, 0}, {1, 0, 0}, {0, 0, 0}};
b = {{2, 3, 0}, {3, 2, 0}, {0, 0, 0}};
c = {{5, 7, 0}, {7, 5, 0}, {0, 0, 11}};

Commutator[x_,y_]=x.y-y.x;

MatrixForm[Commutator[a, b]]
MatrixForm[Commutator[a, c]]
MatrixForm[Commutator[b, c]]


Eigensystem[a]
Eigensystem[b]
Eigensystem[c]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};

Solve[a.{{x11, x12,x13},
{x21, x22,x23},
{x31, x32,x33}}-{{x11, x12,x13},
{x21, x22,x23},
{x31, x32,x33}}.a ==  {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}},{x11, x12,x13,x21, x22,x23,x31, x32,x33}]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};
b = {{0, 0, 2}, {0, 0, 0}, {0, 0, 0}};

Solve[{a.{{x11, x12, x13}, {x21, x22, x23}, {x31, x32, x33}} - {{x11,
       x12, x13}, {x21, x22, x23}, {x31, x32, x33}}.a == {{0, 0,
     0}, {0, 0, 0}, {0, 0, 0}},
  b.{{x11, x12, x13}, {x21, x22, x23}, {x31, x32, x33}} - {{x11, x12,
       x13}, {x21, x22, x23}, {x31, x32, x33}}.b == {{0, 0, 0}, {0, 0,
      0}, {0, 0, 0}}}, {x11, x12, x13, x21, x22, x23, x31, x32, x33}]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};
b = {{0, 0, 2}, {0, 0, 0}, {0, 0, 0}};
c = {{3, 5, 7}, {0, 3, 0}, {0, 0, 3}};

Commutator[x_,y_]=x.y-y.x;

MatrixForm[Commutator[a, b]]
MatrixForm[Commutator[a, c]]
MatrixForm[Commutator[b, c]]
}


{%\color{blue}
 % %\bexample
Let us demonstrate the machinery developed so far by an example.
Consider the normal matrices
$$
\textsf{\textbf{A}} = %\left(
\begin{pmatrix}
0& 1& 0\\ 1& 0& 0\\ 0& 0& 0
\end{pmatrix},\;
\textsf{\textbf{B}} = %\left(
\begin{pmatrix}
2& 3& 0\\ 3& 2& 0\\ 0& 0& 0
\end{pmatrix},\;
\textsf{\textbf{C}} = %\left(
\begin{pmatrix}
5& 7& 0\\ 7& 5& 0\\ 0& 0& 11
\end{pmatrix},
$$
which are mutually commutative; that is,
$
[\textsf{\textbf{A}}, \textsf{\textbf{B}}]=
\textsf{\textbf{A}} \textsf{\textbf{B}}-\textsf{\textbf{B}}\textsf{\textbf{A}}=
[\textsf{\textbf{A}}, \textsf{\textbf{C}}]=
\textsf{\textbf{A}} \textsf{\textbf{C}}-\textsf{\textbf{B}}\textsf{\textbf{C}}=
[\textsf{\textbf{B}}, \textsf{\textbf{C}}]=
\textsf{\textbf{B}} \textsf{\textbf{C}}-\textsf{\textbf{C}}\textsf{\textbf{B}}=0$.

The eigensystems -- that is, the set of the set of eigenvalues and the set of the associated eigenvectors -- of $\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$
and
$\textsf{\textbf{C}}$
are
\begin{equation}
\begin{split}
\{\{1,-1,  0\}, \{(1, 1, 0)^\intercal , (-1, 1, 0)^\intercal , (0, 0, 1)^\intercal \}\} ,\\
\{\{5, -1, 0\},  \{(1, 1, 0)^\intercal , (-1, 1, 0)^\intercal , (0, 0, 1)^\intercal \}\},\\
\{\{12, -2, 11\},  \{(1, 1, 0)^\intercal , (-1, 1, 0)^\intercal , (0, 0, 1)^\intercal \}\}.
\end{split}
\end{equation}
They share a common orthonormal set of eigenvectors
$$
\left\{
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1\\ 1\\ 0
\end{pmatrix},
\frac{1}{\sqrt{2}}
\begin{pmatrix}
-1\\ 1\\ 0
\end{pmatrix},
\begin{pmatrix}
0\\ 0\\ 1\end{pmatrix}
\right\}
$$
which form an orthonormal basis of ${\Bbb R}^3$ or ${\Bbb C}^3$.
The associated projections are obtained by the outer (dyadic or tensor) products
\index{outer product}
\index{dyadic product}
\index{tensor product}
of these vectors; that is,
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_1= \frac{1}{2}
\begin{pmatrix}
1& 1& 0\\
1& 1& 0\\
0& 0& 0
\end{pmatrix},\\
\textsf{\textbf{E}}_2= \frac{1}{2}
\begin{pmatrix}
1& -1& 0\\
-1& 1& 0\\
0& 0& 0
\end{pmatrix},\\
\textsf{\textbf{E}}_3=
\begin{pmatrix}
0& 0& 0\\
0& 0& 0\\
0& 0& 1
\end{pmatrix}.
\end{split}
\end{equation}
Thus the spectral decompositions of
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$  and
$\textsf{\textbf{C}}$ are
\begin{equation}
\begin{split}
\textsf{\textbf{A}}= \textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0  \textsf{\textbf{E}}_3,\\
\textsf{\textbf{B}}= 5\textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0 \textsf{\textbf{E}}_3,\\
\textsf{\textbf{C}}= 12\textsf{\textbf{E}}_1  -2 \textsf{\textbf{E}}_2  + 11\textsf{\textbf{E}}_3,
\end{split}
\label{2011-m-empc}
\end{equation}
respectively.


One way to define the  maximal operator  $\textsf{\textbf{R}}$ for this problem
would be
$$
\textsf{\textbf{R}} = \alpha \textsf{\textbf{E}}_1  + \beta \textsf{\textbf{E}}_2  + \gamma  \textsf{\textbf{E}}_3,
$$
with
$\alpha ,  \beta ,   \gamma \in {\Bbb R}-0$ and
$\alpha  \neq \beta  \neq   \gamma \neq \alpha  $.
The functional coordinates
$f_i(\alpha )$, $f_i(\beta)$, and $f_i(\gamma)$,
$i\in \{\textsf{\textbf{A}},\textsf{\textbf{B}},\textsf{\textbf{C}}\}$,  of the three functions
$ f_\textsf{\textbf{A}}(\textsf{\textbf{R}})$,
$ f_\textsf{\textbf{B}}(\textsf{\textbf{R}})$, and
$ f_\textsf{\textbf{C}}(\textsf{\textbf{R}})$
chosen to match the projection coefficients obtained in Eq. (\ref{2011-m-empc});
that is,
\begin{equation}
\begin{split}
\textsf{\textbf{A}}= f_\textsf{\textbf{A}}(\textsf{\textbf{R}})=  \textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0  \textsf{\textbf{E}}_3,\\
\textsf{\textbf{B}}=  f_\textsf{\textbf{B}}(\textsf{\textbf{R}})= 5\textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0 \textsf{\textbf{E}}_3,\\
\textsf{\textbf{C}}=  f_\textsf{\textbf{C}}(\textsf{\textbf{R}})= 12\textsf{\textbf{E}}_1  -2 \textsf{\textbf{E}}_2  + 11\textsf{\textbf{E}}_3.
\end{split}
\end{equation}
As a consequence, the functions
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$,
$\textsf{\textbf{C}}$ need to satisfy the relations
\begin{equation}
\begin{split}
f_\textsf{\textbf{A}}(\alpha ) =1,\; f_\textsf{\textbf{A}}(\beta ) =-1,\; f_\textsf{\textbf{A}}(\gamma ) =0,\\
f_\textsf{\textbf{B}}(\alpha ) =5,\; f_\textsf{\textbf{B}}(\beta ) =-1,\; f_\textsf{\textbf{B}}(\gamma ) =0,\\
f_\textsf{\textbf{C}}(\alpha ) =12,\; f_\textsf{\textbf{C}}(\beta ) =-2,\; f_\textsf{\textbf{C}}(\gamma ) =11.
\end{split}
\label{2011-m-empc-fsr}
\end{equation}

 % %\eexample
}

It is no coincidence that the projections in the spectral forms of
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$  and
$\textsf{\textbf{C}}$ are identical.
Indeed it can be shown that mutually commuting {\em normal operators} always share the same eigenvectors; and thus also the same projections.
\index{normal operator}
\index{normal transformation}

Let the set $\textsf{\textbf{M}}
=
\{
\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\}
$
be mutually commuting  normal (or Hermitian, or self-adjoint) transformations on an $n$-dimensional inner product space.
Then there exists an orthonormal basis
${\frak B}= \{
\vert {\bf f}_1 \rangle ,
\ldots ,
\vert {\bf f}_n \rangle $
such that every $\vert {\bf f}_j \rangle \in {\frak B}$  is an eigenvector  of each of the $\textsf{\textbf{A}}_i \in  \textsf{\textbf{M}}$.
Equivalently, there exist $n$ orthogonal projections  (let the vectors ${\bf f}_j$ be represented by the coordinates which are column vectors)
$\textsf{\textbf{E}}_j= \vert {\bf f}_j\rangle \langle {\bf f}_j \rangle$
such that every $\textsf{\textbf{E}}_j$, $1\le j\le n$ occurs in the spectral form of each of the $\textsf{\textbf{A}}_i \in  \textsf{\textbf{M}}$.


Informally speaking,
a ``generic'' maximal operator $\textsf{\textbf{R}}$ on an $n$-dimensional Hilbert space ${\frak V}$
can be interpreted in terms of a particular orthonormal basis
$\{\vert {\bf f}_1 \rangle ,\ldots ,\vert {\bf f}_n \rangle \}$ of ${\frak V}$
-- indeed, the $n$ elements of that basis would have to correspond to the projections occurring
in the spectral decomposition of the self-adjoint operators
generated by $\textsf{\textbf{R}}$.



Likewise, the ``maximal knowledge'' about a quantized physical system -- in terms of empirical operational quantities --
would correspond to such a single maximal operator;
or to the orthonormal basis corresponding to the spectral decomposition of it.
Thus it might not be unreasonable to speculate that a particular (pure) physical state is best characterized by a particular orthonormal basis.


\documentclass[%
 %reprint,
 superscriptaddress,
 %groupedaddress,
 %unsortedaddress,
 %runinaddress,
 %frontmatterverbose,
 preprint,
 %onecolumn,
 showpacs,
 showkeys,
 %preprintnumbers,
 nofootinbib,
 %nobibnotes,
 %bibnotes,
  amsmath,amssymb,
  aps,
 % prl,
 %pra,
 %prb,
 % rmp,
 %prstab,
 %prstper,
  longbibliography,
  floatfix,
  %lengthcheck,%
 ]{revtex4-1}

\usepackage[normalem]{ulem}

\usepackage{adjustbox}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm} %bold math
\usepackage{graphicx}

\RequirePackage{times}
\RequirePackage{mathptm}

\usepackage{url}
%\usepackage{yfonts}
%\usepackage{color}
\usepackage[x11names]{xcolor}
\usepackage{eepic}
\usepackage{tikz}
\usepackage{epstopdf}
%\usepackage{pict2e}

\sloppy

\newtheorem{theorem}{Theorem}
\newtheorem{comment}{Comment}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}{Fact}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newcommand{\seq}[1]{\mathbf{#1}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\rest}[2]{#1\!\!\restriction_{#2}}
\newcommand{\reste}[2]{#1\restriction_{#2}}
\newcommand{\N}{\mathbb{N}}%      \N   == \mathbb{N}
\newcommand{\Z}{\mathbb{Z}}%      \Z   == \mathbb{Z}
\newcommand{\Q}{\mathbb{Q}}%      \Q   == \mathbb{Q}
\newcommand{\R}{\mathbb{R}}%      \R   == \mathbb{R}
\newcommand{\C}{\mathbb{C}}
\newcommand{\alphabet}{\{0,1\}}
\newcommand{\B}{B^*}%        \X  == \Sigma^*
\newcommand{\BI}{B^\omega}%        \XI  == \Sigma^\infty
\newcommand{\x}{\mathbf{x}}
\newcommand{\dom}{\text{dom}}
\newcommand{\cl}{\text{cl}}
\newcommand{\dd}{\mathrm{d}}


\newcommand{\bra}[1]{\left< #1 \right|}
\newcommand{\ket}[1]{\left| #1 \right>}

\newcommand{\iprod}[2]{\langle #1 | #2 \rangle}
\newcommand{\mprod}[3]{\langle #1 | #2 | #3 \rangle}
\newcommand{\oprod}[2]{| #1 \rangle\langle #2 |}


\begin{document}

\title{Quantum value indefiniteness put to the extreme}

\author{Karl Svozil}
\email{svozil@tuwien.ac.at}
\homepage{http://tph.tuwien.ac.at/~svozil}

\affiliation{Institute for Theoretical Physics,
Vienna  University of Technology,
Wiedner Hauptstrasse 8-10/136,
1040 Vienna,  Austria}

\affiliation{Department of Computer Science, University of Auckland,
Private Bag 92019, Auckland, New Zealand}

\date{\today}

\begin{abstract}
This is a brief review and interpretation of  recent ``maximal'' theorems on quantum value indefinite observables by Abbott, Calude, Conder and the author,
prepared for the IQOQI Vienna blog.
\end{abstract}

%\pacs{03.65.Aa, 03.65.Ta, 03.65.Ud, 03.67.-a}
\keywords{Kochen-Specker theorem, quantum value indefiniteness, quantum indeterminism, quantum randomness}
%\preprint{CDMTCS preprint nr. x}

\maketitle

One of the mind-boggling features of quantum mechanics is that,
if one is willing to accept counterfactuals in lieu of observables, its formalism
perfectly predicts certain experimentally observed frequencies; whereas
straightforward and seemingly unsuspicious classical assumptions
-- most prominently non-contextual realism and locality -- yield predictions which have been falsified.
While in the Boole-Bell approach~\cite{Pit-94} these discrepancies appear to occur merely probabilistically
and not necessarily for all particles contributing to the measurements~\cite{peres222},
the theorems of Kochen-Specker as well as
Greenberger, Horne and Zeilinger reveal that this clash with classicality can be sharpened to occur with certainty
on particular (finitely many) observables of single particles.

Pointedly stated the situation is this:
relative to the (classical) assumptions -- mainly non-contextuality of the predictions of common observable outcomes in intertwining contexts as well as
overall and total consistent predictability --
there does not exist any truth assignment for certain (finite) collections of observables;
and thus also no overall, uniform value definiteness -- because if there were such a
value definite truth assignment it would be globally inconsistent.
And if there are no consistent truth assignments on such (potentially measurable) observables,
these observables must not all simultaneously have a definite value prior to and independent of measurement;
let alone deterministic laws causing these hypothetical definite values in the sense of the principle of sufficient reason --
simply because if there would be such sufficient reason (in a classical sense) for a totality of observables involved,
these (actually measured and counterfactual) properties would not be indeterminate, and thus again would result in a complete contradiction.
Consequently, relative to the assumptions made, many physicists are inclined to accept
indeterminism as irreducible and ontological~\cite{zeil-05_nature_ofQuantum};
and both formally provable as well as empirically falsifiable.

Alas when it comes to this sort of quantum value indefiniteness, a rather subtle issue needs to be acknowledged:
all the theorems cited so far relied on {\em global}
(at least on the finite sets of observables facilitating their proofs) {\em truth assignments.}
So far and until very recently, any inconsistency proof by contradiction, or any argument about discrepancies with classical predictions, relied on some sufficient violation of classicality
occurring at least once {\em somewhere} along the argument -- in the case of violations of Boole-Bell type inequalities
not even necessarily for every single particle~\cite{peres222}.
Indeed, by design these arguments have never intended to actually {\em quantify} the magnitude of indeterminism; simply because an inevitable breach of classicality at some
intertwining~\cite{Gleason} contexts of observables
(maximal operators~\cite{kochen1})
suffices to render either a proof by contradiction of the respective theorem,
or a consistent -- yet contextual -- truth assignment (which, if contextual, could be perfectly causal;
we shall not pursue this option further).

This makes it impossible to localize and certify quantum indeterminism on any particular observable, because a malicious demon could maintain
value definiteness as well as deterministic causality on
the observable measured whilst breaching classicality elsewhere in the network of intertwined contexts woven more~\cite{cabello:210401} or less tightly.
For the sake of demonstration, let us recall the beautiful
{\em Logical Indeterminacy Principle,} a
Kochen-Specker type theorem of Pitovsky~\cite{pitowsky:218} and Hrushovski and Pitovsky~\cite{hru-pit-2003}:
Any two one-dimensional projection operators $A$ and $B$ (interpretable as propositions~\cite{v-neumann-49,birkhoff-36})
can be interconnected via a finite number of intertwined contexts
such that a total truth value assignment on all these intertwined observables exists if and only if
the initial two  projection operators $A$ and $B$ are both assigned falsity.
That is, if the first observable $A$ is true, then $B$ (by a Kochen-Specker-type proof by contradiction)
must neither be true nor false -- both cases yield a complete contradiction.
However, Pitowsky's Logical Indeterminacy Principle cannot be used to argue that if one prepares a pure state
associated with the one-dimensional projection operator $A$ and measures $B$, then $B$ is necessarily indeterminate:
those proofs merely require a breakdown of at least one (counterfactual) observable ``somewhere'' along the intertwined contexts;
and this could, but need not necessarily, be $B$.

A new type of theorems, as exposed by Abbott, Calude, Conder and the author
in a series of papers~\cite{2012-incomput-proofsCJ,PhysRevA.89.032109,2015-AnalyticKS}
remedies this lack of locatedness of breakdown in classicality by pinpointing the exact location of classicality breach on a per particle basis.
This facilitates a certification of quantum indeterminism -- as always means relative to the physical assumptions entering the argument.
One direct application is the construction of random number generators certified by (and relative to) this type of quantum value indefiniteness.

To present a taste of the argument, the two main assumptions concern the admissibility of classical truth assignments,
which can be readily operationalized by, for instance, generalized lossless beam splitters in $n>2$ dimensions
(as in all such Gleason-Kochen-Specker type arguments, this condition assures the possibility to intertwine~\cite{Gleason} and connect contexts; that is, maximal observables):
a) whenever one output port fires, the other output ports remain dormant; and
b) whenever all but one output ports remain dormant, the only remaining port fires.
Otherwise the measurement cannot be assigned any definite value; that is, in functional terms the value assignment
is {\em partial.}
Again, as in all Gleason-Kochen-Specker type arguments, it is additionally assumed that the observables are non-contextual --
that is, if two or more contexts are intertwined, then the measurement outcomes of the observables they share and have in common are supposed
to be independent of the particular context measured alongside.

Our findings are, with respect to abundance, of the strongest possible form: virtually all observables that could in principle be indeterminate and value indefinite, are so.
In terms of the quantum logical Hilbert space formalism, we prove that, again relative to the assumptions mentioned earlier,
if the system is prepared in a pure state, then all potential propositions (corresponding to one-dimensional projection observables)
which are not collinear or orthogonal to the preparation state
-- that is, the ones whose associated one-dimensional projection operators do not commute with the one-dimensional projection
operator of the state prepared  -- are value indefinite.
That is, besides the original state prepared, and its orthogonal subspace,
quantum value indefiniteness occurs not at any isolated points or regions, but {\em everywhere} on the unit ball of ($n>2$ dimensional) Hilbert space.
The remaining observables  of ``meagre'' measure zero are all pre-defined by the state preparation.

This result applies to any single outcome: suppose a particle is prepared in a given pure state and measured
along an observable which is neither collinear nor orthogonal to the prepared state,
then the assumptions, via the extended Kochen-Specker theorems advertised earlier~\cite{2012-incomput-proofsCJ,PhysRevA.89.032109,2015-AnalyticKS},
render quantum value indefiniteness.
Thus, with respect to collections of observables, the results reveal the ``maximal amount of quantum value indeterminacy.''


(Meta-)physically, this situation inspires various alternatives.
Probably the most prominent, orthodox, interpretation of the theorems is in terms of yet another, in some sense extreme,
corroboration of Born's inclinations~\cite[p.~866]{born-26-1}: to give up determinism in the world of atoms.
Because, evidently, if one succeeds to measure a value indefinite observable, the outcome cannot have a predetermined value.
Consequently, the bits ``enforced'' by measurements occur without any sufficient cause; quasi {\it ex nihilo} --
in theological terms rather by {\it creatio continua}.

I am inclined to consider another, quasi-deterministic interpretation of the theorems (which, I am aware, not necessarily coincides with the opinions of my co-authors)
-- thereby maybe qualifying myself as member of the church of the larger Hilbert space:
if there is a mismatch between preparation and measurement, then any such ``enforced'' measurement outcome cannot reveal (possibly hidden)
properties of the quantum -- because there evidently are none~\cite{zeil-05_nature_ofQuantum,zeil-bruk-99a,zeil-bruk-02}.
Rather the outcome reflects a ``translation process'' facilitated by a quasi-classical device
endowed with a huge number of degrees of freedom -- all contributing to the outcome allegedly and misleadingly reflecting a property of the original quantum.

While
ontologically,
if the quantum mechanical unitary state evolution is supposed to be uniformly valid,
by Cayley's theorem this amounts to a one-to-one evolution, an endless permutation which is the gist of determinism as well as reversibility;
epistemologically, that is, for all practical purposes and relative to the operational
means (that is: money, equipment, graduate students, postdocs and lifetime) available,
measurements involving many degrees of freedom of the measurement apparatus effectively cannot be erased or undone.
Thus irreversibility and the huge number of operationally inaccessible degrees of freedom of the
measurement apparatus amounts to an effective irreversibility and indeterminism.
This situation is almost identical to the means relative status of the second law of thermodynamics~\cite{Myrvold2011237}.
I encourage all those hesitant to accept this view to glance over the Hupty-Dumpty paper by Englert, Schwinger and Scully~\cite{engrt-sg-I,engrt-sg-II}
for some estimates of the difficulties encountered in reversing a Stern-Gerlach type spin measurement!

In my view, one decisive reason of the mind-boggling confusions so far caused by the quantum formalism is due to the fact that operationally we as
intrinsic observers are embedded in, and incapable of escaping, the very physical system we inhabit.
Thus, in platonistic terms, we are inclined to take the reflections and shadows on the wall for reality.
In quantum terms, reality could be identified with the (sometimes inaccessible) pure state in which the system is prepared and permutated;
and the shadow of it with the coherent superposition -- the burlesque cat state in-between life and death~\cite{schrodinger},
{\em ``our surroundings rapidly
turning into a quagmire, or sort of a featureless jelly or plasma, all contours
becoming blurred, we ourselves probably becoming jelly fish,''} as so vividly expressed
by Schr\"odinger~\cite{schroedinger-interpretation} -- of expansions along properties and quantum logical propositions which do not match the prepared and permutated state.
The Everettian~\cite{everett} and Wignerian~\cite{wigner:mb} critique of irreversible measurement does not require any multiverse in parallel --
it is just our pretension and forcing to
measure information the quantized system is neither prepared for nor encoded in which suggests that those superpositions are illusory,
inappropriate catalogues of expectation values.

Of course, this view implies that not all observables which we pretend to be able to measure, actually (that is, ontological)
exist as properties merely of the single quantum measured.
Rather the outcomes result from a translation process of great complexity and apparent stochasticity.
In this line of thought, counterfactuals are not fundamentally different from ``measuring'' an observable ``directly'' --
generally both are not dealing with reality {\it per se},
but are rather translations into the contexts envisioned and accessible to the observer.

Also, entanglement among quanta teaches us that properties can be encoded across quanta
while simultaneously excluding specifications of the properties of the individual quanta involved.
Consequently, any attempt from our side to measure single constituents of
these entanged multipartite states again results in measurement mismatches with respect to the holistic preparation and encoding (of joint properties)
of these entangled states;
and again in epistemic randomness.

In my vision, entanglement
and the one state interpretation of the novel theorems mentioned earlier
also forces us to completely reverse our way of thinking about space-time: rather than perceiving quantum events in a Kantian {\it a priori}
space arena (theatre), space-time needs to be operationally reconstructed from quantum mechanical outcomes.
It is not totally unreasonable to speculate that this may yield a radical revision
of how we perceive space-time as a theory of interconnectivity --resulting in spatio-temporal locations -- of quanta.


\bibliographystyle{aipnum4-1}
\bibliography{svozil}


\end{document}


\chapter{Multilinear Algebra and Tensors}
\label{ch:t}


What follows is a multilinear extension of the previous chapter.
Tensors will be introduced as multilinear forms,
and their transformation properties will be derived.


For many physicists the following derivations might appear confusing and overly formalistic
as they might have difficulties to ``see the forest for the trees.''
For those, a brief overview sketching the most important aspects of tensors might serve as a first orientation.

Let us start by defining, or rather {\em declaring} or supposing the following:  basis vectors of some given (base) vector space are said to ``(co-)vary.''
This is just a ``fixation,'' a designation of notation;
important insofar as it implies that the respective coordinates, as well as the dual basis vectors ``contra-vary;''
and the coordinates of dual space vectors ``co-vary.''

Based on this declaration or rather convention
--
that is,
relative to the behavior with respect to variations of scales of the reference axes (the basis vectors) in the base vector space
--
there exist two important categories: entities which co-vary, and entities which vary inversely, that is, contra-vary, with such changes.
\begin{itemize}

\item  Contravariant entities such as vectors in the base vector space:
These vectors of the base vector space are called contravariant because
their {\em components} contra-vary (that is, vary inversely) with respect to variations of the basis vectors.
By identification the components of contravariant vectors (or tensors) are also contravariant.
In general, a multilinear form on a vector space is called contravariant if its components
(coordinates) are contravariant; that is, they contra-vary with respect to variations of the basis vectors.
\index{contravariant vector}

\item
Covariant entities such as vectors in the dual space:
\marginnote{
The dual space is spanned by all linear functionals on that vector space (cf. Section~\ref{2011-m-dvs} on page \pageref{2011-m-dvs}).
}
The vectors of the dual space are called covariant because
their {\em components} contra-vary with respect to variations of the basis vectors of the dual space,
which in turn contra-vary with respect to variations of the basis vectors of the base space.
Thereby the double contra-variations (inversions) cancel out,
so that effectively the vectors of the dual space co-vary with the vectors of the basis of the base vector space.
By identification the components of covariant vectors (or tensors) are also covariant.
In general, a multilinear form on a vector space is called covariant if its components
(coordinates) are covariant; that is, they co-vary with respect to variations of the basis vectors of the base vector space.
\index{covariant vector}

\item
Covariant and contravariant indices will be denoted by subscripts (lower indices) and superscripts (upper indices), respectively.

\item
Covariant and contravariant entities transform inversely.
Informally, this is due to the fact that their changes must compensate each other,
as covariant and contravariant entities are ``tied together'' by some invariant
(id)entities such as vector encoding and dual basis formation.

\item
Covariant entities can be transformed into contravariant ones
by the application of metric tensors,
and, {\it vice versa,} by the inverse of  metric tensors.
\end{itemize}



\section{Notation}

In what follows, vectors and tensors will be encoded in terms of indexed coordinates or components
(with respect to a specific basis).
The biggest advantage is that such coordinates or components are scalars which can be
exchanged and rearranged according to commutativity, associativity and distributivity,
as well as differentiated.

Let us consider
\marginnote{For a more systematic treatment, see for instance Klingbeil's~\cite{Klingbeil}
or Dirschmid's\cite{Dirschmid} introductions.}
the vector space ${\frak V} = {\Bbb R}^n$ of dimension $n$.
A covariant basis
\marginnote{
For a detailed explanation of covariance and contravariance, see  Section~\ref{2016-m-tensor-cob} on page~\pageref{2016-m-tensor-cob}.
}
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n\}$ of ${\frak V}$
consists of
$n$ covariant basis vectors ${\bf e}_i$.
A contravariant basis
${\mathfrak B}^\ast =\{{\bf e}_1^\ast,{\bf e}_2^\ast,\ldots ,{\bf e}_n^\ast\}
= \{{\bf e}^1,{\bf e}^2,\ldots ,{\bf e}^n\}$ of the dual space ${\frak V}^\ast$
(cf. Section~\ref{2011-m-Dualbasis} on page \pageref{2011-m-Dualbasis})
consists of
$n$ basis vectors ${\bf e}_i^\ast$, where ${\bf e}_i^\ast = {\bf e}^i$ is just a different notation.

Every contravariant vector ${\bf x} \in {\frak V}$ can be coded by,  or expressed in terms of, its contravariant vector components
$X^1, X^2, \ldots , X^n \in {\Bbb R}$
by
${\bf x} =\sum_{i=1}^n X^{i} {\bf e}_{i}$.
Likewise, every covariant vector ${\bf x} \in {\frak V}^\ast$ can be coded by, or expressed in terms of, its covariant vector components
$X_1, X_2, \ldots , X_n \in {\Bbb R}$
by
${\bf x} =\sum_{i=1}^n X_{i} {\bf e}_{i}^\ast =\sum_{i=1}^n X_{i} {\bf e}^{i}$.
\marginnote{
Note that in both covariant and contravariant cases the upper-lower pairings ``${~\cdot_i}~\cdot^i$'' and  ``${~\cdot^i}~\cdot_i$''of the indices match.
}

Suppose that there are $k$ arbitrary contravariant vectors ${\bf x}_1,{\bf x}_2,\ldots ,{\bf x}_k$ in ${\frak V}$ which are indexed by a subscript
(lower index). This lower index should not  be confused with a covariant lower index.
Every such vector ${\bf x}_j$, $1 \le j \le k$ has contravariant vector components
$X^{1_j}_j, X^{2_j}_j, \ldots , X^{n_j}_j \in {\Bbb R}$
with respect to a particular basis ${\mathfrak B}$
such that
\marginnote{
This notation ``$X^{i_j}_j$'' for the $i$th component of the $j$th vector is redundant as it requires two indices $j$; we could have just denoted it by
``$X^{i_j}$.''
The lower index $j$ does {\em not} correspond to any covariant entity but just indexes the $j$th vector ${\bf x}_j$.
}
\begin{equation}
{\bf x}_j =\sum_{{i_j}=1}^n X^{i_j}_j{\bf e}_{{i_j}}.
\label{2016-m-tensor-contrvvs}
\end{equation}

Likewise, suppose that there are $k$ arbitrary covariant vectors ${\bf x}^1,{\bf x}^2,\ldots ,{\bf x}^k$ in the dual space ${\frak V}^\ast$ which are indexed by a superscript
(upper index). This upper index should not  be confused with a contravariant upper index.
Every such vector ${\bf x}^j$, $1 \le j \le k$ has covariant vector components
$X_{1_j}^j, X_{2_j}^j, \ldots , X_{n_j}^j \in {\Bbb R}$
with respect to a particular basis ${\mathfrak B}^\ast$
such that
\marginnote{
Again, this notation ``$X_{i_j}^j$'' for the $i$th component of the $j$th vector is redundant as it requires two indices $j$; we could have just denoted it by
``$X_{i_j}$.''
The upper index $j$ does {\em not} correspond to any contravariant entity but just indexes the $j$th vector ${\bf x}^j$.
}
\begin{equation}
{\bf x}^j =\sum_{{i_j}=1}^n X_{i_j}^j{\bf e}^{{i_j}}.
\label{2016-m-tensor-covvs}
\end{equation}

Tensors are constant with respect to variations of points of ${\Bbb R}^n$.
In contradistinction, {\em tensor fields} depend on points of ${\Bbb R}^n$ in a nontrivial (nonconstant) way.
Thus, the components of a tensor field
depend on the coordinates.
{
\color{blue}
\bexample
For example, the contravariant vector defined by the coordinates
$
\begin{pmatrix}
5.5,
3.7,
\ldots ,
10.9
\end{pmatrix}^T
$
with respect to a particular basis ${\mathfrak B}$
is a tensor; while, again with respect to a particular basis ${\mathfrak B}$,
$
\begin{pmatrix}
\sin X_1,
\cos X_2,
\ldots ,
e^{X_n}
\end{pmatrix}^T
$ or
$
\begin{pmatrix}
X_1,
X_2,
\ldots ,
X_n
\end{pmatrix}^T
$,
which depend on the coordinates $X_1, X_2, \ldots , X_n \in {\Bbb R}$,
are tensor fields.
\eexample
}


We adopt Einstein's summation convention to sum over equal indices.
If not explained otherwise (that is, for orthonormal bases) those pairs have exactly one lower and one upper index.


In what follows, the notations
``$x\cdot y$'',
``$(x,y)$'' and
``$\langle x\mid y\rangle $'' will be used synonymously for the {\em
scalar product}
or
{\em inner product}.
\index{scalar product}
\index{inner product}
Note, however, that the ``dot notation $x\cdot y$''
may be a little bit misleading; for example, in the case of the ``pseudo-Euclidean'' metric
represented by the matrix
 ${\rm diag}(+,+,+,\cdots ,+,-)$, it is no more the standard Euclidean dot product
${\rm diag}(+,+,+,\cdots ,+,+)$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Change of Basis}
\label{2016-m-tensor-cob}



\subsection{Transformation of the covariant basis}

Let
${\mathfrak B}$
and
${\mathfrak B'}$
be two arbitrary bases of
${\Bbb R}^n$.
Then every vector ${\bf f}_i$ of
${\mathfrak B'}$
can be represented as linear combination of basis vectors of
${\mathfrak B}$ [see also Eqs.~(\ref{2011-m-btbe}) and (\ref{2011-m-btbe-r})]:
\begin{equation}
{\bf f}_i=\sum_{j=1}^n {a^j}_i {\bf e}_j, \qquad i=1,\ldots , n
.
\label{2001-mu-tensors}
\end{equation}

%Formally, one may treat  ${\bf e}'_i$ and ${\bf e}_i$ as scalar variables $e'_i$ and $e_j$, respectively; such that ${a_i}^j ={\partial { e}'_i \over \partial { e}_j}$.
The matrix
\begin{equation}
\textsf{\textbf{A}}
\equiv {a^j}_i \equiv
\begin{pmatrix}
{a^1}_1&{a^1}_2& \cdots & {a^1}_n\\
{a^2}_1&{a^2}_2& \cdots & {a^2}_n\\
\vdots &\vdots & \ddots & \vdots \\
{a^n}_1&{a^n}_2& \cdots & {a^n}_n
\end{pmatrix}
.
\label{2016-m-ch-tensor-tm}
\end{equation}
is called the {\em transformation matrix}.
As defined in (\ref{2016-m-ch-fdvs-matrixind}) on page \pageref{2016-m-ch-fdvs-matrixind},
the second (from the left to the right), rightmost (in this case lower) index $i$ varying in row vectors is the column index;
and, the first, leftmost (in this case upper) index $j$  varying in columns is the row index, respectively.

%In terms of the matrix notation~(\ref{2016-m-fdvs-not}) introduced for Eq.~(\ref{2016-m-fdvs-rv}) on page~\pageref{2016-m-fdvs-not},
%Eq.~(\ref{2001-mu-tensors}) can be written as
%\begin{equation}
% (\textsf{\textbf{U}}')^T =  \textsf{\textbf{A}}^T  \textsf{\textbf{U}}^T \textrm{, or }  \textsf{\textbf{U}}' = \textsf{\textbf{U}}  \textsf{\textbf{A}}
%.
%\label{2016-mu-tensors}
%\end{equation}

Note that, as discussed earlier, it is necessary to fix a convention for the transformation of the covariant basis vectors discussed on page~\pageref{2016-m-ch-fdvs-oic}.
This then specifies the exact form of the (inverse, contravariant) transformation of the components or coordinates of vectors.



Perhaps not very surprisingly, compared to the transformation (\ref{2001-mu-tensors}) yielding the ``new'' basis
${\mathfrak B'}$  in terms of elements of the ``old'' basis ${\mathfrak B}$,
a transformation  yielding the ``old'' basis
${\mathfrak B}$  in terms of elements of the ``new'' basis ${\mathfrak B'}$ turns out to be just the inverse ``back'' transformation of the former:
substitution of (\ref{2001-mu-tensors}) yields
\begin{equation}
{\bf e}_i=\sum_{j=1}^n {{a'}^j}_i {\bf f}_j
=\sum_{j=1}^n {{a'}^j}_i  \sum_{k=1}^n {a^k}_j {\bf e}_k
=\sum_{k=1}^n \left(\sum_{j=1}^n  {{a'}^j}_i  {a^k}_j \right) {\bf e}_k,
\end{equation}
which, due to the linear independence of the basis vectors ${\bf e}_i$ of ${\mathfrak B}$,
can only be satisfied if
\begin{equation}
{a^k}_j  {{a'}^j}_i =\delta_i^k
\qquad
{\rm or}
\qquad
\textsf{\textbf{A}}\textsf{\textbf{A}}'={\Bbb I}.
\end{equation}

Thus $\textsf{\textbf{A}}'$ is the {\em inverse matrix}
$\textsf{\textbf{A}}^{-1}$  of $\textsf{\textbf{A}}$. In index notation,
\begin{equation}
{{a'}^j}_i ={{(a^{-1})}^j}_i
,
\label{2001-mu-tensor-tl2}
\end{equation}
and
\begin{equation}
{\bf e}_i=\sum_{j=1}^n {{(a^{-1})}^j}_i  {\bf f}_j
.
\label{2012-m-ch-tlcbv}
\end{equation}

\subsection{Transformation of the contravariant coordinates}


Consider an arbitrary contravariant vector ${\bf x} \in {\Bbb R}^n$ in two basis representations:
(i)
with contravariant components $X^i$ with respect to the basis
${\mathfrak B}$,
and  (ii) with ${ Y }^i$  with respect to the basis
${\mathfrak B'}$.
Then, because both coordinates with respect to the two different bases
have to encode the same vector, there has to be a ``compensation-of-scaling'' such that
\begin{equation}
{\bf x}
=\sum_{i=1}^n X^i {\bf e}_i
=\sum_{i=1}^n { Y }^i {\bf f}_i
.
\end{equation}
Insertion of the basis transformation (\ref{2001-mu-tensors}) and
relabelling of the indices $i \leftrightarrow j$ yields
\begin{equation}
\begin{split}
{\bf x}
=\sum_{i=1}^n X^i {\bf e}_i
=\sum_{i=1}^n { Y }^i {\bf f}_i
=\sum_{i=1}^n { Y }^i \sum_{j=1}^n {a^j}_i {\bf e}_j\\
=
\sum_{i=1}^n\sum_{j=1}^n {a^j}_i { Y }^i  {\bf e}_j
=
\sum_{j=1}^n\left[ \sum_{i=1}^n {a^j}_i  { Y }^i \right] {\bf e}_j
=
\sum_{i=1}^n\left[ \sum_{j=1}^n {a^i}_j  { Y }^j \right] {\bf e}_i
.
\end{split}
\label{2016-m-ch-tensor-tcvc}
\end{equation}
A comparison of coefficients
yields the transformation laws of vector components
[see also Eq.~(\ref{2012-m-ch-e-tl1})]
\begin{equation}
X^i   = \sum_{j=1}^n {a^i}_j  { Y }^j .
\label{2012-m-ch-di-choic}
\end{equation}

In the matrix notation introduced in Eq.~(\ref{2016-m-fdvs-rv}) on page~\pageref{2016-m-fdvs-rv},
(\ref{2012-m-ch-di-choic}) can  be written as
\begin{equation}
X   =  \textsf{\textbf{A}}  Y .
\label{2016-m-ch-di-choic-mn}
\end{equation}



A similar ``compensation-of-scaling'' argument using
(\ref{2012-m-ch-tlcbv})
yields the transformation laws for
\begin{equation}
{ Y }^j   = \sum_{i=1}^n {(a^{-1})^j}_i {X}^i
\label{2015-m-ch-tensor-tlcc}
\end{equation}
%Again, formally, we may treat  ${\bf f}_i$ and ${\bf e}_i$  as scalar variables $e'_i$ and $e_j$, respectively; such that ${{a'}_i}^j ={\partial {e}_i \over \partial { e}'_j}$.
with respect to the covariant basis vectors.
In the matrix notation introduced in Eq.~(\ref{2016-m-fdvs-rv}) on page~\pageref{2016-m-fdvs-rv},
(\ref{2015-m-ch-tensor-tlcc}) can simply be written as
\begin{equation}
 Y    =  \left(\textsf{\textbf{A}}^{-1}\right) X.
\label{2016-m-ch-di-tlcc-mn}
\end{equation}
%In this notation,
%\begin{equation}
%{\bf x} =
%\sum_{i=1}^n { Y }^i {\bf f}_i =
%\sum_{i=1}^n {\bf f}_i { Y }^i
%\equiv
%\textsf{\textbf{U}}'  Y
%=
%\textsf{\textbf{U}} \textsf{\textbf{A}}^T \left(\textsf{\textbf{A}}^{-1}\right)^T X
%=
%\textsf{\textbf{U}} \underbrace{\left(\textsf{\textbf{A}}^{-1}\textsf{\textbf{A}}\right)^T}_{{\Bbb I}_n} X
%= \textsf{\textbf{U}} X.
%\end{equation}


If the basis transformations involve nonlinear coordinate changes -- such as from the
Cartesian to the polar or spherical coordinates discussed later -- we have to employ differentials
\begin{equation}
dX^j   = \sum_{i=1}^n {a^j}_i \,d{ Y }^i  ,
\label{2012-m-ch-di-choic11}
\end{equation}
so that, by partial differentiation,
\begin{equation}
{a^j}_i ={\partial X^j \over \partial  Y ^i}   .
\label{2001-mu-tensor-tl11}
\end{equation}

By assuming that the coordinate transformations are linear, ${a_i}^j$ can be expressed in terms of the coordinates $X^j$
\begin{equation}
{a^j}_i =\frac{  X^j }{   Y ^i}  .
\label{2001-mu-tensor-tl1}
\end{equation}


Likewise,
\begin{equation}
d{ Y }^j   =
\sum_{i=1}^n {{({a^{-1}})}^j}_i\, d{X}^i,
\end{equation}
so that, by partial differentiation,
\begin{equation}
{{(a^{-1})}^j}_i =
{\partial { Y }^j \over \partial X^i}   =J_{ij},
\label{2001-mu-tensor-tl2nl}
\end{equation}
where $J_{ij}$ stands for
the {\em Jacobian matrix}
\index{Jacobian matrix}
\begin{equation}
J\equiv
J_{ij}={\partial { Y }^i\over \partial {X}^j }
\equiv
\begin{pmatrix}
{\partial { Y }^1\over \partial {X}^1}&\cdots&{\partial { Y }^1\over \partial {X}^n}\\
\vdots&\ddots &\vdots\\
{\partial { Y }^n\over \partial {X}^1}&\cdots&{\partial { Y }^n\over \partial {X}^n}
\end{pmatrix} .
\label{2013-m-t-jm}
\end{equation}


\subsection{Transformation of the contravariant basis}
\index{dual basis}
\index{reciprocal basis}
\index{contravariant basis}

Consider again, as a starting point, a covariant basis
${\mathfrak B}=\{{\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n\}$ consisting of
$n$ basis vectors ${\bf e}_i$.
A {\em contravariant basis} can be defined by identifying it with the dual basis
introduced earlier in Section~\ref{2011-m-Dualbasis} on page~\pageref{2011-m-Dualbasis},
in particular, Eq.~(\ref{2011-m-Dualbasis-e1}).
Thus a {\em contravariant} basis
${\mathfrak B^\ast}=\{{\bf e}^1,{\bf e}^2,\ldots ,{\bf e}^n\}$ is a set of $n$ covariant
basis vectors ${\bf e}^i$
which satisfy Eqs.~(\ref{2011-m-Dualbasis-e1})-(\ref{2011-m-Dualbasis-e3})
\begin{equation}
{\bf e}^j \left( {\bf e}_i \right) = \left[{\bf e}_i,{\bf e}^j \right] =  \left[{\bf e}_i,  {\bf e}_j^\ast \right] = \delta^j_i = \delta_{ij}
.
\label{2001-mu-tensors0}
\end{equation}
%[{\bf e}_i,  {\bf e}_j^\ast ]=[{\bf e}_i,{\bf e}^j]=[g_{il} {\bf e}^l, {\bf e}_k]  = [{\bf e}_i, g^{jk} {\bf e}_k]  =  \delta_{ij}
%$g$ stands for the metric tensor which will be defined later in Eqs.~(\ref{2016-m-ch-tensor-gij}) and (\ref{2016-m-ch-tensor-gij2} ).

In terms of the bra-ket notation, (\ref{2001-mu-tensors0}) somewhat superficially transforms into
(a formal justification for this identification is the Riesz representation theorem)
\index{Riesz representation theorem}
\index{Fr\'echet-Riesz representation theorem}
\begin{equation}
\left [ \vert {\bf e}_i \rangle , \langle  {\bf e}^j \vert \right]=
\langle  {\bf e}^j \vert {\bf e}_i \rangle =
  \delta_{ij}
.
\label{2001-mu-tensors0bk}
\end{equation}
Furthermore, the resolution of identity~(\ref{2016-m-ch-fdlws-roi}) can be rewritten as
\begin{equation}
 {\Bbb I}_n = \sum_{i=1}^n \vert {\bf e}^i \rangle \langle {\bf e}_i \vert
.
\label{2016-m-ch-tensor-roi}
\end{equation}

As demonstrated earlier in Eq.~(\ref{2016-m-fdlvs-recoverc}) the vectors ${\bf e}^\ast_i = {\bf e}^i$ of the dual basis can be used to ``retrieve'' the components of arbitrary vectors
${\bf x} = \sum_j X^j {\bf e}_j$  through
\begin{equation}
{\bf e}^i ( {\bf x} ) =
{\bf e}^i \left( \sum_i X^j {\bf e}_j \right) =
\sum_i  X^j {\bf e}^i \left({\bf e}_j \right) =
\sum_i  X^j \delta^i_j =
 X^i.
\label{2016-m-tensor-recoverc}
\end{equation}
Likewise, the basis vectors ${\bf e}_i$ of the ``base space'' can be used to obtain the coordinates of any dual vector ${\bf x} = \sum_j X_j {\bf e}^j$  through
\begin{equation}
{\bf e}_i ( {\bf x} ) =
{\bf e}_i \left( \sum_i X_j {\bf e}^j \right) =
\sum_i  X_j {\bf e}_i \left({\bf e}^j \right) =
\sum_i  X_j \delta_i^j =
 X_i.
\label{2016-m-tensor-recoverc2}
\end{equation}


As also noted earlier,  for orthonormal bases and Euclidean scalar (dot) products (the coordinates of) the dual basis vectors of an orthonormal basis can be coded identically
as  (the coordinates of) the original basis vectors; that is,
in this case,
(the coordinates of) the dual basis vectors are just rearranged as the transposed form of the original basis vectors.


In the same way as argued for changes of covariant bases (\ref{2001-mu-tensors}),
that is, because every vector in the new basis of the dual space can be represented as a linear combination of the vectors of the original dual basis -- we can make the formal {\it Ansatz}:
\begin{equation}
{\bf f}^j=\sum_i{b^j}_i{\bf e}^i,
\label{2016-m-ch-tensor-tocontravb}
\end{equation}
where $ \textsf{\textbf{B}} \equiv {b^j}_i$ is
the transformation matrix associated with the contravariant basis.
How is $b$, the transformation of the contravariant basis, related to $a$,
the transformation of the covariant basis?

Before answering this question, note that, again -- and just as the necessity to fix a convention for the transformation of the covariant basis vectors discussed on page~\pageref{2016-m-ch-fdvs-oic} --
we have to choose by {\em convention} the way transformations are represented.
In particular,
if in (\ref{2016-m-ch-tensor-tocontravb}) we would have reversed the indices ${b^j}_i \leftrightarrow {b_i}^j$, thereby effectively transposing the transformation matrix  $ \textsf{\textbf{B}} $,
this would have resulted in a changed (transposed) form of the transformation laws,
as compared to both the transformation $a$ of the covariant basis, and of the transformation of covariant vector components.

By exploiting (\ref{2001-mu-tensors0}) twice we can find the connection between
the transformation of covariant and contravariant basis elements and thus
tensor components; that is (by assuming Einstein's summation convention we are omitting to write sums explicitly),
\begin{equation}
\begin{split}
\delta_i^j= \delta_{ij}=
{\bf f}^j({\bf f}_i) =
\left[{\bf f}_i , {\bf f}^j \right]=
\left[ {a^k}_i {\bf e}_k ,  {b^j}_l {\bf e}^l \right] =
\\
=
{a^k}_i {b^j}_l  \left[ {\bf e}_k , {\bf e}^l \right]
={a^k}_i {b^j}_l   \delta_k^l
={a^k}_i {b^j}_l   \delta_{kl}
={b^j}_k {a^k}_i  .
\end{split}
\end{equation}
Therefore,
\begin{equation}
\textsf{\textbf{B}}
= \textsf{\textbf{A}}^{-1}
\textrm{, or }
{b^j}_i =  {\left( a^{-1} \right)^j}_i,
\label{2012-m-ch-tensor-tocontrav}
\end{equation}
and
\begin{equation}
{\bf f}^j=
\sum_i{\left( a^{-1}\right)^j}_i{\bf e}^i
.
\label{2012-m-ch-tensor-tocontrav1}
\end{equation}
In short, by comparing (\ref{2012-m-ch-tensor-tocontrav1})  with (\ref{2015-m-ch-tensor-tlcc}), we find that the vectors of the contravariant dual basis transform just like the components of contravariant vectors.

\subsection{Transformation of the covariant coordinates}

For the same, compensatory, reasons yielding the ``contra-varying'' transformation of the contravariant coordinates
with respect to variations of the covariant bases [reflected in Eqs.~(\ref{2001-mu-tensors}), (\ref{2015-m-ch-tensor-tlcc}), and (\ref{2001-mu-tensor-tl2nl})]
the coordinates with respect to the dual, contravariant, basis vectors, transform {\em covariantly.}
\index{contravariance}
We may therefore say that
``basis vectors ${\bf e}_i$, as well as dual components (coordinates) $X_i$ vary covariantly.''
Likewise,
``vector components (coordinates) $X^i$, as well as dual basis vectors ${\bf e}^\ast_i= {\bf e}^i$ vary contra-variantly.''


A similar calculation as for the contravariant components (\ref{2016-m-ch-tensor-tcvc}) yields a transformation for the covariant components:
\begin{equation}
\begin{split}
{\bf x}
=\sum_{i=1}^n X_j {\bf e}^j
=\sum_{i=1}^n { Y }_i {\bf f}^i
=\sum_{i=1}^n { Y }_i \sum_{j=1}^n {b^i}_j {\bf e}^j
=
\sum_{j=1}^n \left( \sum_{i=1}^n {b^i}_j  { Y }_i \right) {\bf e}^j
.
\end{split}
\label{2016-m-ch-tensor-tcontravc}
\end{equation}
Thus, by comparison we obtain
\begin{equation}
\begin{split}
 X_i = \sum_{j=1}^n {b^j}_i  { Y }_j =  \sum_{j=1}^n  {\left( a^{-1} \right)^j}_i  { Y }_j    \textrm{, and }
\\
 Y_i = \sum_{j=1}^n {\left( b^{-1}\right)^j}_i  { X }_j
    = \sum_{j=1}^n {a^j}_i   { X }_j.
\end{split}
\label{2016-m-ch-tensor-tcontravcc}
\end{equation}
In short, by comparing (\ref{2016-m-ch-tensor-tcontravcc})  with (\ref{2001-mu-tensors}), we find that the components of covariant vectors transform just like the vectors of the covariant basis vectors of ``base space.''



\subsection{Orthonormal bases}
For orthonormal bases of $n$-dimensional Hilbert space,
\begin{equation}
\delta_i^j = {\bf e}_i\cdot {\bf e}^j
\textrm { if and only if }
{\bf e}_i= {\bf e}^i  \textrm { for all } 1\le i,j \le n.
\end{equation}
Therefore, the vector space and its dual vector space are ``identical''
in the sense that the coordinate tuples representing their bases are identical
(though relatively transposed).
That is, besides transposition, the two bases are identical
\begin{equation}
{\mathfrak B}\equiv {\mathfrak B}^\ast
\end{equation}
and  formally any distinction between covariant and contravariant vectors becomes
irrelevant. Conceptually, such a distinction persists, though.
In this sense, we might ``forget about the difference between
covariant and contravariant orders.''


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{Tensor as multilinear form}

A {\em multilinear form}
$\alpha :{ \frak V }^k \mapsto {\Bbb R}$  or  ${\Bbb C}$
is a map from (multiple) arguments ${\bf x}_i$ which are elements of some vector space  $\frak V$
into some scalars in ${\Bbb R}$ or ${\Bbb C}$,  satisfying
\begin{equation}
\begin{split}
\alpha ( {\bf x}_1,{\bf x}_2,\ldots , A {\bf y}+ B {\bf z}, \ldots ,{\bf x}_k)
=
A\alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf y}, \ldots ,{\bf x}_k)   \\
   \qquad +
B\alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf z}, \ldots ,{\bf x}_k)
\end{split}
\end{equation}
for every one of its (multi-)arguments.

{
\color{blue}
Note that linear functionals on ${\frak V}$, which constitute the elements of the dual space ${\frak V}^\ast$
(cf. Section~\ref{2011-m-dvs} on page \pageref{2011-m-dvs}) is just a particular example of a  multilinear form
-- indeed rather a  linear form -- with just one argument, a vector in ${\frak V}$.
\eexample
}

In what follows we shall concentrate on {\em real-valued} multilinear forms which map
$k$ vectors in
${\Bbb R}^n$
into
${\Bbb R}$.



\section{Covariant tensors}
\index{covariance}
\index{contravariance}


Mind the notation introduced earlier; in particular in Eqs.~(\ref{2016-m-tensor-contrvvs}) and (\ref{2016-m-tensor-covvs}).
A covariant tensor of rank $k$
\index{rank of tensor}
\index{tensor rank}
\index{tensor type}
\begin{equation}
\alpha:{ \frak V }^k \mapsto {\Bbb R}
\end{equation}
is a multilinear form
\begin{equation}
\alpha ( {\bf x}_1,{\bf x}_2,\ldots ,{\bf x}_k)=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_1}_1 X^{i_2}_2\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k}).
\end{equation}
The
\begin{equation}
A_{{i_1}{i_2}\cdots {i_k}}
\stackrel{{\tiny \textrm{ def }}}{=}
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k})
\end{equation}
 are the
{\em covariant components} or
{\em covariant coordinates}
of the tensor $\alpha $ with respect to the basis
${\mathfrak B}$.

Note that, as each of the $k$ arguments of  a tensor of type (or rank) $k$ has to be evaluated
at each of the $n$ basis vectors ${\bf e}_1,{\bf e}_2,\ldots ,{\bf e}_n$ in an $n$-dimensional vector space,
$A_{{i_1}{i_2}\cdots {i_k}} $ has $n^k$  coordinates.

{\color{OliveGreen}
\bproof
To prove that tensors are multilinear forms, insert
\begin{equation}
\begin{split}
 \alpha ( {\bf x}_1,{\bf x}_2,\ldots , A{\bf x}^1_j+B{\bf x}_j^2,\ldots ,{\bf x}_k)
\\
=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  [A(X^1)^{i_j}_j+B(X^2)^{i_j}_j]\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
= A
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  (X^1)^{i_j}_j\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
\quad +
B
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X^{i_i}_1X^{i_2}_2\ldots  (X^2)^{i_j}_j\ldots X^{i_k}_k
\alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots,{\bf e}_{i_j},\ldots ,{\bf e}_{i_k})
\\
=
A \alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf x}^1_j,\ldots ,{\bf x}_k)+
B \alpha ( {\bf x}_1,{\bf x}_2,\ldots , {\bf x}_j^2,\ldots ,{\bf x}_k)\nonumber
\end{split}
\end{equation}
\eproof
}

\subsection{Transformation of covariant tensor components}

Because of multilinearity  and by insertion into
(\ref{2001-mu-tensors}),
\begin{eqnarray}
&&\alpha ( {\bf f}_{j_1},{\bf f}_{j_2},\ldots ,{\bf f}_{j_k})=
\alpha \left(
\sum_{i_1=1}^n {a^{i_1}}_{j_1} {\bf e}_{i_1},
\sum_{i_2=1}^n {a^{i_2}}_{j_2} {\bf e}_{i_2},
\ldots ,
\sum_{i_k=1}^n {a^{i_k}}_{j_k} {\bf e}_{i_k}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a^{i_1}}_{j_1}{a^{i_2}}_{j_2}\cdots {a^{i_k}}_{j_k} \alpha ( {\bf e}_{i_1},{\bf e}_{i_2},\ldots ,{\bf e}_{i_k})
\label{2012-m-ch-tensor-etotc1}
\end{eqnarray}
or
\begin{equation}
A'_{{j_1}{j_2}\cdots {j_k}}=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{a^{i_1}}_{j_1}{a^{i_2}}_{j_2}\cdots {a^{i_k}}_{j_k} A_{i_1 i_2\ldots i_k}.
\label{2011-m-tvtcov}
\end{equation}

In effect, this yields a transformation factor ``${a^i}_j$'' for every ``old index $i$'' and ``new index $j$.''



\section{Contravariant tensors}

Recall the inverse scaling of contravariant vector coordinates with respect to covariantly varying basis vectors.
Recall further that the dual base vectors are defined in terms of the base vectors by
a kind of ``inversion'' of the latter, as expressed by $[{\bf e}_i,  {\bf e}_j^*]=\delta_{ij}$
in Eq.~\ref{2011-m-Dualbasis-e1}.
Thus, by analogy it can be expected that similar considerations apply to
the scaling of dual base vectors with respect to the scaling of covariant base vectors:
in order to compensate those scale changes, dual basis vectors should contra-vary,
and, again analogously,  their respective dual coordinates, as well as the dual vectors,
should vary covariantly.
Thus, both vectors in the dual space, as well as their components or coordinates, will be called covariant vectors,
as well as covariant coordinates, respectively.
\index{covariance}
\index{covariant vector}
\index{contravariance}



\subsection{Definition of contravariant tensors}

The entire tensor formalism developed so far can be transferred and applied to define {\em contravariant} tensors
as multilinear forms with contravariant components
\begin{equation}
\beta:{ \frak V^\ast }^k \mapsto {\Bbb R}
\end{equation}
by
\begin{equation}
\beta ( {\bf x}^1,{\bf x}^2,\ldots ,{\bf x}^k)=
\sum_{i_1=1}^n
\sum_{i_2=1}^n
\cdots
\sum_{i_k=1}^n
X_{i_1}^1 X_{i_2}^2 \ldots X_{i_k}^k
\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k}).
\end{equation}
By definition
\begin{equation}
B^{{i_1}{i_2}\cdots {i_k}}=\beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k})
\label{2011-m-tvtcontrav}
\end{equation}
 are the contravariant
{\em components} of the contravariant tensor $\beta $ with respect to the basis
${\mathfrak B}^\ast$.








\subsection{Transformation of contravariant tensor components}

The argument concerning transformations of covariant tensors and components
can be carried through to the contravariant case.
Hence, the contravariant components transform as
\begin{eqnarray}
&&\beta ( {{\bf f}}^{j_1},{{\bf f}}^{j_2},\ldots ,{{\bf f}}^{j_k})=
\beta \left(
\sum_{i_1=1}^n {b^{j_1}}_{i_1} {\bf e}^{i_1},
\sum_{i_2=1}^n {b^{j_2}}_{i_2} {\bf e}^{i_2},
\ldots ,
\sum_{i_k=1}^n {b^{j_k}}_{i_k} {\bf e}^{i_k}
\right)
\nonumber \\&& \quad
=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{b^{j_1}}_{i_1}{b^{j_2}}_{i_2}\cdots {b^{j_k}}_{i_k} \beta ( {\bf e}^{i_1},{\bf e}^{i_2},\ldots ,{\bf e}^{i_k})
 \label{2012-m-ch-tensor-etotccon1}
\end{eqnarray}
or
\begin{equation}
B'^{{j_1}{j_2}\cdots {j_k}}=
\sum_{i_1=1}^n\sum_{i_2=1}^n\cdots \sum_{i_k=1}^n
{b^{j_1}}_{i_1}{b^{j_2}}_{i_2}\cdots {b^{j_k}}_{i_k} B^{i_1 i_2\ldots i_k}.
 \label{2012-m-ch-tensor-etotccon2}
\end{equation}

Note that, by Eq.~(\ref{2012-m-ch-tensor-tocontrav}),
$ {b^j}_i =  {\left( a^{-1} \right)^j}_i$.
%
In effect, this yields a transformation factor ``${\left( a^{-1} \right)^j}_i$'' for every ``old index $i$'' and ``new index $j$.''


\section{General tensor}

A (general) Tensor $T$ can be defined as a multilinear form  on the
$r$-fold product of a vector space ${\frak V}$, times the
$s$-fold product of the dual vector space ${\frak V}^\ast$;
that is,
\begin{equation}
T: \left( {\frak V} \right)^r \times \left( {\frak V}^\ast \right)^s
=
\underbrace{{\frak V}\times \cdots \times {\frak V}}_{r\textrm{ \scriptsize copies}}
\times
\underbrace{{\frak V}^\ast \times \cdots \times {\frak V}^\ast}_{s\textrm{ \scriptsize copies}}
\mapsto {\Bbb F}
,
 \label{2012-m-ch-tensor-gdt}
\end{equation}
where, most commonly, the scalar field
${\Bbb F}$
will be identified with the set ${\Bbb R}$ of reals,
or with the set ${\Bbb C}$ of complex numbers.
Thereby,
$r$ is called the
{\em covariant order}, and
\index{covariance}
$s$ is called the
{\em contravariant order}
\index{contravariance}
of $T$.
A tensor of covariant order $r$ and contravariant order $s$
is then pronounced a tensor of
{\em type} (or {\em rank})
\index{tensor rank}
\index{tensor type}
$(r,s)$.
By convention, covariant indices are denoted by {\em subscripts},
whereas the contravariant indices  are denoted by {\em superscripts}.

With the standard, ``inherited'' addition and scalar multiplication,
the set ${\frak T}_r^s$ of all tensors of type $(r,s)$
forms a linear vector space.


Note that a tensor of type $(1,0)$ is called  a
{\em covariant vector}
\index{covariance},
or just a
{\em vector}.
\index{vector}
A tensor of type $(0,1)$ is called a
{\em contravariant vector}.
\index{contravariance}

Tensors can change their type by the invocation of the {\em metric tensor}.
That is, a covariant tensor (index) $i$ can be made into a contravariant tensor (index) $j$
by summing over the index $i$ in a product involving the tensor and $g^{ij}$.
Likewise,  a contravariant tensor (index) $i$ can be made into a covariant tensor (index) $j$
by summing over the index $i$ in a product involving the tensor and $g_{ij}$.


Under basis or other linear transformations,
covariant tensors with index $i$ transform by summing over this index with (the transformation matrix) ${a_i}^j$.
Contravariant tensors with index $i$ transform by summing over this index with the inverse (transformation matrix)  ${(a^{-1})_i}^j$.


\section{Metric}

A {\em metric} or {\em metric tensor} $g$ is a measure of {\em distance} between two points in a vector space.




\subsection{Definition}
\index{metric tensor}
\index{metric}
\label{2011-m-metrict}


Formally, a metric, or metric tensor,  can be defined as a functional $g: {\Bbb R}^n\times{\Bbb R}^n\mapsto {\Bbb R}$
which maps two vectors (directing from the origin to the two points)
into a scalar
with the following properties:
\begin{itemize}
\item
$g$ is symmetric; that is, $g({\bf x},{\bf y})=g({\bf y},{\bf x})$;
\item
$g$ is bilinear; that is,
$g(
\alpha {\bf x} + \beta {\bf y}, {\bf z})
= \alpha g( {\bf x},{\bf z}) + \beta g({\bf y}, {\bf z})
$ (due to symmetry $g$ is also bilinear in the second argument);
\item
$g$ is nondegenerate; that is,
for every ${\bf x}\in {\frak V}$, ${\bf x}\neq 0$, there exists a
${\bf y}\in {\frak V}$ such that $g({\bf x},{\bf y})\neq 0$.
\end{itemize}



\subsection{Construction from a scalar product}

In real Hilbert spaces the {\em metric} tensor can be defined {\it via}  the scalar product  by
\begin{equation}
g_{ij}= \langle {\bf e}_i \mid {\bf e}_j\rangle .
\label{2016-m-ch-tensor-gij}
\end{equation}
and
\begin{equation}
g^{ij}= \langle {\bf e}^i \mid {\bf e}^j\rangle .
\label{2016-m-ch-tensor-gij2}
\end{equation}




For orthonormal bases, the metric tensor can be
represented as a Kronecker delta function, and thus  remains form invariant.
Moreover, its covariant and contravariant components are identical; that is,
$g_{ij}=\delta_{ij}=\delta^i_j=\delta_i^j=\delta^{ij}=g^{ij}$.


\subsection{What can the metric tensor do for you?}

We shall see that with the help of the metric tensor we can ``raise and lower indices;''
that is, we can transform lower (covariant) indices into upper (contravariant) indices, and {\it vice versa}.
This can be seen as follows.
Because of linearity, any contravariant basis vector ${\bf e}^i$
can be written as a linear sum of covariant (transposed, but we do not mark transposition here) basis vectors:
\begin{equation}
{\bf e}^i=A^{ij}{\bf e}_j.
\end{equation}
Then,
\begin{equation}
g^{ik} = \langle {\bf e}^i \vert {\bf e}^k \rangle  =\langle A^{ij}{\bf e}_j\vert  {\bf e}^k  \rangle
=A^{ij}\langle {\bf e}_j \vert  {\bf e}^k\rangle =A^{ij}\delta_j^k=A^{ik}
\end{equation}
and thus
\begin{equation}
{\bf e}^i=g^{ij}{\bf e}_j
\end{equation}
and, by a similar argument,
\begin{equation}
{\bf e}_i=g_{ij}{\bf e}^j.
\end{equation}


This property can also be used to raise or lower the indices not only of basis vectors, but also of tensor components; that is,
to change from contravariant to covariant and conversely from covariant
to contravariant.
For example,
\begin{equation}
{\bf x} =
X^i {\bf e}_i = X^i g_{ij} {\bf e}^j   = X_j {\bf e}^j,
\end{equation}
and hence $X_j = X^i g_{ij}$.

What is  ${g^i}_{j}$?
A straightforward calculation yields, through insertion of Eqs.~(\ref{2016-m-ch-tensor-gij}) and (\ref{2016-m-ch-tensor-gij2}),
as well as the resolution of unity (in a modified form involving upper and lower indices;
cf. Section~\ref{2016-m-ch-fdvsrotio} on page \pageref{2016-m-ch-fdvsrotio}),
\begin{equation}
{g^i}_{j} = g^{ik}g_{kj}  =
\langle {\bf e}^i \underbrace{\mid {\bf e}^k \rangle \langle {\bf e}_k \mid}_{{\Bbb I}} {\bf e}_j\rangle
= \langle {\bf e}^i \mid {\bf e}_j\rangle
= \delta^i_j = \delta_{ij}.
\label{2014-m-ch-tensor-deltag}
\end{equation}
A similar calculation yields ${g_i}^{j} = \delta_{ij}$.




The metric tensor has been defined in terms of the scalar product.
The converse can be true as well.
(Note, however, that the metric need not be positive.)
In Euclidean space with the dot (scalar, inner) product
the metric tensor represents the scalar product between vectors: let
${\bf x}=X^i{\bf e}_i \in {\Bbb R}^n$ and ${\bf y}=Y^j{\bf e}_j \in {\Bbb R}^n$ be two vectors.
Then ("$T$" stands for the transpose),
\begin{equation}
{\bf x}\cdot {\bf y}\equiv ({\bf x},{\bf y})\equiv \langle {\bf x}\mid {\bf y}\rangle
= X^i {\bf e}_i\cdot Y^j {\bf e}_j
= X^iY^j {\bf e}_i\cdot  {\bf e}_j
=X^iY^j g_{ij}= X^T g Y.
\end{equation}

It also characterizes the length of a vector: in the above
equation, set ${\bf y}={\bf x}$. Then,
\begin{equation}
{\bf x}\cdot {\bf x}\equiv ({\bf x},{\bf x})\equiv \langle {\bf x}\mid {\bf x}\rangle
=X^iX^j g_{ij}\equiv X^T g X,
\end{equation}
and thus, if the metric is positive definite,
\begin{equation}
\|  x\|  =\sqrt{X^iX^j g_{ij}}= \sqrt{X^T g X}.
\end{equation}



The square of the {\em line element} or {\em length element}
\index{line element}
\index{length element}
$ds =\| d{\bf x} \| $ of an infinitesimal vector $d{\bf x} $ is
\begin{equation}
d s^2  = g_{ij}dX^i dX^j= d{\bf x}^T g d{\bf x}.
\end{equation}

In (special) relativity with indefinite (Minkowski) metric, $ds^2$, or its finite difference form $\Delta s^2$, is used to define timelike, lightlike and spacelike distances:
\index{spacelike distance}
\index{timelike distance}
\index{lightlike distance}
with $g_{ij}=\eta_{ij}\equiv \text{diag}(1,1,1,-1)$,
$\Delta s^2 >0$ indicates spacelike distances,
$\Delta s^2 <0$ indicates timelike distances,
and $\Delta s^2 >0$ indicates lightlike distances.




\subsection{Transformation of the metric tensor}

Insertion into the definitions and coordinate transformations
 (\ref{2001-mu-tensor-tl2}) and (\ref{2012-m-ch-tlcbv})
yields
\begin{equation}
\begin{split}
g_{ij}={\bf e}_i\cdot {\bf e}_j
={a'^l}_i{\bf e'}_l\cdot {{a'}^m}_j{\bf e'}_m
={a'^l}_i{{a'}^m}_j {\bf e'}_l\cdot {\bf e'}_m \\
= {a'^l}_i{{a'}^m}_j  {g'}_{lm}
= {\partial { Y }^l\over \partial X^i}{\partial { Y }^m\over \partial X^j} {g'}_{lm}
.
\label{2011-m-emtdc}
\end{split}
\end{equation}

Conversely,  (\ref{2001-mu-tensors}) as well as    (\ref{2001-mu-tensor-tl1})
yields
\begin{equation}
\begin{split}
g'_{ij}={\bf f}_i\cdot {\bf f}_j
={a^l}_i{\bf e}_l\cdot {a^m}_j{\bf e}_m
={a^l}_i {a^m}_j {\bf e}_l\cdot {\bf e}_m  \\
= {a^l}_i {a^m}_j  {g}_{lm}
= {\partial {X}^l\over \partial { Y }^i}{\partial {X}^m\over \partial { Y }^j} {g}_{lm}
.
\end{split}
\end{equation}


If the geometry (i.e., the basis) is locally orthonormal, ${g}_{lm}=\delta_{lm}$,
then
$g'_{ij}={\partial {X}^l\over \partial { Y }^i}{\partial {X}_l\over \partial { Y }^j}$.

Just to check consistency with Eq.~(\ref{2014-m-ch-tensor-deltag}) we can compute,
for suitable differentiable coordinates $X$ and $ Y $,
\begin{equation}
\begin{split}
{g'_i}^j={\bf f}_i\cdot {{\bf f}}^j
={a^l}_i{\bf e}_l\cdot {(a^{-1})_m}^j{\bf e}^m
={a^l}_i {(a^{-1})_m}^j {\bf e}_l\cdot {\bf e}^m  \\
= {a^l}_i {(a^{-1})_m}^j \delta_l^m
= {a^l}_i {(a^{-1})_l}^j  \\
= {\partial {X}^l\over \partial { Y }^i}{\partial { Y }_l\over \partial {X}_j}
= {\partial {X}^l\over \partial {X}^j}{\partial { Y }_l\over \partial { Y }_i}
= \delta^{lj}\delta_{li}
= \delta^l_i
.
\end{split}
\end{equation}

In terms of the
{\em  Jacobian matrix} defined in Eq.~(\ref{2013-m-t-jm})
\index{Jacobian matrix}
the metric tensor in Eq. (\ref{2011-m-emtdc})
can be rewritten as
\begin{equation}
g = J^T g' J
\equiv g_{ij}= J_{li}J_{mj}g'_{lm}
.
\label{2011-m-emtdcJ}
\end{equation}
The metric tensor and the Jacobian (determinant)
are thus related by
\begin{equation}
\textrm{det }g = (\textrm{det }J^T) (\textrm{det } g')(\textrm{det } J)
.
\label{2011-m-emtdcJd}
\end{equation}
If the manifold is embedded into an Euclidean space,
then $g'_{lm}=\delta_{lm}$
and  $g = J^T  J $.

\subsection{Examples}

In what follows a few metrics are enumerated and briefly commented.
For a more systematic treatment, see, for instance, Snapper and Troyer's {\em Metric Affine geometry} \cite{snapper-troyer}.

Note also that due to the properties of the metric tensor, its coordinate representation has to be a {\em symmetric matrix}
with {\em nonzero diagonals}.
For the symmetry $g( {\bf x}  ,{\bf y} )=g({\bf y} ,{\bf x} )$ implies that $g_{ij}x_iy^j= g_{ij}y^ix_j=  g_{ij}x_jy^i= g_{ji}x_iy^j$ for all coordinate tuples
$x^i$ and $y^j$. And for any zero diagonal entry (say, in the $k$'th position of the diagonal we can choose a nonzero vector  ${\bf z}$
whose coordinates are all zero except the $k$'th coordinate. Then $g ({\bf z},{\bf x})=0$ for all ${\bf x}$ in the vector space.


\subsection*{$n$-dimensional Euclidean space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{n\; {\rm times}})
\end{equation}

One application in physics is quantum mechanics,
where $n$ stands for the dimension of a complex Hilbert space.
Some definitions can be easily adopted to accommodate the complex numbers.
E.g., axiom 5 of the scalar product becomes
$(x,y)=\overline{(x,y)}$, where ``$\overline{(x,y)}$'' stands for complex conjugation of $(x,y)$.
Axiom 4 of the scalar product becomes
$(x,\alpha y)=\overline{\alpha} (x,y)$.

\subsection*{Lorentz plane}


\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (1,-1)
\end{equation}

\subsection*{Minkowski space of dimension $n$}

In this case the metric tensor is called the
{\em Minkowski metric}
\index{Minkowski metric}
and is often denoted by  ``$\eta$'':
\begin{equation}
\eta \equiv \{\eta_{ij}\}={\rm diag} (\underbrace{1,1,\ldots ,1}_{n-1\; {\rm times}},-1)
\label{2012-m-ch-tensor-minspn}
\end{equation}


One application in physics is the theory of special relativity,
where $D=4$.
Alexandrov's theorem states that the mere requirement of the preservation of
zero distance (i.e., lightcones), combined with bijectivity (one-to-oneness) of the transformation law
yields the Lorentz transformations
\cite{alex1,alex2,alex3,alex-col,borchers-heger,benz,lester,svozil-2001-convention}.



\subsection*{Negative Euclidean space of dimension $n$}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (\underbrace{-1,-1,\ldots ,-1}_{n\; {\rm times}})
\end{equation}

\subsection*{Artinian four-space}

\begin{equation}
g\equiv \{g_{ij}\}={\rm diag} (+1,+1,-1 ,-1)
\end{equation}



\subsection*{General relativity}

In general relativity, the metric tensor $g$ is linked to the energy-mass distribution.
There, it appears as the primary concept when compared to the scalar product.
In the case of zero gravity, $g$ is just the  Minkowski metric (often denoted by  ``$\eta$'')
${\rm diag} (1,1,1,-1) $ corresponding to ``flat'' space-time.

The best known non-flat metric is the Schwarzschild metric
\begin{equation}
g
\equiv
\begin{pmatrix}
(1-2m/r)^{-1}&0&0&0\\
0&r^2&0&0\\
0&0&r^2\sin^2 \theta &0\\
0&0&0&- \left( 1-{2m/r}\right)
\end{pmatrix}
\end{equation}
with respect to the spherical space-time coordinates $r,\theta ,\phi ,t$.

{
\color{blue}
\bexample

\subsection*{Computation of the metric tensor of the circle of radius $r$}
Consider the transformation from the standard orthonormal
threedimensional ``Cartesian'' coordinates
$X_1=x$,
$X_2=y$,
into polar coordinates
\index{spherical coordinates}
$X_1'=r$,
$X_2'=\varphi$.
In terms of  $r$ and $\varphi$, the Cartesian coordinates can be written as
\begin{equation}
\begin{split}
 X_1=r \cos \varphi \equiv X_1' \cos X_2'  , \\
 X_2=r \sin \varphi \equiv X_1'\sin X_2'  .
\end{split}
\end{equation}
Furthermore,  since the basis we start with is the Cartesian orthonormal basis,
$g_{ij}=\delta_{ij}$; therefore,
\begin{equation}
g'_{ij}= {\partial {X}^l\over \partial { Y }^i}{\partial {X}_k\over \partial { Y }^j} g_{lk}
= {\partial {X}^l\over \partial { Y }^i}{\partial {X}_k\over \partial { Y }^j} \delta_{lk}
= {\partial {X}^l\over \partial { Y }^i}{\partial {X}_l\over \partial { Y }^j}.
\end{equation}
More explicitely, we obtain for the coordinates of the transformed metric tensor $g'$
\begin{equation}
\begin{split}
g'_{11}
= {\partial {X}^l\over \partial { Y }^1}{\partial {X}_l\over \partial { Y }^1} \\
=
{\partial (r \cos \varphi) \over \partial {r}}{\partial (r \cos \varphi) \over \partial {r}}
+
{\partial (r \sin \varphi) \over \partial {r}}{\partial (r \sin \varphi) \over \partial {r}}       \\
=
( \cos \varphi )^2
+
(\sin \varphi )^2 =1,\\
g'_{12}
= {\partial {X}^l\over \partial { Y }^1}{\partial {X}_l\over \partial { Y }^2} \\
=
{\partial (r \cos \varphi) \over \partial {r}}{\partial (r \cos \varphi) \over \partial {\varphi }}
+
{\partial (r \sin \varphi) \over \partial {r}}{\partial (r \sin \varphi) \over \partial {\varphi }}  \\
=
(\cos \varphi ) (- r \sin \varphi )
+
(\sin \varphi )( r \cos \varphi )  =0,\\
g'_{21}
= {\partial {X}^l\over \partial { Y }^2}{\partial {X}_l\over \partial { Y }^1} \\
=
{\partial (r \cos \varphi) \over \partial {\varphi }}{\partial (r \cos \varphi) \over \partial {r}}
+
{\partial (r \sin \varphi) \over \partial {\varphi }}{\partial (r \sin \varphi) \over \partial {r}}   \\
=
(- r \sin \varphi ) ( \cos \varphi )
+
( r\cos  \varphi )( \sin \varphi )  =0,\\
g'_{22}
= {\partial {X}^l\over \partial { Y }^2}{\partial {X}_l\over \partial { Y }^2} \\
=
{\partial (r \cos \varphi) \over \partial {\varphi}}{\partial (r \cos \varphi) \over \partial {\varphi }}
+
{\partial (r \sin \varphi) \over \partial {\varphi}}{\partial (r \sin \varphi) \over \partial {\varphi }}      \\
=
(- r \sin \varphi )^2
+
( r \cos \varphi )^2  =r^2;
\end{split}
\end{equation}
that is, in matrix notation,
\begin{equation}
g'
=
\begin{pmatrix}
1&0\\
0&r^2
\end{pmatrix}
,
\end{equation}
and thus
\begin{equation}
(d s')^2  = g_{ij}'d{\bf x'}^i d{\bf x'}^j=   (dr)^2 + r^2 (d\varphi )^2.
\label{2014-gppolarc}
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection*{Computation of the metric tensor of the ball}
Consider the transformation from the standard orthonormal
threedimensional ``Cartesian'' coordinates
$X_1=x$,
$X_2=y$,
$X_3=z$,
into spherical coordinates
%$$$(for a definition of spherical coordinates, see also page \pageref{2011-m-spericalcoo})
\index{spherical coordinates}
$X_1'=r$,
$X_2'=\theta$,
$X_3'=\varphi$.
In terms of  $r,\theta , \varphi$, the Cartesian coordinates can be written as
\begin{equation}
\begin{split}
 X_1=r \sin \theta \cos \varphi \equiv X_1' \sin X_2' \cos X_3'  , \\
 X_2=r \sin \theta \sin \varphi \equiv X_1'\sin X_2' \sin X_3'  ,    \\
 X_3=r \cos \theta  \equiv X_1'\cos X_2'  .
\end{split}
\end{equation}
Furthermore,  since the basis we start with is the Cartesian orthonormal basis,
$g_{ij}=\delta_{ij}$; hence finally
\begin{equation}
g'_{ij}= {\partial {X}^l\over \partial { Y }^i}{\partial {X}_l\over \partial { Y }^j}
\equiv {\rm diag}(1,r^2,r^2\sin^2 \theta ),
\end{equation}
and
\begin{equation}
(ds')^2 =(dr)^2+r^2(d\theta )^2+r^2\sin^2 \theta (d\varphi )^2.
\end{equation}

The expression $ds^2 =(dr)^2+r^2(d\varphi )^2$
for polar coordinates in two dimensions (i.e., $n=2$) of Eq.~(\ref{2014-gppolarc})  is recovered by setting $\theta = \pi/2 $ and $d\theta =0$.

\subsection*{Computation of the metric tensor of the Moebius strip}
The parameter representation of the Moebius strip is
\begin{equation}
\Phi (u,v) =\left(
\begin{array}{c}
(1+v\cos \frac{u}{2})\sin u \\
(1+v\cos \frac{u}{2})\cos u \\
v\sin \frac{u}{2}
\end{array}
\right),
\end{equation}
where
$u\in [0,2\pi ]$ represents the position of the point on the circle,  and where $2a>0$ is the ``width'' of the Moebius strip,
and where $v\in [-a,a]$.


\begin{equation}
\begin{split}
\Phi _{v}=\frac{\partial \Phi }{\partial v}=\allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}
 \\
\Phi _{u}=\frac{\partial \Phi }{\partial u}=\allowbreak
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
\end{split}
\end{equation}


\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial u}
=  \allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}^{T}
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
\\
=
-\frac{1}{2}\left( \cos \frac{u}{2}\sin ^{2}u\right) v\sin \frac{u}{2}-%
\frac{1}{2}\left( \cos \frac{u}{2}\cos ^{2}u\right) v\sin \frac{u}{2}
\\
+%
\frac{1}{2}\sin \frac{u}{2} v\cos \frac{u}{2}=\allowbreak 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial v})^{T}\frac{\partial \Phi }{\partial v}
=\allowbreak
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}^{T}
\begin{pmatrix}
\cos \frac{u}{2}\sin u \\
\cos \frac{u}{2}\cos u \\
\sin \frac{u}{2}
\end{pmatrix}
 \\
=
\cos ^{2}\frac{u}{2}\sin ^{2}u+\cos ^{2}\frac{u}{2}\cos ^{2}u+\sin ^{2}%
\frac{u}{2}=\allowbreak 1
\end{split}
\end{equation}


\begin{equation}
\begin{split}
(\frac{\partial \Phi }{\partial u})^{T}\frac{\partial \Phi }{\partial u}
=
\\
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos
u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin
u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}^{T} \cdot
\\
\cdot
\begin{pmatrix}
-\frac{1}{2}v\sin \frac{u}{2}\sin u+\left( 1+v\cos \frac{u}{2}\right) \cos
u \\
-\frac{1}{2}v\sin \frac{u}{2}\cos u-\left( 1+v\cos \frac{u}{2}\right) \sin
u \\
\frac{1}{2}v\cos \frac{u}{2}
\end{pmatrix}
 \\
=
\frac{1}{4}v^{2}\sin ^{2}\frac{u}{2}\sin ^{2}u+\cos
^{2}u+2 v \cos ^{2}u \cos \frac{u}{2}+
v^{2}\cos ^{2}u  \cos ^{2}\frac{u}{2}
\\
+\frac{1}{4}v^{2}\sin ^{2}\frac{u}{2}\cos^{2}u
+\sin ^{2}u+2  v\sin ^{2}u\cos \frac{u}{2}+ v^{2} \sin^{2}u\cos ^{2}\frac{u}{2}
 \\
+\frac{1}{4}v^{2}\cos ^{2}\frac{1}{2}%
u =\allowbreak \frac{1}{4}v^{2}+v^{2}\cos ^{2}\frac{u}{2}+1+2v\cos \frac{%
1}{2}u
 \\
=\left(1+v\cos \frac{u}{2}\right)^{2}+\frac{1}{4}v^{2}
\end{split}
\end{equation}


Thus the metric tensor is given by
\begin{equation}
\begin{split}
g'_{ij}
= {\partial {X}^s\over \partial { Y }^i}{\partial {X}^t\over \partial { Y }^j}g_{st}
= {\partial {X}^s\over \partial { Y }^i}{\partial {X}^t\over \partial { Y }^j}\delta_{st}\\
\quad \equiv
\left(
\begin{array}{cc}
\Phi _{u}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{u} \\
\Phi _{v}\cdot \Phi _{u} & \Phi _{v}\cdot \Phi _{v}
\end{array}
\right) ={\rm diag}\left(
\left(1+v\cos \frac{u}{2}\right)^{2}+\frac{1}{4}v^{2} , 1\right).
\end{split}
\end{equation}

\eexample
}






\section{Decomposition of tensors}

Although a tensor of type (or rank) $n$ transforms like the tensor product of $n$ tensors of type 1,
not all type-$n$ tensors can be decomposed into a single
tensor product of $n$ tensors of type (or rank) 1.

Nevertheless,
by a generalized Schmidt decomposition (cf. page \pageref{2011-m-Schmidtdecomposition}),
any type-$2$ tensor  can be decomposed into
the sum of
tensor products of two tensors of type 1.

\section{Form invariance of tensors}

A tensor (field) is
form invariant  with respect to some basis change
\index{form invariance}
if its representation in the new basis has the same form as in the old basis.
For instance, if the ``12122--component'' $T_{12122} (x)$ of the tensor $T$
with respect to the old basis and old coordinates $x$   equals some function $f(x)$ (say, $f(x)=x^2$),
then, a necessary condition for $T$ to be form invariant is that, in terms of the new basis,
that component  $T'_{12122} (x')$  equals the same function $f(x')$ as before, but in the new coordinates $x'$
[say, $f(x')=(x')^2$].
A sufficient condition for form invariance of $T$ is that {\em all}
coordinates or components of $T$ are form invariant in that way.


Although form invariance is a gratifying feature for the reasons explained shortly,
a tensor (field) needs not necessarily
be form invariant with respect to all or even any (symmetry) transformation(s).



A physical motivation for the use of form invariant tensors can be given as follows.
What makes some tuples (or matrix, or tensor components in general)  of
numbers or scalar functions a tensor? It is the
interpretation of the scalars as tensor components {\em with respect to
a particular basis}. In another basis, if we were talking about the same
tensor, the tensor components; that is, the numbers or scalar functions,
would be different.
Pointedly stated, the tensor coordinates represent some
encoding of a multilinear function with respect to a particular basis.

Formally, the tensor coordinates are numbers; that is, scalars,
which are grouped together in vector touples or matrices or whatever form we consider useful.
As the tensor coordinates are scalars, they can be treated as scalars.
For instance, due to commutativity and associativity, one can exchange
their order. (Notice, though, that this is generally not the case for
differential operators such as $\partial_i=\partial / \partial {\bf x}^i$.)

A {\em form invariant} tensor with respect to  certain transformations
is a tensor which retains
the same functional form if the transformations are performed; that is,
if the basis changes accordingly.
That is, in this case,
the functional form of mapping numbers or coordinates or other entities remains unchanged, regardless of the coordinate change.
Functions remain the same but with the new parameter components as
argument. For instance; $4\mapsto 4$ and $f(X_1,X_2,X_3)\mapsto
f( Y _1, Y _2, Y _3)$.

Furthermore, if a tensor is invariant with respect to one transformation, it need not
be invariant with respect to another transformation, or with respect to
changes of the scalar product; that is, the metric.

Nevertheless, totally symmetric (antisymmetric) tensors remain totally
symmetric (antisymmetric) in all cases:
\begin{equation}
A_{i_1i_2 \ldots i_si_t\ldots i_k}
=
\pm A_{i_1i_2 \ldots i_ti_s\ldots i_k}
\end{equation}
implies
\begin{equation}
\begin{split}
A'_{j_1i_2 \ldots j_s j_t\ldots j_k}
=
{a^{i_1}}_{j_1}{a^{i_2}}_{j_2}
\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}
\cdots
{a^{i_k}}_{j_k} A_{i_1 i_2\ldots i_s i_t\ldots  i_k}
 \\
=
\pm {a^{i_1}}_{j_1}{a^{i_2}}_{j_2}\cdots
{a_{j_s}}^{i_s}{a_{j_t}}^{i_t}\cdots
{a^{i_k}}_{j_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
  \\
=
\pm {a^{i_1}}_{j_1}{a^{i_2}}_{j_2}
\cdots
{a_{j_t}}^{i_t}{a_{j_s}}^{i_s}
\cdots
{a^{i_k}}_{j_k} A_{i_1 i_2\ldots i_t i_s\ldots  i_k}
  \\
=
\pm A'_{j_1i_2 \ldots j_t j_s\ldots j_k}    .
\end{split}
\end{equation}


In physics, it would be nice if the natural laws could be written into a
form which does not depend on the particular reference frame or  basis
used.
Form invariance thus is a gratifying physical feature, reflecting the
{\em symmetry} against changes of coordinates and bases.

After all, physicists want the formalization of their fundamental laws not to artificially depend on,
say, spacial directions, or on some particular basis, if there is no physical reason why this should be so.
Therefore, physicists tend to be crazy to write down everything in a
form invariant manner.

One strategy to accomplish  form invariance  is to start out with form invariant
tensors and compose -- by tensor products and index reduction -- everything from them. This method guarantees form
invariance.

{
\color{blue}
\bexample

The ``simplest'' form invariant tensor under all transformations is the constant tensor of rank $0$.

Another constant form invariant tensor  under all transformations is represented by the Kronecker symbol $\delta^i_j$,
because
\begin{equation}
{(\delta ')}^i_j= {(a^{-1})^i}_{k} {a^{l}}_j\delta^{k}_{l}={(a^{-1})^i}_{k} {a^{k}}_j= \delta^i_j
.
\end{equation}

A simple form invariant tensor field is a vector ${\bf x}$,
because if $T({\bf x})= x^i t_i= x^i {\bf e}_i={\bf x}$, then
the ``inner transformation''
${\bf x} \mapsto  {\bf x}'$
and the ``outer transformation''
$T \mapsto  T'= \textsf{\textbf{A}}T$
just compensate each other; that is, in coordinate representation, Eqs.(\ref{2012-m-ch-di-choic}) and (\ref{2011-m-tvtcov}) yield
\begin{equation}
T'({\bf x}')= {x'}^i t'_i= {(a^{-1})^i}_l x^l   {a^j}_i t_j = {a^j}_i {(a^{-1})^i}_l  {\bf e}_j  x^l
= \delta_l^j x^l {\bf e}_j  = {\bf x} = T({\bf x}).
\end{equation}



For the sake of another demonstration of form invariance, consider the following two factorizable tensor fields:
while
\begin{equation}
{S}(x)=
\begin{pmatrix}
  {  x}_2  \\
- {  x}_1
\end{pmatrix}
\otimes
\begin{pmatrix}
   {  x}_2  \\
 - {  x}_1
\end{pmatrix}^T
=
\left(    {  x}_2 ,- {  x}_1  \right)^T
\otimes
\left(    {  x}_2 ,- {  x}_1  \right)
\equiv
\begin{pmatrix}
   {  x}_2^2          & -{ x}_1{x}_2  \\
 - {  x}_1{  x}_2     & { x}_1^2
\end{pmatrix}
\label{2012-m-ch-tensor-etotccon1factorized}
\end{equation}
is a form invariant tensor field with respect to the basis $\{(0,1),(1,0)\}$
and orthogonal transformations (rotations around the origin)
\begin{equation}
\begin{pmatrix}
  \cos \varphi & \sin \varphi  \\
 -\sin \varphi & \cos \varphi
\end{pmatrix}
,
\end{equation}
\begin{equation}
{ T}(x)=
\begin{pmatrix}
{  x}_2  \\
{  x}_1
\end{pmatrix}
\otimes
\begin{pmatrix}
{  x}_2  \\
{  x}_1 \end{pmatrix}^T
=
\left(    {  x}_2 ,  {  x}_1  \right)^T
\otimes
\left(    {  x}_2 ,  {  x}_1  \right)
\equiv
\begin{pmatrix}
{  x}_2^2 & { x}_1{  x}_2  \\
{  x}_1{  x}_2          & { x}_1^2
\end{pmatrix}
\end{equation}
is not.

This can be proven by considering the single factors from which $S$ and $T$ are composed.
Eqs. (\ref{2012-m-ch-tensor-etotc1})-(\ref{2011-m-tvtcov})
and
(\ref{2012-m-ch-tensor-etotccon1})-(\ref{2012-m-ch-tensor-etotccon2})
show that the form
invariance of the factors implies the form invariance of the tensor products.

For instance, in our example, the factors $\left(    {  x}_2 ,- {  x}_1  \right)^T$
of $S$ are invariant, as they transform as
$$
\begin{pmatrix} \cos \varphi & \sin \varphi  \\
                         -\sin \varphi & \cos \varphi
\end{pmatrix}
\begin{pmatrix}
{  x}_2  \\
 - {  x}_1
\end{pmatrix}
=
\begin{pmatrix}
 {  x}_2 \cos \varphi  - x_1 \sin \varphi  \\
            - x_2 \sin \varphi         - {  x}_1 \cos \varphi
\end{pmatrix}
=
\begin{pmatrix}
{  x}_2'  \\
 - {  x}_1'
\end{pmatrix},
$$
where the transformation of the coordinates
$$
\begin{pmatrix}
{  x}_1'  \\
  {  x}_2'
\end{pmatrix}
=
\begin{pmatrix}
 \cos \varphi & \sin \varphi  \\
   -\sin \varphi & \cos \varphi
\end{pmatrix}
\begin{pmatrix}
 {  x}_1  \\
 {  x}_2
\end{pmatrix}
=
\begin{pmatrix}
{  x}_1 \cos \varphi  + x_2 \sin \varphi  \\
- x_1 \sin \varphi         + {  x}_2 \cos \varphi
\end{pmatrix}
$$
has been used.


Note that  the notation identifying tensors of type (or rank) two with matrices,
creates an ``artefact'' insofar as the transformation of the ``second index'' must then be represented by
the exchanged multiplication order, together with the transposed transformation matrix;
that is,
$$
a_{ik}a_{jl}A_{kl}
=  a_{ik}A_{kl}a_{jl}
=  a_{ik}A_{kl}\left(a^T \right)_{lj}
\equiv a\cdot A\cdot a^T .
$$
Thus for a
transformation  of
the transposed touple  $\left(    {  x}_2 ,- {  x}_1  \right)$
we must consider the {\em transposed} transformation matrix arranged {\em after} the factor; that is,
$$
\left(   {  x}_2 , - {  x}_1 \right)
\begin{pmatrix}  \cos \varphi & -\sin \varphi  \\
  \sin \varphi & \cos \varphi
\end{pmatrix}
=
\left(
{  x}_2 \cos \varphi  - x_1 \sin \varphi ,
 - x_2 \sin \varphi         - {  x}_1 \cos \varphi
\right)
=
\left(
{  x}_2'  ,
 - {  x}_1'
\right).
$$



In contrast, a similar calculation shows that the factors
$\left(    {  x}_2 ,  {  x}_1  \right)^T$
of $T$ do not transform invariantly.
However, noninvariance with respect to certain transformations does not imply that
$T$ is not a valid, ``respectable'' tensor field; it is just not form invariant under rotations.
\eexample
}

Nevertheless, note again that, while the tensor product of form invariant tensors is again a form invariant tensor,  not every form
invariant tensor might be decomposed into products of form invariant tensors.

{
\color{blue}
\bexample
Let
$\vert + \rangle  \equiv   (0,1)$
and
$\vert - \rangle  \equiv   (1,0)$.
For a nondecomposable tensor, consider the sum of two-partite tensor products (associated with two ``entangled'' particles)
\index{entanglement}
Bell state (cf. Eq. (\ref{2014-m-ch-fdvs-bellbasis}) on page \pageref{2014-m-ch-fdvs-bellbasis}) in the standard basis     \index{Bell state}
\begin{equation}
\begin{split}
\vert \Psi^-\rangle = \frac{1}{\sqrt{2}}\left(\vert +-\rangle   - \vert -+\rangle  \right)   \\
\qquad \equiv  \left( 0,\frac{1}{\sqrt{2}},- \frac{1}{\sqrt{2}} ,  0 \right)     \\
\qquad \equiv  \frac{1}{2}
\begin{pmatrix}
0&0&0&0\\
0&1&-1&0\\
0&-1&1&0\\
0&0&0&0
\end{pmatrix}
.
\end{split}
\label{2011-m-bellstatenondec}
\end{equation}
\marginnote{$\vert \Psi^-\rangle $,   together with the other three Bell states
$\vert \Psi^+\rangle = \frac{1}{\sqrt{2}}\left(\vert +-\rangle   + \vert -+\rangle  \right) $,
$\vert \Phi^+\rangle = \frac{1}{\sqrt{2}}\left(\vert --\rangle   + \vert ++\rangle  \right) $,
and
$\vert \Phi^-\rangle = \frac{1}{\sqrt{2}}\left(\vert --\rangle   - \vert ++\rangle  \right) $,
forms an orthonormal basis of ${\Bbb C}^4$.
}

Why is $\vert \Psi^-\rangle$ not decomposable?
In order to be able to answer this question
(see also Section \ref{2012-m-c-fdvs-entanglement} on page \pageref{2012-m-c-fdvs-entanglement}), consider
the most general two-partite state
\begin{equation}
\vert \psi \rangle
=
\psi_{--}\vert -- \rangle
+
\psi_{-+}\vert -+ \rangle
+
\psi_{+-}\vert +- \rangle
+
\psi_{++}\vert ++ \rangle
,
\end{equation}
with $\psi_{ij}\in {\Bbb C}$,
and compare it to the most general state obtainable through products of single-partite states
$\vert \phi_1\rangle  = \alpha_-  \vert - \rangle    + \alpha_+  \vert + \rangle$,
and
$\vert \phi_2\rangle  = \beta_-  \vert - \rangle    + \beta_+  \vert + \rangle$
with $\alpha_{i}, \beta_i \in {\Bbb C}$;
that is,
\begin{equation}
\begin{split}
\vert \phi \rangle  =\vert \phi_1\rangle    \vert \phi_2\rangle   \\
\qquad =
(\alpha_-  \vert - \rangle    + \alpha_+  \vert + \rangle )
(\beta_-  \vert - \rangle    + \beta_+  \vert + \rangle )   \\
\qquad  =\alpha_- \beta_- \vert -- \rangle    + \alpha_-\beta_+  \vert -+ \rangle +
\alpha_+ \beta_- \vert +- \rangle    + \alpha_+\beta_+  \vert ++ \rangle. \\
\end{split}
\end{equation}
Since the two-partite basis states
\begin{equation}
\begin{split}
\vert -- \rangle  \equiv (1,0,0,0)
,\\
\vert -+ \rangle    \equiv (0,1,0,0)
,\\
\vert +- \rangle     \equiv (0,0,1,0)
,\\
\vert ++ \rangle      \equiv (0,0,0,1)
\end{split}
\end{equation}
are linear independent (indeed, orthonormal),
a comparison of $\vert \psi \rangle  $ with  $\vert \phi \rangle$ yields
\begin{equation}
\begin{split}
\psi_{--}=  \alpha_- \beta_-
,\\
\psi_{-+}=   \alpha_-\beta_+
,\\
\psi_{+-}=  \alpha_+ \beta_-
,\\
\psi_{++}= \alpha_+\beta_+
.
\end{split}
\end{equation}
Hence,
$\psi_{--}/ \psi_{-+} =   \beta_- / \beta_+ =   \psi_{+-} / \psi_{++}$,
and thus a necessary and sufficient condition for a two-partite quantum state to be decomposable
into a product of single-particle quantum states is that its amplitudes obey
 \begin{equation}
\psi_{--}\psi_{++}  =  \psi_{-+}   \psi_{+-} .
\end{equation}
This is not satisfied for the Bell state $\vert \Psi^-\rangle$ in Eq. (\ref{2011-m-bellstatenondec}),
because in this case $\psi_{--}=\psi_{++} =0$
and  $ \psi_{-+} = - \psi_{+-} =1/\sqrt{2}$.
Such nondecomposability is in physics referred to as {\em entanglement}
\cite{CambridgeJournals:1737068,CambridgeJournals:2027212,schrodinger}.
\index{entanglement}

Note also that $\vert \Psi^-\rangle$ is a {\em singlet state},
as it is form invariant under the following generalized rotations in two-dimensional complex Hilbert subspace; that is,
(if you do not believe this please check yourself)
\begin{equation}
\begin{split}
\vert + \rangle =
e^{ i{\frac{\varphi}{2}} }
\left(
\cos \frac{\theta}{2} \vert +'  \rangle
-
\sin \frac{\theta}{2} \vert -'   \rangle
\right),
\\
 \vert - \rangle =
e^{ -i{\frac{\varphi}{2}} }
\left(
\sin \frac{\theta}{2} \vert +'   \rangle
+
\cos \frac{\theta}{2} \vert -'  \rangle
\right)
\end{split}
\end{equation}
in the spherical coordinates $\theta , \varphi$,
but it cannot be composed or written as a product of a {\em single} (let alone form invariant) two-partite tensor product.

\eexample
}


%There exists totally symmetric (antisymmetric) tensors which are form
%invariant under all basis and metric changes.
%The symmetric tensor is  associated with the Kronecker delta
%\begin{equation}
%\delta_i^j =
%\delta^i_j =
%\left\{
% \begin{array}{l}
%1 \mbox{ if } i=j \\
%0 \mbox{ if } i\neq j  \\
%\end{array}
% \right. .
%\end{equation}
%This can be easily seen by evaluating
%${\delta'}_{j_1}^{j_2} = {a^{i_1}}_{j_1}a^{j_2}_{i_2}\delta_{i_1}^{i_2} =
%a_{j_1}^{i}a^{j_2}_i={\delta}_{j_1}^{j_2}$
%
%The antisymmetric tensor is  associated with
%\begin{equation}
%\epsilon_{12\cdots n} = 1, \qquad
%\epsilon_{i_1i_2\cdots i_si_t \cdots i_k} =  -\epsilon_{i_1i_2\cdots
%i_ti_s \cdots i_k}.
%\end{equation}
%

In order to prove form invariance of a constant tensor,
one has to transform the tensor according to the standard transformation laws
(\ref{2011-m-tvtcov}) and (\ref{2011-m-tvtcontrav}), and compare the result with the input;
that is, with the untransformed, original, tensor.
This is sometimes referred to as the ``outer transformation.''


In order to prove form invariance of a tensor field,
one has to additionally transform the spatial coordinates on which the field depends;
that is, the arguments of that field; and then compare.
This is sometimes referred to as the ``inner transformation.''
This will become clearer with the following example.

{
\color{blue}
\bexample


Consider again the tensor field defined
earlier in Eq.
(\ref{2012-m-ch-tensor-etotccon1factorized}),
but let us not choose the ``elegant''
ways of proving form invariance by factoring; rather we explicitly
consider the transformation of all the  components
$$S_{ij}(x_1,x_2)
=
\begin{pmatrix}
 -x_1x_2 & - x_2^2  \\
 x_1^2 & x_1x_2
\end{pmatrix}
$$
with respect to the standard basis  $\{(1,0), (0,1)\}$.

Is $S$ form invariant with respect to rotations around the origin?
That is, $S$ should be form invariant with respect to transformations
$x_i' = a_{ij} x_j$
with
$$
a_{ij}=\begin{pmatrix}
 \cos \varphi & \sin \varphi  \\
  -\sin \varphi & \cos \varphi
\end{pmatrix}.
$$


Consider the ``outer'' transformation first.
As has been pointed out earlier,
the term on the right hand side in $
S_{ij}'= a_{ik}a_{jl}S_{kl}
$
can be rewritten as a product of three matrices; that is,
$$
a_{ik}a_{jl}S_{kl}\left(x_n\right)
=  a_{ik}S_{kl}a_{jl}
=  a_{ik}S_{kl}\left(a^T \right)_{lj}
\equiv a\cdot S\cdot a^T .
$$
$a^T$ stands for the transposed matrix; that is,
$(a^T)_{ij}=a_{ji}$.
$$
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      -x_1x_2 & -x_2^2 \\
      x_1^2   & x_1x_2
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      \cos \varphi  & -\sin \varphi \\
      \sin \varphi & \cos \varphi
    \end{array}
  \right)=
$$

\smallskip

$$
  =\left(
    \begin{array}{cc}
      -x_1 x_2 \cos \varphi + x_1^2 \sin \varphi &
        -x_2^2 \cos \varphi + x_1 x_2 \sin \varphi \\
      x_1 x_2 \sin \varphi + x_1^2 \cos \varphi &
        x_2^2 \sin \varphi + x_1 x_2 \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{cc}
      \cos \varphi  & -\sin \varphi \\
      \sin \varphi & \cos \varphi
    \end{array}
  \right)=
$$

\smallskip

$$
  =\left(\!\!\!
    \begin{array}{cc}
      \cos \varphi
        \left(-x_1 x_2 \cos \varphi + x_1^2 \sin \varphi\right)+\!&\!
      -\sin \varphi
        \left(-x_1 x_2 \cos \varphi + x_1^2 \sin \varphi\right)+\\
      \quad +\sin \varphi
        \left(-x_2^2 \cos \varphi + x_1 x_2 \sin \varphi\right)\!&\!
      \quad +\cos \varphi
        \left(-x_2^2 \cos \varphi + x_1 x_2 \sin \varphi\right)\\
      \\[1ex]
      \cos \varphi
        \left(x_1 x_2 \sin \varphi + x_1^2 \cos \varphi\right)+\!&\!
      -\sin \varphi
        \left(x_1 x_2 \sin \varphi + x_1^2 \cos \varphi\right)+\\
      \quad +\sin \varphi
        \left(x_2^2 \sin \varphi + x_1 x_2 \cos \varphi\right)\!&\!
      \quad +\cos \varphi
        \left(x_2^2 \sin \varphi + x_1 x_2 \cos \varphi\right)
    \end{array}
  \!\right)=
$$

\smallskip

$$
  =\left(
    \begin{array}{cc}
      x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) + &
      2x_1 x_2 \sin \varphi \cos \varphi \\
      \qquad + \left( x_1^2-x_2^2 \right) \sin \varphi \cos \varphi &
      \qquad - x_1^2 \sin^2 \varphi - x_2^2 \cos^2 \varphi \\
      \\[1ex]
      2x_1 x_2 \sin \varphi \cos \varphi + &
      -x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) - \\
      \qquad + x_1^2 \cos^2 \varphi + x_2^2 \sin^2 \varphi &
      \quad -\left(x_1^2-x_2^2\right) \sin \varphi \cos \varphi
    \end{array}
  \right)
$$

Let us now perform the ``inner'' transform
$$
  x'_i =a_{ij}x_j \Longrightarrow
  \begin{array}{rcl}
    x'_1 & = & x_1 \cos \varphi + x_2 \sin \varphi \\
    x'_2 & = & -x_1 \sin \varphi + x_2 \cos \varphi .
  \end{array}
$$

Thereby we assume (to be corroborated) that the functional form in the new coordinates are identical
to the functional form of the old coordinates.
A comparison yields
\begin{eqnarray*}
  -x'_1 \,x'_2 & = &
  -\left(x_1 \cos \varphi + x_2 \sin \varphi \right)
  \left(-x_1 \sin \varphi + x_2 \cos \varphi \right) = \\
  & = &
  -\left(
    -x_1^2 \sin \varphi \cos \varphi +
      x_2^2 \sin \varphi \cos \varphi -
      x_1 x_2 \sin^2 \varphi + x_1 x_2 \cos^2 \varphi
  \right) = \\
  & = &
  x_1 x_2 \left(\sin^2 \varphi - \cos^2 \varphi \right) +
    \left(x_1^2 - x_2^2\right) \sin \varphi \cos \varphi \\
  (x'_1)^2  & = &
    \left(x_1 \cos \varphi + x_2 \sin \varphi \right)
    \left(x_1 \cos \varphi + x_2 \sin \varphi \right) = \\
  & = & x_1^2 \cos^2 \varphi + x_2^2 \sin^2 \varphi +
    2 x_1 x_2 \sin \varphi \cos \varphi \\
  (x'_2)^2  & = &
    \left(-x_1 \sin \varphi + x_2 \cos \varphi \right)
    \left(-x_1 \sin \varphi + x_2 \cos \varphi \right) = \\
  & = & x_1^2 \sin^2 \varphi + x_2^2 \cos^2 \varphi -
    2 x_1 x_2 \sin \varphi \cos \varphi
\end{eqnarray*}
and hence
$$S' (x'_1,x'_2)=\left(
    \begin{array}{cc}
      -x'_1x'_2 & -(x'_2)^2 \\
      (x'_1)^2   & x'_1x'_2
    \end{array}
\right)
$$ is invariant with respect to rotations by angles $\varphi$, yielding the new basis
$\{ (\cos \varphi ,-\sin \varphi
),(\sin
\varphi ,\cos \varphi )\}$.


Incidentally,
as has been stated earlier, $S(x)$ can be written as the product of two invariant tensors $b_i(x)$ and $c_j(x)$:
$$S_{ij}(x)=b_i(x)c_j(x),$$
with
$
b(x_1,x_2)=(-x_2,x_1),
$ and
$
c(x_1,x_2)=(x_1,x_2)
$.
This can be easily checked by comparing the components:
\begin{eqnarray*}
b_1c_1&=&-x_1x_2 = S_{11},\\
b_1c_2&=&-x_2^2 = S_{12},\\
b_2c_1&=&x_1^2 = S_{21},\\
b_2c_2&=&x_1x_2 = S_{22}.\\
\end{eqnarray*}

Under rotations, $b$ and $c$  transform into
\begin{eqnarray*}
a_{ij}b_j&=&
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      -x_2\\
       x_1
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      -x_2\cos \varphi +x_1 \sin \varphi \\
      x_2\sin \varphi +x_1 \cos \varphi
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      -x'_2 \\
      x'_1
    \end{array}
  \right)    \\
 a_{ij}c_j&=&
  \left(
    \begin{array}{cc}
      \cos \varphi  & \sin \varphi \\
      -\sin \varphi & \cos \varphi
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      x_1\\
       x_2
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      x_1\cos \varphi +x_2 \sin \varphi \\
      -x_1\sin \varphi +x_2 \cos \varphi
    \end{array}
  \right)
=
  \left(
    \begin{array}{c}
      x'_1    \\
      x'_2
    \end{array}
  \right) .
\end{eqnarray*}
%Mit beliebigen Tensoren erster Stufe $x=(x_1,x_2)$
%\"uberschnitten, ergeben sich skalare Invarianten:
%\begin{eqnarray*}
%b_ix_i&=&-x_2x_1+x_1x_2=0\\
%c_ix_i&=&x_1^2+x_2^2.
%\end{eqnarray*}

This factorization of $S$ is nonunique, since
Eq.
(\ref{2012-m-ch-tensor-etotccon1factorized})
uses a different factorization; also, $S$ is decomposable into, for example,
$$S(x_1,x_2)=
  \left(
    \begin{array}{cc}
      -x_1x_2 & -x_2^2 \\
      x_1^2   & x_1x_2
    \end{array}
  \right)     =
  \left(
    \begin{array}{cc}
      -x_2^2 \\
      x_1 x_2
    \end{array}
  \right)
\otimes \left({x_1\over x_2},1\right).
$$



\eexample
}



\section{The Kronecker symbol $\delta$}
\index{delta tensor}
For vector spaces of dimension $n$ the totally symmetric Kronecker symbol $\delta$,
sometimes referred to
as the delta symbol $\delta$--tensor, can be defined by
\begin{equation}
\delta_{i_1 i_2\cdots i_k}
=
\left\{
\begin{array}{rl}
+1&\textrm{ if }  i_1 = i_2 = \cdots = i_k \\
0&\textrm{ otherwise (that is, some indices are not identical).}
\end{array}
\right.
\end{equation}

Note that, with the Einstein summation convention,
\index{Einstein summation convention}
\begin{equation}
\begin{split}
\delta_{ij} a_j    = a_j  \delta_{ij} =
\delta_{i1} a_1
+
\delta_{i2} a_2
+
\cdots
+
\delta_{in} a_n  =a_i
,  \\
\delta_{ji} a_j = a_j  \delta_{ji} =
\delta_{1i} a_1
+
\delta_{2i} a_2
+
\cdots
+
\delta_{ni} a_n  =a_i
.
\end{split}
\end{equation}

\section{The Levi-Civita symbol $\varepsilon$}
\index{Levi-Civita symbol}
\index{antisymmetric tensor}
For vector spaces of dimension $n$ the totally antisymmetric Levi-Civita symbol $\varepsilon$, sometimes referred to
as the Levi-Civita symbol $\varepsilon$--tensor, can be defined by the number of permutations of its indices; that is,
\begin{equation}
\varepsilon_{i_1 i_2\cdots i_k}
=
\left\{
\begin{array}{rl}
+1&\textrm{ if } (i_1 i_2\ldots i_k) \textrm{ is an {\em even} permutation of } (1,2,\ldots k)\\
-1&\textrm{ if } (i_1 i_2\ldots i_k) \textrm{ is an {\em odd} permutation of } (1,2,\ldots k)\\
0&\textrm{ otherwise (that is, some indices are identical).}
\end{array}
\right.
\label{2014-m-ch-lcs}
\end{equation}
Hence, $\varepsilon_{i_1 i_2\cdots i_k}$ stands for the sign of the permutation in the case of a permutation, and zero otherwise.

{
\color{blue}
\bexample

In two dimensions,
$$\varepsilon_{ij}\equiv
\left(
\begin{array}{rrrr}
\varepsilon_{11}&\varepsilon_{12}\\
\varepsilon_{21}&\varepsilon_{22}
\end{array}
\right)
=
\left(
\begin{array}{rrrr}
0&1\\
-1&0
\end{array}
\right)
.
$$
\eexample
}

In threedimensional Euclidean space,
the cross product, or vector product
\index{cross product}
\index{vector product}
of two vectors
${\bf x}\equiv x_i$
and
${\bf y}\equiv y_i$
can be written as
${\bf x} \times {\bf y}\equiv \varepsilon_{ijk}x_jy_k$.

{\color{OliveGreen}
\bproof
For a direct proof, consider, for arbitrary threedimensional vectors ${\bf x}$ and ${\bf y}$,
and by enumerating all nonvanishing terms; that is, all permutations,
\begin{equation}
\begin{split}
{\bf x} \times {\bf y}\equiv \varepsilon_{ijk}x_jy_k
\equiv
\begin{pmatrix}
\varepsilon_{123}x_2y_3 + \varepsilon_{132}x_3y_2 \\
\varepsilon_{213}x_1y_3 + \varepsilon_{231}x_3y_1 \\
\varepsilon_{312}x_2y_3 + \varepsilon_{321}x_3y_2
\end{pmatrix}   \\
=
\begin{pmatrix}
\varepsilon_{123}x_2y_3 - \varepsilon_{123}x_3y_2 \\
-\varepsilon_{123}x_1y_3 + \varepsilon_{123}x_3y_1 \\
\varepsilon_{123}x_2y_3 - \varepsilon_{123}x_3y_2
\end{pmatrix}
=
\begin{pmatrix}
x_2y_3 - x_3y_2 \\
-x_1y_3 + x_3y_1 \\
x_2y_3 - x_3y_2
\end{pmatrix}
.
\end{split}
\end{equation}
\eproof
}

\section{Nabla, Laplace, and D'Alembert operators}
\index{nabla operator}
\index{Laplace operator}
\index{D'Alembert operator}

The {\em nabla operator}
\begin{equation}
\nabla_i \equiv \left(
\frac{\partial }{\partial X^1},
\frac{\partial }{\partial X^2},
\ldots ,
\frac{\partial }{\partial X^n}
\right).
\end{equation}
is a vector differential operator in an $n$-dimensional vector space $\frak V$.
In index notation, $\nabla_i$ is also written as
\begin{equation}
\nabla_i  =\partial_i =\partial_{X^i}
= \frac{\partial }{\partial X^i}
.
\end{equation}

Why is the lower index indicating covariance used when differentiation with respect to upper indexed, contravariant coordinates?
The nabla operator transforms in the following manners:
$\nabla_i  =\partial_i =\partial_{X^i}$ transforms like a {\em covariant} basis vector
[compare with Eqs.~(\ref{2012-m-ch-tlcbv}) and (\ref{2001-mu-tensor-tl2nl})], since
\begin{equation}
\partial_i =
\frac{\partial }{\partial X^i}
=
\frac{\partial { Y }^j}{\partial X^i}
\;
\frac{\partial }{\partial { Y }^j}
=
\frac{\partial { Y }^j}{\partial X^i}
\;
\partial'_j
=
{{(a^{-1})}^j}_i
\partial'_j
=
J_{ij}
\partial'_j,
\end{equation}
where $J_{ij}$ stands for the {\em  Jacobian matrix} defined in Eq.~(\ref{2013-m-t-jm}).
\index{Jacobian matrix}

As very similar calculation demonstrates that $\partial^i=\frac{\partial }{\partial X_i}$ transforms like a {\em contravariant} vector.


In three dimensions and in the standard Cartesian basis with the Euclidean metric,
covariant and contravariant entities coincide,
and
\begin{equation}
\begin{split}
\nabla
=
\left(
\frac{\partial }{\partial X^1},
\frac{\partial }{\partial X^2},
\frac{\partial }{\partial X^3}
\right)
={\bf e}^1\frac{\partial }{\partial X^1}
+{\bf e}^2\frac{\partial }{\partial X^2}
+{\bf e}^3\frac{\partial }{\partial X^3}
=
\\
= \left(
\frac{\partial }{\partial X_1},
\frac{\partial }{\partial X_2},
\frac{\partial }{\partial X_3}
\right)^T
={\bf e}_1\frac{\partial }{\partial X_1}
+{\bf e}_2\frac{\partial }{\partial X_2}
+{\bf e}_3\frac{\partial }{\partial X_3}
.
\end{split}
\end{equation}

It is often used to define basic differential operations;
in particular, (i) to denote the {\em gradient} of a scalar field $f(X_1,X_2,X_3)$ (rendering a vector field with respect to a particular basis),
(ii) the {\em divergence} of a vector field ${\bf v}(X_1,X_2,X_3)$
(rendering a scalar field with respect to a particular basis), and
(iii) the {\em curl} (rotation) of a vector field  ${\bf v}(X_1,X_2,X_3)$ (rendering a vector field with respect to a particular basis)
as follows:
\index{gradient}
\index{divergence}
\index{curl}
\begin{eqnarray}
\textrm{grad } f &=& \nabla f = \left(
\frac{\partial f}{\partial X_1},
\frac{\partial f}{\partial X_2},
\frac{\partial f}{\partial X_3}
\right)^T  ,\\
\textrm{div }  {\bf v} &=& \nabla \cdot {\bf v} =
\frac{\partial v_1}{\partial X_1}+
\frac{\partial v_2}{\partial X_2}+
\frac{\partial v_3}{\partial X_3}
  ,\\
\textrm{rot } {\bf v} &=& \nabla \times {\bf v} = \left(
\frac{\partial v_3}{\partial X_2}-
\frac{\partial v_2}{\partial X_3}
,
\frac{\partial v_1}{\partial X_3}-
\frac{\partial v_3}{\partial X_1}
,
\frac{\partial v_2}{\partial X_1}-
\frac{\partial v_1}{\partial X_2}
\right)^T          \\
&\equiv& \varepsilon_{ijk} \partial_j v_k.
\end{eqnarray}

The {\em Laplace operator}
\index{Laplace operator}
is defined by
\begin{equation}
\Delta = \nabla^2= \nabla \cdot \nabla =
\frac{\partial^2 }{\partial (X_1)^2}+
\frac{\partial^2 }{\partial (X_2)^2}+
\frac{\partial^2 }{\partial (X_3)^2}
.
\end{equation}

In special relativity and electrodynamics,  as well as in  wave theory and quantized field theory, with the Minkowski space-time
of dimension four
(referring to the metric tensor with the signature ``$\pm ,\pm ,\pm ,\mp$''),
the {\em D'Alembert operator}
\index{D'Alembert operator}
is defined by the Minkowski metric $\eta = {\rm diag} (1,1,1,-1)$
\begin{equation}
\begin{split}
\Box  = \partial_i \partial^i
=
\eta_{ij}  \partial^i \partial^j=
\nabla^2- \frac{\partial^2 }{\partial t^2}=
\nabla \cdot \nabla - \frac{\partial^2 }{\partial t^2}\\
=
\frac{\partial^2 }{\partial (X_1)^2}+
\frac{\partial^2 }{\partial (X_2)^2}+
\frac{\partial^2 }{\partial (X_3)^2}- \frac{\partial^2 }{\partial t^2}
.
\end{split}
\end{equation}



\section{Index trickery and examples}

We have already mentioned {\em Einstein's summation convention}
\index{Einstein summation convention}
requiring that, when an index variable appears twice in a single term, one has to
sum over all of the possible index values. For instance, $a_{ij}b_{jk}$ stands for $\sum_j a_{ij}b_{jk}$.

There are other tricks which are commonly used.
Here, some of them are enumerated:

\begin{itemize}
\item[(i)]
Indices which appear as internal sums can be renamed arbitrarily
(provided their name is not already taken by some other index).
That is, $a_ib^i=a_jb^j$ for arbitrary $a,b,i,j$.
\item[(ii)]
With the Euclidean metric, $\delta_{ii}=n$.
\item[(iii)]
$\frac{\partial X^i }{\partial X^j}=\delta^i_j=\delta^{ij}$ and
$\frac{\partial X_i }{\partial X_j}=\delta_i^j=\delta^{ij}$.
\item[(iv)]
With the Euclidean metric, $\frac{\partial X^i }{ \partial X^i}=n$.
\item[(v)]
$\varepsilon_{ij}\delta_{ij}=-\varepsilon_{ji}\delta_{ij}=-\varepsilon_{ji}\delta_{ji}=(i \leftrightarrow j)=-\varepsilon_{ij}\delta_{ij}=0$,
since $a=-a$ implies $a=0$;
likewise, $\varepsilon_{ij}x_i x_j=0$.
In general, the Einstein summations $s_{ij\ldots }a_{ij\ldots}$ over objects $s_{ij\ldots }$ which are {\em symmetric} with respect to index exchanges
over objects $a_{ij\ldots}$ which are {\em antisymmetric}  with respect to index exchanges yields zero.
\item[(vi)]
For threedimensional vector spaces ($n=3$)  and the Euclidean metric,
the {\em Grassmann identity} holds:
\index{Grassmann identity}
\begin{equation}
 \varepsilon_{ijk}\varepsilon_{klm}
=  \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}.
\label{2011-m-egi}
\end{equation}
{\color{OliveGreen}
\bproof
For the sake of a proof, consider
\begin{equation}
\begin{split}
{\bf x} \times ({\bf y} \times {\bf z}) \equiv \\
\textrm{in index notation}\\
x_j  \varepsilon_{ijk} y_l z_m\varepsilon_{klm}   =
x_j y_l z_m  \varepsilon_{ijk} \varepsilon_{klm}   \equiv \\
\textrm{in coordinate notation}\\
\begin{pmatrix}
x_1 \\ x_2\\ x_3
\end{pmatrix}
\times
\left[
\begin{pmatrix}
y_1 \\y_2\\y_3
\end{pmatrix}
\times
\begin{pmatrix}
z_1 \\z_2\\z_3
\end{pmatrix}
\right] =
\begin{pmatrix}
x_1 \\ x_2\\ x_3
\end{pmatrix}
\times
\begin{pmatrix}
y_2z_3 -y_3z_2 \\
y_3z_1 -y_1z_3 \\
y_1z_2 -y_2z_1 \\
\end{pmatrix}
 =  \\
\begin{pmatrix}
x_2 (y_1z_2 -y_2z_1) - x_3(y_3z_1 -y_1z_3) \\
x_3 (y_2z_3 -y_3z_2) - x_1(y_1z_2 -y_2z_1) \\
x_1 (y_3z_1 -y_1z_3) - x_2(y_2z_3 -y_3z_2)
\end{pmatrix}
 =  \\
\begin{pmatrix}
x_2 y_1z_2 -x_2y_2z_1 - x_3y_3z_1 +x_3y_1z_3 \\
x_3 y_2z_3 -x_3y_3z_2 - x_1y_1z_2 +x_1y_2z_1 \\
x_1 y_3z_1 -x_1y_1z_3 - x_2y_2z_3 +x_2y_3z_2
\end{pmatrix}
 =  \\
\begin{pmatrix}
y_1 (x_2z_2 + x_3z_3) - z_1(x_2y_2 + x_3y_3) \\
y_2 (x_3z_3 + x_1z_1) - z_2(x_1y_1 + x_3y_3) \\
y_3 (x_1z_1 + x_2z_2) - z_3(x_1y_1 + x_2y_2)
\end{pmatrix}
\end{split}
\end{equation}
The ``incomplete''  dot products can be completed through addition and subtraction of the same term, respectively; that is,
\begin{equation}
\begin{split}
\begin{pmatrix}
y_1(x_1z_1 + x_2z_2 + x_3z_3) - z_1(x_1y_1 + x_2y_2 + x_3y_3)   \\
y_2(x_1z_1 + x_2z_2 + x_3z_3) - z_2(x_1y_1 + x_2y_2 + x_3y_3)     \\
y_3(x_1z_1 + x_2z_2 + x_3z_3) - z_3(x_1y_1 + x_2y_2 + x_3y_3)       \\
\end{pmatrix}
\equiv   \\
\textrm{in vector notation}\\
{\bf y} \left( {\bf x} \cdot {\bf z}\right)
-
{\bf z} \left( {\bf x} \cdot {\bf y}\right)
\equiv   \\
\textrm{in index notation}\\
x_j y_l z_m \left(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}\right).
\end{split}
\end{equation}
\eproof
}

\item[(vii)]
For threedimensional vector spaces ($n=3$) and the Euclidean metric the Grassmann identity implies\\
$\| a\times b \| =
\sqrt{\varepsilon_{ijk}\varepsilon_{ist}a_j a_s b_k b_t}  =
\sqrt{\| a\|^2
\| b\|^2
-(a\cdot b)^2}=\sqrt{{\rm det}
\left(
\begin{array}{cc}
a\cdot a&a\cdot b\\
a\cdot b&b\cdot b
\end{array}\right)}=
 =
\sqrt{\| a\|^2
\| b\|^2 \left (1- \cos^2 \angle_{ab} \right)}
\| a\|
\| b\|
\sin \angle_{ab}.$
\item[(viii)]
Let $u,v\equiv X_1',X_2'$ be two parameters associated with an
orthonormal Cartesian basis $\{(0,1),(1,0)\}$, and let
$\Phi :(u,v)\mapsto {\Bbb R}^3$
be a mapping from some area of ${\Bbb R}^2$ into a twodimensional
surface of ${\Bbb R}^3$. Then the metric tensor is given by
$g_{ij}=
{\partial \Phi^k \over \partial  Y ^i}
{\partial \Phi^m \over \partial  Y ^j} \delta_{km}.$

\end{itemize}






{
\color{blue}
\bexample

Consider the following examples in threedimensional vector space.
Let $r^2 = \sum_{i=1 }^3 x_i^2$.

\begin{enumerate}
\item
\begin{equation}
\begin{split}
  \partial_jr =  \partial_j \sqrt{\sum_ix_i^2} =
  \frac{1}{2}\frac{1}{\sqrt{\sum_ix_i^2}}\,2x_j =
  \frac{x_j}{r}
\end{split}
\end{equation}
By using the chain  rule one obtains
\begin{equation}
\begin{split}
  \partial_jr^\alpha =
  \alpha r^{\alpha-1}\left(\partial_jr\right) =
  \alpha r^{\alpha-1}\left(\frac{x_j}{r}\right)=
  \alpha r^{\alpha-2}x_j
\end{split}
\label{2011-m-eet1}
\end{equation}
and thus $\nabla r^\alpha = \alpha r^{\alpha-2}{\bf x}$.


\item
\begin{equation}
\begin{split}
  \partial_j \log r=\frac{1}{r}\left(\partial_jr\right)
\end{split}
\end{equation}
With $
  \partial_jr = \frac{x_j}{r}
$  derived earlier in Eq. (\ref{2011-m-eet1}) one obtains
$
 \partial_j \log r= \frac{1}{r}\frac{x_j}{r}=
  \frac{x_j}{r^2}
$,
and thus $\nabla  \log r =\frac{{\bf x}}{r^2}$.

\item
\begin{equation}
\begin{split}
  \partial_j
  \left[
    \left(
      \sum_i\left(x_i-a_i\right)^2
    \right)^{-\frac{1}{2}}+
    \left(
      \sum_i\left(x_i+a_i\right)^2
    \right)^{-\frac{1}{2}}
  \right]=
\\
  = -\frac{1}{2}\left[\frac{1}{\left(\sum_i\left(x_i-a_i\right)^2\right)^
    \frac{3}{2}}\,2\left(x_j-a_j\right)
  +\frac{1}{\left(\sum_i\left(x_i+a_i\right)^2\right)^\frac{3}{2}}\,
    2\left(x_j+a_j\right)\right]= \\
 -\left(\sum_i\left(x_i-a_i\right)^2\right)^{-\frac{3}{2}}\left(x_j-a_j\right)-
    \left(\sum_i\left(x_i+a_i\right)^2\right)^{-\frac{3}{2}}\left(x_j+a_j\right)   .
\end{split}
\end{equation}

\item
For three dimensions and for $r \neq 0$,
\begin{equation}
\nabla \bigl({{\bf r} \over r^3} \bigr)\equiv
  \partial_i\left(\frac{r_i}{r^3}\right)=
  \frac{1}{r^3}\underbrace{\partial_i r_i}_{=3}+
  r_i\left(-3\frac{1}{r^4}\right)\left(\frac{1}{2r}\right)2r_i=
  3\frac{1}{r^3}-3\frac{1}{r^3}=0 .
\label{2011-m-eet2}
\end{equation}


\item  With this solution (\ref{2011-m-eet2}) one obtains, for three dimensions and $r \neq 0$,
\begin{equation}
\Delta \bigl({1 \over r} \bigr)\equiv
  \partial_i\partial_i\frac{1}{r}=\partial_i\left(-\frac{1}{r^2}\right)
  \left(\frac{1}{2r}\right)2r_i=-\partial_i\frac{r_i}{r^3}=0   .
\end{equation}


\item  With the earlier solution (\ref{2011-m-eet2}) one obtains
\begin{equation}
\begin{split}
\Delta \bigl({{\bf r} {\bf p} \over r^3} \bigr)\equiv \\
  \partial_i\partial_i\frac{r_jp_j}{r^3}=
%****** unklar: siehe Vorlagen
    \partial_i
    \left[
      \frac{p_i}{r^3}+r_jp_j\left(-3\frac{1}{r^5}\right)r_i
    \right]= \\
  = p_i\left(-3\frac{1}{r^5}\right)r_i+
    p_i\left(-3\frac{1}{r^5}\right)r_i+ \\
   +r_jp_j
    \left[
      \left(15\frac{1}{r^6}\right)
      \left(\frac{1}{2r}\right)2r_i
    \right]r_i+
    r_jp_j\left(-3\frac{1}{r^5}\right)
    \underbrace{\partial_i r_i}_{=3}= \\
  = r_i p_i \frac{1}{r^5}(-3-3+15-9)=0
\end{split}
\end{equation}



\item    With $r\neq 0$ and constant $\bf p$ one obtains
\marginnote{Note that, in three dimensions, the Grassmann identity (\ref{2011-m-egi})
 $\varepsilon_{ijk}\varepsilon_{klm}       =          \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$
holds.}
\begin{equation}
\begin{split}
  \nabla \times ({\bf p} \times \frac{{\bf r}}{r^3})
 \equiv
  \varepsilon_{ijk}\partial_j\varepsilon_{klm}p_l\frac{r_m}{r^3}=
  p_l \varepsilon_{ijk} \varepsilon_{klm}
  \left[\partial_j\frac{r_m}{r^3}\right]  \\
 = p_l
    \varepsilon_{ijk}\varepsilon_{klm}
  \left[
    \frac{1}{r^3}\partial_j r_m + r_m
    \left(-3\frac{1}{r^4}\right)\left(\frac{1}{2r}\right)2r_j
  \right]  \\
  = p_l\varepsilon_{ijk}\varepsilon_{klm}
  \left[
    \frac{1}{r^3}\delta_{jm} - 3\frac{r_j r_m}{r^5}
  \right]  \\
  = p_l(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl})
  \left[
    \frac{1}{r^3}\delta_{jm} - 3\frac{r_j r_m}{r^5}
  \right]  \\
  = p_i \underbrace{\left(3\frac{1}{r^3}-3\frac{1}{r^3}\right)}_{=0}-
  p_j
  \Biggl({1\over {r^3}}
    \underbrace{{\partial_j r_i}}_{=\delta_{ij}}-
    3\frac{r_j r_i}{r^5}
  \Biggr)  \\
  = -\frac{{\bf p}}{r^3}+3\frac{\left({\bf r} {\bf p}\right){\bf r}}{r^5}
.
\end{split}
\end{equation}



\item
\begin{equation}
\begin{split}
{\nabla} \times({\nabla }\Phi )\\
\equiv
\varepsilon_{ijk} \partial_j \partial_k \Phi \\ =
\varepsilon_{ikj} \partial_k \partial_j \Phi  \\=
\varepsilon_{ikj} \partial_j \partial_k \Phi  \\=
-\varepsilon_{ijk} \partial_j \partial_k \Phi =0.
\end{split}
\end{equation}
This is due to the fact that $\partial_j \partial_k$ is  symmetric, whereas
$\varepsilon_{ijk}$ is totally antisymmetric.


\item        For a proof that
$({{\bf x} } \times {{\bf y}})\times {{\bf z}}
\neq
{{\bf x} } \times ({{\bf y}}\times {{\bf z}})$ consider
\begin{equation}
\begin{split}
({{\bf x} } \times {{\bf y}})\times {{\bf z}}\\
\equiv
\underbrace{\varepsilon_{ijm}}_{\text{ second } \times}
\underbrace{\varepsilon_{jkl}}_{\text{ first } \times}
x_k y_l z_m\\=
-\varepsilon_{imj}
\varepsilon_{jkl}
x_k y_l z_m\\ =
-(\delta_{ik}\delta_{ml}-
\delta_{im}\delta_{lk})
x_k y_l z_m\\ =
-x_i {{\bf y}}\cdot {{\bf z}}+
y_i {{\bf x}}\cdot {{\bf z}}.
\end{split}
\end{equation}
{\it versus}
\begin{equation}
\begin{split}
{{\bf x} } \times ({{\bf y}}\times {{\bf z}})\\
\equiv
\underbrace{\varepsilon_{ikj}}_{\text{ first } \times}
\underbrace{\varepsilon_{jlm}}_{\text{ second } \times}
x_k y_l z_m\\ =
(\delta_{il}\delta_{km}-
\delta_{im}\delta_{kl})
x_k y_l z_m\\ =
y_i {{\bf x}}\cdot {{\bf z}}-
z_i {{\bf x}}\cdot {{\bf y}}.
\end{split}
\end{equation}





\item
Let ${\bf w} = { {{\bf p}} \over r} $ with  $p_i=p_i\left(t  - {r \over c}\right)$,
whereby   $t$ and $c$  are constants. Then,
\begin{eqnarray*}
  \mbox{div}{\bf w}& = &
\nabla \cdot {\bf w}\\
\equiv \partial_i w_i & = & \partial_i
  \left[
    \frac{1}{r} p_i \left(t-\frac{r}{c}\right)
  \right]= \\
  & = & \left(-\frac{1}{r^2}\right)\left(\frac{1}{2r}\right)
    2r_i p_i+
    \frac{1}{r}p_i'\left(-\frac{1}{c}\right)
    \left(\frac{1}{2r}\right)2 r_i= \\
  & = & -\frac{r_i p_i}{r^3}-\frac{1}{c r^2}p_i' r_i
.
\end{eqnarray*}
Hence,
$
  \mbox{div}{\bf w}=\nabla \cdot {\bf w}=
  -\left(\frac{{\bf r} {\bf p}}{r^3}+\frac{{\bf r} {\bf p}'}{c r^2}\right)
$.



\begin{eqnarray*}
\mbox{rot}{\bf w}& =& \nabla \times {\bf w}  \\
  \varepsilon_{ijk}\partial_j w_k & = &
   \equiv  \varepsilon_{ijk}
    \left[
      \left(-\frac{1}{r^2}\right)\left(\frac{1}{2r}\right)
      2 r_j p_k +
      \frac{1}{r}p_k'
      \left(-\frac{1}{c}\right)\left(\frac{1}{2r}\right)2r_j
    \right]= \\
  & = & -\frac{1}{r^3}\varepsilon_{ijk}r_j p_k -\frac{1}{cr^2}
    \varepsilon_{ijk}r_j p_k' = \\
  & \equiv & -\frac{1}{r^3}\left({\bf r} \times {\bf p}\right)-
    \frac{1}{cr^2}\left({\bf r} \times {\bf p}'\right)   .
\end{eqnarray*}

\item
Let us verify  some specific examples of Gauss' (divergence) theorem,
\index{Gauss' theorem}
stating that the outward flux of a vector field through a closed surface
is equal to the volume integral of the divergence of the region inside the surface.
That is, the sum of all sources subtracted by the sum of all sinks represents the net flow out of a region or volume of threedimensional space:
\begin{equation}
\int \limits_V \nabla \cdot {\bf w} \, dv   =\int \limits_{F_V} {\bf w} \cdot d{\bf f}
.   \label{2011-m-gauss}
\end{equation}

Consider the vector field ${\bf w} = \begin{pmatrix} 4x , -2y^2 , z^2\end{pmatrix}^T$
and the (cylindric) volume bounded by the planes  $z=0$ und $z=3$,
as well as by the surface
$x^2 + y^2 = 4$.


Let us first look at the left hand side $\int \limits_V \nabla \cdot {\bf w} \, dv $
of Eq. (\ref{2011-m-gauss}):
$$
  \nabla {\bf w}=\textrm{div } {\bf w}=4-4y+2z
$$
\begin{eqnarray*}
  \Longrightarrow \int \limits_V \! \textrm{div } {\bf w} dv& = &
  \int \limits_{z=0}^3 \! dz \int \limits_{x=-2}^2 \!\! dx
  \int \limits_{y=-\sqrt{4-x^2}}^{\sqrt{4-x^2}} \!\!\!dy\,
    \left(4-4y+2z\right)= \\
  & & \mbox{cylindric coordinates: }
  \Biggl[
    \begin{array}{rcl}
      x & = & r \cos \varphi \\
      y & = & r \sin \varphi \\
%***** \"z-Koordinatentransformation\" hinzugefuegt
      z & = & z
    \end{array}
  \Biggr] \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \int \limits_0^{2\pi} \!\! d\varphi \left(4-4r \sin \varphi+2z\right)= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r \, dr
  \left(4 \varphi +4r\cos\varphi+2\varphi z\right)
  \Biggl|_{\varphi=0}^{2 \pi}= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \left(8 \pi +4r+4 \pi z -4r\right)= \\
  & = & \int \limits_{z=0}^3 \!\! dz \int \limits_0^2 r\, dr
  \left(8 \pi +4 \pi z\right) \\
  & = & 2 \left( 8 \pi z +4 \pi \frac{z^2}{2}\right)\Biggl|_{z=0}^{z=3}=
    2 (24+18) \pi = 84 \pi
\end{eqnarray*}

Now consider the right hand side $\int \limits_F {\bf w} \cdot d{\bf f}$
of Eq. (\ref{2011-m-gauss}).
The surface consists of three  parts:
the lower plane $F_1$ of the cylinder is characterized by $z=0$;
the upper plane $F_2$  of the cylinder is characterized by  $z=3$;
the surface on the side of the cylinder $F_3$
 is characterized by   $x^2+y^2=4$.
$d {\bf f}$ must be normal to these surfaces, pointing outwards; hence
 \begin{eqnarray*}
  F_1 : \int \limits_{{\cal F}_1} {\bf w} \cdot d{\bf f}_1  & = &
    \int \limits_{{\cal F}_1}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2=0
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        0 \\
        0 \\
        -1
      \end{array}
    \right)
    \, d\, x d\, y = 0 \\
  F_2 : \int \limits_{{\cal F}_2} {\bf w} \cdot d{\bf f}_2 & = &
    \int \limits_{{\cal F}_2}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2=9
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        0 \\
        0 \\
        1
      \end{array}
    \right)
    \, d\, x d\, y = \\
  & = & 9 \int \limits_{K_{r=2}} \!\! d\, f=9 \cdot 4 \pi=36 \pi \\
  F_3 : \int \limits_{{\cal F}_3} {\bf w} \cdot d{\bf f}_3 & = &
    \int \limits_{{\cal F}_3}
    \left(
      \begin{array}{c}
        4x \\
        -2y^2 \\
        z^2
      \end{array}
    \right)
    \left(
      \frac{\partial {\bf x}}{\partial \varphi} \times
      \frac{\partial {\bf x}}{\partial z}
    \right)
    \, d\varphi \, dz \quad (r=2=\mbox{const.})
\end{eqnarray*}
$$
  \frac{\partial {\bf x}}{\partial \varphi} =
  \left(
    \begin{array}{c}
      -r \sin \varphi \\
       r \cos \varphi \\
       0
    \end{array}
  \right)=
  \left(
    \begin{array}{c}
      -2 \sin \varphi \\
       2 \cos \varphi \\
       0
    \end{array}
  \right); \enspace
  \frac{\partial {\bf x}}{\partial z} =
  \left(
    \begin{array}{c}
      0 \\
      0 \\
      1
    \end{array}
  \right)
$$
$$
   \Longrightarrow
  \left(
    \frac{\partial {\bf x}}{\partial \varphi} \times
    \frac{\partial {\bf x}}{\partial z}
  \right) =
  \left(
    \begin{array}{c}
      2 \cos \varphi \\
      2 \sin \varphi \\
      0
    \end{array}
  \right)
$$
\begin{eqnarray*}
  \Longrightarrow
  F_3 & = & \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \int \limits_{z=0}^3 \!\! dz
    \left(
      \begin{array}{c}
        4 \cdot 2 \cos \varphi \\
        -2(2 \sin \varphi)^2 \\
        z^2
      \end{array}
    \right)
    \left(
      \begin{array}{c}
        2 \cos \varphi \\
        2 \sin \varphi \\
        0
      \end{array}
    \right)= \\
    &= & \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \int \limits_{z=0}^3 \!\! dz
    \left(16 \cos^2 \varphi -16 \sin^3 \varphi\right) = \\
  & = & 3 \cdot 16 \int \limits_{\varphi =0}^{2 \pi} \!\! d\varphi
    \left( \cos^2\varphi - \sin^3 \varphi \right) = \\
  & = &
    \Biggl[
      \begin{array}{rcl}
        \int \cos^2 \varphi \, d\varphi & = & \frac{\varphi}{2}+
          \frac{1}{4} \sin 2 \varphi \\
        \int \sin^3 \varphi \, d\varphi & = & -\cos \varphi+
          \frac{1}{3} \cos^3 \varphi
      \end{array}
    \Biggr] \, = \\
    & = & 3 \cdot 16
    \Biggl\{
      \frac{2 \pi}{2}-
      \underbrace
        {\left[
          \left(1+\frac{1}{3}\right)-\left(1+\frac{1}{3}\right)
        \right]}
      _{=0}
    \Biggr\}=48 \pi
\end{eqnarray*}
For the flux through the surfaces one thus obtains
$$ \oint \limits_F {\bf w} \cdot d{\bf f}=F_1+F_2+F_3=84 \pi .$$





\item
Let us verify  some specific examples of Stokes' theorem in three dimensions,
\index{Stokes' theorem}
stating that
\begin{equation}
\int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f}   =\oint \limits_{{\cal C}_{\cal F}} {\bf b} \cdot d{\bf s}
.   \label{2011-m-stokes}
\end{equation}

Consider the vector field ${\bf b} = \begin{pmatrix} yz , -xz , 0 \end{pmatrix}^T$
and the volume bounded by spherical cap
formed by the plane at $z = a / \sqrt{2}$ of a sphere of radius $a$ centered around the origin.

Let us first look at the left hand side $\int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f} $
of Eq. (\ref{2011-m-stokes}):
$$
  {\bf b}=
  \left(
    \begin{array}{c}
      yz \\
      -xz \\
      0
    \end{array}
  \right)
  \Longrightarrow
  \textrm{rot } {\bf b} = \nabla \times {\bf b} =
  \left(
    \begin{array}{c}
      x \\
      y \\
      -2z
    \end{array}
  \right)
$$
Let us transform this into spherical coordinates:
$$
  {\bf x}=
  \left(
    \begin{array}{c}
      r\sin \theta \cos \varphi \\
      r\sin \theta \sin \varphi \\
      r\cos \theta
    \end{array}
  \right)
$$
$$
  \Rightarrow
  \frac{\partial {\bf x}}{\partial \theta}=
  r
  \left(
    \begin{array}{c}
      \cos \theta \cos \varphi \\
      \cos \theta \sin \varphi \\
      -\sin \theta
    \end{array}
  \right);\quad
  \frac{\partial {\bf x}}{\partial \varphi}=
  r
  \left(
    \begin{array}{c}
      -\sin \theta \sin \varphi \\
      \sin \theta \cos \varphi \\
      0
    \end{array}
  \right)
$$
$$
  d{\bf f}=
  \left(
    \frac{\partial {\bf x}}{\partial \theta} \times
    \frac{\partial {\bf x}}{\partial \varphi}
  \right)
  d\theta \, d\varphi=
  r^2
  \left(
    \begin{array}{c}
      \sin^2 \theta \cos \varphi \\
      \sin^2 \theta \sin \varphi \\
      \sin \theta \cos \theta
    \end{array}
  \right)
  d\theta \, d\varphi \label{eqn:1.14.1}
$$
$$
  \nabla \times {\bf b}=
  r
  \left(
    \begin{array}{c}
      \sin \theta \cos \varphi \\
      \sin \theta \sin \varphi \\
      -2\cos \theta
    \end{array}
  \right)
$$

\begin{eqnarray*}
  \int \limits_{\cal F} \textrm{rot } {\bf b} \cdot d{\bf f} & = &
  \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
  \int \limits_{\varphi=0}^{2 \pi} \!\! d\varphi \, a^3
  \left(
    \begin{array}{c}
      \sin \theta \cos \varphi \\
      \sin \theta \sin \varphi \\
      -2\cos \theta
    \end{array}
  \right)
  \left(
    \begin{array}{c}
      \sin^2 \theta \cos \varphi \\
      \sin^2 \theta \sin \varphi \\
      \sin \theta \cos \theta
    \end{array}
  \right)= \\
  & = & a^3
  \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
  \int \limits_{\varphi=0}^{2 \pi} \!\! d\varphi
  \Biggl[
    \sin^3\theta
    \underbrace{\left(\cos^2\varphi+\sin^2\varphi\right)}_{=1}-
      2\sin\theta\cos^2\theta
  \Biggr] = \\
  & = & 2 \pi a^3
  \left[
    \int \limits_{\theta=0}^{\pi/4} \!\! d\theta
    \left(1-\cos^2\theta\right)\sin\theta-
    2\int \limits_{\theta=0}^{\pi/4} \!\! d\theta
    \sin \theta \cos^2 \theta
  \right] = \\
  & = & 2 \pi a^3
  \int \limits_{\theta=0}^{\pi/4}d\theta
  \sin \theta \left(1-3\cos^2\theta\right) = \\
  && \left[
    \begin{array}{l}
      \mbox{transformation of variables: } \\
      \cos \theta = u \Rightarrow du=-\sin \theta d\theta
      \Rightarrow d\theta=-\frac{du}{\sin \theta}
    \end{array}
  \right] \\
  & = & 2 \pi a^3
  \int \limits_{\theta=0}^{\pi/4}(-du)\left(1-3u^2\right)=
    2 \pi a^3
    \left( \frac{3u^3}{3}-u \right)\Biggr|_{\theta=0}^{\pi/4}= \\
  & = & 2 \pi a^3
    \left(\cos^3\theta-\cos\theta \right)\Biggr|_{\theta=0}^{\pi/4}=
    2 \pi a^3
    \left(\frac{2\sqrt{2}}{8}-\frac{\sqrt{2}}{2} \right) = \\
  & = & \frac{2 \pi a^3}{8}\left(-2\sqrt{2}\right)=
    -\frac{\pi a^3 \sqrt{2}}{2}
\end{eqnarray*}


Now consider the right hand side $\oint \limits_{{\cal C}_{\cal F}} {\bf b} \cdot d{\bf s}$
of Eq. (\ref{2011-m-stokes}).
The radius $r'$ of the circle  surface
$\{(x,y,z) \mid x,y \in {\Bbb R} ,z=a/\sqrt{2}\}$ bounded by the sphere with radius $a$
is determined by
$ a^2 =(r')^2 +(a/ \sqrt{2})^2$; hence, $r' =a/\sqrt{2}$.
The curve of integration ${\cal C}_{\cal F}$ can be parameterized by
$$\{(x,y,z) \mid
x={a\over \sqrt{2}} \cos \varphi,
y={a\over \sqrt{2}} \sin \varphi,
z={a\over \sqrt{2}}\}.$$
Therefore,
$$
  {\bf x} = a
  \left(
    \begin{array}{c}
      \frac{1}{\sqrt2}\cos\varphi \\[1ex]
      \frac{1}{\sqrt2}\sin\varphi \\[1ex]
      \frac{1}{\sqrt2}
    \end{array}
  \right)=
  \frac{a}{\sqrt{2}}
  \left(
    \begin{array}{c}
      \cos\varphi \\
      \sin\varphi \\
      1
    \end{array}
  \right)
\in {\cal C}_{\cal F}
$$
Let us transform this into polar coordinates:
$$
  d{\bf s}=\frac{d{\bf x}}{d\varphi} \,d\varphi =
  \frac{a}{\sqrt{2}}
  \left(
    \begin{array}{c}
      -\sin\varphi \\
      \cos\varphi \\
      0
    \end{array}
  \right) d\varphi
$$
$$
  {\bf b}=
  \left(
    \begin{array}{c}
      \frac{a}{\sqrt{2}}\sin\varphi\cdot\frac{a}{\sqrt{2}} \\
      -\frac{a}{\sqrt{2}}\cos\varphi\cdot\frac{a}{\sqrt{2}} \\
      0
    \end{array}
  \right)=
  \frac{a^2}{2}
  \left(
    \begin{array}{c}
      \sin\varphi \\
      -\cos\varphi \\
      0
    \end{array}
  \right)
$$
Hence the circular integral is given by
$$
  \oint \limits_{{\cal C}_F} {\bf b} \cdot d{\bf s}=
  \frac{a^2}{2}\frac{a}{\sqrt{2}}
  \int \limits_{\varphi=0}^{2 \pi}
  \underbrace
    {\left(-\sin^2\varphi-\cos^2\varphi\right)}
  _{=-1}
  \, d\varphi =
  -\frac{a^3}{2\sqrt{2}}2 \pi=-\frac{a^3 \pi}{\sqrt{2}}
.
$$


\item
In machine learning, a linear regression {\it Ansatz}~\cite{Goodfellow-et-al-2016-Book} is to find a linear model for the prediction of some unknown
\index{linear regression}
observable, given some anecdotal instances of its performance.
More formally, let
$y$ be an arbitrary observable which depends
on $n$ parameters $x_1, \ldots , x_n$  by linear means; that is, by
\begin{equation}
y = \sum_{i=1}^n x_i r_i = \langle  {\bf x}\vert {\bf r} \rangle,
\label{2016-ml-ansatz-lr}
\end{equation}
where $\langle {\bf x} \vert = (\vert {\bf x} \rangle )^T$ is the transpose
of the vector $\vert {\bf x} \rangle$.
The tuple
\begin{equation}
\vert {\bf r} \rangle = \begin{pmatrix} r_1, \ldots , r_n \end{pmatrix}^T
\label{2016-ml-ansatz-vectorweights}
\end{equation}
contains the unknown weights of the approximation --
the ``theory,'' if you like --
and $\langle  {\bf a}\vert {\bf b} \rangle = \sum_i a_ib_i$ stands for the Euclidean scalar product of the tuples interpreted
as (dual) vectors in $n$-dimensional (dual) vector space $\mathbb{R}^n$.

Given are $m$ known instances of (\ref{2016-ml-ansatz-lr}); that is, suppose $m$ pairs
$\begin{pmatrix}z_j, \vert {\bf x}_j \rangle \end{pmatrix}$ are known.
These data can be bundled into an $m$-tuple
\begin{equation}
\vert {\bf z} \rangle \equiv \begin{pmatrix} z_{j_1}, \ldots , z_{j_m} \end{pmatrix}^T,
\end{equation}
and an $(m \times n)$-matrix
\begin{equation}
\textsf{\textbf{X}} \equiv
\begin{pmatrix}
x_{{j_1}{i_1}} & \ldots & x_{{j_1}{i_n}}\\
\vdots & \vdots & \vdots \\
x_{{j_m}{i_1}} & \ldots & x_{{j_m}{i_n}}
\end{pmatrix}
\end{equation}
where $j_1,\ldots , j_m$ are arbitrary permutations of $1,\ldots ,m$,
and the matrix rows are just the vectors
$\vert {\bf x}_{j_k} \rangle \equiv \begin{pmatrix} x_{{j_k}{i_1}} & \ldots , x_{{j_k}{i_n}} \end{pmatrix}^T$.

The task is to compute a ``good'' estimate of $\vert {\bf r} \rangle$;
that is, an estimate of $\vert {\bf r} \rangle$
which allows an ``optimal'' computation of the prediction $y$.

Suppose that a good way to measure the performance
of the prediction from some particular definite but unknown $\vert {\bf r} \rangle $
with respect to the $m$ given data
$\begin{pmatrix}z_j, \vert {\bf x}_j \rangle \end{pmatrix}$
is by the mean squared error (MSE)
\begin{equation}
\begin{split}
\text{MSE}
=
\frac{1}{m}
\left\|
\vert {\bf y} \rangle - \vert {\bf z} \rangle
\right\|^2
=
\frac{1}{m}
\left\|
\textsf{\textbf{X}} \vert {\bf r} \rangle
 - \vert {\bf z}   \rangle
\right\|^2
\\
=
\frac{1}{m}
\left(
\textsf{\textbf{X}} \vert {\bf r} \rangle
 - \vert {\bf z}   \rangle
\right)^T
\left(
\textsf{\textbf{X}} \vert {\bf r} \rangle
 - \vert {\bf z}   \rangle
\right)
\\
=
\frac{1}{m}
\left(
\langle {\bf r} \vert \textsf{\textbf{X}}^T
- \langle {\bf z} \vert
\right)
\left(
\textsf{\textbf{X}} \vert {\bf r} \rangle
- \vert {\bf z}   \rangle
\right)
\\
=
\frac{1}{m} \left(
\langle {\bf r} \vert \textsf{\textbf{X}}^T \textsf{\textbf{X}} \vert {\bf r} \rangle
- \langle {\bf z} \vert  \textsf{\textbf{X}} \vert {\bf r} \rangle
- \langle {\bf r} \vert \textsf{\textbf{X}}^T   \vert {\bf z}   \rangle
+ \langle {\bf z} \vert    {\bf z}   \rangle
\right)
\\
=
\frac{1}{m} \left[
\langle {\bf r} \vert \textsf{\textbf{X}}^T \textsf{\textbf{X}} \vert {\bf r} \rangle
- \langle {\bf z} \vert \left( \langle  {\bf r} \vert \textsf{\textbf{X}}^T\right)^T
- \langle {\bf r} \vert \textsf{\textbf{X}}^T   \vert {\bf z}   \rangle
+ \langle {\bf z} \vert    {\bf z}   \rangle
\right]
\\
=
\frac{1}{m} \left\{
\langle {\bf r} \vert \textsf{\textbf{X}}^T \textsf{\textbf{X}} \vert {\bf r} \rangle
- \left[  \left( \langle  {\bf r} \vert \textsf{\textbf{X}}^T \right)^T  \right]^T \vert {\bf z} \rangle
\right .\\ \left.
- \langle {\bf r} \vert \textsf{\textbf{X}}^T   \vert {\bf z}   \rangle
+ \langle {\bf z} \vert    {\bf z}   \rangle
\right\}
\\
=
\frac{1}{m} \left(
\langle {\bf r} \vert \textsf{\textbf{X}}^T \textsf{\textbf{X}} \vert {\bf r} \rangle
- 2 \langle {\bf r} \vert \textsf{\textbf{X}}^T   \vert {\bf z}   \rangle
+ \langle {\bf z} \vert    {\bf z}   \rangle
\right)
.
\end{split}
\label{2016-ml-MSE}
\end{equation}

In order to minimize the mean squared error (\ref{2016-ml-MSE}) with respect to variations of $\vert {\bf r} \rangle$
one obtains a condition for ``the linear theory'' $\vert {\bf y} \rangle$
by setting its derivatives (its gradient) to zero; that is
\begin{equation}
\partial_{ \vert {\bf r} \rangle } \text{MSE}
=
{\bf 0}.
\label{2016-ml-MSE-min}
\end{equation}
A lengthy but straightforward computation yields
\begin{equation}
\begin{split}
\frac{\partial }{\partial r_i}
\left(
r_j  \textsf{\textbf{X}}^T_{jk} \textsf{\textbf{X}}_{kl} r_l -2 r_j \textsf{\textbf{X}}^T_{jk}z_k + z_jz_j
\right)
\\
=
\delta_{ij}  \textsf{\textbf{X}}^T_{jk} \textsf{\textbf{X}}_{kl} r_l
+
r_j \textsf{\textbf{X}}^T_{jk} \textsf{\textbf{X}}_{kl}  \delta_{il}
-2 \delta_{ij} \textsf{\textbf{X}}^T_{jk}z_k
\\
=
\textsf{\textbf{X}}^T_{ik} \textsf{\textbf{X}}_{kl} r_l
+
r_j \textsf{\textbf{X}}^T_{jk} \textsf{\textbf{X}}_{ki}
-2 \textsf{\textbf{X}}^T_{ik}z_k
\\
=
\textsf{\textbf{X}}^T_{ik} \textsf{\textbf{X}}_{kl} r_l
+
\textsf{\textbf{X}}^T_{ik} \textsf{\textbf{X}}_{kj} r_j
-2 \textsf{\textbf{X}}^T_{ik}z_k
\\
=
2 \textsf{\textbf{X}}^T_{ik} \textsf{\textbf{X}}_{kj} r_j
-2 \textsf{\textbf{X}}^T_{ik} z_k
\\
\equiv
2\left( \textsf{\textbf{X}}^T \textsf{\textbf{X}} \vert {\bf r} \rangle - \textsf{\textbf{X}}^T   \vert {\bf z} \rangle
\right)
=0
\end{split}
\label{2016-ml-MSE-min-res-der}
\end{equation}
and finally, upon multiplication with
$ \left( \textsf{\textbf{X}}^T \textsf{\textbf{X}}  \right)^{-1}$ from the left,
\begin{equation}
\vert {\bf r} \rangle
=  \left( \textsf{\textbf{X}}^T \textsf{\textbf{X}} \right)^{-1}
\textsf{\textbf{X}}^T \vert {\bf z} \rangle
.
\label{2016-ml-MSE-min-res}
\end{equation}
A short plausibility check for $n=m=1$ yields the linear dependency
$\vert {\bf z} \rangle  =  \textsf{\textbf{X}} \vert {\bf r} \rangle$.




\end{enumerate}

\eexample
}

\section{Some common misconceptions}

\subsection{Confusion between component representation and ``the real thing''}
Given a particular basis, a tensor is uniquely characterized by its components.
However, without reference to a particular basis, any components are just blurbs.

Example (wrong!): a type-1 tensor (i.e., a vector) is given by
$(1,2)$.

Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a rank-1 tensor (i.e., a vector) is given by
$(1,2)$.


\subsection{A matrix is a tensor}

See the above section.
{
\color{blue}
\bexample
Example (wrong!): A matrix is  a  tensor of type (or rank) 2.
\eexample
}
Correct: with respect to the basis $\{(0,1),(1,0)\}$,  a matrix represents a type-2 tensor.
The matrix components are the tensor components.

Also, for non-orthogonal bases, covariant, contravariant, and mixed tensors correspond to different matrices.


\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright \decofourleft
%\aldine X \decoone c
\floweroneright
% \aldineleft ] \decosix g \leafleft
% \aldineright Y \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
% \decofourright b \floweroneleft
}
\end{center}


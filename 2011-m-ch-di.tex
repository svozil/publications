\chapter{Distributions as generalized functions}
\label{2011-m-ch:gf}

In the days when Dirac developed quantum mechanics, there was a need to define
``singular scalar products'' such as ``$\langle x \mid y \rangle = \delta (x-y)$,''
with some generalization of the Kronecker delta function $\delta_{ij}$
which is zero whenever $x\neq y$ and ``large enough'' needle shaped (see Fig. \ref{2011-m-fdeltaplot}) to yield unity when
integrated over the entire reals; that is, ``$\int_{-\infty}^\infty \langle x \mid y \rangle dy =\int_{-\infty}^\infty \delta (x-y) dy =1$.''
\begin{marginfigure}%
%TeXCAD (http://texcad.sf.net/) Picture. File: [1.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{4.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(105,100)(0,0)
\put(0,0){\line(1,0){100}}
\put(50,0){\line(0,1){100}}
\thicklines
\put(50,0){{\color{orange}\vector(0,1){90}}}
\put(0,0){{\color{orange}\line(1,0){100}}}
\put(65,100){\makebox(0,0)[cc]{{\color{orange}$\delta(x)$}}}
\put(105,0){\makebox(0,0)[cc]{$x$}}
\end{picture}
\caption{Dirac's $\delta$-function as a ``needle shaped'' generalized function.}
  \label{2011-m-fdeltaplot}
\end{marginfigure}

\begin{marginfigure}%
%TeXCAD (http://texcad.sf.net/) Picture. File: [1.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{4.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(105,120)(0,0)
\put(0,0){\line(1,0){100}}
\put(50,0){\line(0,1){100}}
\thicklines
\put(50,0){{\color{orange}\vector(0,1){110}} }
\put(60,110){\makebox(0,0)[cc]{{\color{orange}$\delta(x)$}}}
\put(105,0){\makebox(0,0)[cc]{$x$}}
%
{\color{blue}
\put(45,0){\circle*{2.5}}
\put(40,0){\circle*{2.5}}
\put(30,0){\circle*{2.5}}
\put(10,0){\circle*{2.5}}
\put(55,0){\circle*{2.5}}
\put(60,0){\circle*{2.5}}
\put(70,0){\circle*{2.5}}
\put(90,0){\circle*{2.5}}
%-
\thicklines
\put(45,100){\circle{2.5}}
\put(40,50){\circle{2.5}}
\put(30,25){\circle{2.5}}
\put(10,12.5){\circle{2.5}}
\put(55,100){\circle{2.5}}
\put(60,50){\circle{2.5}}
\put(70,25){\circle{2.5}}
\put(90,12.5){\circle{2.5}}
%-
\put(45,100){\line(1,0){10}}
\put(40,50){\line(1,0){20}}
\put(30,25){\line(1,0){40}}
\put(10,12.5){\line(1,0){80}}
% base lines
\put(10,0){\line(1,0){35}}
\put(55,0){\line(1,0){35}}
\put(0,0){\line(1,0){45}}
\put(55,0){\line(1,0){45}}
%
\thinlines
%\dottedline(10,0)(10,12.5)
\multiput(9.824,-.176)(0,.96154){14}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(90,0)(90,12.5)
\multiput(89.824,-.176)(0,.96154){14}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(30,0)(30,25)
\multiput(29.824,-.176)(0,.96154){27}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(70,0)(70,25)
\multiput(69.824,-.176)(0,.96154){27}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(40,.25)(40,50)
\multiput(39.824,.074)(0,.995){51}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(60,.25)(60,50)
\multiput(59.824,.074)(0,.995){51}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(45,0)(45,100)
\multiput(44.824,-.176)(0,.990099){102}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(55,0)(55,100)
\multiput(54.824,-.176)(0,.990099){102}{{\rule{.4pt}{.4pt}}}
%\end
\put(105,12.5){\makebox(0,0)[cc]{$\delta_1(x)$}}
\put(85,25){\makebox(0,0)[cc]{$\delta_2(x)$}}
\put(75,50){\makebox(0,0)[cc]{$\delta_3(x)$}}
\put(65,100){\makebox(0,0)[cc]{$\delta_4(x)$}}
}
\end{picture}
\caption{Delta sequence approximating Dirac's $\delta$-function as a more and more ``needle shaped'' generalized function.}
  \label{2011-m-fdeltaplotnseq}
\end{marginfigure}
One of the first attempts to formalize these objects was in terms
of functional limits.
Take, for instance, the {\em delta sequence}
\index{delta sequence}
which is a sequence of strongly peaked functions for which
in some limit
the sequences $\{f_n(x-y)\}$ with, for instance,
\begin{equation}
\delta_n(x-y) =
\left\{
\begin{array}{rl}
n & \textrm{ for } y - \frac{1}{2n}  < x < y+ \frac{1}{2n} \\
0& \textrm{ else }
\end{array}
\right.
\label{2011-m-deltseq}
\end{equation}
 become the delta function $\delta (x-y)$.
That is,
in the fuctional sense (see below)
\begin{equation}
\lim_{n\rightarrow \infty} \delta_n(x-y)= \delta (x-y) .
\end{equation}
Note that the area of this particular $\delta_n(x-y)$ above the $x$-axes
is independent of $n$, since its width is $1/n$ and the height is $n$.



Other delta sequences are
\begin{eqnarray}
\delta_n(x)
&=& \frac{n}{\sqrt{\pi}} e^{-n^2 x^2},\\
&=&
\frac{1}{\pi}   \frac{\sin (n x)}{ x},\\
&=&
= (1\mp i)\left( {n\over 2\pi }\right)^{1\over 2} e^{\pm inx^2}  \\
&=&
\frac{1}{\pi x}  \frac{e^{inx}-e^{-inx}}{2i} ,\\
&=&
\frac{1}{\pi}  \frac{n  e^{-x^2}}{1+n^2x^2} ,\\
&=&
\frac{1}{2\pi } \int_{-n}^n e^{ixt} dt  = \frac{1}{2\pi i x} \left. e^{ixt}\right|_{-n}^n    ,\\
&=&
\frac{1}{2\pi} \frac{\sin \left[\left( n+\frac{1}{2}\right) x \right]  }{\sin \left( \frac{1}{2}x \right)   },\\
&=&
\frac{1}{\pi}\frac{n}{1+ n^2x^2},     \\
&=&
\frac{n}{ \pi}\left(\frac{\sin (nx)}{nx}\right)^2.
\end{eqnarray}
Other commonly used limit forms of the $\delta $-function are the Gaussian, Lorentzian, and Dirichlet forms
\begin{eqnarray}
\delta_\epsilon (x) &=& \lim_{\epsilon \rightarrow 0} \frac{1}{\sqrt{\pi } \epsilon } e^{-\frac{x^2}{\epsilon^2}} ,  \\
&=& \lim_{\epsilon \rightarrow 0} \frac{1}{\pi} \frac{\epsilon }{x^2+\epsilon^2} ,  \\
&=& \lim_{\epsilon \rightarrow 0} \frac{1}{\pi x} \sin{\epsilon }{x} ,
\end{eqnarray}
respectively.
Again, the limit
\begin{equation}
\delta (x)= \lim_{\epsilon \rightarrow 0} \delta_\epsilon (x)
\end{equation}
has to be understood in the functional sense (see below).

Naturally, such ``needle shaped functions'' were viewed suspiciouly by many mathematicians
at first, but later they embraced these types of functions
\cite{gelfand:1964:gf} by developing a theory of
{\em functional analysis}
\index{functional analysis}
or
{\em distributions}.
\index{distributions}

Distributions $q$ are quasi-functions of one or more variables
which can be interpreted in the ``weak sense'' as {\em functionals};
that is, by integrating them with suitable, ``good'' test functions $\varphi$
of the class $D$ of test functions
\begin{equation}
F(\varphi )=\int_{-\infty}^\infty
q(x) \varphi (x) dx.
\end{equation}
Recall that a functional is some mathematical entity which maps a function or another mathematical object
into scalars in a linear manner; that is,
\begin{equation}
F(c_1\varphi_1+c_2\varphi_2)=
c_1F(\varphi_1)  +
c_2F(c_2\varphi_2\varphi_2).
\end{equation}

In particular, the $\delta$ function maps to
\begin{equation}
F(\varphi )=\int_{-\infty}^\infty
\delta(x-y) \varphi (x) dx = \varphi(y).
\end{equation}


{
\color{blue}
\bexample

Let us see if the sequence $\{ \delta_n\}$ with
$$\delta_n(x-y) =
\left\{
\begin{array}{rl}
n & \textrm{ for } y - \frac{1}{2n}  < x < y+ \frac{1}{2n} \\
0& \textrm{ else }
\end{array}
\right.$$
defined in Eq. (\ref{2011-m-deltseq}) and depicted
in Fig.
\ref{2011-m-fdeltaplotnseq}
is a delta sequence;
that is, if, for large $n$, it converges to $\delta$ in a functional sense.
In order to verify this claim, we have to integrate $\delta_n(x)$
with ``good'' test functions $ \varphi (x)$ and take the limit $n\rightarrow \infty$;
if the result is $ \varphi (0)$, then we can identify $\delta_n(x)$ in this limit with $\delta (x)$
(in the functional sense).
\begin{equation}
\begin{array}{l}
\lim_{n\rightarrow \infty} \int_{-\infty}^\infty \delta_n(x) \varphi (x) dx \\
\qquad =
\lim_{n\rightarrow \infty} \int_{- \frac{1}{2n}}^\frac{1}{2n} n \varphi (x) dx    \\
\qquad\qquad\textrm{[variable transformation:}  \\
\qquad\qquad\quad  u=2n(x-y),x=\frac{u}{2n}+y,    \\
\qquad\qquad\quad  du = 2n dx, -1\le u \le 1\textrm{]} \\
\qquad = \lim_{n\rightarrow \infty} \int_{- 1}^1 n \varphi (\frac{ u}{2n}+y) \frac{du}{2n}       \\
\qquad =
\lim_{n\rightarrow \infty} \frac{1}{2} \int_{- 1}^1 \varphi (\frac{ u}{2n}+y) du     \\
\qquad =
\frac{1}{2} \int_{- 1}^1 \lim_{n\rightarrow \infty} \varphi (\frac{ u}{2n}+y) du    \\
\qquad =
\frac{1}{2} \varphi (y) \int_{- 1}^1  du     \\
\qquad =
\varphi (y)
.
\end{array}
\end{equation}
Hence, in the functional sense,
this limit yields the $\delta$-function.

\eexample
}

By invoking test functions, we would like to be able to differentiate distributions very much like ordinary functions.
We would also like to transfer differentiations to the functional context.
How can this be impemented in terms of possible ``good'' properties we require from the behaviour of test functions, in accord with our wishes?

Consider the partial integration
obtained from $(uv)' = u'v+uv'$; thus
$\int (uv)' = \int u'v+\int uv'$,
and finally   $\int u'v = \int (uv)'  -\int uv'$.
By identifying $u$ with the generalized function $g$ (such as, for instance  $\delta$),
and $v$ with the test function $\varphi$, respectively, we obtain
\begin{equation}
\begin{array}{l}
\int_{-\infty}^\infty
g'(x)\varphi(x)
dx      \\
\qquad
=
\left.
g(x)\varphi(x)\right|_{-\infty}^\infty
-  \int_{-\infty}^\infty
g(x)\varphi'(x)
dx           \\
\qquad
=

g(\infty)\varphi(\infty) - g(-\infty)\varphi(-\infty)
-  \int_{-\infty}^\infty
g(x)\varphi'(x)
dx
.
\end{array}
\end{equation}
We can see here the two main requirements for ``good'' test functions:
\begin{enumerate}
\item
that they vanish at infinity -- that is, that their support
(the set of arguments $x$ where $g(x)\neq 0$) is finite; and
\item
that they are continuosly differentiable -- indeed, by induction, that they are arbitrarily often differentiable.
\end{enumerate}
Hence
we assume \cite{schwartz} that the tests functions $\varphi$
are infinitely often differentiable, and that their support is compact.
Compact support means that $\varphi (x)$ does not vanish only at a finite, bounded region of $x$.


Such a ``good'' test function is, for instance,
\begin{equation}
\varphi_{\sigma ,a}(x)=
 \cases{e^{-{1\over 1-((x-a)/\sigma )^2}} &for $\vert
 {x-a\over \sigma }\vert <1$;\cr
                           0 &else. \cr}
\end{equation}

{\color{OliveGreen}
\bproof
In order to show that $\varphi_{\sigma ,a}$ is a suitable test function,
we have to proof its infinite differetiability, as well as the compactness of its support
$M_{\varphi_{\sigma, a}}$.
Let
$$
   \varphi_{\sigma,a}(x):=\varphi\left({ x- a
   \over\sigma}\right)
$$
and thus
$$
   \varphi(x)=\left\{\begin{array}{cl}
                 \displaystyle e^{-{1\over1-x^2}}&\mbox{for}\quad |x|<1\\
                 0 &\mbox{for}\quad |x|\geq 1
              \end{array}\right.
$$
This function is drawn in Fig. \ref{2011-m-fd1}.
\begin{marginfigure}
 % GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.12pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\begin{picture}(809,809)(0,0)
\font\gnuplot=cmr10 at 10pt
\gnuplot
{\color{black}
\put(361,82.0){\line(1,0){809}}
\put(767,645){\makebox(0,0)[l]{$0.37$}}
\put(361,12){\makebox(0,0){$-1$}}
\put(1100,12){\makebox(0,0){$1$}}
\put(767,786){\makebox(0,0)[l]{$\varphi( x)$}}
\put(731,82){\line(0,1){704}}
}
 \thicklines
{\color{orange}
\put(100,82.0){\line(1,0){290}}
\put(378,81.34){\rule{2.650pt}{0.800pt}}
\multiput(378.00,80.34)(5.500,2.000){2}{\rule{1.325pt}{0.800pt}}
\multiput(389.00,85.40)(0.700,0.520){9}{\rule{1.300pt}{0.125pt}}
\multiput(389.00,82.34)(8.302,8.000){2}{\rule{0.650pt}{0.800pt}}
\multiput(401.40,92.00)(0.512,0.838){15}{\rule{0.123pt}{1.509pt}}
\multiput(398.34,92.00)(11.000,14.868){2}{\rule{0.800pt}{0.755pt}}
\multiput(412.40,110.00)(0.512,1.187){15}{\rule{0.123pt}{2.018pt}}
\multiput(409.34,110.00)(11.000,20.811){2}{\rule{0.800pt}{1.009pt}}
\multiput(423.41,135.00)(0.511,1.349){17}{\rule{0.123pt}{2.267pt}}
\multiput(420.34,135.00)(12.000,26.295){2}{\rule{0.800pt}{1.133pt}}
\multiput(435.40,166.00)(0.512,1.636){15}{\rule{0.123pt}{2.673pt}}
\multiput(432.34,166.00)(11.000,28.453){2}{\rule{0.800pt}{1.336pt}}
\multiput(446.40,200.00)(0.512,1.636){15}{\rule{0.123pt}{2.673pt}}
\multiput(443.34,200.00)(11.000,28.453){2}{\rule{0.800pt}{1.336pt}}
\multiput(457.40,234.00)(0.512,1.586){15}{\rule{0.123pt}{2.600pt}}
\multiput(454.34,234.00)(11.000,27.604){2}{\rule{0.800pt}{1.300pt}}
\multiput(468.40,267.00)(0.512,1.536){15}{\rule{0.123pt}{2.527pt}}
\multiput(465.34,267.00)(11.000,26.755){2}{\rule{0.800pt}{1.264pt}}
\multiput(479.41,299.00)(0.511,1.349){17}{\rule{0.123pt}{2.267pt}}
\multiput(476.34,299.00)(12.000,26.295){2}{\rule{0.800pt}{1.133pt}}
\multiput(491.40,330.00)(0.512,1.337){15}{\rule{0.123pt}{2.236pt}}
\multiput(488.34,330.00)(11.000,23.358){2}{\rule{0.800pt}{1.118pt}}
\multiput(502.40,358.00)(0.512,1.287){15}{\rule{0.123pt}{2.164pt}}
\multiput(499.34,358.00)(11.000,22.509){2}{\rule{0.800pt}{1.082pt}}
\multiput(513.40,385.00)(0.512,1.187){15}{\rule{0.123pt}{2.018pt}}
\multiput(510.34,385.00)(11.000,20.811){2}{\rule{0.800pt}{1.009pt}}
\multiput(524.40,410.00)(0.512,1.038){15}{\rule{0.123pt}{1.800pt}}
\multiput(521.34,410.00)(11.000,18.264){2}{\rule{0.800pt}{0.900pt}}
\multiput(535.41,432.00)(0.511,0.897){17}{\rule{0.123pt}{1.600pt}}
\multiput(532.34,432.00)(12.000,17.679){2}{\rule{0.800pt}{0.800pt}}
\multiput(547.40,453.00)(0.512,0.888){15}{\rule{0.123pt}{1.582pt}}
\multiput(544.34,453.00)(11.000,15.717){2}{\rule{0.800pt}{0.791pt}}
\multiput(558.40,472.00)(0.512,0.838){15}{\rule{0.123pt}{1.509pt}}
\multiput(555.34,472.00)(11.000,14.868){2}{\rule{0.800pt}{0.755pt}}
\multiput(569.40,490.00)(0.512,0.739){15}{\rule{0.123pt}{1.364pt}}
\multiput(566.34,490.00)(11.000,13.170){2}{\rule{0.800pt}{0.682pt}}
\multiput(580.40,506.00)(0.512,0.639){15}{\rule{0.123pt}{1.218pt}}
\multiput(577.34,506.00)(11.000,11.472){2}{\rule{0.800pt}{0.609pt}}
\multiput(591.41,520.00)(0.511,0.536){17}{\rule{0.123pt}{1.067pt}}
\multiput(588.34,520.00)(12.000,10.786){2}{\rule{0.800pt}{0.533pt}}
\multiput(603.40,533.00)(0.512,0.539){15}{\rule{0.123pt}{1.073pt}}
\multiput(600.34,533.00)(11.000,9.774){2}{\rule{0.800pt}{0.536pt}}
\multiput(613.00,546.40)(0.543,0.514){13}{\rule{1.080pt}{0.124pt}}
\multiput(613.00,543.34)(8.758,10.000){2}{\rule{0.540pt}{0.800pt}}
\multiput(624.00,556.40)(0.611,0.516){11}{\rule{1.178pt}{0.124pt}}
\multiput(624.00,553.34)(8.555,9.000){2}{\rule{0.589pt}{0.800pt}}
\multiput(635.00,565.40)(0.611,0.516){11}{\rule{1.178pt}{0.124pt}}
\multiput(635.00,562.34)(8.555,9.000){2}{\rule{0.589pt}{0.800pt}}
\multiput(646.00,574.40)(0.913,0.526){7}{\rule{1.571pt}{0.127pt}}
\multiput(646.00,571.34)(8.738,7.000){2}{\rule{0.786pt}{0.800pt}}
\multiput(658.00,581.38)(1.432,0.560){3}{\rule{1.960pt}{0.135pt}}
\multiput(658.00,578.34)(6.932,5.000){2}{\rule{0.980pt}{0.800pt}}
\multiput(669.00,586.38)(1.432,0.560){3}{\rule{1.960pt}{0.135pt}}
\multiput(669.00,583.34)(6.932,5.000){2}{\rule{0.980pt}{0.800pt}}
\put(680,590.34){\rule{2.400pt}{0.800pt}}
\multiput(680.00,588.34)(6.019,4.000){2}{\rule{1.200pt}{0.800pt}}
\put(691,593.84){\rule{2.650pt}{0.800pt}}
\multiput(691.00,592.34)(5.500,3.000){2}{\rule{1.325pt}{0.800pt}}
\put(702,596.34){\rule{2.891pt}{0.800pt}}
\multiput(702.00,595.34)(6.000,2.000){2}{\rule{1.445pt}{0.800pt}}
\put(714,597.84){\rule{2.650pt}{0.800pt}}
\multiput(714.00,597.34)(5.500,1.000){2}{\rule{1.325pt}{0.800pt}}
\multiput(736.00,598.34)(5.500,-1.000){2}{\rule{1.325pt}{0.800pt}}
\put(747,596.34){\rule{2.891pt}{0.800pt}}
\multiput(747.00,597.34)(6.000,-2.000){2}{\rule{1.445pt}{0.800pt}}
\put(759,593.84){\rule{2.650pt}{0.800pt}}
\multiput(759.00,595.34)(5.500,-3.000){2}{\rule{1.325pt}{0.800pt}}
\put(770,590.34){\rule{2.400pt}{0.800pt}}
\multiput(770.00,592.34)(6.019,-4.000){2}{\rule{1.200pt}{0.800pt}}
\multiput(781.00,588.06)(1.432,-0.560){3}{\rule{1.960pt}{0.135pt}}
\multiput(781.00,588.34)(6.932,-5.000){2}{\rule{0.980pt}{0.800pt}}
\multiput(792.00,583.06)(1.432,-0.560){3}{\rule{1.960pt}{0.135pt}}
\multiput(792.00,583.34)(6.932,-5.000){2}{\rule{0.980pt}{0.800pt}}
\multiput(803.00,578.08)(0.913,-0.526){7}{\rule{1.571pt}{0.127pt}}
\multiput(803.00,578.34)(8.738,-7.000){2}{\rule{0.786pt}{0.800pt}}
\multiput(815.00,571.08)(0.611,-0.516){11}{\rule{1.178pt}{0.124pt}}
\multiput(815.00,571.34)(8.555,-9.000){2}{\rule{0.589pt}{0.800pt}}
\multiput(826.00,562.08)(0.611,-0.516){11}{\rule{1.178pt}{0.124pt}}
\multiput(826.00,562.34)(8.555,-9.000){2}{\rule{0.589pt}{0.800pt}}
\multiput(837.00,553.08)(0.543,-0.514){13}{\rule{1.080pt}{0.124pt}}
\multiput(837.00,553.34)(8.758,-10.000){2}{\rule{0.540pt}{0.800pt}}
\multiput(849.40,540.55)(0.512,-0.539){15}{\rule{0.123pt}{1.073pt}}
\multiput(846.34,542.77)(11.000,-9.774){2}{\rule{0.800pt}{0.536pt}}
\multiput(860.41,528.57)(0.511,-0.536){17}{\rule{0.123pt}{1.067pt}}
\multiput(857.34,530.79)(12.000,-10.786){2}{\rule{0.800pt}{0.533pt}}
\multiput(872.40,514.94)(0.512,-0.639){15}{\rule{0.123pt}{1.218pt}}
\multiput(869.34,517.47)(11.000,-11.472){2}{\rule{0.800pt}{0.609pt}}
\multiput(883.40,500.34)(0.512,-0.739){15}{\rule{0.123pt}{1.364pt}}
\multiput(880.34,503.17)(11.000,-13.170){2}{\rule{0.800pt}{0.682pt}}
\multiput(894.40,483.74)(0.512,-0.838){15}{\rule{0.123pt}{1.509pt}}
\multiput(891.34,486.87)(11.000,-14.868){2}{\rule{0.800pt}{0.755pt}}
\multiput(905.40,465.43)(0.512,-0.888){15}{\rule{0.123pt}{1.582pt}}
\multiput(902.34,468.72)(11.000,-15.717){2}{\rule{0.800pt}{0.791pt}}
\multiput(916.41,446.36)(0.511,-0.897){17}{\rule{0.123pt}{1.600pt}}
\multiput(913.34,449.68)(12.000,-17.679){2}{\rule{0.800pt}{0.800pt}}
\multiput(928.40,424.53)(0.512,-1.038){15}{\rule{0.123pt}{1.800pt}}
\multiput(925.34,428.26)(11.000,-18.264){2}{\rule{0.800pt}{0.900pt}}
\multiput(939.40,401.62)(0.512,-1.187){15}{\rule{0.123pt}{2.018pt}}
\multiput(936.34,405.81)(11.000,-20.811){2}{\rule{0.800pt}{1.009pt}}
\multiput(950.40,376.02)(0.512,-1.287){15}{\rule{0.123pt}{2.164pt}}
\multiput(947.34,380.51)(11.000,-22.509){2}{\rule{0.800pt}{1.082pt}}
\multiput(961.40,348.72)(0.512,-1.337){15}{\rule{0.123pt}{2.236pt}}
\multiput(958.34,353.36)(11.000,-23.358){2}{\rule{0.800pt}{1.118pt}}
\multiput(972.41,320.59)(0.511,-1.349){17}{\rule{0.123pt}{2.267pt}}
\multiput(969.34,325.30)(12.000,-26.295){2}{\rule{0.800pt}{1.133pt}}
\multiput(984.40,288.51)(0.512,-1.536){15}{\rule{0.123pt}{2.527pt}}
\multiput(981.34,293.75)(11.000,-26.755){2}{\rule{0.800pt}{1.264pt}}
\multiput(995.40,256.21)(0.512,-1.586){15}{\rule{0.123pt}{2.600pt}}
\multiput(992.34,261.60)(11.000,-27.604){2}{\rule{0.800pt}{1.300pt}}
\multiput(1006.40,222.91)(0.512,-1.636){15}{\rule{0.123pt}{2.673pt}}
\multiput(1003.34,228.45)(11.000,-28.453){2}{\rule{0.800pt}{1.336pt}}
\multiput(1017.40,188.91)(0.512,-1.636){15}{\rule{0.123pt}{2.673pt}}
\multiput(1014.34,194.45)(11.000,-28.453){2}{\rule{0.800pt}{1.336pt}}
\multiput(1028.41,156.59)(0.511,-1.349){17}{\rule{0.123pt}{2.267pt}}
\multiput(1025.34,161.30)(12.000,-26.295){2}{\rule{0.800pt}{1.133pt}}
\multiput(1040.40,126.62)(0.512,-1.187){15}{\rule{0.123pt}{2.018pt}}
\multiput(1037.34,130.81)(11.000,-20.811){2}{\rule{0.800pt}{1.009pt}}
\multiput(1051.40,103.74)(0.512,-0.838){15}{\rule{0.123pt}{1.509pt}}
\multiput(1048.34,106.87)(11.000,-14.868){2}{\rule{0.800pt}{0.755pt}}
\multiput(1061.00,90.08)(0.700,-0.520){9}{\rule{1.300pt}{0.125pt}}
\multiput(1061.00,90.34)(8.302,-8.000){2}{\rule{0.650pt}{0.800pt}}
\put(1072,81.34){\rule{2.650pt}{0.800pt}}
\multiput(1072.00,82.34)(5.500,-2.000){2}{\rule{1.325pt}{0.800pt}}

\put(725.0,600.0){\rule[-0.400pt]{2.650pt}{0.800pt}}
\put(1083.0,82.0){\rule[-0.400pt]{48.662pt}{0.800pt}}
}
\end{picture}
\caption{Plot of a test function $\varphi(x)$. }
\label{2011-m-fd1}
\end{marginfigure}


First, note, by definition, the support  $M_\varphi=(-1,1)$,
because $\varphi (x)$   vanishes outside  $(-1,1)$).

Second, consider the differentiability of $\varphi (x)$;
that is $\varphi\in C^\infty({\Bbb R})$?
Note that
$\varphi^{(0)}=\varphi$ is continuous;
and that $\varphi^{(n)}$ is of the form
$$
   \varphi^{(n)}(x)=\left\{\begin{array}{cl}
                         {P_n(x)\over(x^2-1)^{2n}}    e^{1\over x^2-1}&\mbox{for $|x|<1$}\\
                         0&\mbox{for $|x|\geq1$,}
                    \end{array}\right.
$$
where $P_n(x)$ is a finite polynomial in $x$
 ($\varphi(u)=e^u\Longrightarrow
\varphi'(u)={d\varphi\over du}{du\over dx^2}{dx^2\over dx}=\varphi(u)
\left(-{1\over(x^2-1)^2}\right)2x$ etc.) and $[x=1-\varepsilon]\Longrightarrow
x^2=1-2\varepsilon+\varepsilon^2\Longrightarrow x^2-1=
\varepsilon(\varepsilon-2)$
\begin{eqnarray*}
   \lim_{x\uparrow1}\varphi^{(n)}(x)&=&\lim_{\varepsilon\downarrow0}
      {P_n(1-\varepsilon)\over\varepsilon^{2n}(\varepsilon-2)^{2n}}
      e^{1\over\varepsilon(\varepsilon-2)}=\\
   &=&\lim_{\varepsilon\downarrow0}{P_n(1)\over\varepsilon^{2n}2^{2n}}
      e^{-{1\over2\varepsilon}}=\left[\varepsilon={1\over R}\right]=
      \lim_{R\to\infty}{P_n(1)\over2^{2n}}R^{2n}e^{-{R\over2}}=0,
\end{eqnarray*}
because the power $e^{-x}$ of $e$
decreases stronger
than any polynomial  $x^n$.

Note that the complex continuation
$\varphi (z)$ is not an analytic function and cannot be expanded as a Taylor series on the
entire complex plane ${\Bbb C}$ although it is infinitely often
differentiable on the real axis; that is, although
$\varphi\in C^\infty({\Bbb R})$.
This can be seen from a uniqueness theorem of complex analysis.
%(z.\,B.\ Weinm\"uller-Skriptum, Seite 291):\medskip\\
Let
 $B\subseteq{\Bbb C}$ be a domain, and
let
$z_0\in B$ the limit of a sequence
$\{z_n\}\in B$, $z_n\ne z_0$.
Then it can be shown that, if two
analytic functions
$f$ und $g$ on $B$  coincide in the points $z_n$,
then they coincide on the entire domain $B$.

Not, take  $B={\Bbb R}$ and
the  vanishing analytic function $f$; that is,
$f(x)=0$.
$f(x)$ coincides with $\varphi (x)$ only in
 ${\Bbb R}/M_\varphi$.
As a result, $\varphi$ cannot be analytic.
Indeed, $\varphi_{\sigma,\vec a}(x)$  diverges at $x=a\pm\sigma$.
Hence $\varphi(x)$ cannot be Taylor expanded, and
$$
   C^\infty({\Bbb R}^k){\not\Longrightarrow\atop\Longleftarrow}
   \mbox{analytic function \quad (\"`$C^\infty({\Bbb C}^k)$\/\"')}
$$
\eproof
}

Other ``good'' test function are \cite{schwartz}
\begin{equation}
\left\{\phi_{c,d}(x)\right\}^\frac{1}{n}
\end{equation}
obtained by choosing $n\in {\Bbb N} - 0$
and $a\le c<d\le b$ and by defining
\begin{equation}
\phi_{c,d}(x)=
\cases{e^{-\left( \frac{1}{x-c} + \frac{1}{d-x}
\right)} &for $c<x<d$, and  \cr
                           0 &else. \cr}
\end{equation}

If $\varphi (x)$ is a ``good'' test functen, then
\begin{equation}
x^\alpha P_n (x)\varphi (x)
\end{equation}
with any Polynomial $P_n (x)$, and in particular $x^n\varphi (x)  $ also is
 a ``good'' test function.


Equipped with ``good'' test functions which have a finite support and are infinitely often differentiable,
we can now give meaning to the transference of differential quotients from
the objects entering the integral towards the test function by {\em partial integration}.
First note again that $(uv)' = u'v+uv'$
and thus
$\int (uv)' = \int u'v+\int uv'$
and finally   $\int u'v = \int (uv)'  -\int uv'$.
Hence,     by identifying $u$ with $g$, and $v$ with the test function $\varphi$, we obtain
\begin{equation}
\begin{array}{l}
F'(\varphi):=
\int_{-\infty}^\infty
\left( \frac{d}{dx} q(x)\right) \varphi (x) dx
\\
\qquad =
\left. q(x) \varphi (x) \right|_{x=-\infty}^\infty
- \int_{-\infty}^\infty
q(x)\left( \frac{d}{dx} \varphi (x) \right) dx \\
\qquad =
- \int_{-\infty}^\infty
q(x)\left( \frac{d}{dx} \varphi (x) \right) dx \\
\qquad =-F(\varphi ').
\end{array}
\end{equation}
By induction we obtain
\begin{equation}
F^{(n)}(\varphi)=\int_{-\infty}^\infty
\left( \frac{d^n}{dx^n } q(x)\right) \varphi (x) dx
 =(-1)^n F(\varphi ^{(n)})=F((-1)^n \varphi ^{(n)}).
\end{equation}

\section{Heaviside step function}
\index{Heaviside step function}

One of the possible definitions of the Heaviside step function $H(x)$, and maybe the most common one --
they differ in the value $H(0)$of  at the origin $x=0$, a difference which is irrelevant measure theoretically for ``good'' functions
since it is only about an isolated point --
is
\begin{equation}
H(x-x_0)
=
\left\{
\begin{array}{rl}
1&\textrm{ for } x\ge x_0\\
0&\textrm{ for } x < x_0
\end{array}
\right.
\label{2011-m-di-edhf}
\end{equation}
It is plotted in Fig. \ref{2011-m-fhsf}.

\begin{marginfigure}
%TeXCAD (http://texcad.sf.net/) Picture. File: [1.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{4.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(105,60)(0,0)
\put(0,0){\line(1,0){100}}
\put(50,0){\line(0,1){100}}
\thicklines
\put(105,0){\makebox(0,0)[cc]{$x$}}
%
{\color{orange}
\put(50,0){\circle{2.5}}
\put(50,50){\circle*{2.5}}
% base lines
\put(0,0){\line(1,0){50}}
\put(50,50){\line(1,0){50}}
\put(60,60){\makebox(0,0)[cc]{$H(x)$}}
}
\end{picture}
\caption{Plot of the Heaviside step function  $H(x)$.}
\label{2011-m-fhsf}
\end{marginfigure}

It is also very common to define the  Heaviside step function as the
{\em integral of the $\delta$ function};
likewise the delta function is the derivative of the Heaviside step function; that is,
\begin{equation}
\begin{array}{l}
H(x-x_0)
=
\int_{-\infty}^{x-x_0} \delta (t) dt,\\
\frac{d}{dx} H(x-x_0)=\delta (x-x_0).
\end{array}
\end{equation}
In the spirit of the above definition, it might have been more appropriate to define $H(0)=\frac{1}{2}$; that is,
\begin{equation}
H(x-x_0)
=
\left\{
\begin{array}{rl}
1&\textrm{ for } x > x_0\\
\frac{1}{2}&\textrm{ for } x = x_0\\
0&\textrm{ for } x < x_0
\end{array}
\right.
\end{equation}
and, since this affects only an isolated point at $x=0$, we may happily do so if we prefer.

{\color{OliveGreen}
\bproof
The latter equation can -- in the sense of functionals --
be proved through integration by parts as follows.
Take
\begin{equation}
\begin{array}{l}
\int _{-\infty}^\infty \left[\frac{d}{dx} H(x-x_0)\right] \varphi (x) dx       \\
\qquad =
\left. H(x-x_0) \varphi (x)\right| _{-\infty}^\infty - \int _{-\infty}^\infty H(x-x_0) \left[\frac{d}{dx} \varphi (x)\right] dx \\
\qquad = H(-\infty)\varphi (-\infty ) - \varphi (+\infty ) - \int _{x_0}^\infty \left[\frac{d}{dx} \varphi (x)\right] dx  \\
\qquad =  - \int _{x_0}^\infty \left[\frac{d}{dx} \varphi (x)\right] dx \\
\qquad =    -  \left.  \varphi (x)   \right| _{x_0}^\infty \\
\qquad =    - [  \varphi (\infty)  - \varphi (x_0)] \\
\qquad =     \varphi (x_0).
\end{array}
\end{equation}


\eproof
}



Some other formulae{}  involving the Heaviside step function
are
 \begin{equation}
H (\pm x)=\lim_{\epsilon \rightarrow 0^+}{\mp i\over 2\pi
 }\int_{-\infty}^{+\infty} {e^{ikx}\over k\mp i\epsilon}dk,
 \end{equation}
and
 \begin{equation}
H (x)
={1\over 2}
+
\sum_{l=0}^\infty (-1)^l {(2l)!(4l+3)\over 2^{2l+2}l!(l+1)!}
P_{2l+1} (x),
 \end{equation}
where $P_{2l+1} (x)$ is a Legendre polynomial.
Furthermore,
\begin{equation}
\delta(x)=
\lim_{\epsilon \rightarrow 0}  \frac{1}{\epsilon } H\left( \frac{\epsilon }{2} -\vert x\vert\right) .
\end{equation}

An integral representation of $H(x)$ is
 \begin{equation}
H (x)
=\lim{\epsilon \downarrow 0^+} \mp \frac{1}{2\pi i}
\int_{-\infty}^\infty
 \frac{1}{t \pm i\epsilon}e^{\mp ixt} dt.
 \end{equation}


One commonly used limit form  of the Heaviside step function is
\begin{equation}
H(x)= \lim_{\epsilon \rightarrow 0} H_\epsilon (x)=\lim_{\epsilon \rightarrow 0} \frac{1}{\pi x} \left[ \frac{\pi }{2} + \tan^{-1}  \frac{x}{\epsilon } \right] .
\end{equation}
respectively.

Another limit representation is by a variant of the {\em sine integral function}
\index{sine integral}
$\textrm{Si}(x)=\int_0^x (\sin t)/t \,dt$ as follows \cite{maor1998}
\begin{equation}
\begin{array}{l}
H(x)= \lim_{t \rightarrow \infty} H_t (x)
\\ \qquad
=\lim_{n \rightarrow \infty} \frac{2}{\pi }\int_0^n \frac{\sin (kx)}{x} dx
\\
\qquad
=
 \frac{2}{\pi }\int_0^\infty \frac{\sin (kx)}{x} dx
.
\end{array}
\end{equation}


\section{The sign function}

The
{\em sign function}
\index{sign function}
is defined by
\begin{equation}
\begin{array}{l}
\textrm{sgn}(x-x_0)
=
\left\{
\begin{array}{rl}
-1&\textrm{ for} x < x_0\\
0&\textrm{ for} x = x_0 \\
+1&\textrm{ for} x > x_0
\end{array}
\right.
.
\end{array}
\end{equation}
It is plotted in Fig. \ref{2011-m-fsf}.
\begin{marginfigure}
%TeXCAD (http://texcad.sf.net/) Picture. File: [1.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{4.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(105,110)(0,-10)
\put(0,50){\line(1,0){100}}
\put(50,-5){\line(0,1){110}}
\thicklines
\put(105,50){\makebox(0,0)[cc]{$x$}}
%
{\color{orange}
\put(50,0){\circle{2.5}}
\put(50,50){\circle*{2.5}}
\put(50,100){\circle{2.5}}
% base lines
\put(0,0){\line(1,0){50}}
\put(50,100){\line(1,0){50}}
\put(60,110){\makebox(0,0)[cc]{$\textrm{sgn}(x)$}}
}
\end{picture}
\caption{Plot of the sign function  $\textrm{sgn}(x)$.}
\label{2011-m-fsf}
\end{marginfigure}

In terms of  the  Heaviside step function it can be written by ``stretching'' the former by a factor of two,
and shifting it for one negative unit as follows
\begin{equation}
\begin{array}{l}
\textrm{sgn}(x-x_0) = 2H(x-x_0) -1,\\
H(x-x_0) = \frac{1}{2} \left[ \textrm{sgn}(x-x_0)+1\right] \textrm{; and also}
\textrm{sgn}(x-x_0) = H(x-x_0) - H(x_0-x).
\end{array}
\label{2011-m-cbhsf}
\end{equation}

Note that
\begin{eqnarray}
\mbox{sgn}(x)
&=&{4\over \pi }\sum_{n=0}^\infty {\sin [
(2n+1)x]\over
(2n+1)}\\
&=&{4\over \pi }\sum_{n=0}^\infty (-1)^n{\cos [
(2n+1)(x-\pi /2)]\over
(2n+1)}\;,\; -\pi <x<\pi  .
 \end{eqnarray}


\section{Useful formul\ae{} involving $\delta$}

 \begin{equation}
 \delta (x)=\lim_{\epsilon \rightarrow 0 }
 {1\over \pi }{\epsilon \over x^2+\epsilon^2}
 \end{equation}
 %S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische  Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 \delta (x)=\lim_{\epsilon \rightarrow 0_+ }
 {1\over 2\pi i}\left({1 \over x-i\epsilon }
 -
 {1 \over x+i\epsilon }
 \right)
 =
 \lim_{\epsilon \rightarrow 0 }
 {1\over \pi }{\epsilon \over x^2+\epsilon^2}
 \end{equation}
 % W. Thirring, {\sl private communication.}
 \begin{equation}
 \lim_{\epsilon \rightarrow 0 }
 {1 \over x\pm i\epsilon }={P\over x}\mp i\pi \delta (x)
 \end{equation}
 % S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 \delta (x)=\delta (-x)
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 \delta (x)=\lim_{\epsilon \rightarrow 0}{\theta (x+\epsilon )
 -\theta (x)\over \epsilon }={d\over dx}\theta (x) \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 \varphi (x)\delta (x-x_0)
 =
 \varphi (x_0)\delta (x-x_0)
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 x\delta (x)=0
 \end{equation}
 % S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
For $a\neq 0$,
 \begin{equation}
 \delta (ax)={1\over \vert a\vert }\delta (x)
\label{2011-m-distdp}
 \end{equation}
 % S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)

{\color{OliveGreen}
\bproof
For the sake of a proof,  consider the case $a>0$ first:
 \begin{equation}
 \begin{array}    {l}
\int _{-\infty}^\infty \delta (ax)  \varphi (x)  dx          \\
\qquad =
\frac{1}{a}\int _{-\infty}^\infty \delta (y)  \varphi (\frac{y}{a}) dy  \\
\qquad =    \frac{1}{a}  \varphi (0);
 \end{array}
 \end{equation}
and now the case $a<0$:
 \begin{equation}
 \begin{array}    {l}
\int _{-\infty}^\infty \delta (ax)  \varphi (x)  dx     \\
\qquad =
\frac{1}{a}\int _\infty^{-\infty} \delta (y)  \varphi (\frac{y}{a}) dy
- \frac{1}{a}\int _{-\infty}^\infty \delta (y)  \varphi (\frac{y}{a}) dy       \\
\qquad =   - \frac{1}{a}  \varphi (0);
 \end{array}
 \end{equation}
\eproof
}

If there exists a simple singularity $x_0$ of $f(x)$ in the
integration interval, then
\begin{equation}
 \delta (f(x))={1\over \vert f'(x_0)\vert }\delta(x-x_0)
.
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
More generally,  if $f$ has only simple roots and $f'$ is nonzero there,
\begin{equation}
 \delta (f(x))=\sum_{x_i}{\delta(x-x_i)\over \vert f'(x_i)\vert }
\label{2011-m-distdp1}
 \end{equation}
where the sum extends over all simple roots $x_i$ in
the integration interval.
In particular,
 \begin{equation}
 \delta (x^2-x_0^2)={1\over 2\vert x_0\vert }[\delta (x-x_0)+\delta
 (x+x_0)] \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)

{\color{OliveGreen}
\bproof
For a proof, note that, since $f$ has only simpleroots,
it can be expanded around these roots by
$$
f(x) \approx (x-x_0) f'(x_0)
$$
with nonzero $f'(x_0) \in {\Bbb R}$.
By identifying  $f'(x_0)$ with $a$ in
Eq. (\ref{2011-m-distdp})
we obtain
Eq. (\ref{2011-m-distdp1}).

\eproof
}


 \begin{equation}
 \delta '(f(x))=
\sum_{i=0}^N
{f'' (x_i)\over \vert f'(x_i)\vert ^3}
\delta (x-x_i) +
\sum_{i=0}^N
{f' (x_i)\over \vert f'(x_i)\vert ^3}
\delta ' (x-x_i)
 \end{equation}
% Testbeispiel Lakatha
 \begin{equation}
 \vert x\vert \delta (x^2)=\delta (x)
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 -x\delta '(x)=\delta (x)
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 \delta^{(m)}(x)=(-1)^m\delta^{(m)}(-x)
,
 \end{equation}
where the index $^{(m)}$ denotes $m$--fold differentiation;
 % A. Messiah, {\sl Quantum Mechanics, Volume I} (North Holland, Amsterdam 1961)
 \begin{equation}
 x^{m+1}\delta^{(m)}(x)=0
,
 \end{equation}
 where the index $^{(m)}$ denotes $m$--fold differentiation;
 %A. Messiah, {\sl Quantum Mechanics, Volume I} (North Holland, Amsterdam 1961)
 \begin{equation}
 x^2\delta '(x)=0
 \end{equation}
 %A. Messiah, {\sl Quantum Mechanics, Volume I} (North Holland, Amsterdam 1961)

 \begin{equation}
 \delta (x)={d^2\over dx^2}[x\theta (x)]
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)

If $ \delta^{3} ({\vec r})=
\delta (x)
\delta (y)
\delta (r)$ with ${\vec r}=(x,y,z)$, then
 \begin{equation}
 \delta^{3} ({\vec r})=\delta (x)\delta (y)\delta (z)=-{1\over 4\pi }\Delta {1\over  r }
 \end{equation}
% S. Gro\ss mann, {\sl Funktionalanalysis I} (Akademische Verlagsgesellschaft, Wiesbaden 1975)
 \begin{equation}
 \delta^{3}  ({\vec r})=-{1\over 4\pi }(\Delta +k^2){e^{ikr}\over r}
 \end{equation}
 % A. Messiah, {\sl Quantum Mechanics, Volume I} (North Holland, Amsterdam 1961)
 \begin{equation}
 \delta^{3}  ({\vec r})=-{1\over 4\pi }(\Delta +k^2){\cos kr\over r}
 \end{equation}
 %A. Messiah, {\sl Quantum Mechanics, Volume I} (North Holland, Amsterdam 1961)

In quantum field theory,  phase space integrals of the form
 \begin{equation}
 {1\over 2E}=\int dp^0 \, \theta (p^0)\delta (p^2-m^2)
 \end{equation}
 if $E=({\vec p}^2+m^2)^{(1/2)}$
 are exploited.
 % H. Pietschmann, Formul\ae \& Results in Weak Interactions (Springer, Wien 1974)

\section{Fourier transforms of $\delta$ and $H$}

If $\{ f_x(x)\}$ is a sequence of functions converging, for $n\rightarrow \infty$
toward a function $f$ in the functional sense (i.e. {\it via}
integration of $f_n$ and $f$ with ``good'' test functions),
then the Fourier transform $\widetilde f$ of $f$ can be defined by \cite{Lighthill,Howell}
\begin{equation}
\begin{array}{l}
 \widetilde{f}(k)= \lim_{n\rightarrow \infty}
 \int_{-\infty}^\infty  f_n(x) e^{-i{kx}} dx
.
\end{array}
\end{equation}
Since $e^{-i{kx}}$ is not a particularly ``good'' function,  $\widetilde f$  needs not be an ordinary function
\cite{doi:10.1080/0020739900210418}.



The Fourier transform of the $\delta$-function can be obtained by insertion into Eq. (\ref{2011-m-eft})
\begin{equation}
\begin{array}{l}
 \widetilde{\delta}(k)=   \int_{-\infty}^\infty  \delta(x) e^{-i{kx}} dx   \\
\qquad =    e^{-i{0k}}  \int_{-\infty}^\infty  \delta(x)  dx   \\
\qquad =    1
.
\end{array}
\label{2011-m-eftdelta}
\end{equation}
That is, the Fourier transform of the $\delta$-function is just a constant.
$\delta$-spiked signals carry all frequencies in them.

The Fourier transform of the Heaviside step function can also be obtained by insertion into Eq. (\ref{2011-m-eft})
\begin{equation}
\begin{array}{l}
 \widetilde{H}(k)=   \int_{-\infty}^\infty  H(x) e^{-i{kx}} dx   \\
\qquad =    \frac{1}{2}\left[\delta(k) -\frac{i}{\pi k}\right]
.
\end{array}
\end{equation}


{\color{OliveGreen}
\bproof
A {\it caveat} first: the following proof should {\em not} be understood in the usual functional,
but rather in a functional sense.

\begin{equation}
\begin{array}{l}
 \widetilde{H}(k)
=   \int_{-\infty}^\infty  H(x) e^{-i{kx}} dx   \\
\qquad
=   \int_{0}^\infty   e^{-i{kx}} dx
;   \\
 \tilde{H}(-k)
=   \int_{-\infty}^\infty  H(x) e^{+i{kx}} dx   \\
\qquad
=   \int_{0}^\infty  H(x) e^{+i{kx}} dx   \\
\textrm{ [variable transformation: }x\rightarrow -x \textrm{]}  \\
\qquad
=  - \int_0^{-\infty} e^{-i{kx}} dx   \\
\qquad
=   \int_{-\infty}^0 e^{-i{kx}} dx
.
\end{array}
\end{equation}

Now separate the even part $E$ from the odd part $O$ of $H$; that is, define
\begin{equation}
\begin{array}{l}
E(k) = (H(k) + H(-k))\\
\qquad =   \int_{-\infty}^{+\infty} e^{-i{kx}} dx  ,   \\
O(k) = (H(k) - H(-k))\\
\qquad =   \int_{0}^\infty \left[  e^{-i{kx}}- e^{+i{kx}}\right]  dx ,   \\
\qquad =   -2 i \int_{0}^\infty \sin (kx)  dx ,   \\
H(k) = \frac{1}{2}(E(k)+O(k))
.
\end{array}
\end{equation}

The two integrals defining $E$ and $O$
do not converge.
However, one may approximate them by cutting the
integrals off at some large value $L$
\begin{equation}
\begin{array}{l}
E_L(k) =   \int_{-L}^{+L} e^{-i{kx}} dx  ,   \\
\qquad =  \left. -\frac{e^{-i{kx}}}{ik}   \right|_{x=-L}^{+L},   \\
\qquad =   -\frac{e^{-i{kL}}-e^{+i{kL}}}{ik} ,   \\
\qquad =  2 \frac{\sin(kL)}{k} ,\textrm{ and }   \\
O_L(k) = - 2 i \int_{0}^L \sin (kx)  dx    \\
\qquad    = \left.- 2 i  \frac{\cos (kx)}{k}   \right|_{x=0}^{+L},   \\
\qquad    =  2 i \frac{\left[1- \cos (kL)   \right]}{k},   \\
\qquad    =  4 i \frac{\sin^2 (\frac{kL}{2})}{k}
.
\end{array}
\end{equation}

Now observe that, interpreted as distribution,
$E_L(k)$ is proportional to a delta sequence, rendering
\begin{equation}
 \lim_{L\rightarrow \infty} E_L(k) =   \frac{1}{2} \delta(k)
.
\end{equation}

In the same intepretation, when integrating over a test function,
one can show that
\begin{equation}
 \lim_{L\rightarrow \infty} O_L(k) =   -\frac{i}{\pi k}
.
\end{equation}

It is possible to derive this result directly from the linearity of the Fourier transform: since
by Eq. (\ref{2011-m-cbhsf}) $H(x) = \frac{1}{2} \left[ \textrm{sgn}(x)+1\right]$,
\begin{equation}
\begin{array}{l}
 \widetilde{H}(k) = \frac{1}{2} \left[ \widetilde{\textrm{sgn}}(k)+\tilde{1}\right] \\
\qquad    =\frac{1}{2} \left[ \frac{1}{i\pi k}+\delta (k)\right]
.
\end{array}
\end{equation}

%http://www.mathkb.com/Uwe/Forum.aspx/math/12158/How-to-find-Fourier-Transform-for-special-functions-how-to-evaluate
% Daryl McCullough Ithaca, NY
\eproof
}

{
\color{blue}
\bexample

Let us compute some concrete examples related to distributions.
\begin{enumerate}
\item
For a start, let us proof that
\begin{equation}
\lim_{\epsilon \rightarrow 0}{\epsilon \sin^2 \frac{x}{\epsilon}\over \pi
 x^2}= \delta (x).\end{equation}
As a hint, take  $\int_{-\infty}^{+\infty} {\sin^2x\over x^2}dx =\pi $.

Let us proof this conjecture by integrating over a good test function $\varphi$
\begin{equation}
\begin{array}{l}
   {1\over\pi}\lim_{\epsilon \rightarrow 0}\int\limits_{-\infty}^{+\infty}
   {\varepsilon\sin^2\left({x\over\varepsilon}\right)\over x^2} \varphi(x) dx
\\
\qquad \textrm{[variable substitution}  y={x\over\varepsilon} , {dy\over dx}={1\over\varepsilon}, dx=\varepsilon dy
\textrm{]}
\\ \qquad =
  {1\over\pi}\lim_{\epsilon \rightarrow 0}
   \int\limits_{-\infty}^{+\infty}\varphi(\varepsilon y)
   {\varepsilon^2\sin^2(y)\over \varepsilon^2y^2}dy
\\ \qquad =    {1\over\pi}
   \varphi(0)\int\limits_{-\infty}^{+\infty}{\sin^2(y)\over y^2}dy
\\ \qquad =    \varphi(0).
\end{array}
\end{equation}
Hence we can identify
\begin{equation}
   \lim_{\varepsilon \rightarrow 0}{\varepsilon\sin^2 \left({x\over\varepsilon}\right)\over\pi x^2}=\delta(x).
\end{equation}

 \item
In order to prove that $\frac{1}{\pi} \frac{n  e^{-x^2}}{1+n^2x^2} $ is  a  $\delta$-sequence
we proceed again by integrating over a good test function $\varphi$,
and with the hint that $\int\limits_{-\infty}^{+\infty} dx/ (1+x^2) =\pi$ we obtain
 \begin{equation}
\begin{array}{l}
\lim_{n \rightarrow \infty}  \frac{1}{\pi}
\int\limits_{-\infty}^{+\infty}
\frac{n  e^{-x^2}}{1+n^2x^2}
   \varphi(x)
dx
\\ \qquad \qquad \textrm{[variable substitution}  y={xn} , x={y\over n}, {dy\over dx}={n}, dx={dy\over n}
\textrm{]}
\\ \qquad =
\lim_{n \rightarrow \infty}   \frac{1}{\pi}
\int\limits_{-\infty}^{+\infty}
\frac{n  e^{-\left( \frac{y}{n} \right)^2}}{1+y^2}
   \varphi \left( \frac{y}{n} \right)
\frac{dy}{n}
\\ \qquad =
\frac{1}{\pi}
\int\limits_{-\infty}^{+\infty}
\lim_{n \rightarrow \infty}  \left[     e^{-\left( \frac{y}{n} \right)^2}  \varphi \left( \frac{y}{n} \right) \right]
\frac{1}{1+y^2}
dy
\\ \qquad =       \frac{1}{\pi}
\int\limits_{-\infty}^{+\infty}
\left[     e^{0}  \varphi \left( 0\right) \right]
\frac{1}{1+y^2}
dy
\\ \qquad =    \frac{\varphi \left( 0\right)}{\pi}
\int\limits_{-\infty}^{+\infty}
\frac{1}{1+y^2}
dy
\\ \qquad =
\frac{\varphi \left( 0\right)}{\pi} \pi
\\ \qquad =
\varphi \left( 0\right).
\end{array}
\end{equation}
Hence we can identify
\begin{equation}
   \lim_{n \rightarrow \infty}{\frac{1}{\pi} \frac{n  e^{-x^2}}{1+n^2x^2}}=\delta(x).
\end{equation}

\item
Let us proof that
$x^n\delta^{(n)}(x)=C\delta (x)$    and determine the constant $C$.
We proceed again by integrating over a good test function $\varphi$.
First note that if
$\varphi (x)$ is a good test function, then so is
$x^n\varphi (x)$.
\begin{eqnarray*}
   \int dx x^n\delta^{(n)}(x)\varphi(x)&=&
   \int dx \delta^{(n)}(x)\bigl[x^n\varphi(x)\bigr]=\\
   &=&(-1)^n \int dx \delta(x)\bigl[x^n\varphi(x)\bigr]^{(n)}=\\
   &=&(-1)^n \int dx \delta(x)\bigl[nx^{n-1}\varphi(x)+x^n\varphi'(x)
   \bigr]^{(n-1)}=                                              \\
&&\cdots \\
   &=&(-1)^n \int dx \delta(x)\left[\sum_{k=0}^n
           \left(
          \begin{array}{c}
          n\\ k
           \end{array}\right) (x^n)^{(k)}\varphi ^{(n-k)}(x)\right]
   =\\
   &=&(-1)^n \int dx
\delta(x)\bigl[n!\varphi(x)+n\cdot n!x\varphi'(x) +
\cdots +x^n\varphi^{(n)}(x)
   \bigr]=\\
   &=&(-1)^n n! \int dx \delta(x)\varphi(x);
\end{eqnarray*}
hence, $ C=(-1)^n n!$.
Note that
$\varphi (x)$ is a good test function then so is
$x^n\varphi (x)$.

\item
Let us simplify $\int_{-\infty}^\infty \delta (x^2-a^2)g(x)\; dx$.
First recall Eq. (\ref{2011-m-distdp1})  stating that
$$
   \delta(f(x))=\sum_{i}{\delta(x-x_i)\over|f'(x_i)|},
$$
whenever $x_i$ are simple roots of  $f(x)$, and $f'(x_i)\neq 0$.
In our case, $
   f(x)=x^2-a^2=(x-a)(x+a)
$, and the roots are  $x=\pm a$.
Furthermore,
$$
   f'(x)=(x-a)+(x+a);
$$
hence
$$
   |f'(a)|=2|a|,\qquad |f'(-a)|=|-2a|=2|a|.
$$
As a result,
$$
    \delta(x^2-a^2)=\delta\bigl((x-a)(x+a)\bigr)={1\over|2a|}
   \bigl(\delta(x-a)+\delta(x+a)\bigr).
$$
Taking this into account we finally obtain
 \begin{equation}
\begin{array}{l}
  \int\limits_{-\infty}^{+\infty}\delta(x^2-a^2)
   g(x)dx\\
\qquad =
\int\limits_{-\infty}^{+\infty}{\delta(x-a)+\delta(x+a)\over
   2|a|}g(x)dx\\
\qquad =    {g(a)+g(-a)\over2|a|}.
\end{array}
\end{equation}


\item
Let us evaluate \begin{equation}   I=
\int_{-\infty}^\infty
\int_{-\infty}^\infty
\int_{-\infty}^\infty
\delta (x_1^2+x_2^2+x_3^2-R^2) d^3x
\end{equation} for  $R \in {\Bbb R}, R >0 $.
We may, of course, remain tn the standard ccartesian coordinate system and evaluate the integral by ``brute force.''
Alternatively,
a more elegant way is to use the spherical symmetry of the problem and use spherical coordinates $r, \Omega (\theta ,\varphi )$ by rewriting $I$
into
\begin{equation}
I=  \int_{r,\Omega} r^2  \delta (r^2-R^2) d\Omega dr.
\end{equation}
As the integral kernel $\delta (r^2-R^2)$ just depends on the radial coordinate $r$
the angular coordinates just integrate to $4\pi$.
Next we make use of Eq. (\ref{2011-m-distdp1}), eliminate the solution for $r=-R$, and obtain
\begin{equation}
\begin{array}{l}
I= 4\pi   \int_0^\infty r^2  \delta (r^2-R^2)  dr \\
\qquad =  4\pi   \int_0^\infty  r^2 \frac{\delta (r+R) + \delta (r-R)}{2R}  dr \\
\qquad =   4\pi   \int_0^\infty  r^2 \frac{\delta (r-R)}{2R}  dr \\
\qquad =   2 \pi R.
\end{array}
\end{equation}

\item
Let us compute
\begin{equation}
\int_{-\infty}^\infty \int_{-\infty}^\infty  \delta
 (x^3-y^2+2y)\delta (x+y)H (y-x-6)f(x,y) \,dx\,dy.
\end{equation}

First, in dealing with
$\delta(x+y)$, we evaluate the $y$ integration at $x=-y$ or $y=-x$:
$$
   \int\limits_{-\infty}^\infty \delta(x^3-x^2-2x)H(-2x-6)f(x,-x)  dx
$$
Use of Eq. (\ref{2011-m-distdp1})
$$
   \delta(f(x))=\sum_{i}{1\over|f'(x_i)|}\delta(x-x_i),
$$
at the roots
\begin{eqnarray*}
   x_1&=&0\\
   x_{2,3}&=&{1\pm\sqrt{1+8}\over 2}={1\pm3\over2}=\left\{{2\atop-1}\right.
\end{eqnarray*}
of the argument $f(x)=x^3-x^2-2x=x(x^2-x-2)=x(x-2)(x+1)$ of the remaining $\delta$-function,
together with
$$
 f'(x)=  {d\over dx}(x^3-x^2-2x)=3x^2-2x-2 ;
$$
yields
\begin{eqnarray*}
   &&\int\limits_{-\infty}^\infty dx
         {\delta(x)+\delta(x-2)+\delta(x+1)\over|3x^2-2x-2|}
         H(-2x-6)f(x,-x)=\\
   &=&{1\over|-2|}\underbrace{H(-6)}_{\mbox{$=0$}}f(0,-0)+
      {1\over|12-4-2|}\underbrace{H(-4-6)}_{\mbox{$=0$}}f(2,-2)+\\
   & &+\ {1\over|3+2-2|}\underbrace{H(2-6)}_{\mbox{$=0$}}f(-1,1)\\
&=&0
\end{eqnarray*}


\item
When simplifying derivativs of generalized functions it is always useful to evaluate their properties
--
such as $x\delta(x)=0$, $f(x)\delta(x-x_0)=f(x_0)\delta(x-x_0)$, or $\delta (-x)=\delta (x)$
--
first and  before proceeding with the next differentiation or evaluation.
We shall present some applications of this ``rule'' next.

First, simplify
\begin{equation}
\left({d\over dx}-\omega \right)H (x)e^{\omega x}
\end{equation}
as follows
\begin{equation}
\begin{array}{l}
 {d\over dx}\left[H(x)e^{\omega
x}\right]-\omega H(x)e^{\omega x}\\
\qquad =
   \delta(x)e^{\omega x}+\omega H(x)e^{\omega x}-\omega H(x)
   e^{\omega x}\\
\qquad =
   \delta(x)e^{0}
   \\
\qquad =  \delta(x)
\end{array}
\end{equation}


\item
Next, simplify
\begin{equation}
\left({d^2\over dx^2}+\omega^2 \right){1\over \omega }H
 (x)\sin (\omega x)
\end{equation}
as follows
\begin{equation}
\begin{array}{l}
{d^2\over dx^2}\left[{1\over\omega}H(x)\sin(\omega
x)\right]+\omega H(x)
      \sin(\omega x)\\
   \qquad =   {1\over\omega}{d\over dx}\Bigl[\underbrace{\delta(x)\sin(\omega x)}_
      {\mbox{$=0$}}+\omega H(x)\cos(\omega x)\Bigr]+\omega H(x)
      \sin(\omega x)\\
  \qquad =   {1\over\omega}\Bigl[\omega\underbrace{\delta(x)\cos(\omega x)}_
      {\mbox{$\delta(x)$}}-\omega^2H(x)\sin(\omega x)\Bigr]+
      \omega H(x)\sin(\omega x)=\delta(x)
\end{array}
\end{equation}



\item
Let us compute the $n$th derivative of
 \begin{equation} f (x) =\cases{0, &f\"ur $ x< 0 $;\cr
                           x, &f\"ur $0\le x\le 1$;\cr
                           0, &f\"ur $ x>1$.\cr}
\end{equation}

\begin{marginfigure}
{\color{black}
\begin{center}
\unitlength=0.5cm
\begin{picture}(8,17)
   \put(0,12){\begin{picture}(8,5)
      \put(5.05,0.5){\line(1,0){2.45}}
      \put(0.5,0.5){\line(1,0){4.45}}
      \put(5,0.5){\circle{0.1}}
      \put(3,0.5){\line(0,1){3}}
      \put(3,0.5){\circle*{0.1}}
      \put(5,2.5){\circle*{0.1}}
      \put(5,0){\makebox(0,0)[cb]{$1$}}
      \put(3,0){\makebox(0,0)[cb]{$0$}}
      \put(8,0.5){\makebox(0,0)[rc]{$x$}}
      \put(3,4){\makebox(0,0)[ct]{$f(x)$}}
      \thicklines
{\color{orange}
      \put(5.05,0.5){\line(1,0){2.45}}
      \put(0,0.5){\line(1,0){3}}
      \put(3,0.5){\line(1,1){2}}
}
   \end{picture}}
   \put(0,7){\begin{picture}(8,5)
      \put(5.05,0.5){\line(1,0){2.45}}
      \put(3.05,0.5){\line(1,0){1.9}}
      \put(5,0.5){\circle{0.1}}
      \put(3,0.5){\circle{0.1}}
      \put(3,0.55){\line(0,1){2.95}}
      \put(3,2.5){\circle*{0.1}}
      \put(5,2.5){\circle*{0.1}}
      \put(5,0){\makebox(0,0)[cb]{$1$}}
      \put(3,0){\makebox(0,0)[cb]{$0$}}
      \put(8,0.5){\makebox(0,0)[rc]{$x$}}
      \put(3,4){\makebox(0,0)[ct]{$f_1(x)$}}
      \put(5.2,2.5){\makebox(0,0)[lc]{
          $\begin{array}{rcl}
             f_1(x)&=&H(x)-H(x-1)\\
                   &=&H(x)H(1-x)
           \end{array}$}}
      \thicklines
{\color{orange}
      \put(0,0.5){\line(1,0){3}}
      \put(3,2.5){\line(1,0){2}}
      \put(5.05,0.5){\line(1,0){2.45}}
}
   \end{picture}}
   \put(0,2){\begin{picture}(8,5)
      \put(0,0.5){\line(1,0){7.5}}
      \put(3,0.5){\line(0,1){3}}
      \put(3,4){\makebox(0,0)[ct]{$f_2(x)$}}
      \put(8,0.5){\makebox(0,0)[rc]{$x$}}
      \thicklines
      \put(0.5,-2){{\color{orange}\line(1,1){5}}}
      \put(5.2,2.5){\makebox(0,0)[lc]{$f_2(x)=x$}}
   \end{picture}}
\end{picture}
\end{center}
}
\caption{Composition of
$f (x)$
}
\label{2011-m-fcof1}
\end{marginfigure}

As depicted in Fig. \ref{2011-m-fcof1},
$f$ can be composed from two functions $f(x)=f_2(x)\cdot f_1(x)$;
and this composition can be done in at least two ways.


Decomposition {(i)}
\begin{eqnarray*}
   f(x)&=&x\bigl[H(x)-H(x-1)\bigr]=xH(x)-xH(x-1)\\
   f'(x)&=&H(x)+x\delta(x)-H(x-1)-x\delta(x-1)
\end{eqnarray*}
Because of $x\delta(x-a)=a\delta(x-a)$,
\begin{eqnarray*}
   f'(x)&=&H(x)-H(x-1)-\delta(x-1)\\
   f''(x)&=&\delta(x)-\delta(x-1)-\delta'(x-1)
\end{eqnarray*}
and hence   by induction
$$
f^{(n)}(x)=\delta^{(n-2)}(x)-\delta^{(n-2)}(x-1)-
   \delta^{(n-1)}(x-1)
$$
for $n>1$.

Decomposition {(ii)}
\begin{eqnarray*}
   f(x)&=&xH(x)H(1-x)\\
   f'(x)&=&H(x)H(1-x)+
           \underbrace{x\delta(x)}_{\mbox{$=0$}}
           H(1-x)+\underbrace{xH(x)(-1)\delta(1-x)}_
           {\mbox{$-H(x)\delta(1-x)$}}=\\
   &=&H(x)H(1-x) - \delta(1-x)=[\delta(x)=\delta(-x)]=
           H(x)H(1-x) - \delta(x-1)\\
   f''(x)&=&\underbrace{\delta(x)H(1-x)}_{\mbox{$=\delta(x)$}}
           +\underbrace{(-1)H(x)\delta(1-x)}_{\mbox{$-\delta(1-x)$}}
           -\delta'(x-1)=\\
        &=&\delta(x)-\delta(x-1)-\delta'(x-1)
\end{eqnarray*}
and hence   by induction
$$
f^{(n)}(x)=\delta^{(n-2)}(x)-\delta^{(n-2)}(x-1)-
   \delta^{(n-1)}(x-1)
$$
for $n>1$.


\item
Let us compute the $n$th derivative of
 \begin{equation} f (x) =\cases{\vert \sin x\vert , &for $-\pi \le x\le \pi $;\cr
                           0, &for $ \vert x\vert >\pi $.\cr}
\end{equation}

$$
   f(x)=|\sin x|H(\pi+x)H(\pi-x)
$$
$$
   |\sin x|=\sin x\mbox{ sgn}(\sin x)=\sin x\mbox{ sgn\,}x\qquad\mbox{f\"ur}
   \quad -\pi<x<\pi ;
$$
hence we start from
$$
  f(x)=\sin x\mbox{ sgn\,}xH(\pi+x)H(\pi-x),
$$
Note that
\begin{eqnarray*}
   \mbox{ sgn\,}x&=&H(x)-H(-x),\\
   (\mbox{ sgn\,}x)'&=&H'(x)-H'(-x)(-1)=\delta(x)+\delta(-x)=
                   \delta(x)+\delta(x)=2\delta(x).
\end{eqnarray*}
\begin{eqnarray*}
   f'(x)   &=   &\cos x\mbox{ sgn\,}xH(\pi+x)
           H(\pi-x)+\sin x2\delta(x)H(\pi+x)H(\pi-x)+\\
           &    &+\sin x\mbox{ sgn\,}x\delta(\pi+x)H(\pi-x) +
           \sin x\mbox{ sgn\,}x H(\pi+x)\delta(\pi-x)(-1)=\\
           &=   &\cos x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)\\
   f''(x)   &=   & -\sin x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)+
             \cos x2\delta(x)H(\pi+x)H(\pi-x)+\\
           &    &+\cos x\mbox{ sgn\,}x\delta(\pi + x)H(\pi - x)
              + \cos x\mbox{ sgn\,}x H(\pi + x)\delta(\pi - x)(-1)=\\
           &=   & -\sin x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)+
             2\delta(x)+\delta(\pi+x)+\delta(\pi-x)\\
   f'''(x)   &=   & -\cos x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)-
             \sin x2\delta(x)H(\pi+x)H(\pi-x)-\\
           &    &-\sin x\mbox{ sgn\,}x\delta(\pi+x)H(\pi-x) -
             \sin x \mbox{ sgn\,}x H(\pi+x)\delta(\pi-x)(-1)+\\
           &    &+2\delta'(x)+\delta'(\pi+x)-\delta'(\pi-x)=\\
           &=   & -\cos x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)+
             2\delta'(x)+\delta'(\pi+x)-\delta'(\pi-x)\\
   f^{(4)}(x)   &=   & \sin x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)-
             \cos x2\delta(x)H(\pi+x)H(\pi-x)-\\
           &    &-\cos x\mbox{ sgn\,}x\delta(\pi+x)H(\pi-x) -
             \cos x \mbox{ sgn\,}x H(\pi+x)\delta(\pi - x)(-1)+\\
           &    &+2\delta''(x)+\delta''(\pi+x)+\delta''(\pi-x)=\\
           &=   & \sin x\mbox{ sgn\,}xH(\pi+x)H(\pi-x)-
             2\delta(x)-\delta(\pi+x)-\delta(\pi-x)+\\
           &    &+2\delta''(x)+\delta''(\pi+x)+\delta''(\pi-x);
\end{eqnarray*}
hence
\begin{eqnarray*}
   f^{(4)}   &=   &f(x)-2\delta(x)+2\delta''(x)-
             \delta(\pi+x)+\delta''(\pi+x)-\delta(\pi-x)+\delta''(\pi-x), \\
   f^{(5)}   &=   &f'(x)-2\delta'(x)+2\delta'''(x)-
             \delta'(\pi + x)+\delta'''(\pi + x)+\delta'(\pi - x)-
             \delta'''(\pi - x);
\end{eqnarray*}
and thus  by induction
\begin{eqnarray*}
 f^{(n)}&=&f^{(n-4)}(x)-2\delta^{(n-4)}(x)+
             2\delta^{(n-2)}(x)-\delta^{(n-4)}(\pi+x)+\\
   &&+\delta^{(n-2)}(\pi+x)+(-1)^{n-1}
             \delta^{(n-4)}(\pi-x)+(-1)^n\delta^{(n-2)}(\pi-x)\\
   &&(n=4,5,6,\dots)
\end{eqnarray*}



\end{enumerate}

\eexample
}


\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright \decofourleft
%\aldine X \decoone c \floweroneright
% \aldineleft ] \decosix g \leafleft
% \aldineright Y \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
% \decofourright b
\floweroneleft
}
\end{center}

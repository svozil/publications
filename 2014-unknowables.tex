\documentclass[%
 %reprint,
  twocolumn,
 %superscriptaddress,
 %groupedaddress,
 %unsortedaddress,
 %runinaddress,
 %frontmatterverbose,
 % preprint,
 showpacs,
 showkeys,
 preprintnumbers,
 %nofootinbib,
 %nobibnotes,
 %bibnotes,
 amsmath,amssymb,
 aps,
 % prl,
 % pra,
 % prb,
  rmp,
 %prstab,
 %prstper,
  longbibliography,
 %floatfix,
 %lengthcheck,%
 ]{revtex4-1}

%\usepackage{cdmtcs-pdf}

\usepackage{mathptmx}% http://ctan.org/pkg/mathptmx

\usepackage{amssymb,amsthm,amsmath}

\usepackage{tikz}
\usepackage[breaklinks=true,colorlinks=true,anchorcolor=blue,citecolor=blue,filecolor=blue,menucolor=blue,pagecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{graphicx}% Include figure files
\usepackage{url}

\usepackage{xcolor}

\begin{document}


\sloppy
%
\title{Physical unknowables}
%
%\thanks{Contribution to
%the international symposium ``Horizons of Truth''
%celebrating the 100th birthday of Kurt G\"odel
%at  the University of Vienna from April 27th-29th, 2006}
%
\author{Karl Svozil}
\email{svozil@tuwien.ac.at}
\homepage{http://tph.tuwien.ac.at/~svozil}
\affiliation{Institute for Theoretical Physics, Vienna University of Technology,  \\
Wiedner Hauptstra\ss e 8-10/136, A-1040 Vienna, Austria}
%
%
\begin{abstract}
A variety of physical unknowables are discussed. Provable lack of physical omniscience, omnipredictability and omnipotence is derived by reduction to problems which are known to be recursively unsolvable. ``Chaotic'' symbolic dynamical systems are unstable with respect to variations of initial states. Quantum unknowables include the random occurrence of single events, complementarity and value indefiniteness.
\end{abstract}
%
%
%
\pacs{01.70.+w,01.65.+g,02.30.Lt,03.65.Ta}
\keywords{Unknowables, Church-Turing thesis, induction and forecast, n-body problem, quantum indeterminism}
%
\maketitle
%
\tableofcontents
%
%
\newpage

\begin{quote}
\begin{flushright}
{\footnotesize
{
As we know, there are known knowns; \\
there are things we know we know. \\
We also know there are known unknowns; \\
that is to say we know there are some things we do not know. \\
But there are also unknown unknowns --\\
the ones we don't know we don't know.} \\
{ --~United States Secretary of Defense Donald H. Rumsfeld \\
at a Department of Defense news briefing on February 12, 2002}
% http://www.defenselink.mil/Transcripts/Transcript.aspx?TranscriptID=2636
\\ $\;$
\\ $\;$
{ Ei mihi, qui nescio saltem quid nesciam!}\\
{ (Alas for me, that I do not at least know the extent of my own ignorance!)}  \\
{  --~Aurelius Augustinus, 354--430, ``Confessiones'' (Book XI, chapter 25)}
%http://www.stoa.org/hippo/text11.html
%http://de.wikipedia.org/wiki/Confessiones
%http://en.wikipedia.org/wiki/Confessiones
%http://www.ccel.org/a/augustine/confessions/confessions.html
%http://www.ccel.org/ccel/augustine/confessions.xiv.html
%http://www.newadvent.org/fathers/110111.htm
}
\end{flushright}
\end{quote}

\section{Rise and fall of determinism}

In what follows, a variety of physical unknowables will be discussed.
Provable lack of physical omniscience, omnipredictability and omnipotence is derived by reduction to problems that are known to be recursively unsolvable.
``Chaotic'' symbolic dynamical systems are unstable with respect to variations of initial states.
Quantum unknowables include the random occurrence of single events, complementarity, and value indefiniteness.

From antiquity onward, various waves of (in)determinism have influenced human thought.
Regardless of whether they were shaped by some {\it Zeitgeist,}
or whether, as Goethe's {\it Faust} puts it,
{``what you the Spirit of the Ages call,
is nothing but the spirit of you all,
wherein the Ages are reflected,''}
their proponents have sometimes vigorously defended their stance in irrational,  unscientific, and ideologic ways.
Indeed, from an emotional point of view, may it not appear frightening
to be ``imprisoned'' by remorseless, relentless predetermination,
even in a dualistic setup \citep{descartes-meditation}; and, equally frightening,
to accept that one's fate depends on total arbitrariness and chance?
Does determinism expose freedom, self-determination and human dignity as an idealistic illusion?
On the other extreme, what kind of morale, merits and efforts appear worthy in a universe governed by pure chance?
Is there some reasonable in-between straddling those extreme positions that may also be consistent with science?

We shall, for the sake of separating the scientific debate
from emotional overtones and possible bias, adopt a contemplative strategy of {\em evenly-suspended attention}
outlined by  \citet*{Freud-1912}, who admonishes analysts to be aware of the dangers
caused by {``temptations to project,
what  [the analyst]  in dull self-perception recognizes as the peculiarities of his own personality,
as generally valid theory into science.''}
Nature is thereby treated as a  client-patient,  and whatever findings come up are accepted  as is  without any
immediate emphasis or judgment.


\subsection{Toward explanation and feasibility}

Throughout history,
the human desire to foresee and manipulate the physical world for survival and prosperity,
and in accord with personal wishes and  fantasies,
has been confronted with the inability to predict and manipulate
large portions of the habitat.
As time passed, people have figured out various ways to tune
ever increasing fragments of the world according to their needs.
From a purely behavioral perspective, this is brought about in the way of
pragmatic quasi-causal conditional rules  of the following kind,
``if one does this, one obtains that.''
A typical example of such a rule is ``if I rub my hands, they get warmer.''


How does one arrive at those kinds of rules?
Guided by suspicions, thoughts, formalisms and by pure chance,
inquiries start by  roaming around,  inspecting portions of the world
and examining their behavior.
Repeating phenomena or  patterns of behavior are observed and  pinned down  by reproducing and evoking them.
A physical behavior is anything that can be observed and thus operationally obtained and measured;
for example, the rise and fall of the sun, the ignition of fire, the formation and  melting of ice (in principle even time series of financial entities traded
at stock exchanges or over-the-counter).

%Note that, due to the finiteness of the resolution, all kinds of physical behaviors,
%even the ones that appear continuous, can be discretized.
%Ultimately, all physical experiences can be broken down into yes-no propositions
%representable by zeroes and ones, by sequences of single clicks in detectors.

As physical behaviors are observed,
people attempt to  understand  them by trying to figure out some
 cause \citep{schlick,frank} or reason for their occurrences.
Researchers invent virtual parallel worlds of thoughts
and intellectual concepts such as ``electric field'' or ``mechanical force''
to  explain and manipulate the physical behaviors,
calling these creations of their minds ``physical theories.''
Contemporary physical theories are heavily formalized and spelled out in the language of mathematics.
A good theory provides people with the feeling of a key unlocking new ways of world comprehension and manipulation.
Ideally, an explanation should be as compact as possible
and should apply to as many behavioral patterns as possible.

Ultimately, theories of everything \citep{schlick-35,barrow-TOE,Kragh-qg} should be able to predict and manipulate all phenomena.
In the extreme form, science becomes omniscient and omnipotent,
and we envision ourselves almost as becoming empowered with  magic:
we presume that our ability to manipulate and tune the world is limited by our fantasies alone,
and any constraints whatsoever can be bypassed or overcome one way or another.
Indeed, some of what in the past has been called ``supernatural,'' ``mystery,'' and ``the beyond''
has been realized in everyday life.
Many wonders of witchcraft have been transferred into the realm of the physical sciences.
Take, for example, our abilities to fly,
to transmute mercury into gold \citep{PhysRev.60.473},
to listen and speak to far away friends,
or to cure bacterial diseases with a few pills of antibiotics.

Until about 1900, the fast-growing natural sciences, guided  by
rational \citep{Descartes-Discourse} and empirical \citep{lock-thu,hume-thu} thinking,
and seconded by the European Enlightenment, prospered under the assumption of physical determinism.
Under the {\em aegis} of physical determinism, all incapacities to predict and manipulate physical behavior were
interpreted to be merely {\em epistemic} in nature, purporting that, with growing precision of measurements and
improvements of theory, all physical unknowables will eventually be overcome and turned into knowables;
that is,   everything should in principle be knowable.
Even  statistical  quantities would describe underlying deterministic behaviors.
Consequently, there could not exist any physical behavior or entity
without a cause  stimulating or pushing  it into existence.


The uprise of determinism culminated in the following statement by \citet[chap.~2]{laplace-prob}:
\begin{quote}
{
Present events are connected with preceding ones
by a tie based upon the evident principle that a thing
cannot occur without a cause which produces it. This
axiom, known by the name of the principle of sufficient
reason, extends even to actions which are considered
indifferent $\ldots$


We ought then to regard the present state of the
universe as the effect of its anterior state and as the
cause of the one which is to follow. Given for one
instant an intelligence which could comprehend all the
forces by which nature is animated and the respective
situation of the beings who compose it an intelligence
sufficiently vast to submit these data to analysis it
would embrace in the same formula the movements of
the greatest bodies of the universe and those of the
lightest atom; for it, nothing would be uncertain and
the future, as the past, would be present to its eyes.
}
\end{quote}
The invention of (analytic) functions reflects this paradigm quite nicely:
some dispersionless point coordinate $x(t)$ of infinite precision
serves as the representation \citep{hertz-94} of a physical state
as a (unique) function of physical time~$t$.

Indeed, the possibility to formulate theories {\it per se},
and in particular, the applicability of formal, mathematical models, comes as a mind-boggling surprise and
cannot be taken for granted;
there appears to be what \citet{wigner}  called an
``unreasonable effectiveness of mathematics in the natural sciences.''
Even today, there is a Pythagorean consensus that there is no limit to
dealing with physical entities in terms of mathematical formalism.
And, as mathematics increasingly served as a proper representation of reality,
and computational deduction systems were increasingly  introduced to delineate formalizable truth,
algorithmics started to become a metaphor for physics.
In algorithmic terms, nature computes, and can be (re)programmed to perform certain tasks.

The natural sciences continued to be uninhibited by any sense of limits until
about {\it fin-de-si\`ecle}, around 1900.
In parallel, the formalization of mathematics progressed in an equally uninhibited way.
\citet[170]{hilbert-26} argued that
nobody should ever expel mathematicians from the paradise created by Cantor's set theory
and posed a challenge \citep{hilbert-1900e}
to search for a consistent, finite system of formal axioms which would be able to render all
mathematical and physical truths; just like quasi-finitistic ways to cope with infinitesimal calculus had been found.

This type of belief system that claims omniscience could be called ``deterministic conjecture''
because no proof for its validity can be given,
nor is there any way of falsification \citep{popper-en}.
Alas, from a pragmatic point of view, omniscience  can be  effectively  disproved
on a daily basis by tuning in to local weather forecasts.

Furthermore, it seems to be an enduring desire of human nature to be able not merely to trust
the rules and theories syntactically and operationally \citep{bridgman}
but also to be able to semantically interpret them as implying and carrying some
ontological significance or truth -- as if
reality would communicate with us, mediated through our senses, thereby revealing the laws governing nature.
Stated pointedly, we not only wish to accept physical theories as pure abstractions and constructions of our own mind \citep{berkeley}
but we associate meaning and truth to them
so much so that only very reluctantly do we admit their preliminary, transient, and changing character \citep{lakatosch}.

\subsection{Rise of indeterminism}

Almost unnoticed,  the tide of indeterminism started to build
 toward the end of the nineteenth century \citep{purrington,Kragh-qg}.
At that time, mechanistic theories faced an increasing number of anomalies:
Poincar\'e's discovery of  instabilities  of trajectories of celestial bodies
(which made them extremely sensible to initial conditions),
radioactivity \citep{Kragh-1997AHESradioact,Kragh-2009_RePoss5},
X-rays,
specific heats of gases and solids,
emission and absorption of light
(in particular, blackbody radiation),
the (ir)reversibility dichotomy
between classical reversible mechanics and
Boltzmann's statistical-mechanical theory of entropy {\it versus} the second law of thermodynamics,
and the experimental refutation of classical  constructions  of the ether as a medium for the propagation of light waves.

After the year 1900 followed a short period of revolutionary new physics, in particular,
quantum theory and relativity theory,
without any strong inclination toward (in)determinism.
Then indeterminism erupted with Born's claim that quantum mechanics has it both ways:
the quantum state evolves strictly deterministically,
whereas the individual event or measurement outcome occurs indeterministically.
Born also stated that he {\em believed} that there is no cause for an individual quantum event;
that is, such an outcome occurs irreducibly at random.

There followed a fierce controversy, with many researchers such as Born, Bohr, Heisenberg, and Pauli
taking the indeterministic stance,
whereas others,
like Planck \citep{born-55}, Einstein \citep{epr,ein-reply}, Schr\"odinger, and De Brogli, leaning toward determinism.
This latter position was pointedly put forward by Einstein's {\it dictum} in a letter to Born,
dated December~12, 1926 \cite[113]{born-69}:
{``In any case I am convinced that he [the Old One] does not throw dice.''}
At present, indeterminism is clearly favored, the canonical position being expressed by \citet{zeil-05_nature_ofQuantum}:
{``The discovery that individual events are
irreducibly random is probably one of the
most significant findings of the twentieth
century. $\ldots$~For the individual event in quantum physics,
not only do we not know the cause, there is no cause.''}

The last quarter of the twentieth century saw the rise of yet another form of physical indeterminism,
originating in Poincar\'e's aforementioned
discovery of instabilities of the motion of classical bodies
against variations of initial conditions \citep{Campbell-1882,poincare14,Diacu96-ce}.
This scenario of {\em deterministic chaos} resulted in a plethora of claims regarding indeterminism
that resonated with a general public susceptible to fables and fairy tales \citep{bricmont}.

In parallel, G\"odel's incompleteness theorems \citep{godel1,tarski:32,davis-58,davis,smullyan-92},
as well as related findings in the computer sciences \citep{turing-36,chaitin3,calude:02,gruenwald-vitanyi},
put an end to Hilbert's program of finding a finite axiom system for all mathematics.
G\"odel's incompleteness theorems also established formal
bounds on provability, predictability, and induction.
(The incompleteness theorems also put an end to philosophical contentions
expressed by \citet[101]{schlick-35} that, beyond epistemic unknowables and
the ``essential incompetence  of human knowledge,'' there is ``not a single real
question for which it would be {\em logically} impossible to find a solution.'')

Alas, just like determinism, physical indeterminism cannot be proved, nor can there be given any reasonable criterion for its falsification.
After all, how can one check against all laws and find none applicable?
Unless one is willing to denote any system whose laws are currently unknown
or whose behavior is hard to predict with present techniques as indeterministic,
there is no scientific substance to such absolute claims,
especially  if one takes into account the bounds imposed by the theory of recursive functions discussed later.
So, just as in the deterministic case, this position should be considered conjectural.


In discussing the present status of physical (in)determinism,
we shall first consider provable unknowables through reduction to incompleteness theorems of recursion theory,
then discuss classical deterministic chaos, and finally deal with the three types of quantum indeterminism:
the occurrence of certain single events, complementarity, and value indefiniteness.
The latter quantum unknowables are not commonly accepted by the entire community of physicists;
a minority is still hoping for a more complete quantum theory than the present statistical theory.


\section{Provable physical unknowables}

In the past century,
unknowability has been formally defined and {\em derived} in terms of a precise,
formal notion of unprovability \citep{godel1,tarski:32,tarski:56,turing-36,rogers1,davis-58,odi:89,smullyan-92}.
This is a remarkable departure from informal suspicions and observations regarding the limitations
of our worldview.
No longer is one reduced to informal, heuristic contemplations and comparisons about what one knows and can do {\it versus}
one's ignorance and incapability.
Formal unknowability is about formal proofs of unpredictability and impossibility.

There are several pathways to formal undecidability.
For contemporaries accustomed to computer programs (and their respective codes),
a straight route may be algorithmic.
What is an algorithm? In Turing's \citeyearpar[34]{Turing-Intelligent_Machinery} own words,
 \begin{quote}
{
a man provided with paper, pencil and rubber, and subject to strict discipline [carrying out a set of rules of procedure written down] is in effect a universal computer.
}
\end{quote}
From a purely syntactic point of view,
formal systems in mathematics  can be identified with computations
and {\it vice versa}.
Indeed, as stated by G\"odel \citeyearpar[369-370]{godel-ges1} in a {\em postscript,} dated from  June 3, 1964:
 \begin{quote}
 {
 due to A. M. Turing's work,
 a precise and unquestionably
 adequate definition of the general concept of formal system can now be
 given, the existence of undecidable arithmetical propositions and the
 non-demonstrability of the consistency of a system in the same system
 can now be proved rigorously for {\em every} consistent formal system
 containing a certain amount of finitary number theory.

 Turing's work gives an analysis of the
 concept of ``mechanical
 procedure'' (alias ``algorithm'' or ``computation procedure'' or
 ``finite combinatorial procedure''). This concept is shown to be
 equivalent with that of a ``Turing machine.'' A formal system can
 simply be defined to be any mechanical procedure for producing
 formulas, called provable formulas.
}
 \end{quote}


Almost since its discovery, attempts \citep{popper-50i,popper-50ii} have been made to translate
formal incompleteness into physics,
mostly by reduction to some provable undecidable problem
of recursion theory  such as the halting
problem \citep{wolfram84,kanter,moore,wolfram85b,dc-d91a,dc-d91b,suppes-1993,svozil-93,1994IJTP...33.1085H,casti:94-onlimits_book,casti:96-onlimits,barrow-impossibilities}.
Here the term {\em reduction} indicates that physical undecidability is linked or reduced to logical undecidability.
A typical example is the embedding of a Turing machine or any type of computer capable of
universal computation into a physical system.
As a consequence, the physical system inherits
any type of unsolvability derivable for universal computers such as the
unsolvability of the halting problem:
because the computer is part of the physical system, so are its behavioral patterns
[and {\em vice versa} \citep{bridgman,landauer:86,landauer}].

Note that these logical and recursion-theoretical
types of physical unknowables are only derivable within deterministic systems that are
strong enough to express {\em self-reference}, {\em substitution} \cite[chap.~1]{smullyan-92},
and {\em universal computation}.
Indeterministic systems are not deterministic by definition,
and too-weak forms of expressibility are trivially incomplete \citep{bruk-08},
as they are incapable of expressing universal computation or self-reference and substitution.


G\"odel himself did not believe that his incompleteness theorems had any relevance for physics,
especially not for quantum mechanics.
The author was told by professor Wheeler that G\"odel's resentments
[also mentioned in \citet[140--141]{bernstein}]
may have been due to Einstein's negative opinion about quantum theory,
because Einstein may have brainwashed G\"odel
into believing that all efforts in this direction were in vain.


\subsection{Intrinsic self-referential observers}


Embedded \citep{toffoli:79}, intrinsic observers \citep{svozil-94} cannot leave their Cartesian prison \cite[Meditation~1.12]{descartes-meditation}
and step outside the universe examining it from some  Archimedean point \cite[sect.~11,  405--409]{bos1}.
Thus every physical observation is reflexive \citep{nagel-ViewFromNowhere,sosa-rk2} and circular \citep{Kauffman198753}.
The self-referential and substitution capability of observers results in very diverse,
unpredictable forms of behavior  and in provable unknowables.

For the sake of the further analysis, suppose that there exist observers measuring objects  and that
observers and objects are distinct from one another, separated by a cut.
Through that cut, information is exchanged.
Symbolically, we may regard the object as an agent contained in a black box,
whose only relevant emanations are representable by finite strings of zeroes and ones
appearing on the cut, which can be modeled by any kind of screen or display.
According to this purely syntactic point of view,
a physical theory should be able to render identical symbols like the ones appearing through the cut;
that is, a physical theory should be able to mimic or emulate the black box to which it purports to apply.
This view is often adapted in quantum mechanics \citep{fuchs-peres},
where the question regarding any  meaning of the quantum formalism is notorious \cite[129]{feynman-law}.


A sharp distinction between a physical object and an extrinsic
outside observer is a rarely affordable abstraction.
Mostly the observer is part of the system to be observed.
In such cases,
the measurement process is modeled symmetrically,
and information is exchanged between observer and object bidirectionally.
This symmetrical configuration makes a distinction between observer and object
purely conventional \citep{svozil-2001-convention}.
The cut is constituted by the information exchanged.
We tend to associate with the  measurement apparatus
one of the two subsystems that, in comparison, is  larger, more classical,
and up-linked with some conscious observer \citep{wigner:mb}.
The rest of the system can then be called the measured object.


Intrinsic observers face all kinds of paradoxical self-referential situations.
These have been expressed informally as puzzling amusement and artistic perplexity,
and as a formalized, scientifically valuable resource.
The {\em liar paradox}, for instance, is already mentioned in the Bible's Epistle to Titus 1:12, stating that
``one of Crete's own prophets has said it: `Cretans are always liars, evil brutes, lazy gluttons.'
He has surely told the truth.''
In what follows, paradoxical self-referentiality will be applied to argue
against the solvability of the general induction problem
as well as for a pandemonium of undecidabilities related to physical systems
and their behaviors. All are based on intrinsic observers embedded
in the systems they observe.

It is not totally unreasonable to speculate that the
limits of intrinsic self-expression seems to be
what G\"odel himself
considered the gist of his incompleteness theorems.
In a reply to a letter by Burks
[reprinted in \citet[55]{v-neumann-66}; see also \citet[554]{fef-84}],
G\"odel states:
 \begin{quote}
 {
that a complete epistemological description
 of a language $A$ cannot be given in the same language $A$, because
 the concept of truth of sentences of $A$ cannot be defined in $A$. It
 is this theorem which is the true reason for the existence of
 undecidable propositions in the formal systems containing arithmetic.
}
 \end{quote}

One of the first researchers to become interested in the application
of paradoxical self-reference to physics
was the philosopher \citet{popper-50i,popper-50ii}
who published two almost forgotten papers
discussing, among other issues, Russell's paradox of
Tristram Shandy \citep{sterne}:
In volume 1, chapter 14, Shandy finds that he could publish
two volumes of his life every year,
covering a time span far shorter than the time it took him to write
these volumes. This de-synchronization, Shandy concedes,
will rather increase than diminish as he advances; one may thus have serious doubts about
whether he will ever complete his autobiography.
This relates to a question of whether there can be a physical computer that can be assured
of correctly {\em processing information faster than the universe does.}
\citet[016128-1]{PhysRevE.65.016128} states that [see also \citet[sect.~5]{CalCamSvo-Stef-1995}]
{``In a certain sense, the universe is more powerful
than any information-processing system constructed within it
could be. This result can alternatively be viewed as a restriction
on the computational power of the universe -- the universe
cannot support the existence within it of a computer
that can process information as fast as it can.''}


\subsection{Unpredictability}
\label{2010-unknowable-s2}

For any deterministic system strong enough to support
universal computation,  the general forecast or prediction
problem is provable unsolvable.
This proposition will be argued by reduction to the halting problem, which is provable unsolvable.
A straightforward embedding of a universal computer
into a physical system results in the fact that,
owing to the reduction to the recursive undecidability of the halting problem,
certain future events cannot be predicted
and are thus provable indeterministic.
Here reduction again means that physical undecidability is linked or reduced
to logical undecidability.

A clear distinction should be made between {\em determinism}
(such as {\em computable evolution laws}) and {\em predictability} \citep{suppes-1993}.
Determinism does not exclude unpredictability in the long run.
The local (temporal), step-by-step evolution of the system can be perfectly deterministic and computable,
whereas recursion-theoretic unknowables correspond to global observables at unbounded time scales.
Indeed, (nontrivial) provable unpredictability requires determinism,
because formalized proofs require formal systems or algorithmic behavior.



Unpredictability in indeterministic systems is tautological and trivial.
At the other extreme, one should also keep in mind that there exist rather straightforward
pre-G\"odelian impossibilities \citep{bruk-08}  to
express certain mathematical truths
 in weak systems that are incapable of
representing universal computation or Peano arithmetic.

For the sake of exploring (algorithmically)
what paradoxical self-reference is like,
one can consider the sketch of a proof by contradiction
of the unsolvability of the halting problem.
The halting problem is about whether or not a computer will eventually halt on a given input,
that is, will evolve into a state indicating the completion of a computation task or will stop altogether.
Stated differently, a solution of the halting problem will be an algorithm that
decides whether another arbitrary algorithm on arbitrary input will finish running or will run forever.

The scheme of the proof by contradiction is as follows:
the existence of a hypothetical halting algorithm
capable of solving the halting problem will be {\em assumed.}
This could, for instance, be a subprogram of some suspicious supermacro library
that takes the code of an arbitrary program as input and outputs 1 or 0,
depending on whether or not the program halts.
One may also think of it as a sort of oracle or black box analyzing an arbitrary
program in terms of its symbolic code and outputting one of two symbolic states, say, 1 or 0,
referring to termination or nontermination of the input program, respectively.

On the basis of this {\em hypothetical halting algorithm}
one constructs another {\em diagonalization program} as follows:
on receiving some arbitrary {\em input program} code as input, the {diagonalization program}
consults the {\em hypothetical halting algorithm} to find out whether or not this
{input program} halts; on receiving the answer, it does the {\em opposite:}
If  the   hypothetical halting algorithm  decides that the   input program  {\em halts,}
the   diagonalization program  does {\em not halt} (it may do so easily by entering an infinite loop).
Alternatively, if  the   hypothetical halting algorithm  decides that the  input program  does {\em not halt,}
the {diagonalization program} will {\em halt} immediately.

The {diagonalization program} can be forced to execute a paradoxical task by
receiving {\em its own program code} as input.
This is so because, by considering the {diagonalization program,}
the {hypothetical halting algorithm} steers the {diagonalization program} into
{\em halting} if it discovers that it {\em does not halt;}
conversely,  the {hypothetical halting algorithm} steers the {diagonalization program} into
{\em not halting} if it discovers that it {\em halts.}

The contradiction obtained in applying the {\em  diagonalization program} to its own code proves that this program
and, in particular, the {hypothetical halting algorithm} cannot exist.
A slightly revised form of the proof (using quantum diagonalizaton operators that are equivalent to a classical {\em derangement} or {\em subfactorial})
holds for quantum diagonalization \citep{1612095},
as quantum information could be in a fifty-fifty fixed-point halting state.
Procedurally, in the absence of any fixed-point halting state,
the aforemetioned task might turn into a nonterminating
alteration of oscillations between halting and nonhalting states \citep{Kauffman198753}.


A universal computer
can in principle be embedded into, or realized by, certain physical systems designed to universally compute.
An example of such a physical system is the computer on which I am currently typing this chapter.
Assuming unbounded space [i.e., memory \citep{calude-staiger-09}] and time,
it follows by
reduction \citep{wolfram84,kanter,moore,wolfram85b,dc-d91a,dc-d91b,suppes-1993,svozil-93,1994IJTP...33.1085H,casti:94-onlimits_book,CalCamSvo-Stef-1995,casti:96-onlimits,barrow-impossibilities}
that there exist physical observables,
in particular, forecasts about whether or not an embedded computer will ever
halt in the sense sketched earlier,
that are provably undecidable.





\subsection{The busy beaver function as the maximal recurrence time}

The busy beaver function \citep{rado,chaitin-ACM,dewdney,brady}
addresses the following
question: suppose one considers all  programs (on a particular computer)
up to length (in terms of the number of symbols) $n$.
What is the {\em largest number} producible by such a program before halting?
(Note that non-halting programs, possibly producing an infinite number, e.g., by a non-terminating loop, do not apply.)
This number may be called the {\em  busy beaver function} of $n$.
The first values of a certain universal computer's busy beaver function with two states and n symbols
 are, for $n=$ 2, 3, 4, 5, 7 and 8, known to be, or estimated by \citep{dewdney,brady},
 4, 6, 13, greater than $10^{3}$, greater than  $10^{4}$, and greater than
$10^{44}$.

Consider a related question: what is the upper bound of running time  --  or,
alternatively, recurrence time  --  of a program of length $n$ bits before
terminating  or, alternatively, recurring?
An answer to this question will explain just how long we have to
wait for the most time-consuming program of length $n$ bits to
halt. That, of course, is a worst-case scenario. Many programs of
length $n$ bits will have halted long  before the maximal halting time.
We mention without proof \citep{chaitin-ACM,chaitin-bb}  that
this bound can be represented by the busy beaver function.

Knowledge of the maximal halting time would solve the halting
problem quantitatively
because if the maximal halting time were known
and bounded by any computable function of the program size of $n$ bits,
one would have to wait
just a little longer than the maximal halting time to make sure
that every program of length $n$  --  also this particular program, if it is destined for termination  --
has terminated.
Otherwise, the program would run forever.
Hence, because of the recursive unsolvability of the halting problem
the maximal halting time cannot be a computable function.
Indeed, for large values of $n$, the maximal halting time explodes and
grows faster than any computable function  of $n$.


By reduction, upper bounds for the recurrence of any kind of physical behavior can be obtained;
for deterministic systems representable by $n$ bits,
the maximal recurrence time grows faster than any computable number
of $n$.
This bound from below for possible behaviors may be interpreted quite generally
as a measure
of the impossibility to predict and forecast such behaviors by algorithmic means.


\subsection{Undecidability of the induction problem}

Induction, in physics, is the inference of general rules
dominating and generating physical behaviors from these behaviors alone.
For any deterministic system strong enough to support
universal computation, the general induction problem
is provable unsolvable.
Induction is thereby reduced to the unsolvability of
the rule inference problem \citep{go-67,blum75blum,angluin:83,ad-91,li:92}
of identifying a rule or law reproducing the behavior of a deterministic system
by observing its input-output performance by purely algorithmic means
(not by intuition).

Informally, the algorithmic idea of the proof is to take any sufficiently powerful
rule or method of induction and, by using it, to define some
functional behavior that is not identified by it.
This amounts
to constructing an algorithm which
(passively)
fakes the guesser by simulating some particular function
until the guesser
pretends to be able to guess the function correctly.
In a second,  diagonalization step, the faking algorithm then switches to a different
 function to invalidate the guesser's guess.

%
%More formally, assume two (universal) computers $U$ and $U'$.
%Suppose that the second computer $U'$ executes an arbitrary
%algorithm $p$ unknown to computer $U$, the ``guesser.''
% The task of $U$,
% which is called the rule inference problem,
% is to conjecture the ``law'' or algorithm $p$ by analysing the
% behavior of $U'(p)$.
% The recursive unsolvability of the rule inference problem \citep{go-67} states that this task cannot be
% performed by any effective computation.
%
%For the sake of contradiction, assume \citep{li:92}
%that there exists a ``perfect guesser'' $U$ which can identify
%all total recursive functions (wrong).
%Then it is possible to construct a function $\varphi^\ast:{\Bbb N} \rightarrow
%\{0,1\}$, such that the guesses of $U$ are wrong infinitely often,
%thereby contradicting the above assumption.
%
%Define $\varphi^\ast (0)=0$.
%One may construct $\varphi^\ast $ by simulating $U$. Suppose the values
%$\varphi^\ast (0)$, $\varphi^\ast (1)$, $\varphi^\ast (2)$, $\cdots$,
%$\varphi^\ast (n-1)$ have already been constructed. Then, on input $n$,
%simulate $U$, based on the previous series
%$
%\{0, \varphi^\ast (0)\}$,
%$
%\{1, \varphi^\ast (1)\}$,
%$
%\{2, \varphi^\ast (2)\},
%\cdots ,
%\{n-1, \varphi^\ast (n-1)\}$,
% and define
%$\varphi^\ast (n)$ equal to 1 plus the guess of $U$ of
%$\varphi^\ast (n)$ mod 2. In this way, $U$ can never guess
%$\varphi^\ast $ correctly; thereby making an infinite number of mistakes.

One can also interpret this result in terms of the recursive
unsolvability of the halting problem, which in turn is related to the busy beaver function;
there is no recursive bound on the
time the guesser has to wait  to make sure that the guess is
correct.

\subsection{Impossibility}

Physical tasks which would result in paradoxical
behavior \citep{hilbert-26} are impossible to perform.
One such task is the solution of the general halting problem, as discussed earlier.
Thus omnipotence appears infeasible, at least as long as one sticks to the usual
formal rules opposing inconsistencies \cite[163]{hilbert-26}.

Another such paradoxical task (requiring substitution and self-reference) can be forced upon  {\it La Bocca della Verit\'a} (Mouth of Truth),
located in the {\it portico} of the church of {\it Santa Maria in Cosmedin} in Rome.
It is believed that if one tells a lie with one's hand in the mouth of the sculpture,
the hand will be bitten off; another less violent legend has it that anyone sticking a hand in the mouth while
uttering a false statement will never be able to pull the hand back out.
\citet[178]{rucker} once allegedly put in his hand in the sculpture's mouth uttering, {``I will not be able to pull my hand back out.''}
The author leaves it to the reader to imagine {\it La Bocca della Verit\'a}'s confusion when confronted with such as statement!

There is a {\em pandemonium} of conceivable physical tasks \citep{barrow-impossibilities},
some quite entertaining \citep{smullyan-78}, which would result in paradoxical behavior and
are thus impossible to perform.
Some of these tasks are pre-G\"odelian and merely require {\em substitution}.

For the sake of demonstrating paradoxical substitution and the resulting impossibility,
consider the following  {\em printing task} discussed by \citet[2--4]{smullyan-92}.
Let the expressions  (not), (printable), (self-substitute),
have a standard interpretation in terms of negation, printing,
 and  self-reference by substitution
[i.e., if $X$ is some expression formed by the earlier three expressions and brackets, then
{(self-substitute)}$(X)=X(X)$], respectively,
and define {(not)(printable)}$(X)$  for arbitray expressions $X$
to be true if and only if $X$ cannot be printed.
Likewise,   {(not)(printable)}{(self-substitute)}$(X)$  is defined
to be true if and only if {(self-substitute)}$X$ cannot be printed.
Whatever the rules deriving expressions (subject to the notion of truth defined earlier) may be,
as long as the system is consistent and produces only true propositions (and no false ones),
within this small system,
the following proposition is {\em true but unprintable:}
{(not)(printable)(self-substitute)}$[${(not)(printable)(self-substitute)}$]$.
By definition, this proposition is true if and only if  (self-substitute)$[${(not)(printable)(self-substitute)}$]$
cannot be printed.
As per definition, (self-substitute)$[${(not)(printable)(self-substitute)}$]$
is just {(not)(printable)(self-substitute)}$[${(not)(printable)(self-substitute)}$]$,
the proposition is true if and only if it is not printable.
Thus the proposition  is either true and cannot be printed,
or it is printable and thus false.
The latter alternative is excluded by the assumption of consistency.
Thus one is left with the only consistent alternative that the proposition
{(not)(printable)(self-substitute)}$[${(not)(printable)(self-substitute)}$]$
is true but unprintable.
Note also that, since its negation {(printable)(self-substitute)}$[${(not)(printable)(self-substitute)}$]$
is false, it is also not printable (by the consistency assumption),
and hence {(printable)(self-substitute)}$[${(not)(printable)(self-substitute)}$]$
is an example of a proposition which is undecidable within the system --
neither it nor its negation will ever be printed in a consistent formalized system with the
notion of truth defined earlier.

\subsection{Results in classical recursion theory with implications for theoretical physics}


The following theorems of recursive  analysis \citep{aberth-80,Weihrauch} have some
implications for theoretical physics \citep{kreisel}:
(1)
There exist recursive monotone bounded sequences of rational numbers
whose limit is no computable number
\citep{Specker49}.
A concrete example of such a number is Chaitin's Omega number \citep{chaitin3,calude:02,calude-dinneen06},
the halting probability for a computer (using prefix-free code),
which can be defined by a sequence of rational numbers
with no computable rate of convergence.
(2)
There exist a recursive real function which has its maximum in the unit interval
at no recursive real number \citep{Specker57}.
This has implications for the principle of least action.
(3)
There exists a real number $r$ such that $G(r) = 0$ is recursively undecidable for $G(x)$
in a class of functions which involves polynomials and the sine function
\citep{wang}.
This, again, has some bearing on  the principle of least action.
(4)
There exist incomputable solutions of the wave equations for computable initial values
\citep{pr1,bridges1}.
(5)
On the basis of theorems of recursive analysis \citep{Scarpellini-63,richardson68},
many questions in dynamical systems theory are provable undecidable \citep{1985cfd..book.....F,dc-d93,Stewart-91,calude:037103}.
%\end{description}



\section{Deterministic chaos}

The wording {\em deterministic chaos} appears to be a {\it contradictio in adjecto},
indicating a hybrid form of chaotic behavior in deterministic systems \citep{li-83,nld-chaos}.
Operationally, it is characterized by the practical impossibility of forecasting the future
because the system is unstable \citep{Lyapunov-92} and very sensitive
to tiny variations of the initial state.
Because the initial state can only be determined with finite accuracy,
its evolution will soon become totally unpredictable.

\subsection{Instabilities in classical motion}

In 1885 King Oscar II of Sweden and Norway, stimulated by Weierstrass, Hermite, and Mittag-Leffler,
offered a prize to anybody contributing toward the solution of the so-called {\em $n$-body problem} \cite[2]{weierstrass-1885}:
\begin{quote}
{
Given a system of arbitrarily many mass points that attract each
according to Newton's law, try to find, under the assumption that no two points ever collide,
 a representation of the coordinates of each point
as a series in a variable that is some known function of time and for
all of whose values the series converges uniformly.
}
\end{quote}

The prize-winning work was expected to render systematic techniques toward a solution to {\em stable} motion
such that systems whose states start out close together will stay close together forever \cite[69]{Diacu96-ce}.
To everyone's surprise, the exciting course of events \citep{peterson-NC,Diacu96,Diacu96-ce}
resulted in Poincar{\'e}'s prize-winning centennial revised contribution \citep{poincare-1890},
which predicted unexpected and irreducible {\em instabilities} in the mechanical motion of bodies.
%Stimulated by a hint of Phragm{\'e}n,
Poincar{\'e} was led to the conclusion that sometimes small
variations in the initial state could lead to huge variations in the
evolution of a physical system at later times.
In Poincar{\'e}'s own words \cite[chapt.~4, sect.~2,  56--57]{poincare14}:
\begin{quote}
{
If we would know the laws of nature and the state of the Universe precisely
for a certain time,
we would be able to predict with certainty
the state of the Universe for any later time.
But
$\ldots$
it can be the case that small differences in the initial values
produce great differences in the later phenomena;
a small error in the former may result in a large error in the latter.
The prediction becomes impossible and we have a ``random phenomenon.''}
\end{quote}

Note that Poincar{\'e} adheres to a Laplacian-type determinism
but recognizes the possibility that systems whose states start out close together
will stay close together {\em for a while} \cite[69]{Diacu96-ce}
and then diverge into totally different behaviors.
Today such behaviors are subsumed under the name {\em deterministic chaos.}
In chaotic systems, it is practically impossible to specify
the initial value precise enough to allow long-term predictions.

Already in 1873, Maxwell mentioned \cite[211-212]{Campbell-1882}
\begin{quote}
{
When an infinitely small variation in the present state may bring about a finite difference in the state of the
system in a finite time, the condition of the system is said to be unstable.
It is manifest that the existence of unstable conditions renders impossible the prediction of future events, if our
knowledge of the present state is only approximate, and not accurate.
}
\end{quote}
Maxwell also discussed unstable states of high potential energy whose
spontaneous \citep{frank}  decay or change \cite[212]{Campbell-1882}
{``requires an expenditure of work, which in certain cases may be
infinitesimally small, and in general bears no definite proportion to the energy developed in consequence thereof.''}

Today, after more than a century of research into unstable chaotic motion, {\em symbolic dynamics}
identified the
 {\em Poincar{\'e} map near a homocyclic orbit},
the {\em horseshoe map} \citep{smale-hm},
and the {\em shift map}
as equivalent origins of classical deterministic chaotic motion,
which is characterized by a {\em computable evolution law}
and the {\em sensitivity}  and instability with respect to variations of the
{\em initial value} \citep{shaw,li-83,nld-chaos}.

This scenario can be demonstrated by considering the shift map $\sigma$ as it
pushes up dormant information residing in the successive bits of the initial state represented by the sequence
$s=0.\text{(bit~1)}\text{(bit~2)}\text{(bit~3)}\cdots$,
thereby truncating the bits before the comma;
that is, $\sigma (s)= 0.\text{(bit~2)}\text{(bit~3)}\text{(bit~4)}\cdots$,
$\sigma (\sigma (s))= 0.\text{(bit~3)}\text{(bit~4)}\text{(bit~5)}\cdots$, and so on.
Suppose a measurement device operates with a precision of, say, two bits after the comma,
indicated by a two bit window of measurability;  thus intially
all information beyond the second bit after the comma is hidden to the experimenter.
Consider two initial states
$s=[0.\text{(bit~1)}\text{(bit~2)}] \text{(bit~3)}\cdots$ and
$s'=[0.\text{(bit~1)}\text{(bit~2)}] \text{(bit~3)}'\cdots$,
where the square brackets
indicate the boundaries of the window of measurability (two bits in this case).
Initially, as the representations of both states start with the same two bits after the comma
$[0.\text{(bit~1)}\text{(bit~2)}]$,
these states appear operationally identical and cannot be discriminated experimentally.
Suppose further that, after the second bit, when compared,
the successive bits
$\text{(bit }i\text{)}$ and $\text{(bit }i\text{)}'$
in both state representations at identical positions $i=3,4,\ldots$ are totally
independent and uncorrelated.
After just two iterations of the shift map $\sigma$, $s$ and
$s'$
may result in totally  different, diverging observables
$\sigma (\sigma (s))= [0.\text{(bit~3)}\text{(bit~4)}]\text{(bit~5)}\cdots$
and
$\sigma (\sigma (s'))= [0.\text{(bit~3)}'\text{(bit~4)}']\text{(bit~5)}'\cdots$.

If the initial values are {\em defined} to be elements of
a continuum, then almost all (of measure one) of them
are not representable by any algorithmically compressible number;
in short,  they are random \citep{MartinLöf1966602,calude:02}.
Classical deterministic chaos results from the assumption of such a random initial value
 --  drawn somehow [one needs the {\em axiom of choice} \citep{wagon1,svozil-set} for doing this]
from the continuum urn  --  and the unfolding of the information contained therein by a recursively enumerable (computable),
deterministic (temporal evolution) function.
Of course, if one restricts the initial values to finite sets,
or, say, to the rationals, then the behavior will be periodic.
The randomness of classical, deterministic chaos resides
in the {\em assumption of the continuum};
an assumption which might be considered a convenience (for the sake of applying the infinitesimal calculus),
as it is difficult to conceive of any convincing
physical operational evidence supporting the full structure of continua.
If the continuum assumption is dropped, then what remains is Maxwell's
and Poincar{\'e}'s observation of the unpredictability
of the behavior of a deterministic system
due to instabilities and diverging evolutions from almost identical initial states \citep{Lyapunov-92}.


\subsection{Rate of convergence}

The connections between symbolic dynamical systems and universal computation
result in provable unknowables \citep{dc-d93,Stewart-91}.
These symbolic dynamic unknowables are different in type from the dynamical instabilities,
and should be interpreted recursion theoretically, as outlined in Section~\ref{2010-unknowable-s2}.

Let us come back to the original $n$-body problem.
About one hundred years after its formulation, as quoted earlier,
the $n$-body problem has been solved \citep{1969VeLen...7..121B,Babadzanjanz-1979,Wang91,Diacu96,Wang01,Babadzanjanz-1993,Babadzanjanz-2006}.
The three-body problem was already solved by \citet{Sundman12}.
The solutions are given in terms of  convergent power series.

Yet, to be practically applicable,
the rate of convergence of the series must be computable and even reasonably good.
One might already expect from symbolic dynamics,
in particular, from chaotic motion,
that these series solutions could converge very slowly.
Even the short-term prediction of future behaviors
may require the summation of a huge number of terms,
making these series unusable for all practical purposes
\citep{Diacu96,rousseau-2004}.

Alas, the complications regarding convergence may be more serious.
Consider a universal computer based on the $n$-body problem.
This can, for instance, be achieved by ballistic computation, such as the
``Billiard Ball'' model of computation
\citep{fred-tof-82,margolus-02}
that effectively embeds a universal computer into an  $n$-body system \citep{svozil-2007-cestial}.
It follows by reduction that certain predictions,
say, for instance, the general halting problem, are impossible.

What are the consequences of this reduction for the convergence of the series solutions?
It can be expected that not only do the series converge very slowly,
like in deterministic chaos,
but that, in general, there does not exist any computable rate of convergence
for the series solutions of particular observables.
This is very similar to the busy beaver function or to Chaitin's Omega number \citep{chaitin3,calude:02},
representing the halting probability of a universal computer.
The Omega number can be enumerated
by series solutions from quasi-algorithms
computing its very first digits \citep{calude-dinneen06}.
Yet, because of the incomputable growth of the time
required to determine whether certain summation terms corresponding to halting programs possibly contribute,
the series lack any computable rate of convergence.

Though it may be possible to evaluate
the state of the $n$ bodies by Wang's power series solution
for any finite time with a computable rate of convergence,
global observables, referring to (recursively) unbounded times, may be incomputable.
Examples of global observables correspond to solutions of certain decision problems
such as the stability of some solar system (we do not claim that this is provable incomputable), or the
halting problem.

This, of course,
stems from the metaphor and robustness of universal computation
and the capacity of the $n$-bodies to implement universality.
It is no particularity or peculiarity of Wang's power series solution.
Indeed, the troubles reside in the capacity to implement substitution, self-reference, universal computation,
and Peano arithmetic
by $n$-body problems.
Because of this capacity, there cannot exist other formalizable methods,
analytic solutions, or approximations capable of deciding and computing certain decision problems
or observables for the $n$-body problem.

\section{Quantum unknowables}

In addition to provable physical unknowables by reduction to recursion-theoretic ones,
and chaotic symbolic dynamic systems, a third group of physical unknowables resides in the quantum domain.
Although it has turned out to be a highly successful theory,
quantum mechanics, in particular, its interpretation and meaning,
has been controversially received within the physics community.
Some of its founding fathers, like Schr\"odinger and, in particular,  Einstein,
considered quantum mechanics to be an unsatisfactory theory:
Einstein, Podolsky and Rosen \citeyearpar{epr,ein-reply} argued
that there exist counterfactual \citep{svozil-2006-omni,vaidman:2009}
ways to infer observables from experiment that, according to quantum mechanics, cannot coexist simultaneously;
hence quantum mechanics cannot predict what experiment can (counterfactually) measure.
Thus quantum mechanics is {\em incomplete} and should eventually be substituted by a more complete theory.
Others, among them Born, Bohr, and Heisenberg,
claimed that unknowability in quantum mechanics is irreducible,  is ontic, and will remain so forever.
Over the years, the latter view seems to have prevailed
\citep{fuchs-peres,Bub-1999}, although not totally unchallenged
\citep{jammer:66,jammer1,jammer-92}.
Already Sommerfeld warned his students not to get
into the meaning behind quantum mechanics,
and as mentioned by \citet{clauser-talkvie},
not long ago, scientists working in that field
had to be very careful not to become discredited as {\em quacks.}
Richard Feynman \cite[129]{feynman-law}
once mentioned the
\begin{quote}
{perpetual torment that results
from [the question], ``But how can it be like that?'' which
is a reflection of uncontrolled but utterly vain desire to see
[quantum mechanics] in terms of an analogy with something familiar.
$\ldots$
Do not keep saying to yourself, if you can possibly avoid it,
``But how can it be like that?''
because you will get ``down the drain,'' into a blind alley from which nobody has yet
escaped.}
\end{quote}
This antirationalistic postulate of  irreducible indeterminism and meaninglessness came
after a period of fierce debate on the quantum foundations,
followed by decades of vain attempts to complete quantum mechanics in any operationally testable way,
and after the discovery of proofs of the incompatibility of local, realistic, context-independent ways to complete
quantum mechanics \citep{clauser,mermin-93}.


In what follows, we shall discuss three realms of quantum unknowables:
(1) randomness of single events,
(2) complementarity, and
(3) value indefiniteness.

\subsection{Random individual events}

In 1926, \citet[866]{born-26-1} [see an English translation in \citet[54]{wheeler-Zurek:83}] postulated that
\begin{quote}
{``from the standpoint of our quantum mechanics, there is no quantity
which in any individual case causally fixes the consequence of the collision;
but also experimentally we have so far no reason to believe that there are some inner properties of the atom
which condition a definite outcome for the collision.
Ought we to hope later to discover such properties $\ldots$  and determine them in individual cases?
Or ought we to  believe that the agreement of theory and experiment  --  as to the impossibility
of prescribing conditions? I myself am inclined  to give up determinism in the world of atoms.''
}
\end{quote}

Furthermore, Born suggested that, though {\em individual particles behave irreducibly indeterministic},
the {\em quantum state evolves deterministically}  in a strictly Laplacian causal way.
Indeed, between (supposedly irreversible) measurements the (unitary) quantum state evolution
is even reversible, that is, one-to-one, and amounts to a  generalized (distance preserving) rotation in complex Hilbert space.
In Born's \citeyearpar[804]{born-26-2} [see an English translation in \citet[302]{jammer:89}] own words,
\begin{quote}
{the motion of particles conforms to the laws of probability, but the probability itself
is propagated in accordance with the law of causality.
[This means that knowledge of a state in all points in a given time determines the distribution of
the state at all later times.]
}
\end{quote}

This distinction between a reversible, deterministic evolution of the quantum state, on one hand,
and the irreversible measurement, on the other hand, has left some physicists with an uneasy feeling;
in particular, because of the possibility to erase
\citep{PhysRevD.22.879,PhysRevA.25.2208,greenberger2,Nature351,Zajonc-91,PhysRevA.45.7729,PhysRevLett.73.1223,PhysRevLett.75.3783,hkwz}
measurements by reconstructing the quantum state,
accompanied by a complete loss of the information obtained from the quantum state before the (undone) measurement --
unlike in classical reversible computation \citep{bennett-73,bennett-82,maxwell-demon},
which still allows copying, that is, one-to-many operations, the quantum state evolution is strictly one-to-one.
Indeed, the possibility to undo measurements on quantum states appears to be  not bound by any fundamental principle,
and limited merely by the experimenter's  technological capacities.
Stated pointedly, it would in principle be possible to undo all measurements,
yet this cannot be accomplished  most of the time (for almost all measurements)
for all practical purposes~\cite{bell:a1}.
But then, one could speculate, Born's statement seems to suggest that the deterministic state evolution uniformly prevails.
Pointedly stated, if, at least in principle, there is no such thing as an irreversible measurement,
and the quantum state evolves uniformly deterministically, why should there exist indeterministic individual events?
In this view, the insistence in irreversible measurements as well as in an irreducible indeterminism associated with individual quantum events
appears to be an idealistic, subjective illusion -- in fact, this kind of indeterminism depends
on measurement irreversibility and decays into thin air if the latter is denied.

Similar arguments have been brought forth by \citet{everett} and \citet{schroedinger-interpretation}.
Note that it is not entirely clear [and indeed remains conventional \citep{svozil-2001-convention}]
where exactly the measurement cut \citep{wigner:mb,roessler-98}
between the observer and the object is located.
By assuming the universal applicability of quantum mechanics,
the object and the measurement apparatus could be uniformly {\em combined} into a
larger system whose quantum mechanical evolution should be deterministic;
otherwise quantum mechanics would not be universally valid.
Such frameworks hardly offer objective opportunities for indeterminism besides subjective ones --
in the {\em many worlds} resolution \citep{everett}, every one
of many simultaneous observers branching off to different universes
subjectively experiences the arbitrariness of the occurrence of events as indeterminism.
(This resembles the perception of a particular sequence of bits as compared to all possible ones.)

Alas, the deterministic evolution of the quantum state could result in the
{\em superposition} of classically contradictory states.
One of the mind-boggling, perplexing and counterintuitive consequences associated with this coexistence of classical contradictions is
Schr\"odinger's \citeyearpar[812]{schrodinger} cat paradox
implying the simultaneous coexistence of death and life of a macroscopic object such as a mammal.
Another one is Everett's \citeyearpar{everett} aforementioned many-worlds interpretation suggesting that
our universe perpetually branches
off into zillions of consistent alternatives.

Thus one is faced with a {\em dilemma:}
either to accept a somehow spurious nonuniformity in the evolution of the quantum state
during (irreversible) measurement processes -- an {\it ad hoc} assumption challenged by quantum erasure experiments --
or being confronted with the counterintuitive decay of quantum states into superpositions of
classically mutually exclusive states -- a sort of jelly -- not backed by our everday
experience as conscious beings (although often ambivalent we usually dont reside in mental ambiguity for too long).
\citet[19--20]{schroedinger-interpretation} sharply addressed the difficulties of a quantum theorist
coping with this aspect of the quantum formalism:
\begin{quote}
{
The idea that  [the alternate measurement outcomes] be not alternatives but {\em all} really happening simultaneously
seems lunatic to [the quantum theorist], just {\em impossible.}
He thinks that if the laws of nature took {\em this} form for,
let me say,
a quarter of an hour, we should find our surroundings rapidly turning into a quagmire, a sort of a featureless jelly or plasma,
all contours becoming blurred, we ourselves probably becoming jelly fish.
It is strange that he should believe this.
For I understand he grants that unobserved nature does behave this way -- namely according to the wave equation.
$\ldots$ according to the quantum theorist, nature is prevented from rapid
jellification only by our perceiving or observing it.
}
\end{quote}

If, however, an additional irreducible irreversible evolution or some other,
possibly environmental \citep{PhysRevD.22.879,RevModPhys.75.715},
effect associated with measurements (and  the collapse of the quantum wave function) is  postulated
or somehow emerges,
individual events may occur indeterministically.
The considerations might appear to be sophistries,  but
they have direct consequences for the supposedly most advanced random number generators of our time.
These devices operate with beam splitters
\citep{svozil-qct,rarity-94,zeilinger:qct,stefanov-2000,wang:056107,PhysRevA.82.022102},
which are strictly reversible
\citep{Mandel-Ou1987118,green-horn-zei,zeilinger:882,svozil-2004-analog}
-- one could demonstrate reversibility on beam splitters by  forming a Mach-Zehnder interforemeter with two
serially connected ones --
or parametric down-conversions  and entanglement \citep{0256-307X-21-10-027,fiorentino:032334,10.1038/nature09008}.

Born did not address these questions, nor did he specify the formal notion of indeterminism to which he was relating.
So far, no mathematical characterization of quantum randomness has been proved \citep{2008-cal-svo}.
In the absence of any indication to the contrary, it is mostly implicitly assumed
that quantum randomness is of the strongest possible kind,
which amounts to postulating that the symbolic sequences
associated with measurement outcomes are uncomputable
or even algorithmically incompressible.


Indeed, the quantum formalism does not predict the outcome of single events
when there is a mismatch between the context in which a state was prepared,
and the context in which it is measured.
Here, the term {\em context} \citep{svozil-2006-omni,svozil-2008-ql}
denotes a maximal collection of comeasurable observables,
or, more technically,
the maximal operator from which all commuting operators can be functionally derived
\cite[sect.~84]{halmos-vs}.
Ideally, a quantized system can be prepared
to yield exactly one answer in exactly one context \citep{zeil-99,DonSvo01,svozil-2002-statepart-prl}.
Other outcomes associated with other contexts occur indeterministically \citep{2008-cal-svo}.

Furthermore, the quantum formalism is incapable of predicting deterministically the radioactive decay of individual particles.
Attempts to find causal laws lost steam \citep{Kragh-1997AHESradioact,Kragh-2009_RePoss5} at the time of Born's suggestion
of the indeterministic interpretation of individual measurement outcomes,
and nobody has come up with a operationally satisfactory deterministic prediction since then.

In the absence of other explanations,
it is not too unreasonable to pragmatically presume that these single events occur without any causation
and thus at random.
Presently, this appears to be the prevalent opinion among physicists.
Such random quantum coin tosses
\citep{svozil-qct,rarity-94,zeilinger:qct,stefanov-2000,0256-307X-21-10-027,wang:056107,fiorentino:032334,svozil-2009-howto,10.1038/nature09008}
have been used for various purposes, such as delayed choice experiments
\citep{wjswz-98,zeilinger:qct}.

Note that randomness of this type \citep{Cris04,calude-dinneen05}
is postulated rather than proved and thus, unless disproved, remains conjectural.
This is necessarily so, for any claim of randomness can only be corroborated  {\em relative} to, and
{\em with respect} to, a more or less large class of laws or behaviors;
it is impossible to inspect the hypothesis against an infinity of  --  and even less so all  --  conceivable laws.
To rephrase a statement about computability \cite[11]{davis-58}, how can we ever exclude the possibility of our
presented, some day (perhaps by some extraterrestrial visitors), with a (perhaps
extremely complex) device  that computes and predicts
a certain type of hitherto random physical phenomenon?


\subsection{Complementarity}




{\em Complementarity} is the impossibility of measuring
two or more complementary observables
with arbitrary precision simultaneously.
In 1933, \citet[7]{pauli:58} gave the first explicit definition of {\em complementarity} stating that
[see the partial English translation in \cite[369]{jammer:89}]
\begin{quote}
{
in the case of  an indeterminacy of a property of a system at a certain configuration
(at a certain state of a system), any attempt to measure the respective property (at least partially)
annihilates the influence of the previous knowledge of the system on the (possibly statistical) propositions
about possible later measurement results.
$\ldots$
The impact
on the system by the  measurement apparatus for momentum (position) is such that
within the limits of the uncertainty relations
the value of the knowledge of the previous position (momentum) for the
prediction of later measurements of position and momentum is lost.
}
\end{quote}


Einstein, Podolsky, and Rosen \citeyearpar{epr} challenged quantum complementarity (and doubted the completeness of quantum theory)
by utilizing a configuration
of two entangled \citep{schrodinger,CambridgeJournals:1737068,CambridgeJournals:2027212} particles.
They
claimed to be able to empirically infer two different complementary contexts counterfactually simultaneously,
thus circumventing quantum complementarity.
Thereby, one context is measured on one side of the setup, whereas the other context is measured on the other side of it.
By the uniqueness property \citep{svozil-2006-uniquenessprinciple}  of certain two-particle states,
knowledge of a property of one particle entails the certainty
that, if this property were measured on the other particle as well, the outcome of the measurement would be
a unique function of the outcome of the measurement performed.

This makes possible the measurement of one context {\em as well as} the {\em simultaneous counterfactual inference}
of a different complementary context.
Because, one could argue, although one has actually measured on one side a different,
incompatible context compared to the context measured on the other side,
if, on both sides, the same  context {\em would be measured},
the outcomes on both sides {\em would be uniquely correlated}.
(This can indeed be verified in another experiment.)
Hence, the Einstein, Podolsky, and Rosen argument continues,
measurement of one context per side is sufficient,
for the outcome could be counterfactually inferred on the other side.
Thus, effectively two complementary contexts are knowable.
Based on this argument, Einstein, Podolsky, and Rosen suggested that quantum mechanics must be considered incomplete,
because it cannot predict what can be measured; thus a more complete theory is needed.

Complementarity was first encountered in quantum mechanics,
but it is a phenomenon also observable in the classical world.
To get  better intuition of complementarity, we shall consider generalized urn models \citep{wright,wright:pent} or,
equivalently \citep{svozil-2001-eua},
finite deterministic automata \citep{e-f-moore,svozil-93,schaller-96,dvur-pul-svo,cal-sv-yu} in an unknown initial state.
Both quasi-classic examples mimic complementarity to the extent that even quasi-quantum cryptography
can be performed with them \citep{svozil-2005-ln1e} as long as value indefiniteness is not a feature of the protocol
\citep{PhysRevLett.85.3313,2010-qchocolate},
that is, for instance,
the Bennett and  Brassard \citeyearpar{benn-84} protocol \citep{benn-92} can be implemented with generalized urn models,
whereas the Ekert protocol
\citep{ekert91} cannot.


A generalized urn model is
characterized by an ensemble of balls with black background color.
Printed on these balls are some color symbols.
Every ball contains just one  symbol per color.
Further assume some filters or eyeglasses that are
perfect because they totally absorb light of all other colors
but a particular  one.
In that way, every color can be associated with a particular pair of eyeglasses and {\it vice versa.}
%This, of course, is a system science trick related to intrinsic color perception.

When a spectator looks at a ball through such a particular pair of eyeglasses,
the only operationally recognizable symbol will be the one in the particular
color that is transmitted through the eyeglasses.
All other colors are absorbed, and the symbols printed on them will appear black
and therefore will not be differentiable from the black background.
Hence the ball will appear to carry a different message or symbol,
depending on the color with which it is viewed.

For the sake of demonstration, let us consider a generalized urn model with four
ball types, two colors, say red and green, and two symbols, say ``0'' and ``1,'' per color, that is,
ball type~1:~({\color{red}red~0}~{\color{green}green~0}),
ball type~2:~({\color{red}red~0}~{\color{green}green~1}),
ball type~3:~({\color{red}red~1}~{\color{green}green~0}),  and
ball type~4:~({\color{red}red~1}~{\color{green}green~1}).
The {green} pair of eyeglasses associated with the green observable
allows the observer to differentiate between
ball types 1 or 3 (associated with the green symbol ``0''),
and
ball types 2 or 4 (associated with the green symbol ``1'').
The {red} pair of eyeglasses  associated with the red observable
allows the observer to differentiate between
ball types 1 or 2 (associated with the green symbol ``0''),
and
ball types 3 or 4 (associated with the green symbol ``1'').
[Without going into details in general this yields sets of partitions of the set of ball types
resulting in partition logics \cite[chapt.~10]{svozil-93}.]




The difference between the balls and the quanta is the possibility
of viewing all the different symbols on the balls
in all different colors by taking off the eyeglasses;
also, one can consecutively look at one and the same ball with differently colored pair of eyeglasses,
thereby identifying the ball completely.
Quantum mechanics does not provide us with  a possibility to look across the quantum veil,
as it allows neither a global, simultaneous measurement of all complementary observables
nor a measurement of one observable without disturbing the measurement of another complimentary observable
(with the exception of Einstein, Podolsky, and Rosen counterfactual measurements discussed earlier).
On the contrary, there are strong formal arguments suggesting
that the assumption of a simultaneous
physical coexistence of such complementary observables yields a complete contradiction.
These issues will be discussed next.


\subsection{Value indefiniteness {\it versus} omniscience}

Still another quantum unknowable results from the fact that no global
(in the sense of all or at least certain finite sets of  complementary observables)
classical truth
assignment exists which is consistent with even a finite number of local (in the sense of comeasurable) ones,
that is, no consistent classical truth table can be given by pasting together the possible outcomes of measurements of certain complementary observables.
This phenomenon is also known as {\em value indefiniteness} or, by an option to interpret this result, {\em contextuality}  (see later).
Here the term {\em local} refers to a particular context \citep{svozil-2008-ql}
that, operationally, should be thought of as the collection of all comeasurable or copreparable \citep{zeil-99} observables.
The structure of quantum propositions \citep{birkhoff-36,kochen3,kalmbach-83,kalmbach-86,pulmannova-91,nav:91,svozil-ql}
can be obtained by pasting contexts together.

As by definition, only {\em one} such context is directly measurable,
arguments based on more than one context must necessarily involve counterfactuals \citep{svozil-2006-omni,vaidman:2009}.
A {\em counterfactual} is a would-be-observable or
{\em contrary-to-fact conditional}
\citep{chisholm-46}
which has not been measured but potentially could have been measured
if an observer would have decided to do so; alas the observer decided to measure a different, presumably complementary, observable.

Already scholastic philosophy,
for instance, Thomas Aquinas,
considered similar questions such as whether God has knowledge of
non-existing  things \cite[part one, question 14, article 9]{Aquinas} or things
that are not yet \cite[part one, question 14, article 13]{Aquinas};
see also Specker's \citeyearpar[243]{specker-60}  reference to {\it infuturabilities}.
Classical omniscience, at least its naive expression that,
if a proposition is true, then an omniscient agent (such as God) knows that it is true,
is plagued by controversies and paradoxes.
Even without evoking quantum mechanics, there exist bounds on omniscience because of the self-referential
perception of intrinsic observers endowed with free will:
if such an observer is omniscient and has absolute predictive power,
then free will could counteract omniscience and, in particular, the observer's own predictions.
Within a consistent formal framework, the only alternative is to either abandon free will,
stating that it is an idealistic illusion,
or  accept that omniscience and absolute predictive power is bound by paradoxical self-reference.

The empirical sciences implement classical omniscience by assuming that
in principle, all observables of classical physics are comeasurable without any restrictions,
regardless of whether they are actually measured.
No ontological distinction is made between an observable obtained by an actual and a potential or counterfactual measurement.
[In contrast, compare Schr\"odinger's \citeyearpar[sect.~7]{schrodinger} own epistemological interpretation of the wave function as a
{\em catalog of expectations.}]
Classically, precision and comeasurability are limited only by the technical capacities of the experimenter.
The principle of empirical classical omniscience has given rise to the realistic believe that
all observables exist regardless of their observation, that is, regardless and independent of
any particular measurement.

Physical (co-)existence is thereby related to the realistic assumption
[sometimes referred to as the ``ontic'' \citep{atman:05} viewpoint] that \citep{stace}
``some entities sometimes exist without being experienced by any finite mind.''
With regards to such unexperienced counterfactual entities,
\citet[364, 365, 368]{stace}  questions their existence (compare also Schr\"odinger's remark quoted earlier):
\begin{quote}
In front of me is a piece of paper. I assume that the realist believes
that this paper will continue to exist when it is put away in my desk for the night,
and when no finite mind is experiencing it.
$\ldots$
I will state clearly at the outset that I cannot prove that no entities exist without being experienced
by minds. For all I know completely unexperienced entities may exist,
but what I shall assert is that
$\ldots$
%we have not the slightest reason for believing that they do exist.
%And from this it will follow that the realistic position that they
%do exist is perfectly groundless and gratuitous, and one which ought not to be belived.
%It will be in exactly the same position as the proposition ``there is a unicorn on the planet Mars.''
%I cannot prove that there is no unicorn on Mars.
%But since there is not the slightest reason to suppose that there is one,
%it is a proposition which ought not to be believed.
there is absolutely no reason for asserting that these non-mental, or physical,
entities ever exist except when they are being experienced,
and the proposition that they do so exist is utterly groundless and gratuitous,
and one which ought not to be believed.
$\ldots$
As regards [a] unicorn on Mars, the correct position,
as far as logic is concerned,
is obviously that if anyone asserts that there is a unicorn there,
the onus is on him to prove it;
and that until they do prove it,
we ought not to believe that they exist.
\end{quote}

One might criticize Stace's idealistic position by responding that
suppose an experimenter can choose which observable among a collection of  different, complementary,
observables is actually measured.
Regardless of this choice, a measurement of any  observable that {\em could} be measured
{\em would} produce some result.
This contrary-to-fact conditional
could be interpreted as an existing {\em element of physical reality.}
Furthermore, according to the argument of Einstein, Podolsky and Rosen \citeyearpar[777]{epr},
even certain sets of {\em complementary} counterfactual elements of physical reality
coexist
``if, without in any way disturbing a system, we can predict with certainty (i.e.,
with probability equal to unity) the value of [these] physical quantit[ies].''
The idealist  might repond that these arguments are unconvincing
because they are merely based on conterfactual inference and are thus empirically ``utterly groundless and gratuitous.''

The formal expression of classical omniscience is the Boolean algebra of observable propositions \citep{Boole},
in particular the abundance of two-valued states interpretable
as omniscience about the system.
Thereby, any such dispersionless quasi-classical two-valued state  --  associated with a truth assignment
 --  can be defined for all observables,
regardless of whether they have been actually observed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After the discovery of complementarity, a further indication against quantum omniscience came from Boole's \citeyearpar{Boole-62}
{\em conditions of possible (classical) experience} which
are bounds for the occurrence of (classical) events that are derivable within classical probability theory
\citep{pitowsky-89a,pitowsky,Pit-94,2000-poly} for quantum probabilities and
quantum expectation functions.
\citet{bell-66} pointed out that experiments
based on counterfactually inferred observables
discussed by Einstein, Podolsky and Rosen \citeyearpar{epr} discussed earlier violate these  conditions of possible (classical) experience and thus
seem to indicate the impossibility
of a faithful embedding (i.e., preserving the logical structure)
of quantum observables into classical Boolean algebras.
Stated pointedly, under some (presumably mild) side assumptions,
{\em unperformed experiments have no results} \citep{peres222};
that is,
there cannot exist a table enumerating all
actual and hypothetical context independent (see later) experimental
outcomes consistent with the observed quantum frequencies \citep{zeilinger-epr-98,svozil_2010-pc09}.
As any such table could be interpreted as omniscience with respect to
the observables in the Boole-Bell-Einstein-Podolsky-Rosen-type experiments,
the impossibility to consistently enumerate such tables (under the noncontextual assumption)
appears to be a very serious indication against
omniscience in the quantum domain.

The quantum nonlocal (i.e., the particles are spatially separated)
correlations among observables in the Boole-Bell-Einstein-Podolsky-Rosen-type experiments
are stronger than classical in the sense that {\it ex post facto,} when the two outcomes are communicated and compared,
in the case of dichotomic observables, say ``0'' and ``1,''
for some measurement parameter regions,
there appear to be
{\em more equal} occurrences ``00'' or ``11'' and thus
{\em fewer unequal} occurrences ``01'' or ``10'' than could be classically accounted for;
likewise,
for other  measurement parameter regions,
there appear to be
{\em fewer equal} occurrences ``00'' or ``11'' and thus
{\em more unequal} occurrences ``01'' or ``10'' than could be classically accounted for.
These conclusions can only be drawn {\it in retrospect}, that is, after bringing together and comparing the outcomes.
Individual outcomes occur indeterministically and, in particular, independently of the  measurement parameter regions
[but not of outcomes \citep{shimony2}]
of other distant, measurements.
No faster-than-light signaling can occur.
Indeed, even stronger-than-quantum correlations would, in this scenario,
not violate relativistic causality \citep{pop-rohr,popescu-97b,svozil-krenn,svozil-2004-brainteaser}.


The reason that it is impossible to describe all quantum observables
simultaneously by classical tables of experimental outcomes
can be understood in terms of a stronger conclusion that,
for quantum systems whose Hilbert space is of dimension greater than two,
there does not exist any dispersionless quasi-classical, two-valued state
interpretable as truth assignment.
This conclusion, which is known as the
Kochen-Specker theorem
\citep{specker-60,kochen1,ZirlSchl-65,Alda,Alda2,kamber64,kamber65,mermin-93,svozil-ql,svozil-tkadlec,cabello-96,svozil-2008-ql},
has a finitistic proof by contradiction.
Proofs of the Kochen-Specker theorem  amount to  brain teasers in graph coloring
resulting in the fact that, for the geometric configurations considered,
there does not exist any possibility to consistently and context independently
enumerate and tabulate the values of all the observables occurring in a  Kochen-Specker-type argument \citep{cabello-96}.

The violations of conditions of possible classical experience in Boole-Bell-type experiments or
the Kochen-Specker theorem do not exclude realism restricted to a single context
but (noncontextual) realistic omniscience beyond it.
It may thus not be totally unreasonable to suspect that the assumption of \mbox{(pre-)}determined observables outside
a single context may be unjustified \citep{svozil-2003-garda}.

If one nevertheless {\em insists} in the simultaneous physical coexistence
of counterfactual observables,
any {\em forced} tabulation \citep{peres222,svozil_2010-pc09} of truth values for Boole-Bell-type or Kochen-Specker-type configurations
would  either result in a complete contradiction or
in {\em context dependence}, also termed {\em contextuality}, that is,
the outcome of a measurement of an observable would depend on what other comeasurable observables are measured
alongside  it \citep{bohr-1949,bell-66,hey-red,redhead,svozil-2008-ql}.

Indeed, the current mainstream interpretation
of the Boole-Bell-type or Kochen-Specker-type theorems is in terms of contextuality, that is, by assuming a dependence of the outcome
of a single observable {\em on what other observables} are actually measured
or at least what could have been consistently known alongside it.
This insistence in the coexistence of complementary observables could be interpreted as an attempt to rescue
classical omniscience accompanied by ontological realism at the price of accepting contextuality.
The realist \citet[451]{bell-66}
suggested that
``the result of an observation may reasonably depend $\ldots$
on the complete disposition of the apparatus.''
(Already \citet{bohr-1949} mentioned
``the impossibility of any sharp separation between the behaviour of atomic
objects and the interaction with the measuring instruments which serve to define the conditions
under which the phenomena appear.'')

For the sake of demonstrating contextuality \citep{svozil_2010-pc09} consider a dichotomic observable
(with outcomes ``0'' or ``1'').
Contextuality predicts that, when measured together with some particular set of  observables,
this observable yields a certain outcome, say ``0,''
whereas when measured together with another, complementary,
set of other observables, the observable may yield a different outcome, say ``1.''

However, statistically the quantum probability and expectation value of this observable is
noncontextual and thus  {\em independent} of the set of co-observables.
Thus contextuality is a hypothetical (counterfactual)
phenomenon regarding complementary measurements on an individual particle, making it inaccessible for direct tests.
Alas, as far as Einstein-Podolsky-Rosen-type measurements might reproduce such contextual behavior
for individual particles,
quantum mechanics predicts noncontextuality \citep{svozil:040102} and thus contradicts the assumption of quantum contextuality.
(Often claims of experimental evidence of quantum contextuality
do not deal with its individual particle character but deal with statistical violations of Boole-Bell-type or Kochen-Specker-type configurations.
The terms which contribute to (in)equalities are not measured on one and the same particle;
operationally they even originate in very different measurement setups.)
One may argue that contextuality occurs only when absolutely necessary, that is,
when the set of observables allows only an insufficient number of two-valued states for a
homeomorphic embedding into (classical) Boolean algebras; but
in view of the fact that quantum noncontextuality for single events occurs for configurations
which can be pasted together to construct a Kochen-Specker-type scheme,
any such argument might appear {\it ad hoc.}.

On the basis of the aforementioned lack of quantum omniscience, it is possible to postulate
the existence of absolute sources of indeterminism;
if there are no (preexisting) observables,
and no causal laws yielding individual outcomes,
the occurrence of any such outcome can only be unpredictable and incomputable \citep{2008-cal-svo}.
This quantum dice approach has first been proposed \citep{svozil-qct,rarity-94,zeil-99}
and realized \citep{zeilinger:qct,stefanov-2000,0256-307X-21-10-027,wang:056107}
in setups which utilize {\em complementarity}, yet still allow omniscience.
More recently, it was suggested \citep{svozil-2009-howto,10.1038/nature09008}
to utilize quantum systems with more than two exclusive outcomes  that are  are subject to {\em value indefiniteness}
(two-dimensional systems cannot be proven to be value indefinite).
The additional advantage over devices utilizing merely complementarity is that these new type of quantum oracles \citep{fiorentino:032334,1367-2630-12-1-013019,10.1038/nature09008}
are ``quantum mechanically certified''
by Boole-Bell-type, Kochen-Specker-type, and Greenberger-Horne-Zeilinger-type \citep{ghsz} theorems not to allow omniscience.
Of course, all these devices operate under the assumption that there are no hidden variables that could complete the quantum mechanical description of nature,
especially no contextual ones,
as well as no quasi-indeterminism caused by environmental influences
[such as in the context translation principle \citep{svozil-2003-garda}].
Thus, ultimately, these sources of quantum randomness are grounded in our belief
that quantum mechanics is the most complete representation of physical phenomenology.


\section{Miracles due to gaps in causal description}

A different issue, discussed by \citet{frank},
is the possible occurrence of miracles in the presence of {\em gaps} of physical determinism.
Already Maxwell has considered {\em singular points} \cite[212--213]{Campbell-1882}, {``where prediction,
except from absolutely perfect data, and guided by the omniscience of contingency, becomes impossible.''}
One might perceive individual events occurring
outside the validity of classical and quantum physics without any apparent cause as miracles.
For if there is no cause to an event,
why should such an event occur altogether rather than not occur?

Although such thoughts remain highly speculative, miracles
could be the basis for an operator-directed evolution in otherwise deterministic physical systems.
Similar models have  been applied to dualistic models of the mind \citep{popper-eccles,Eccles22051986,eccles:papal}.
The objection that this scenario is unnecessarily complicating an otherwise monistic model
should be carefully reevaluated in view of computer-generated {\em virtual realities} \citep{descartes-meditation,putnam:81,svozil-nat-acad}.
In such algorithmic universes, there are computable evolution laws as well as inputs from interfaces.
From the intrinsic perspective \citep{svozil-94}, the inputs cannot be causally accounted for,
and hence they remain irreducibly transcendental with respect to the otherwise algorithmic universe.





\section{Concluding thoughts}

\subsection{Metaphysical status of (in)determinism}

Hilbert's \citeyearpar{hilbert-1900e} sixth problem is about the axiomatization of physics.
Regardless of whether this goal is achievable,
omniscience cannot be gained
via the formalized, syntactic route,
which will remain blocked forever by the paradoxical self-reference
to which intrinsic observers and operational methods are bound.
Even if the universe were a computer \citep{zuse-70,fredkin,wolfram-2002,svozil-2005-cu},
we would intrinsically experience unpredictability and complementarity.


With regard to conjectures about the (in)deterministic evolution of physical events,
the situation is unsettled and can be expected to remain unsettled forever.
The reason for this is the provable impossibility to formally prove (in)determinism:
it is not possible to ensure that physical behaviors are causal and will remain so forever,
nor is it possible to exclude all causal behaviors.



The postulate of indeterministic behavior in physics or elsewhere is impossible to {\em prove} by
considering a finite operationally obtained encoded phenotype
such as a finite sequence of (supposedly random) bits from physical experiments
alone.
Furthermore, recursion theory and algorithmic information theory \citep{chaitin3,calude:02,gruenwald-vitanyi} imply that
an unbounded system of axioms is required to prove the unbounded
algorithmic information content of an unbounded symbolic sequence.
There also exist irreducible complexities in pure mathematics \citep{chaitin-04,s00032-006-0064-2}.

The opportunistic approach that (as historically,  many ingenious scientists have failed to come up with a causal description)
indeterminism will prevail appears to be anecdotal, at best, and  misleading, at worst.
Likewise, the advice of authoritative researchers to
avoid asking questions related
to completing a theory,
or to avoid thinking about the meaning of quantum mechanics or any  kind of rational interpretation,
and to avoid searching for causal laws for phenomena which are, at the same time,
postulated to occur indeterministically by the same authorities --  even wisely and benevolently posted  --
hardly qualify as proof.

Any kind of lawlessness can thus be claimed only
{\em with reference to,} and {\em relative to,} certain criteria, laws, or quantitative statistical or algorithmic tests.
For instance, randomness could be established merely {\em with respect to} certain tests,
such as some batteries of tests of randomness, for instance, {\em diehard} \citep{diehard}, {\it NIST} \citep{Rukhin-nist},  {\it TestU01} \citep{1268777},
or algorithmic \citep{calude-dinneen05,PhysRevA.82.022102} tests.
Note, however, that even the decimal expansion of $\pi$, the ratio between the circumference and the diameter of an ideal circle \citep{bailey97,bailey05},
behaves reasonably random \citep{PhysRevA.82.022102};
$\pi$ might even be a good source of randomness for many Monte Carlo calculations.


Thus, both from a  formal as well as from an operational point of view,
any rational investigation into, or claim of, absolute (in)determinism is metaphysical and can only be proved
{\em relative to} a limited number of statistical or algorithmic tests
which some specialists happen to choose;
with very limited validity for the formal and the natural sciences.

\subsection{Harnessing unknowables and indeterminism}

Physical indeterminism need not necessarily be perceived negatively as the absence of causal laws
but rather as a {\em valuable resource.}
Indeed, ingenious quasi-programs to compute the {\em halting probability} \citep{chaitin3,calude-dinneen06,rtx100200236p}
through summation of series without any computable rate of convergence could,
at least in principle, and in the limit of unbounded computational resources,
be interpreted as generating  provable random sequences.
However, as has already been expressed by  \citet[768]{von-neumann1},
{``anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.''}


Besides recursion-theoretic undecidability,
there appear to be at least two principal sources of indeterminism and randomness in physics:
(1) one scenario is associated with instabilities of classical physical systems
and with a strong dependence of future behaviors on the initial value, and
(2) quantum indeterminism, which can be subdivided into three subcategories, including  random outcomes of individual events,
 complementarity, and
value indefiniteness.

The production of random numbers by  physical generators has a long history \citep{rand-55}.
The similarities and differences between classical and quantum randomness can be conceptualized
in terms of two  black boxes: the first  of them,  called the {\em ``Poincar{\'e} box,''}
containing a classical, deterministic, chaotic source of randomness and
the second,  called the {\em ``Born box,''}
containing a quantum source of randomness.

A Poincar{\'e} box could be realized by operating a classical dynamical system in the shift map region.
Major principles for  Born boxes utilizing beam
splitters or parametric down conversion
include the following:
(1) there should be at least three mutually exclusive outcomes to ensure value indefiniteness
\citep{PhysRevLett.85.3313,2008-cal-svo,svozil-2009-howto,1367-2630-12-1-013019,10.1038/nature09008};
(2) the states prepared and measured should be pure and in mutually  [possibly interlinked \citep{svozil:040102}]
unbiased bases or contexts; and
(3) events should be independent  to be able to apply proper normalization procedures \citep{von-neumann1,Samuelson-1968}.


Suppose an agent is being presented with both boxes without any label on, or hint about, them;
that is, the origin of indeterminism
is unknown to the agent.
In a modified Turing test, an agent's task would be to find out which is the Born and which is
the Poincar{\'e} box solely by observing their output.
In the absence of any criteria, there should not exist any operational  method or procedure
capable of discriminating among these boxes.
Moreover, both types of indeterminism appear to be based on speculative assumptions:
in the classical case, it is the existence of continua and the possibility to randomly choose
elements thereof, representing the initial values;
in the quantum case, it is the irreducible indeterminism of single events.



\subsection{Personal remarks}

It is perpetually amazing, perplexing and  mind-boggling
how many laws and mathematical form\ae~can be found to express and program or induce physical behavior
with high precision.
There definitely is substance to the Pythagorean belief that, at least in a restricted manner,
nature is numbers and God computes; maybe also throwing dice sometimes.

The apparent impossibility to explain certain phenomena by any causal law should be perceived carefully and cautiously in a historic, transient perspective.
The author has the impression that in their attempts to canonize beliefs in the irreducible randomness of (quantum) mechanics,
many physicists, philosophers, and communicators may have prematurely thrown out a thorough rationalistic worldview with
the provably unfounded claims of total omniscience and omnipotence.

Let me sketch some very speculative attempts to undo the {\it Goridan Knot}
that haunts the perception of randomness in the classical and quantum domains in recent times.
(1) G\"odel-Turing-Tarski-type undecidability will remain with us forever, at least as long
one allows substitution, self-reference,  and universal computation.
(2) Most classical as well quantum unknowables might  be epistemic and not ontic.
(3) The classical continua might be convenient abstractions
that will have to be abandoned in favor of
granular, course-graining structures eventually.
As a consequence, classical randomness originating from deterministic chaos
might turn out to be formally computable but for all practical purposes impossible to predict.
(4) Space and time might turn out to be intrinsic constructions to represent dichotomic events
 in a world dominated by  one-to-one state evolution.
(5) There might only exist pure quantum states that can be associated with a unique (measurement and preparation) context.
Mixed quantum states might turn out to be purely epistemic, that is,
based on our ignorance of the pure state we are dealing with.
(6) Kochen-Specker and Boole-Bell-type arguments should be interpreted to indicate value indefiniteness
beyond a single context. The idea that there is physical existence beyond a single context at a time (and, associated with it, contextuality)
might  be misleading.
(7) Quantum randomness originate in the process of  context translation between
different, mismatching preparation and measurement contexts.
It might thus be induced by the environment of the measurement apparatus and our technologic inability to
maintain universal coherence.
(8) Dualistic operator controlled scenarios might present an option that are consistent
or at least in {\em peaceful coexistence}  with a certain type of
determinism (leaving room for miracles or gaps of causality). The information flow from and through the interface might either
be experienced as miracle, or, within the statistical bounds, as incomputable event or input.
Whether these specutations and feelings are justified only generations to come will know.

\section*{Acknowledgements}

The author gratefully acknowledges discussions with
Matthias Baaz,
Norbert Brunner,
Cristian S. Calude,
Elena Calude,
John Casti,
Gregory Chaitin,
Robert K. Clifton,
Michael~J. Dinneen,
Paul Adrien Maurice Dirac
Anatolij Dvure{\v{c}}enskij,
Klaus Ehrenberger,
Kurt Rudolf Fischer,
Daniel Greenberger,
Hans Havlicek,
Gudrun Kalmbach,
G\"unther Krenn,
Eckehart K{\"o}hler,
Alexander Leitsch,
David Mermin,
Mirko Navara,
Pavel Pt{\'{a}}k,
Sylvia Pulmannov{\'{a}},
Werner DePauli Schimanovich,
Ernst Specker,
Friedrich Stadler
Johann Summhammer,
Josef Tkadlec,
John Archibald Wheeler,
Ron Wright,
and
Anton Zeilinger.
None of these persons should be blamed for my ignorance.




% END BODY OF CHAPTER
% -------------------------------------------------------------- [REFERENCES]
% BEGIN REFERENCES SECTION


\bibliography{svozil}




\end{document}

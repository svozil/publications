\chapter{Formal (in)computability and randomness}
\label{2016-pu-book-chapter-ranform}

Hilbert's dream in particular, and the formalistic axiomatic program in general, was to ground mathematics
by a finite formal system --
a set of axioms and deterministic rules of derivation, the latter (by the Curry-Howard correspondence)
\index{Curry-Howard correspondence}
operating like an algorithm on the former like an input, which
would proof all true theorems of mathematics.
G\"odel and Turing, among others, put an end to this formalistic dream~\cite{Heijenoort-fftg,davis}, as vividly
expressed by G\"odel   in a {\em postscript,} dated from  June 3, 1964~\cite[p.~369-370]{godel-ges1}:
 {\em ``due to A. M. Turing's work,
 a precise and unquestionably
 adequate definition of the general concept of formal system can now be
 given, the existence of undecidable arithmetical propositions and the
 non-demonstrability of the consistency of a system in the same system
 can now be proved rigorously for {\em every} consistent formal system
 containing a certain amount of finitary number theory.

 Turing's work gives an analysis of the
 concept of ``mechanical
 procedure'' (alias ``algorithm'' or ``computation procedure'' or
 ``finite combinatorial procedure''). This concept is shown to be
 equivalent with that of a ``Turing machine.'' A formal system can
 simply be defined to be any mechanical procedure for producing
 formulas, called provable formulas.''
}


We shall present a very brief survey of the consequences of these findings,
and first hint on the fact that almost all elements of the continuum, and, in particular, almost all reals, are incomputable.
That is, they are inaccessibly to any computation.

Then we head on to modern, algorithmic, definitions of randomness, and , in particular, of random reals.
That is, random reals are algorithmically incompressible, and cannot
be produced by any program whose code has much smaller length (as the original phenotype or number).

Thereby we shall mention quantitative incompleteness theorems and introduce the busy beaver function. In a certain sense,
those constructions give a glimpse on how fast a computation may diverge, and how difficult it is to compute or represent certain objects.
We shall also speculate how primordial chaos may give rise to unbounded complexities.


Finally we consider Chaitin's halting probability Omega,
which serves as a sort of Rosetta stone for comprehending at least mild forms of random reals in perplexing ways.

\section{Abundance of incomputable reals}

For the sake of an orientation for the reader a very brief expose of formal definitions related to indeterminism and randomness is offered.
Mostly the concepts will be presented without proofs.

Let us start with the set ${\Bbb R}$ of real numbers [see for instance, Refs.~\cite{drobot,Stillwell-2013,ehrlich-1994}, or Ref.~\cite{Hlawka-zz} (in German)].
Reals will be coded by, or written as, infinite decimals.
\index{real number}
\index{set of reals}


By Cantor's theorem using diagonalization~\cite[Section~3.5.1, p.~70-71]{Stillwell-2013}
the set of real numbers ${\Bbb R}$  (or, say, the real unit interval $[0,1]$) is nondenumerable.
More generally
--
because one needs not proceed along the diagonal and process the entries therein to produce a
real which does not occur in any type of enumeration of reals
--
the same result can be obtained by applying
self-reference and the existence of some map without any fixed points~\cite[p.~368]{Yanofsky-BSL:9051621},
As a result ${\Bbb R}$  cannot be brought into a one-to-one correspondence with the natural numbers ${\Bbb N}$  (such as, for instance,
the integers ${\Bbb Z}$  or the rationals ${\Bbb Q}$).
Sets of this ${\Bbb R}$ type will be called {\em continua}.
\index{continua}
Sets of type ${\Bbb N}$ will be called {\em denumerable}.
\index{denumerable}



But we can go quantitatively further than that: we can show that, from the point of view of
measure theory, denumerable sets are ``meagre;'' that is, almost all reals are not in any such set of denumerable numbers~\cite[Section~3.5.2, p.~71-72]{Stillwell-2013}.
For a sketch of a proof suppose  that we are ``covering'' the $i$'th element of the denumerable set with an interval $\delta^{-i}\varepsilon$, with $1 <\ delta <\infty$
and $\varepsilon$ a tiny number.
Summing over all such cover intervals can be readily performed, as the respective set is denumerable. By the geometric series summation formula, the entire
length covered is at most (for nonoverlapping intervals) $\varepsilon /(1-\delta^{-1})= r\varepsilon$ . We can make this covering length arbitrary small by making
either $\delta$ larger or $\epsilon$ smaller.
That is, in this measure theoretic, quantitative, sense, ``almost all'' reals are not in any particular denumerable set.

Next we define a computable real by the property that it is produced -- that is, it is the output of -- some algorithm ``running'' on a (supposedly universal) computer.
The set of computable reals is denumerable because we can find ways to enumerate all of them:
for a sketch of the idea how to perform this task, imagine
the numbers produced by successively generated
(by their code lengths in lexicographic order) algorithms at successive times.

As a consequence we find that ``almost all'' reals are incomputable.
That is, if one considers the real unit interval as a ``continuum urn''
--
one needs the axiom of choice in order to ``draw'' a general element of this urn,
as no computable ``handle'' exists to fetch it
--
then with probability $1$ it will be incomputable.

It may appear amazing that all denumerable sets are so ``meagre,'' and its members so ``thinly distributed and embedded''  in the real continuum.
In particular, such sets -- such as the rational numbers and also the computable ones which include ``many more'' irrational numbers --
are dense
\index{dense set}
in the sense that in-between two arbitrary numbers $a$ and $b$ with $a < b$ of a dense set there always lies another number $c$ in that set, such that $c$ is
larger than $a$ but smaller than $b$; that is, $a < c < b$.


\section{Random reals}
\label{2016-pu-book-chapter-ranform--s-rr}

A real can be defined to be random if its (decimal) expansion cannot be algorithmically
compressed~\cite{kolmogorov1,chaitin-66c,Kolmogorov-1968,chaitin-ait-82-handbook,lambalgen-89,lambalgen4,calude:02,li-vitanyi-2008,DH}.
In particular, {\em ``it may perhaps not appear entirely arbitrary
to define a patternless or random finite binary sequence as a sequence which, in
order to be calculated, requires, roughly speaking, at least as long a program as any
other binary sequence of the same length.''}~\cite{chaitin-66c}.

Randomness {\it via} algorithmic incompressibility implies that the respective random sequences  or random reals
{\em ``possess all conceivable computable statistical properties of randomness''}~\cite{MartinLoef1966602};
that is, they pass all conceivable computable  statistical tests of randomness.
What is such a ``conceivable computable  statistical test?''
It is based on all conceivable computable laws -- that is,
all algorithms. More precisely,
a single conceivable computable  statistical test is based on a single algorithm: it is the hypothesis that the random sequences  or random real
{\em cannot} be generated by this algorithm. Because if it were, it would be algorithmically compressible by that algorithm.
Herein lies the connection between statistical test and algorithm: that any algorithm constitutes an algorithmic test against nonrandomness
(algorithmic compressibility); and {\it vice versa}, every computable statistical test is representable by an algorithm.


\section{Algorithmic information}

\subsection{Definition}

Let us be more precise and, for the sake of avoiding difficulties related to subadditivity~\cite{chaitin-ait-82-handbook},
restrict ourselves to {\em prefix} or {\em instantaneous} program codes~\cite{levin,ch:75}
\index{prefix codes}
\index{instantaneous codes}
which have the ``prefix (free) property.''
This property requires that there is no whole code word in the system that is a prefix (initial segment) of any other code word in that same system.

Define the
{\em algorithmic information (content)},
\index{algorithmic information}
or, used synonymously, the (Kolmogorov)  program-size complexity,
\index{program-size complexity}
or the {\em information-theoretic complexity}
\index{information-theoretic complexity}
of an {\em individual}
object is a measure or criterion how difficult it is to {\em algorithmically specify} (but not in terms of time it takes to  produce) that object~\cite{chaitin-ait-82-handbook}.
In particular,
the algorithmic information content $I(x)$ of
binary string $x$
as the size/length (encoded in bits, that is, binary digits) of the
{\em shortest/smallest program} running on some (Turing-type) universal computer $U$ to calculate $x$, plus the information content
$ I( \vert s \vert )$ of the length $\vert s \vert $ of this sequence (since this also contributes); that is, if $\vert s \vert$ stands
for the length of the binary sequence $s_n$ in bits, and the order
$O(f)$ of $f$ stands for a function whose absolute value is bounded by a constant times $f$ [and thus $O(1)$ just stands for a constant], then
\begin{equation}
I(x) =\vert s \vert + I( \vert s \vert ) + O(1)  =  \vert s \vert + O( \log_2 \vert s\vert ).
\end{equation}


\subsection{Algorithmic information of a single random sequence}

Instead of delving into joint and mutual information content
we head straight to a formal definition of randomness.
A finite {\em random} binary sequence
\index{random sequence}
$s_n$ of length $n$ is defined to be (nearly) algorithmically incompressible; that is, its algorithmic information content
$I(s_n)$ is not (much) less than $n$.
An infinite binary sequence $s$ is random if its initial segments $s_n$ are random finite binary sequences.
That is, $s$ is random if and only if there exists some constant $c$,
such that, for all $n \in {\Bbb N}$,
 the algorithmic information content of its initial segments $s_n$ is bounded from below by $n-c$; that is,
\begin{equation}
s \textrm{ is random } \Leftrightarrow \exists c \forall n \left[ I(s_n) > n-c \right].
\end{equation}
A random real (in arbitrary base notation) is one whose base~$2$ expansion of its fractional part (forgetting the integer part as long as it is finite)
is a random infinite binary sequence.

\subsection{Bounds from above}

Let us go a little further and mention a bound from above on the algorithmic information content
of a string of length $n$: it must be less than $n+O(1)$.
Because, to paraphrase Chaitin~\cite[p.~11]{Chaitin-1974}, {\em ``the algorithmic information content
of a string of length $n$ must be less than $n+O(1)$,
because any string of length $n$ can be calculated by putting it directly into a program as a table.
This requires $n$ bits, to which must be added $O(1)$ bits of instructions for printing the table.
In other words, if nothing betters occurs to us, the string itself can be used as its definition,
and this requires only a few more bits than its length.''}

\subsection{Abundance of random reals}

Almost all reals of the continuum are not only incomputable, as we have argued previously by a measure theoretic argument,
but they are also random. Rather than rephrasing the argument,  Chaitin's argument~\cite[p.~11]{Chaitin-1974}
can be paraphrased as follows:
{\em ``the algorithmic information content of the great majority of strings of length $n$ is approximately $n$,
and very few strings of length $n$ are of algorithmic information content much less than $n$.
The reason is simply that there are much fewer programs of length appreciably less than $n$ than strings of length $n$.
More exactly, there are $2^n$ binary strings of length $n$, and less than $2^{n-k}$ binary encoded programs of length less than $n-k$.
Thus the number of strings of length $n$ and algorithmic information content less than $n-k$ decreases exponentially as $k$ increases,
and increases exponentially as $n$ increases.''}

As a consequence, most of the sequences of length $n$ are of algorithmic information content close to $n$, and,
according to the definition earlier, appear random.
Therefore, if one chooses some nonalgorithmic method generating such sequences --
if they are operational, that is, they can be produced by some physically process;
say, by tossing a fair coin and hoping for the best that this process is
not deterministic or biased as alleged in Ref.~\cite{diaconis:211})
--
then chances are high that the algorithmic information content of such a string will be as long as the length of that sequence.
Pointedly stated: ``grabbing and picking'' a random real from the continuum with nonalgorithmic means,
facilitated by the axiom of choice,
\index{axiom of choice}
will almost always yield a random real.


\section{Information-theoretic limitations of formal systems}
\label{2016-pu-book-chapter-ranform-itlfs}

By reduction to the halting problem it can be argued that the algorithmic information content $I$  in general is incomputable.
Because computability of  the algorithmic information content $I(s_n)$ would require that it would be possible to compute
whether or not particular programs of length up to $n+O(n)$  halt (after output of $s_n$). But this is clearly impossible for large enough
(and even for small) $n$; see also the busy beaver function discussed later.

It is therefore impossible that in general it is possible to prove (non)randomness or (in)computability of a particular individual infinite sequence.
(Any particular finite sequence is provable computable, as by the earlier mentioned tabulation technique,
an algorithm outputting it can be constructed by putting it directly into a program as a table.
This is no contradiction to the earlier definition of randomness of a finite sequence because this is means relative and not absolute.)
That is, all statements (e.g., Ref.~\cite{zeil-05_nature_ofQuantum})
such as ``this string is irreducibly random,'' at least as far as they relate to ontology, are
provably unprovable hypotheses. Epistemically they are inclinations at best, as expressed by Born's statement~\cite[p.~866]{born-26-1}
(English translation in \cite[p.~54]{wheeler-Zurek:83})
{\em ``I myself am inclined  to give up determinism in the world of atoms.''}
At worst they are ideologies which remain unfalsifiable.

In a quantitative sense one could go beyond the G\"odel-Turing reduction.
Let us follow Chaitin~\cite{Chaitin-1974} and employing Berry's paradox,
as reported by Russell~\cite[p.~153, contradiction~(4), footnote~3]{Heijenoort-fftg}:
{\em  ``But `the least
integer not nameable in fewer than nineteen syllables' is itself a name consisting of
eighteen syllables; hence the least integer not nameable in fewer than nineteen
syllables can be named in eighteen syllables, which is a contradiction.''}

Chaitin's paradoxical construction which is based upon the Berry paradox can be expressed by the following sentence~\cite{Chaitin-1974,chaitin-ait-82-handbook}
which cannot be valid:
{\em ``the program which yields the shortest proof that its algorithmic information content is much greater than its length, say, 1 billion bits.''}

Because such a program, if it existed, purports to prove that its algorithmic information content is much greater than its length, say, 1 billion bits.
And yet, this statement is quite short, and certainly less than a billion bits.
This is contradictory; and as a consequence no such program can exist.
Stated differently:
For every formal system deriving statements of the form $I(s) > n$, there is a number $k$
such that no such statement is provable using the given rules of that formal system  for any $n > k$~\cite[p~265,266]{Davis1978}.
$k$ can be called the ``strength'' of such a formal system.
This strength is a limiting measure for the capacity of the formal system to prove statements about the algorithmic information content of sequences.

One way of interpreting this result is in terms of {\em independence:}
\index{independence}
because more axioms specify more theorems, different axioms specify different theorems.
That is, it is the choice of the formalist which deductive mathematical universe is created by the assumptions~\cite[p.~38]{Hazewinkel-e5}.

\section{Abundance of true yet unprovable statements}
\label{2016-pu-book-chapter-ranform-rp}

In view of the aforementioned incompleteness and independence result
one may ask~\cite[p.~148]{chaitin:92}: {\em ``How common is incompleteness and unprovability? Is
it a very bizarre pathological case, or is it pervasive and quite common?''}

Indeed, just as the set of computable reals is ``meagre'' in the set of reals,
so is the set of provable (by constructive, algorithmic methods)
theorems ``meagre'' with respect to all true theorems of mathematics.
This has been proven in a topological sense of ``meagre'' in the context of G\"odel-Turing type incompleteness~\cite{calude:94b}
(and not in the sense of independence of, say, the continuum hypothesis).

{\em Rice's theorem}~\cite{Rice-1953}
\index{Rice's theorem}
asserts that all non-trivial, (semantic) functional properties of programs are undecidable.
A functional property
is one  (i) describing how some functions performs in terms of its input/output behavior, and
(ii) which is non-trivial in the sense that some (of all perceivable) programs which have this input/output behavior,
and other programs which don't.
Rice's theorem can be algorithmically proven by reduction to the halting problem:
Suppose there is an algorithm $A$ deciding whether or not any given function
or algorithm $B$ has any functional property.
Then we can define another program $C$ which first solves the halting problem for some other arbitrary function $D$,
clears the memory, and consecutively executes a program $E$ with has the respective functional property decided by $A$.
As long as  $D$ halts, all may go well.
But if $D$ does not halt, the program $C$ never clears the memory,
and can never execute a program $E$ with the respective functional property.
Now, if one inserts $C$ into $A$,
in order to be able to decide (positively) about the functional property of $E$ -- and thus of $C$ --
$A$ would have to be able to solve the halting problem for $D$ first;
a task which is provable impossible by algorithmic means.

%https://www.youtube.com/watch?v=xbr9K-u0wmE
%https://www.youtube.com/watch?v=F5tQ7DlkNf8


As Yanofsky observes this bears some similarity to the
{\em downward L\"owenheim-Skolem},
\index{L\"owenheim-Skolem}
{\em ``stating that if there is a
consistent way of using a language [[statements in mathematics $\ldots$ written with a
finite set of symbols]] to talk about such a system [[with an uncountably infinite number
of elements]], then that language
might very well be talking about a system with only a countably infinite number
of elements. That is, the axioms might be intended for discussing something
uncountably infinite, but we really cannot show that it is has more than a countably
infinite number of elements''}~\cite[footnote~20, p.~375]{Yanofsky-ol}.




\section{Halting probability $\Omega$}
\label{2016-pu-book-chapter-ranform-Omega}
\index{Omega}

The program lengths $\vert p_i \vert$ of $q$ algorithms $p_i$, $i=1,\ldots , q$ encoded by binary prefix free codes on a universal computer satisfies the
{\em Kraft inequality}
\index{Kraft inequality}
\begin{equation}
0 \le \sum_{i=1}^q 2^{ - \vert p_i \vert } \le 1.
\end{equation}

These bounds are motivation to define Chaitin's {\em halting probability}~\cite{ch:75,s00032-006-0064-2,chaitin-04}
$\Omega$
by the sum of the weighted length of all binary prefix free encoded programs $p$ on a given universal computer  which halt:
\begin{equation}
\Omega = \sum_{ p\textrm{ halt} } 2^{ - \vert p \vert }.
\end{equation}

$\Omega$ is
Borel normal in any base~\cite{chaitin:89},
and
``highly incomputable'' as it requires the solution to all halting problems. Conversely, knowledge of $\Omega$,
at least up to some degree, entails the solution of decision problems associated with halting problems:
for instance, the Goldbach conjecture
\index{Goldbach conjecture}
(``every even number greater than $2$ can be represented as the sum of two primes'')
can be rephrased as a halting problem by parsing through all cases and halting if one of them fails.

Despite this obvious computational hardness, the initial bits of $\Omega$
can be computed, or at least estimated up to some small degree~\cite{chaitin3,chaitin:01,2002-glimpseofran}.
This is related to the fact that very small-size programs still ``converge fast'' --
that is, they soon halt -- if ever; and, because of the exponentially decreasing weight with length,
those programs contribute more to $\Omega$ as longer ones.
But because of the recursive unsolvability of the halting problem there exists no computable rate of convergence.
In particular, as we shall see next, halting times grow faster than every computable function of program length.

\section{Busy beaver function and maximal execution and recurrence time}
\label{2016-pu-book-chapter-ranform-s-bb}

Suppose one considers all  programs (on a particular computer)
up to length  $n$.
The busy beaver function  $\Sigma (n)$  of $n$
is the {\em largest number} producible by such a programs of length $n$ before halting~\cite{rado,dewdney,brady}.
(Note that non-halting programs, possibly producing infinite numbers, for example by a non-terminating loop, do not apply.)

Alternatively,  in terms of algorithmic information content,
the busy beaver function $\Sigma (n)$
\index{busy beaver function}
can be defined as the largest number (of bits) whose algorithmic information content is less than or equal to $n$~\cite{chaitin-ACM,chaitin-bb};
that is, $\Sigma (n) = \max_{I(k)\le n} k$~\cite[Definition~5.3, p.~414]{chaitin-ACM}.
%From this definition follows
%$
%I(\Sigma (n)) \le  n \le \Sigma (I(n))
%$~\cite[Theorem~5.1(a,d), p.~414]{chaitin-ACM}.

$\Sigma (n)$ {\em grows faster} than any computable function of $n$ and therefore is incomputable.
Let us follow Chaitin~\cite{chaitin-bb}
and suppose that $n$ is greater than $I(f)+O(1)$, the algorithmic information content of $f$ (in terms of its binary code) plus a positive constant.

For the computation of $f(n)+1$ it suffices to know a minimal-size program to compute $f$, as well as the value of $n$,
or, even more economically, the value of $n - I(f)$.
Thus,
$
I(f(n)+1)
\le
I(f) +  I( \log_2 \vert n- I(f)\vert )
\le
I(f) +  I( \log_2 \vert I(f) + O(1)- I(f)\vert )
=
I(f) +  I( \log_2   O(1) )
<
I(f) +  I(  O(1) )
<
I(f) +   O(1)
< n
$.
Therefore, by the definition of  $\Sigma (n)$,  $f(n)+1$ is included  in $\Sigma (n)$; that is, $\Sigma (n) \ge f(n)+1$  if  $n > I(f)+O(1)$.


A related question is about the maximal execution or run-time of a halting algorithm of length smaller than or equal to $n$:
what is minimum time $S(n)$  --  or,
alternatively, recurrence time  --  such that all programs of length at most $n$ bits which halt
have done so; that is, have
terminated  or, alternatively, are recurring?

An answer to this question will explain just how long it may take for the most time-consuming program of length $n$ bits to
halt. That, of course, is a worst-case scenario. Many programs of
length $n$ bits will have halted long  before this maximal halting time~\cite{CALUDE2008295}.
$S(n)$ can be estimated in terms of $\Sigma (n)$
by two bounds; one from below and one from above.
The bound from below is rather straightforward:
Since the printout of any symbol requires at least a unit time step,
$\Sigma (n)$ can be interpreted as a sort of counter variable.
Thus a first estimate is $S(n )\ge \Sigma (n)$;
that is, $S(n)$ grows faster than any computable function of $n$.

A bound from above can be conceptualized in terms of an ``inner dialogue'' of an algorithm  which can be
{\em published} -- that is, printed -- with little
algorithmic overhead.
Stated differently, every ``algorithmic contemplation'' or ``symbolic computation'' could,
with a little overhead [symbolized by ``of the order of'' $O(\cdot )$],
be transformed into a ``printout'' of this ``monologue'' directed towards the outside world,
whose size in turn cannot exceed  $\Sigma (n+O(1))$~\cite{chaitin-ACM}.
This yields a bound from above $S(n )\le O\left(\Sigma (n+O(1))\right)$~\cite{calude:pr}.
Thus the busy beaver function can serve as some sort of measure of what some algorithm  can(not) do before it halts.
In this sense ``expressing something to the world'' can be equated to ``contemplating internally.''


A {\em simulation} of the original computation yields bounds
$\Sigma (3n+O(1))$~\cite{Ben-Amram1996,Ben-Amram2002} for
any program of size $n$ bits to either halt,  or else never to halt.



Knowledge of the maximal halting time -- in particular, some computable upper bound on $\Sigma$ -- would solve the halting
problem quantitatively
because if the maximal halting time were known
and bounded by any computable function of the program size of $n$ bits,
one would have to wait
just a little longer than the maximal halting time to make sure
that every program of length $n$  --  also this particular program, if it is destined for termination  --
has terminated.
Otherwise, the program would run forever.
Hence, because of the recursive unsolvability of the halting problem
the maximal halting time cannot be a computable function.



As a consequence, the upper bounds for the recurrence of any kind of physical behaviour can be expressed
in terms of the busy beaver function~\cite{svozil-93}.
In particular,
for deterministic systems representable by $n$ bits
the maximal recurrence time grows faster than any computable number
of $n$.
This maximal estimate related to possible behaviours may be interpreted quite generally
as a measure
of the impossibility to predict and forecast such behaviours by algorithmic means.

Just as for  $\Omega$,   knowledge of busy beaver function and thus the maximal halting time
at least up to some degree, entails the solution of decision problems associated with halting problems.
But these capacities, at least with computable means, are forever blocked by recursion theoretic incomputability.


\section{Some speculations on primordial chaos and unlimited information content}



Chaitin's independence theorem discussed earlier  imposes quantitative bounds on formal  expressability:
essentially [that is, up to $O(1)$] it is impossible to ``squeeze out''
(in terms of proofs of theorems, and with a {\it caveat}~\cite{lambalgen-89})
of a formal system much more than one has put in.
If one wants more provable theorems one has to assume more. There appears to be no ``pay once eat all scenario'' envisaged by Hilbert.
This, flamboyantly speaking, ``garbage in, garbage out'' situation fits well with {\em means relativity}
\index{means relativity} and intrinsic perception of embedded observers: as there is no external
point of view from which to execute omniscience the only possibility is to perform relative to the (intrinsic) means available.

There is, however, another option not excluded by Chaitin's limiting theorems:
the possibility to obtain a system of arbitrary algorithmic information content
by considering subsequences of infinite sequences interpreted as formal systems, or as axioms thereof.
There are two extreme scenarios which could be imagined: in the first scenario, a ``primordial chaos'' is taken as a resource.
In the second scenario, the continuum of all infinite binary sequences $2^\omega$ is approximated by a nonterminating process of generating it.

In both cases partial sequences could in principle be taken as a basis representation for axiomatic systems.
And in both cases the encoded axiomatic systems are potentially infinite, with an unlimited algorithmic information content.

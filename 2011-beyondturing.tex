\documentclass{comjnl}

\usepackage{amsmath}

%\copyrightyear{2009} \vol{00} \issue{0} \DOI{000}

\begin{document}


\title[On Demons and Oracles]{On Demons and Oracles}
%\author{Kumaara Velan}
\author{Alastair A. Abbott,$^{1}$  Cristian S. Calude,$^{1}$ Karl Svozil$^{2}$}

\affiliation{$^{1}$Department of Computer Science,
University of Auckland\\
Private Bag 92019, Auckland,
New Zealand\\
$^{2}$Institut f\"ur Theoretische Physik,
 Vienna University of Technology,  \\  Wiedner Hauptstra\ss e 8-10/136 A-1040 Vienna, Austria
}
\email{aabb009@aucklanduni.ac.nz, cristian@cs.auckland.ac.nz, svozil@tuwien.ac.at}

\shortauthors{A. A. Abbott,  C. S. Calude, K. Svozil}

\received{2011}
\revised{2011}


%\category{C.2}{Computer Communication Networks}{Computer Networks}
%\category{C.4}{Performance of Systems}{Analytical Models}
%\category{G.3}{Stochastic Processes}{Queueing Systems}
%\terms{Internet Technologies, E-Commerce}
\keywords{Church-Turing Thesis, accelerated Turing machine, quantum oracle}


\begin{abstract}
This paper presents a personal, subjective view on Church-Turing Thesis and some attempts to trespass the Turing barrier.
\end{abstract}

\maketitle


%\section{Introduction}
\section{Turing's barrier}
The concept of digital computation which emerged  from the works of Church~\cite{church36},
Turing~\cite{turing-36} and G\"odel~\cite{godel1} is an important achievement of  the last century.
A large variety  of mathematical models  of computers and computations have been developed.
 Turing machines,
lambda-calculus,
combinatory logic, recursive functions,
Markov algorithms,
register machines
are among the most known classical
models. Newer models range from  programming-oriented models including concurrent models like actor model and process calculi to
quantum Turing machines, DNA computers, molecular computers, wetware computers and many others. A remarkable result
was gradually proved: in spite of  diversity, the computational capability of every model of computation is the same. All models are
computationally equivalent. This strong mathematical evidence motivated a more general belief: the Turing model of computation
is the right and most general concept for digital computation.

Turing~\cite{turing-36} proved that Hilbert's Entscheidungs-\ problem---the decision problem for the predicate calculus\footnote{Find an effective method to determine whether an arbitrary formula    of the predicate calculus system is provable in the system.}---is unsolvable by any Turing machine. Independently, Church~\cite{church30} obtained the same negative result by using his lambda-calculus, so by proposing (\cite{church30}, p.\ 356) to

\begin{quote}
define the notion ... of an effectively calculable function of positive integers by identifying it with the notion of a recursive function of positive integers (or of a lambda-definable function of positive integers).
\end{quote}
he argued that Hilbert's Entscheidungsproblem {\em is unsolvable} (not only unsolvable by any lambda-definable function). Motivated by a similar  identification  proposed by Turing~\cite{turing-36}, and the (mathematical) equivalence between the
sets of functions computed by Turing machines,  lambda-definable functions and recursive functions,  Kleene (\cite{kleene-52}, p.\ 232),  introduced the
Church-Turing Thesis:

\begin{quote}
A function of positive integers is effectively calculable only if recursive.
\end{quote}

  In the quest to give meaning to their negative solutions to Hilbert's Entscheidungsproblem,  Turing and Church were interested in describing what humans could ``in principle'' compute, so originally the Church-Turing Thesis' main scope was purely mathematical. However, the Thesis itself is  not a mathematical statement as one of the terms involved---the notion of  effectively calculable function---is
not mathematically defined. In particular,  the Thesis cannot be (mathematically) proved: it needs empirical verification which can continue for ever or it may lead to a plausible refutation. This fact was recognised almost immediately by
Post (\cite{post-1936}, p.\ 10), who  objected to its presentation as a definition because it ``blinds us to the need of its continual verification''.

In time the scope of the  Church-Turing Thesis shifted towards a more general goal, the ultimate limits of digital computation. In this  new context the  Church-Turing Thesis can be stated as\footnote{Sometimes this is called the Physical Church-Turing Thesis.}:

\begin{quote}
Every function  of positive integers which can be computed in a  physical system is recursive, or equivalently,  it can be computed by a Turing machine.
\end{quote}

 In this form the Church-Turing Thesis is a statement  about what can be computed in a system of physical laws. Two components are involved: i) the mathematical component  determines the dynamics of evolution of  physical states into others and the relation between inputs and outputs, and ii) the physical component determines which dynamics can be performed in  the given system of physical laws.  Once we fix a system of physical laws described mathematically, the corresponding Church-Turing Thesis becomes a well-posed mathematical question which can be mathematically investigated~\cite{yao-2003}: it can be proved, disproved, or proved undecidable.
 G\"odel, who was initially unconvinced by Church's argumentation~\cite{church30}, but changed his mind after reading Turing's paper~\cite{turing-36},  suggested the idea of an axiomatic approach for the notion of
``effective calculability'' meant to capture its generally accepted properties. In this spirit  Gandy~\cite{gandy2}
 proposed a programme where physical laws (like bounded velocity and finite density of information) are used to `prove' the Church-Turing Thesis; this approach was extended to quantum theory by Arrighi and Dowek~\cite{arrighi-dowek}.
  The
 Church-Turing Thesis has morphed into a class of Church-Turing Theses, each depending on the underlying system of physical laws;
 in some cases it is true, in some false (see, for example, Smith~\cite{Smith2006154}).

 The physical determination of the Church-Turing Thesis was rightly  pointed out by  Deutsch~\cite{deutsch}:
 \begin{quote}
The reason why we find it possible to construct, say, electronic
calculators, and indeed why we can perform mental arithmetic, cannot be
found in mathematics or logic. The reason is that the laws of
physics ``happen" to permit the existence of physical models for the
operations of arithmetic such as addition, subtraction and
multiplication.
\end{quote}

But omitting the mathematical component

\begin{quote}
Computers are physical objects, and computations are physical processes. What computers can or cannot
compute is determined by the laws of physics and not by pure mathematics. (Deutsch~\cite{deutsch-1997}, p.\ 98)
\end{quote}
is wrong. The laws of physics can determine what dynamics can be performed in a given  system of physical laws, but not what
such dynamics ``compute'': this  a mathematical issue. According to Timpson~\cite{timpson-2004}

\begin{quote}
We must recognise that their [mathematical determinants] place is {\em prior} to that of physical determinants.
\end{quote}
\noindent Here is an example. Consider an undecidable problem, say the halting problem\footnote{Does there exist a Turing machine capable of deciding whether
an arbitrary Turing machine halts on a given input?}. In every system of physical laws the halting problem will
be undecidable  because of Turing's proof. This proof and its conclusion---the undecidability of the halting problem---tell us nothing about  the system of physical laws, as
no possible dynamics can be a solution to the problem: this is a mathematical fact true in any system of physical laws.
Of course, the halting problem can be solved by other types ``machines'': such a ``solution''---obtained by mathematical or physical means---does not challenge the validity of Turing's result which concerns only the mathematical concept of Turing machine.


\section{The land of hypercomputation}
Hypercomputation  studies models of computations in the hope of breaking the Turing barrier.  By placing precise physical constraints on computations, hypercomputation contributes to the program of
continuous verification of the Church-Turing Thesis suggested by Post.

 The possibility of executing infinitely many ``operations'' in a finite amount of time is the core
 of the quest. This idea is not new:  Zeno's analysis of motion paved the way for the accelerated Turing machines featured in the next section.

 In 1939 Turing~\cite{turing-1939} introduced the seminal notion of {\em oracle} Turing machine, a standard machine having access to an infinite sequence of bits---the oracle---coding answers to as many questions, and made this  machine compute with finite approximations of the infinite oracle. If the oracle
 is computable the resulting computation is equivalent to a standard computation, but in case the oracle is incomputable the machine trespasses the Turing barrier. In the expert hands of recursion-theorists (see \cite{cooper-2003} for an understandable glimpse) oracle Turing machines haven been used to scrutinise the land of incomputable.  The  ``crucial question'', in the words of the mathematician M.~Davis,  is:

 \begin{quote}
Are there real physical  processes that can be harnessed to do the work
of Turing `oracles'? \end{quote}

Davis' unequivocally negative answer (see \cite{Davis-2004,Davis-2006}):

 \begin{quote}
  I think it would be profoundly surprising if the physics of our world can fully be given
without departing from the set of Turing-machine-computable functions.
\end{quote}
will be re-visited
 in Section~5.

 Rewind to 1970:  In a footnote  to~\cite{kreisel-1970} (p.\ 143) the logician G. Kreisel makes an astonishing suggestion:   a collision problem related to the 3--body problem\footnote{The problem of predicting the motion of a group of celestial objects that interact with each other gravitationally. }
 could be regarded as ``an analogue computation of a non-recursive function'', so an instance of hypercomputation. This possibility  gets a new dimension with Xia's~\cite{xia-92}   construction of no-collisions singularities in small Newtonian systems.  Harnessing the incomputability identified  in different physical systems (see \cite{pr1,penrose:90,cooper-2003}) becomes a possible  source of hypercomputation.


   Hypercomputation models have been constructed using neural networks~\cite{siegel95}, quantum mechanics (\cite{nielsen-1997,kieu-02a,2002-cal-pav}),
 relativity theory (\cite{Hogarth92,Etesi-02,Nemeti2006118,andreka-2009}), inductive Turing machines~\cite{burgin-2005} and many other ideas (see \cite{ord-2006,burgin-2005} for overviews).




 \section{A case study: accelerated Turing machines}


Already the ancient greek philosophers worried about the potentials of an actual infinite divisibility of space and time.
For instance, Zeno of Elea pointed out that in such cases there could be no motion,
since the slightest finite move would require an infinity of actions~\cite{zeno,Sainsbury,salmon-01}.
Subsequently, differential calculus suggests to formally overcome these issues by taking the finite differential
quotient of spatial and temporal change.

The revival of these ancient ideas came with computation.
Already Weyl (see also Blake~\cite[p.~651]{Blake26} and Russell~\cite[p.~144]{Russell-36}) notes the potentiality to ``complete'' infinite computations in
finite proper physical time as follows:
\begin{quote}
The impossibility of conceiving the continuum as rigid being cannot
be formulated more concisely than by Zeno's well-known paradox of
the race between Achilles and the tortoise. The remark that the
successive partial sums $1 - {1\over 2^n}\;  (n = 1, 2, 3, \cdots )$ of the series
$$
{1\over 2}+
{1\over 2^2}+
{1\over 2^3}+
\cdots
$$
do not increase beyond all bounds but converge to 1, by which one
nowadays thinks to dispose of the paradox, is certainly relevant and
elucidating. Yet, if the segment of length 1 really consists of infinitely
many subsegments of lengths $1/2$, $1/4$, $1/8$, $\ldots$, as of `chopped-off'
wholes, then it is incompatible with the character of the infinite as the
`incompletable' that Achilles should have been able to traverse them
all. If one admits this possibility, then there is no reason why a
machine should not be capable of completing an infinite sequence of
distinct acts of decision within a finite amount of time; say, by supply-
ing the first result after $1/2$ minute, the second after another $1/4$ minute,
the third $1/8$ minute later than the second, etc. In this way it would
be possible, provided the receptive power of the brain would function
similarly, to achieve a traversal of all natural numbers and thereby a
sure yes-or-no decision regarding any existential question about natural
numbers!
(\cite{weyl:49}, pp.~41-42; see also  Ref.~\cite{benna:64}, p.~20)
\end{quote}

A growing number of research articles documents the increasing interest in the subject;
both from a computer science perspective~\cite{1011191,potgieter-06}, as well as from a more physical point of view~\cite{gruenbaum:68,pit:90,sv-aut-rev,1612095}.
With regards to physics, several proposals exist to harness infinite divisibility of space and time, and maybe action,
and to utilise physical processes for the construction of such infinity machines.

Some of these physically inspired proposals
involve investigating the state of a lamp with ever decreasing switching cycles~\cite{thom:54,benna:62,thom:54a,salmon-01,1612095}.
Ultrarelativistic methods put observers in ``fast orbits;'' or throw them toward
black holes~\cite{Hogarth92,hogarth1,hogarth2,Hogarth04,ear-nor:93,en96,siegel95,nemeti-06,DBLP:conf/mcu/Durand-Lose04,Nemeti2006118,andreka-2009}.
Alternatively, automata can  be densely embedded in physical space-time~\cite{2008-sica}.

Note that these hypothetical machines do not require the full structure of the continuum,
and in particular not the existence of certain convergence criteria, but just need the possibility to operationally
obtain a point in between two points which have already been operationally obtained; a process which,
if allowed to go on {\it ad infinitum} by complete induction, would formally coincide with density.

It is also important to consider constraints on memory costs and other resources.
For zeno squeezed machines to hypercompute they must by necessity use an infinite amount of space~\cite{calude-staiger-09}.

\section{Statistical physics}

A particular sort of hypercomputation might also have some bearing on statistical physics by possibly yielding potentiality to at least
improve, if not reverse, energy dissipation. Thereby, intuitively speaking, potentially infinite computational capacities are used as an ``entropy or energy reservoir.''

For the sake of demonstration, consider Maxwell's demon~\cite{maxwell-demon2},
which is supposed to be capable of separating lower-energy from higher-energy particles in an isolated flasket.
This might be accomplished by drawing a wall with an opening demon-controlled by a shutter which only opens for suitable particles.
In this way, a temperature gradient might be produced which would, at least in the long run, contradict the second law of thermodynamics
(in the form that no process exists whose sole purpose is the transformation of heat into work).

Again intuitively speaking, the contemporary ``exorcism'' of the demon seems to presuppose that nature behaves reversibly;
that is, the evolution of physical microstates is one-to-one.
Thus every separation or contraction of microstates in one (not necessarily spatial but some sort of general phase space) region
has to be accompanied by an equal compensating amount of mixing or expansion.
In particular, any sorting action of the demon,
associated with a decrease of entropy of the rest system,
should be compensated by an increase of the demon's ``mind'' memory which is at least as big as the entropy decrease caused by the demon.
One elegant way of demonstration~\cite[pp.~927-929]{bennett-82} is by a one-molecule Szil\'ard engine~\cite[pp.~843--844]{Szilard-1929}
whose phase space volume is contracted by a factor of two by the demon's action.
However, in order to complete the cycle, one bit of the demon's memory has to be reset---an irreversible, two-to-one transition causing an increase
of entropy at least as big as  $k_B \log 2$ \cite{landauer:61}.

But what if the demon's memory and computational capacity is, at least in principle, unbounded?
This case corresponds to certain types of (infinite) hypercomputability.
In such cases, the contraction in physical configuration or phase space
could go on forever at the price of consuming more and more memory of the demon;
thereby realizing a sort of {\em Hilbert's Hotel} scenario.

Likewise, what if the demon's capacity to compress information is unbounded, and thus it would be possible to ``compute'' the algorithmic
information content of the information acquired?
Present literature~\cite{zurek,li-vitanyi-2008,681318} postulates that only the optimal compression
yields optimally small compensation on the demon's side;
other less optimal compressions result in an overall increase in entropy.
Thus, in order to attain optimal performance,  due to
quantitative bounds on determining the algorithmic information~\cite[Section~8.2]{calude:02},
unbounded computational capacities have to be assumed.


\section{Quantum oracles}


Progress in the natural sciences during the 20th century was marked by two distinct departures from classical thought:
Einstein's theories of relativity and the theory of quantum mechanics.
Given the exploration of possibilities of hypercomputation based on relativity theory mentioned in Section~4, and that quantum mechanics if anything presents an even greater departure from classical omniscience, it is perhaps a little surprising that the possibility of hypercomputation by exploiting quantum mechanics has been explored so little.
Research into the use of Quantum mechanics for computation has primarily explored the alternative computational model of quantum computing~\cite{Gruska}, but since its conception in Deutsch~\cite{deutsch} it has been viewed as yet another model equivalent in computational power to Turing machines~\cite{be-va}.

There are, however, possibilities of attaining hypercomputational power through the more subtle (although perhaps more direct) approach of considering quantum oracles instead of quantum computational models.

In contrast to quantum computation which utilises the simple and well known unitary dynamics of the Schr\"odinger equation, the construction of the quantum oracle exploits the heart of non-classicality in quantum mechanics: the measurement process.
This is outside the reach of the Schr\"odinger equation and, while there are extremely strong results on the impossibility of a classical description of measurement which lie at the centre of our argument, there are several competing interpretations of the ontological structure of quantum systems which alter the way measurement is viewed~\cite{wheeler-Zurek:83}.
As such, the quantum oracle we describe relies on the validity of the assumptions we make in forming the mathematical model of the strange, counterintuitive features of quantum measurements.


In quantum mechanical theory even simple systems can exist in states that are superpositions of other states, for which any attempt to measure the state will yield one of the possible outcomes seemingly at random~\cite{zeil-99}.
Formally, the theory only describes the probability distribution of this process;
the fact that, after measurement, a subsequent measurement of the state will yield the same result seems to indicate that the measurement process irreversibly changes the state of the system at random.
The nature of this ``state collapse'' is outside the theory and in the realm of ``interpretations'', of which many exist:
the standard Copenhagen interpretation(s)~\cite{wheeler-Zurek:83}, the de Broglie-Bohm theory~\cite{bohm52}, the many-worlds interpretation of Everett~\cite{everett}, and many more exotic ones; among them
the claim that measurements can be ``undone''~\cite{PhysRevD.22.879,PhysRevA.25.2208,greenberger2,Nature351,Zajonc-91,PhysRevA.45.7729,PhysRevLett.73.1223,PhysRevLett.75.3783,hkwz}.

A natural interpretation of these state of affairs, and one argued early on by Einstein, Podolsky and Rosen (EPR) in their seminal paper~\cite{epr}, is that
\begin{quote}
	the description of reality as given by a [quantum mechanical] wave function is not complete
\end{quote}
in that the result of a measurement is not probabilistic but in fact determined by some unknown, yet pre-existing, ``element of physical reality''.
However, the failures of a classical, deterministic viewpoint to account for the predictions of quantum mechanics are exemplified by the ``no-go'' results of Bell \cite{bell} and Kochen and Specker~\cite{kochen1}.
Bell's results show the impossibility of any hidden variable theory to reproduce the statistical predictions of quantum mechanics under the assumption of locality.
However, of more interest  is the result of Kochen and Specker applicable to individual quanta.
The Kochen-Specker Theorem proves that it is impossible to assign pre-existing values to the outcomes of measurements under the conditions of i)  \emph{value indefiniteness}: all observables, even those which are not compatible (cannot be simultaneously measured) have definite values corresponding the result of a measurement of them; and  ii) \emph{non-contextuality}: the value corresponding to the result of a measurement does not depend on which other compatible measurement are made alongside of it.

In a  bid to maintain realism, Bell proposed that~\cite{bell-66}
\begin{quote}
	the result of an observation may reasonably depend not only on the state of the system \dots but also on the complete disposition of the apparatus.
\end{quote}
% After all, why should the result of the measurement no depend on the ``complete disposition of the apparatus'' [].
Attempts to give complete, contextual, interpretations for quantum theory exist, such as the de Broglie-Bohm theory~\cite{bohm52}, and while such interpretations reproduce certain quantum mechanical predictions, they must by necessity embrace non-locality and remain distinctly non-classical and counterintuitive.

The many-worlds interpretation~\cite{everett} offers a rather different way out.
In this theory, measurement corresponds to the measuring apparatus becoming ``entangled'' with the state being measured, hence it makes no sense to even speak of the unique result of a measurement.

At the other end of the spectrum, Born opted~\cite{born-26-1} to
\begin{quote}
	give up determinism in the world of atoms.
\end{quote}
This served as the basis for the Copenhagen interpretation~\cite{wheeler-Zurek:83}, due largely to Born and Heisenberg amongst others, and has been the predominant view since.


We argue the following: if we opt to accept indeterminism (which subtly requires the acceptance of measurement as a real process, ruling out the many worlds type scenario) as the conclusion to be drawn from the Kochen-Specker Theorem, then we can construct a simple device acting as an incomputable oracle.
The Kochen-Specker Theorem, however, does not give us a straight choice between contextual and indeterministic realities.
Even if we choose to reject the notion of a contextual reality, the theorem only excludes the possibility of every observable having well-defined hidden variables---it is a mathematical possibility that we need only give up determinism for some observables.
However, the proof of the theorem is intrinsically co-ordinate free and only concerns itself with orthogonality relationships between observables.
If no direction in Hilbert space is privileged by the proof, then how can one argue in favour of this partially-deterministic regime?
Rather than being fundamental to the state, the asymmetry could manifest itself during the measurement process.
Put bluntly, one can conceive of a {\em demon} possessing the observer and ensuring only those observables with definite values are ever measured;
conversely,  a {\em demon} could inhabit the state ensuring the observable we choose to measure is assigned a definite value, while those we do not measure are allowed to be indefinite.
Such ``super-deterministic'' loopholes are known to exist in, and would invalidate tests of Bell inequalities~\cite{Pearle:1970fk,Hooft:2007fk}.
If we are to use a quantum system as an oracle, we must refuse to accept such demons from existing and conspiring to make our output be due to predetermined elements of reality.
We hence choose to consider a complete departure from classical omniscience and only allow those observables in which the state was prepared as an eigenstate to have definite values; elsewhere we have complete value indefiniteness.

Before we can construct our oracle we must make one final connection between value definiteness and computability by returning to EPR.
Specifically, we ask what does it mean to be able to assign a definite value to a measurement outcome?
According to EPR~\cite{epr},
\begin{quote}
	if, without in any way disturbing a system, we can predict with certainty the value of a physical quantity, then there exists an element of physical reality [hidden variable] corresponding to this physical quantity.
\end{quote}
A definite value exists exactly when there is a value allowing us to predict exactly the result of a measurement.
Thus, if we repeat the state preparation and measurement process \textit{ad infinitum} and the sequence produced by the concatenation of measurement outcomes is computable, then \emph{every} measurement can be predicted with certainty and was thus of a value definite observable.
This final assumption makes the important connection between computability and the classical notion of determinism that quantum mechanics appears to have abandoned.

From here it can be argued that, if each measurement is of a value indefinite observable (i.e.\ not of the observable which the state is in an eigenstate of) then the infinite sequence of results considered above must be incomputable.
If it were computable, this would mean the measured observables were all value definite, contradicting the assertion of value indefiniteness everywhere.

Under these assumptions we are hence guaranteed that such a device would produce an incomputable sequence and act as an oracle.
Such devices have in fact been considered for the use of random number generation~\cite{Abbott:2010uq}, so perhaps Davis' verdict on the existence of physical process that can be harnessed as Turing oracles was a little premature; one of the most plausible physical interpretation of the conundrums presented by quantum mechanics allows us to do just that.

It is useful to compare and contrast this envisaged quantum oracle to a hypothetical realisation of a probabilistic Turing machine---a Turing machine which chooses transitions probabilistically from some predefined computable probability distribution.
If we envisage a trivial such devise which, regardless of the input, accepts with probability one-half, and rejects also with probability one-half, and consider the infinite sequence generated by running this machine on inputs $1,2,\dots$ (where ``accepting'' on input $i$ means the $i$th bit is 1), is this device different in any real respect to our quantum oracle?
Na\"ively it would seem not: both devices act as oracles where the $i$th bit is 1 with probability one-half.
However, the sequences produced by this probabilistic oracle are only uniformly distributed---we cannot rule out the crucial probability-zero possibility of a computable sequence being produced.
This probabilistic oracle would hence, in practice, be a Turing oracle with probability-one.
While this may appear to contradict the well known Turing-equivalence of probabilistic Turing machines~\cite{de-Leeuw:1956aa}, we note the crucial distinction that this device does not formally compute any sequence at all---we are simply envisaging a single output of an infinite run of it being used as an oracle.
The existence of a physical realisation of such a probabilistic Turing machine is, of course, as difficult a problem to solve as that of a Turing oracle; we simply note that such a device is not the same as our proposed quantum oracle which is stronger in that it is unable to produce any computable output.

While our quantum oracles behave as oracles in the Turing sense, we know of no way of saying more about the set which they are an oracle for.
The most important open question to answer about these kind of oracles is: \emph{what is the computational power of a Turing machine working with a quantum oracle?}
We believe that such an oracle cannot solve the halting problem, and it is an open problem to determine what problems such devices can solve.

\section{The known, the unknown, and the unknowable}
The body of every subject can be divided into three parts: the known, the unknown, and the unknowable.
In time, the unknown shrinks, with some facts migrating to the known and the unknowable parts.
The unknowable is the most problematic part as to prove that a condition is impossible one has to show
that it implies a contradiction or an absurdity. A limit implies an impossibility, but the converse implication is false.
Impossibilities are provable, hence objective; limits tend to be subjective and temporal.
In contrast to mathematics where the triad is sharp and its poles---the known and the unknowable---are rather stable,
the division fluctuates in science. Mathematical limits, like G\"odel's incompleteness
theorem or incomputability results, cannot be automatically transferred to physics. Hypercomputation is
a subject at the intersection of mathematics, computer science and various particular sciences, physics, chemistry,
biology, so here impossibility and limits  are difficult to obtain and tend to be temporal.

%\section{Concluding remarks}
%\ack{To be added}


%\nocite{*}

\bibliographystyle{compj}
\bibliography{svozil.bib,alastairBib}%,alastairBib.bib,hypercomputation.bib}


\end{document}
By stating that
\begin{quote}
LCMs\footnote{LCM=Logical computing machine, Turing's name for today's Turing machine.} can do anything that could be described
as ``rule of thumb'' or ``purely mechanical''.\end{quote}

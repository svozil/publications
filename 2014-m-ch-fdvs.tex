\chapter{Finite-dimensional vector spaces}
\label{ch:lvs}

\newthought{Vector Spaces} are prevalent in physics;
\marginnote{{\it ``I would have written a shorter letter, but I did not have the time.''}
(Literally: {\it ``I made this [letter] very long, because I did not have the leisure to make it shorter.''})
Blaise Pascal, {\it Provincial Letters: Letter XVI (English Translation)}\\
{\it ``Perhaps if I had spent more time I should have been able to make a shorter report~$\ldots$''}
James Clerk Maxwell~\cite{garber}, Document~15,  p.~426
}
they are essential for an understanding
of mechanics, relativity theory, quantum mechanics, and statistical physics.


\section{Basic definitions}

In what follows excerpts from Halmos' beautiful treatment
``Finite-Dimensional Vector Spaces'' will be reviewed \cite{halmos-vs}.
Of course, there exist zillions of other very nice presentations, among them
Greub's ``Linear algebra,'' and
Strang's ``Introduction to Linear Algebra,''
among many others, even freely downloadable  ones
\cite{Greub75,Strang:2009:ILA,Homes-rorres,lipschutz:schaul-la,Hefferon}
%Homes and Chris Rorres'  ``Elementary Linear Algebra: Applications Version'' \cite{Homes-rorres},
%Mathews' ``Elementary Linear Algebra'' \cite{Mathews-LA} (an eprint),
%or Schaum's Outlines \cite{lipschutz:schaul-la}, to name just a very few among the many
competing for your attention.


The more physically oriented notation in Mermin's book on
quantum information theory \cite{mermin-04,mermin-07} is adopted.
Vectors are typed in bold face, or in Dirac's ``bra-ket'' notation.
Thereby,
the vector ${\bf x}$ is identified with the ``ket vector'' $\vert {\bf x}\rangle$.
The vector ${\bf x}^\ast$  from the dual space (see Section \ref{2011-m-dvs} on page \pageref{2011-m-dvs})
 is identified with the ``bra vector'' $\langle {\bf x}\vert$.
\index{ket vector}
\index{bra vector}
Dot (scalar or inner) products between two vectors ${\bf x}$ and ${\bf y}$   in Euclidean space are then
denoted by ``$\langle \textrm{bra} \vert  (\textrm{c}) \vert \textrm{ket}  \rangle$''  form;
that is, by $\langle x \vert  y  \rangle$.

The overline sign stands for complex conjugation; that is,
if
${a}= \Re a +i\Im a $ is a complex number, then
$\overline{a}= \Re a -i\Im a$.

Unless stated differently, only
finite-dimensional vector spaces are considered.

For an $n \times m$ matrix $\textsf{\textbf{A}}$ we shall us the {\em index notation}
\index{index notation}
\begin{equation}
\textsf{\textbf{A}}
\equiv
\begin{pmatrix}
a_{11}&a_{12}& \cdots & a_{1m}\\
a_{21}&a_{22}& \cdots & a_{2m}\\
\vdots &\vdots & \ddots & \vdots \\
a_{n1}&a_{n2}& \cdots & a_{nm}
\end{pmatrix}
\equiv a_{ij}
.
\end{equation}

A {\em matrix multiplication}
\index{matrix multiplication} (written with or without dot)
$\textsf{\textbf{A}} \cdot \textsf{\textbf{B}} = \textsf{\textbf{A}}  \textsf{\textbf{B}}$
of an $n \times m$ matrix $\textsf{\textbf{A}}\equiv a_{ij}$
with an $m \times l$ matrix $\textsf{\textbf{B}}\equiv b_{pq}$
can then be written as an $n \times l$ matrix
$\textsf{\textbf{A}} \cdot \textsf{\textbf{B}} \equiv a_{ij}b_{jk}$,
$1\le i\le n$,
$1\le j\le m$,
$1\le k\le l$.
Here the {\em Einstein summation convention}
\index{Einstein summation convention} has been used,
which requires that, when an index variable appears twice in a single term, one has to
sum over all of the possible index values.
Stated differently, if $\textsf{\textbf{A}}$ is an $n \times m$ matrix and $\textsf{\textbf{B}}$ is an $m \times l$ matrix,
their matrix product $\textsf{\textbf{A}}\textsf{\textbf{B}}$ is an $n \times l$ matrix, in which the $m$
entries across the rows of $\textsf{\textbf{A}}$ are multiplied with the $m$ entries down the columns of $\textsf{\textbf{B}}$.

Often, a {\em vector} will be coded -- with respect to a basis or coordinate system (see below) -- as an $n$-touple of numbers;
which in turn can be interpreted as either a $1 \times n$ matrix (row vector),
or a $n \times 1$ matrix (column vector).
We can then write certain terms very compactly (alas often misleadingly).
Suppose, for instance, that
${\bf x}\equiv (x_1, x_2,\ldots ,x_n)$
and
${\bf y}\equiv (y_1, y_2,\ldots ,y_n)$
are two (row) vectors (with respect to a given basis).
Then, $x_iy_j a_{ij}\equiv {\bf x} \textsf{\textbf{A}} {\bf y}^T$ can (somewhat superficially) be represented as a matrix multiplication of
a row vector with a matrix and a column vector  yielding a scalar; which in turn can be interpreted as a $1 \times 1$ matrix.
Here ``$T$'' indicates transposition; that is,
${\bf y}^T\equiv \begin{pmatrix}
y_1\\
y_2\\
\vdots \\
y_n
\end{pmatrix}
$ represents a column vector (with respect to a particular basis).

\subsection{Fields of real and complex numbers}

In physics, scalars occur either as real or complex numbers.
Thus we shall restrict our attention to these cases.

A {\em field}  $\langle  {\Bbb F} , +, \cdot , -, ^{-1}, 0, 1\rangle$
\index{field}
is a set together with two operations,
usually called {\em addition} and {\em multiplication},   denoted by ``$+$'' and ``$\cdot$''
(often  ``$a\cdot b$'' is identified with the expression ``$ab$'' without the center dot)
respectively, such that the following conditions (or, stated differently, axioms) hold:
\begin{itemize}
\item[(i)]
closure of ${\Bbb F}$ with respect to addition and multiplication:
for all $a, b \in {\Bbb F}$, both $a + b$ and $a   b$ are in ${\Bbb F}$;
\item[(ii)]
associativity of addition and multiplication:
for all $a$, $b$, and $c$ in ${\Bbb F}$,
the following equalities hold: $a + (b + c) = (a + b) + c$,
and
$a (b c) = (a  b) c$;
\item[(iii)]
commutativity of addition and multiplication:
for all $a$ and $b$ in ${\Bbb F}$, the following equalities hold: $a + b = b + a$ and $a b = b  a$;
\item[(iv)]
additive and multiplicative identity:
there exists an element of ${\Bbb F}$,
called the additive identity element and denoted by $0$, such that for all $a$ in ${\Bbb F}$,
$a + 0 = a$.
Likewise, there is an element, called the multiplicative identity element and denoted by $1$,
such that for all $a$ in ${\Bbb F}$, $1 \cdot a  = a$.
(To exclude the trivial ring, the additive identity and the multiplicative
identity are required to be distinct.)
\item[(v)]
additive and multiplicative inverses:
for every $a$ in ${\Bbb F}$, there exists an element $-a$ in ${\Bbb F}$, such that $a + (-a) = 0$.
Similarly, for any $a$ in ${\Bbb F}$ other than $0$, there exists an element $a^{-1}$ in ${\Bbb F}$,
such that $a \cdot a^{-1} = 1$.
(The elements $+ (-a)$ and  $a^{-1}$
are also denoted $-a$ and $\frac{1}{a}$, respectively.)
Stated differently: subtraction and division operations exist.
\item[(vi)]
Distributivity of multiplication over addition:
For all $a$, $b$ and $c$ in ${\Bbb F}$, the following equality holds:
$a (b + c) = (a  b) + (a  c)$.
\end{itemize}

\subsection{Vectors and vector space}
\marginnote{For proofs and additional information see \S 2 in   \cite{halmos-vs}}

Vector spaces are merely structures allowing the
sum (addition) of objects called ``vectors,'' and multiplication
of these objects by scalars; thereby remaining in this structure.
That is, for instance, the ``coherent superposition''  ${\bf a} + {\bf b}\equiv \vert {\bf a} + {\bf b} \rangle $
\index{coherent superposition}
of two vectors ${\bf a} \equiv \vert {\bf a} \rangle $ and ${\bf b} \equiv \vert {\bf b} \rangle $ can be guaranteed to
be a vector.
At this stage, little can be said about the length or relative direction or orientation of these ``vectors.''
Algebraically, ``vectors'' are elements of vector spaces.
Geometrically a vector may be interpreted as ``a quantity which is usefully represented by an arrow'' \cite{Weinreich}.
\marginnote{In order to define {\em length}, we have to engage an additional structure, namely the {\em norm}
$\|  {\bf a} \|$ of a vector ${\bf a}$.
\index{length}
And in order to define relative {\em direction} and {\em orientation}, and, in particular,
{\em orthogonality} and {\em collinearity} we have to define the {\em scalar product}
$\langle {\bf a} \vert {\bf b} \rangle$ of two vectors ${\bf a}$ and ${\bf b}$.
\index{direction}
\index{scalar product}
}



A {\em linear vector space}      $\langle  \frak V , +, \cdot , -,  0, 1\rangle$
\index{linear vector space}
is a set $\frak V$ of elements called {\em vectors},
\index{vector}
here denoted by  bold face  symbols such as
$
{\bf a},
{\bf x},
{\bf v},
{\bf w},
\ldots
$,
or, equivalently, denoted by
$
\vert {\bf a}\rangle,
\vert {\bf x}\rangle,
\vert {\bf v}\rangle,
\vert {\bf w}\rangle,
\ldots
$,
satisfying certain conditions (or, stated differently, axioms); among them,
with respect to addition of vectors:
\begin{itemize}
\item[(i)]
commutativity,
\item[(ii)]
associativity,
\item[(iii)]
the uniqueness of the origin or null vector $0$,
as well as
\item[(iv)]
the uniqueness of  the negative vector;
\item[ ]
with respect to multiplication of vectors with scalars associativity:
\item[(v)]
the existence of a unit factor $1$; and
\item[(vi)]
distributivity with respect to scalar and vector additions; that is,
\begin{equation}
\begin{split}
(\alpha +\beta ){\bf x} = \alpha {\bf x}+\beta  {\bf x}, \\
\alpha ({\bf x} +{\bf y}) = \alpha {\bf x}+\alpha {\bf y},
\end{split}
\end{equation}
with ${\bf x}, {\bf y} \in \frak V$ and scalars $\alpha ,\beta \in  {\Bbb F}$,
respectively.
\end{itemize}

{
\color{blue}
\bexample
Examples of vector spaces are:
\begin{itemize}
\item[(i)]
The set ${\Bbb C}$ of complex numbers: ${\Bbb C}$  can be interpreted as a complex vector space by  interpreting as vector addition and scalar multiplication
as the usual addition and multiplication of complex numbers, and with $0$ as the null vector;
\item[(ii)]
The set ${\Bbb C}^n$, $n \in {\Bbb N}$ of $n$-tuples of complex numbers:
Let
${\bf x}=
(x_1,\ldots , x_n)$
and
${\bf y}=
(y_1,\ldots , y_n)$.
 ${\Bbb C}^n$  can be interpreted as a complex vector space by  interpreting
the ordinary addition  $ {\bf x} + {\bf y} =  (x_1+y_1,\ldots , x_n+y_n)$
and the multiplication $\alpha {\bf x}=
(\alpha  x_1,\ldots ,\alpha  x_n)$ by a complex number $\alpha$ as vector addition
 and scalar multiplication, respectively;
the null tuple $0 =
(0,\ldots ,0)$ is the neutral element of vector addition;
\item[(iii)]
The set ${\frak P}$
 of all polynomials with complex coefficients in a variable~$t$:
${\frak P}$  can be interpreted as a complex vector space by  interpreting
the ordinary addition of polynomials and the multiplication of a polynomial by a complex number as vector addition and scalar multiplication,
respectively;
the null polynomial is the neutral element of vector addition.  \eexample
\end{itemize}
}


\section{Linear independence}

A set ${\frak S}=\{
{\bf x}_1,
{\bf x}_2,
\ldots ,
{\bf x}_k\} \subset {\frak V}$
of vectors ${\bf x}_i$ in a linear vector space
is {\em linearly independent}
\index{linear independence}
if ${\bf x}_i\neq 0\,\forall 1\le i\le k$,
and additionally, if either $k=1$,
or if no vector in ${\frak S}$ can be written as a linear combination of other vectors in this set ${\frak S}$;
that is, there are no scalars $\alpha_j$ satisfying
 ${\bf x}_i=\sum_{1\le j \le k, \; j\neq i}\alpha_j {\bf x}_j$.

Equivalently, if $\sum_{i=1}^k \alpha_i {\bf x}_i = 0$
 implies $\alpha_i =0$ for each $i$, then the set
${\frak S}=\{
{\bf x}_1,
{\bf x}_2,
\ldots ,
{\bf x}_k\}  $ is linearly independent.


Note that a the vectors of a basis are linear independent and ``maximal''
insofar as any inclusion of an additional vector results in a linearly dependent set;
that ist, this additional vector can be expressed in terms of a linear combination of the
existing basis vectors; see also Section~\ref{2012-m-ch-fdvs-Basis} on page \pageref{2012-m-ch-fdvs-Basis}.

\section{Subspace}
\label{2011-m-subspace}
\marginnote{For proofs and additional information see \S 10 in  \cite{halmos-vs}}
A nonempty subset ${\frak M}$ of a vector space is a {\em subspace}
\index{subspace}
or, used synonymuously,
a {\em linear manifold}\index{linear manifold},
 if, along with every pair of vectors ${\bf x}$   and  ${\bf y}$
 contained in  ${\frak M}$,
 every linear combination
$\alpha {\bf x} + \beta {\bf y}$ is also contained in  ${\frak M}$.

If
${\frak U}$
and
${\frak V}$
are two subspaces of a vector space,
then
${\frak U}+{\frak V}$
is the subspace spanned by
${\frak U}$
and
${\frak V}$;
that is,
it contains all vectors
${\bf z}={\bf x}+{\bf y}$, with
${\bf x}\in {\frak U}$  and
${\bf y}\in {\frak V}$.

${\frak M}$ is the {\em linear span}
\index{linear span}
\index{span}
\begin{equation}
{\frak M}
= \textrm{span}({\frak U},{\frak V})
= \textrm{span}({\bf x},{\bf y}) =
\{\alpha {\bf x} +\beta {\bf y}\mid \alpha ,\beta \in {\Bbb F}, {\bf x} \in {\frak U},
{\bf y} \in {\frak V}\}.
\end{equation}


A generalization to more than two vectors and more than two subspaces is straightforward.


For every vector space ${\frak V}$, the vector space containing only the null vector,
 and the vector space ${\frak V}$ itself are subspaces of ${\frak V}$.

\subsection{Scalar or inner product}
\marginnote{For proofs and additional information see \S 61 in  \cite{halmos-vs}}
\index{scalar product}
\index{inner product}
\label{2011-m-scalarproduct}

A {\em scalar} or {\em inner} product presents some form of measure of ``distance'' or ``apartness''
of two vectors in a linear vector space.
It should not be confused with the bilinear functionals (introduced on page \pageref{2011-m-dvs}) that connect a vector space with its dual vector space,
although for real Euclidean vector spaces these may coincide,
and although the scalar product is also bilinear in its arguments.
It should also not be confused with the tensor product introduced on page \pageref{2011-m-tensorp}.

An inner product space is a vector space $\frak V$,
together with an inner product; that is, with a map
 $\langle \cdot \mid \cdot \rangle :  \frak V  \times  \frak V  \longrightarrow {\Bbb F}$
 (usually ${\Bbb F}={\Bbb C}$ or ${\Bbb F}={\Bbb R}$)
 that satisfies the following three conditions (or, stated differently, axioms) for all vectors  and all scalars:
\begin{itemize}
\item[(i)]
Conjugate symmetry:
$
\langle {\bf x}\mid {\bf y}\rangle
=
\overline{\langle {\bf y}\mid {\bf x}\rangle }$.
\marginnote{For real, Euclidean vector spaces, this function is symmetric; that is
$
\langle {\bf x}\mid {\bf y}\rangle
=
{\langle {\bf y}\mid {\bf x}\rangle}$.
}
\item[(ii)]
Linearity in the first argument:
$$
\langle  \alpha{\bf x} + \beta {\bf y}  \mid {\bf z}\rangle
=
\alpha {\langle {\bf x}\mid {\bf z}\rangle}
+
\beta {\langle {\bf y}\mid {\bf z}\rangle}
.
$$

\item[(iii)]
Positive-definiteness:
$
\langle {\bf x}\mid {\bf x}\rangle
\ge
0$;  with equality if and only if ${\bf x} = 0$.
\end{itemize}

 Note that from the first two properties, it follows that the inner product is
{\em antilinear}, or synonymously,
{\em conjugate-linear}, in its first argument:
 $$
 \langle \alpha {\bf x} + \beta {\bf y} \mid {\bf z}\rangle
 =
 \overline{\alpha} {\langle {\bf x}\mid {\bf z}\rangle}
 +\overline{\beta} {\langle {\bf y}\mid {\bf z}\rangle}
.
 $$



The {\em norm} of a vector ${\bf x}$
\index{norm}
is defined by
\begin{equation}
\|
{\bf x}
\|
=\sqrt{\langle {\bf x}\mid {\bf x}\rangle }
\end{equation}

{
\color{blue}
\bexample
One example is the
{\em dot product}
\index{dot product}
\begin{equation}
\langle  {\bf x} \vert {\bf y} \rangle
=
\sum_{i=1}^n \overline{x_i}y_i
\end{equation}
of two vectors ${\bf x}=
(x_1,\ldots , x_n)$
and
${\bf y}=
(y_1,\ldots , y_n)$ in ${\Bbb C}^n$,
which, for real Euclidean space,  reduces to the well-known dot product
$\langle  {\bf x} \vert {\bf y} \rangle
=
{x_1}y_1 + \cdots + {x_n}y_n  = \| {\bf x}\| \| {\bf y}\| \cos \angle ({\bf x},{\bf y})$.


It is mentioned without proof that the most general form of an inner product in ${\Bbb C}^n$
is
$\langle  {\bf x} \vert {\bf y} \rangle
=  {\bf y} \textsf{\textbf{A}} {\bf x}^\dagger$,
where the symbol ``$\dagger$'' stands for the {\em conjugate transpose} (also denoted as
{\em Hermitian conjugate} or {\em Hermitian adjoint}),
\index{conjugate transpose}
\index{Hermitian conjugate}
\index{Hermitian adjoint}
and $ \textsf{\textbf{A}} $ is a positive definite Hermitian matrix (all of its eigenvalues are positive).
\eexample
}

Two nonzero vectors $  {\bf x} , {\bf y} \in {\frak V}$,  $  {\bf x},   {\bf y}\neq 0$
are {\em orthogonal}, denoted by ``${\bf x} \perp {\bf y}$''
\index{othogonality}
if their scalar product vanishes; that is, if
\begin{equation}
\langle  {\bf x} \vert {\bf y} \rangle   = 0.
\end{equation}


Let ${\frak E}$ be any set of vectors in an inner product space ${\frak V}$.
The symbol
\begin{equation}
{\frak E}^\perp  = \left\{ {\bf x}\mid  \langle  {\bf x} \vert {\bf y} \rangle=0,  {\bf x} \in {\frak V},
\forall {\bf y} \in {\frak E} \right\}
\end{equation}
 denotes the set of all vectors in  ${\frak V}$ that are
orthogonal to every vector in  ${\frak E}$.

Note that, regardless of whether or not ${\frak E}$ is a subspace,
\marginnote{See page \pageref {2011-m-subspace} for a definition of subspace.}
${\frak E}^\perp$ is a subspace.
Furthermore,
${\frak E}$  is contained in $({\frak E}^\perp)^\perp= {\frak E}^{\perp\perp}$.
In case ${\frak E}$ is a subspace, we call ${\frak E}^\perp$
the {\em orthogonal complement}
\index{orthogonal complement}
of ${\frak E}$.

The following {\em projection theorem}
\index{projection theorem}
is mentioned without proof.
If ${\frak M}$ is any subspace of a finite-dimensional inner product space ${\frak V}$,
then ${\frak V}$ is the direct sum of ${\frak M}$ and ${\frak M}^\perp$;
that is, ${\frak M}^{\perp \perp}={\frak M}$.

{
\color{blue}
\bexample
For the sake of an example, suppose ${\frak V}={\Bbb R}^2$,
and take ${\frak E}$ to be the set of all vectors spanned by the vector $(1,0)$;
then ${\frak E}^\perp$ is the set of all vectors spanned by $(0,1)$.
\eexample
}


\subsection{Hilbert space}


A (quantum mechanical) {\em Hilbert space}  is a linear
\index{Hilbert space}
vector space ${\frak V}$ over the field${\Bbb C}$ of complex numbers
(sometimes only ${\Bbb R}$ is used)
equipped with vector addition, scalar multiplication, and some inner (scalar) product.
Furthermore, {\em closure} is an additional requirement,
but nobody has made operational sense of that so far:
If ${\bf x}_n\in {\frak V}$, $n=1,2,\ldots$, and if $\lim_{n,m\rightarrow
\infty} ({\bf x}_n-{\bf x}_m,{\bf x}_n-{\bf x}_m)=0$,
then there exists an ${\bf x}\in {\frak V}$ with
$\lim_{n\rightarrow \infty} ({\bf x}_n-{\bf x},{\bf x}_n-{\bf x})=0$.





Infinite dimensional vector spaces and continuous spectra are nontrivial
extensions of the finite
dimensional Hilbert space treatment. As a heuristic rule -- which is not
always correct -- it might be
stated that the sums become integrals, and the Kronecker delta function
$\delta_{ij}$ defined by
\index{Kronecker delta function}
\begin{equation}
\delta_{ij} =\begin{cases}
0  &\text{ for }i\neq j , \\
1  &\text{ for }i = j.
\end{cases}
\end{equation}
becomes the Dirac delta function $\delta (x-y)$, which is a
generalized function in the continuous variables $x,y$.
In the Dirac bra-ket notation, unity is given by
${\bf 1}=\int_{-\infty}^{+\infty} \vert x\rangle \langle  x\vert \, dx$.
For a careful treatment, see, for instance,
the books by
Reed and Simon \cite{reed-sim1,reed-sim2}.


\section{Basis}
\label{2012-m-ch-fdvs-Basis}
\marginnote{For proofs and additional information see \S 7 in    \cite{halmos-vs}}

We shall use bases of vector spaces to formally represent vectors (elements) therein.

A (linear) {\em basis}
\index{basis}
[or a {\em coordinate system}, or a {\em frame (of reference)}]
\index{coordinate system}
\index{frame}
is a set    $\frak B$
of linearly independent vectors
such that every vector
in $\frak V$ is a linear combination of the vectors in the basis; hence
$\frak B$ spans $\frak V$.

What particular basis should one choose?
{\em A priori} no basis is privileged over the other.
Yet, in view of certain (mutual) properties of elements of some bases (such as orthogonality or orthonormality)
we shall prefer (s)ome over others.


Note that a vector is some directed entity with a particular length,
oriented in some (vector) ``space.''
It is ``laid out there'' in front of our eyes, as it is: some directed entity.
{\it A priori}, this space, in its most primitive form,
is not equipped with a basis, or synonymuously, frame of reference, or reference frame.
Insofar it is not yet coordinatized.
In order to formalize the notion of a vector, we have to code this vector by ``coordinates''
or ``components'' which are the coeffitients with respect to a (de)composition into basis elements.
Therefore, just as for numbers (e.g., by different numeral bases, or by prime decomposition),
there exist many ``competing'' ways to code a vector.

Some of these ways appear to be rather straightforward,
such as, in particular, the {\em Cartesian basis},
also synonymuosly called the  {\em standard basis}.
\index{Cartesian basis}
\index{standard basis}
It is, however, not in any way {\it a priori}
``evident'' or ``necessary'' what should be specified to be ``the Cartesian basis.''
Actually, specification of a ``Cartesian basis'' seems to be mainly motivated by
physical inertial motion --
and thus identified with some inertial frame of reference --
``without any friction and
forces,'' resulting in a ``straight line motion at constant speed.''
(This sentence is  cyclic, because heuristically any such absence of ``friction and
force''  can only be operationalized by testing if the motion is a
``straight line motion at constant speed.'')
If we grant that in this way straight lines can be defined, then
Cartesian bases in Euclidean vector spaces can be characterized by
orthogonal (orthogonality is defined {\it via} vanishing scalar products between nonzero vectors)
straight lines spanning the entire space.
In this way, we arrive, say for a planar situation, at the coordinates
characteried by some basis $\{(0,1),(1,0)\}$,
where, for instance, the basis vector ``$(1,0)$'' literally and physically
means ``a unit arrow pointing in some particular, specified direction.''

Alas, if we would prefer, say, cyclic motion in the plane,
we might want to call a frame based on the polar coordinates $r$ and $\theta$ ``Cartesian,''
resulting in some ``Cartesian basis'' $\{(0,1),(1,0)\}$;
but this ``Cartesian basis'' would be very different from the Cartesian
basis mentioned earlier,
as ``$(1,0)$'' would refer to some specific unit radius,
and ``$(0,1)$'' would refer to some specific unit angle (with respect to a specific zero angle).
In terms of the ``straight'' coordinates (with respect to ``the usual Cartesian basis'')
$x,y$, the polar coordinates are $r = \sqrt{x^2+y^2}$ and $\theta = \mathrm{tan}^{-1} (y/x)$.
We obtain the original ``straight'' coordinates (with respect to ``the usual Cartesian basis'')
back if we take
$x=r\cos \theta$
and
$y=r\sin \theta$.

Other bases than the ``Cartesian'' one may be less suggestive at first; alas it may be ``economical'' or pragmatical to use them;
mostly to cope with, and adapt to, the {\em symmetry} of a physical configuration:
if the physical situation at hand is, for instance, rotationally invariant,
we might want to use rotationally invariant bases --
such as, for instance, polar coordinares in two dimensions, or spherical coordinates in three dimensions --
to represent a vector, or, more generally, to code any given representation of a physical entity
(e.g., tensors, operators) by such bases.


\section{Dimension}
\marginnote{For proofs and additional information see \S 8 in    \cite{halmos-vs}}
The {\em dimension}
\index{dimension}
of $\frak V$ is the number of elements in $\frak B$.

All bases $\frak B$ of $\frak V$ contain the same number of elements.

A vector space is finite dimensional if its bases are finite; that is, its bases
contain a finite number of elements.

{\color{Purple}
In quantum physics, the dimension of a quantized system is associated with
the {\em number of mutually exclusive measurement outcomes}.
For a spin state measurement of an electron
along a particular direction,
as well as for a measurement of the linear polarization
of a photon in a particular direction,
the dimension is two, since both measurements
may yield two distinct outcomes
which we can
interpret as vectors in two-dimensional Hilbert space,
which, in Dirac's bra-ket notation \cite{dirac}, can be written as
$
\mid \uparrow \rangle$ and $\mid \downarrow \rangle$,
or $\mid + \rangle$ and $
\mid - \rangle
$,
or
$
\mid H \rangle $ and $
\mid V \rangle
$,
or
$
\mid 0 \rangle $ and $
\mid 1 \rangle
$,
or
$
\mid$\begin{tikzpicture}
    \emoticon\pupils
    %% mouth
    \draw[very thick,red,line cap=round] (-1ex,-1ex)
               ..controls (-0.5ex,-1.5ex)and(0.5ex,-1.5ex)..(1ex,-1ex);
    %% eyelashes
    \draw (0.60ex,1.20ex)--(0.60ex,1.60ex)
          (0.85ex,1.25ex)--(0.95ex,1.45ex)
          (1.00ex,1.00ex)--(1.20ex,1.10ex)
          (0.35ex,1.15ex)--(0.25ex,1.35ex)
          (-0.60ex,1.20ex)--(-0.60ex,1.60ex)
          (-0.85ex,1.25ex)--(-0.95ex,1.45ex)
          (-1.00ex,1.00ex)--(-1.20ex,1.10ex)
          (-0.35ex,1.15ex)--(-0.25ex,1.35ex);
\end{tikzpicture}$\rangle $ and $
\mid$\begin{tikzpicture}
\emoticon\pupils
    %% pupils
    \fill[shift={( 0.5ex,0.5ex)},rotate=90] (0,0) ellipse (0.3ex and 0.15ex);
    \fill[shift={(-0.5ex,0.5ex)},rotate=90] (0,0) ellipse (0.3ex and 0.15ex);
    %% mouth
    \draw[thick] (-1ex,-1ex)..controls
        (-0.5ex,-0.5ex)and(0.5ex,-0.5ex)..(1ex,-1ex);
    %% eyebrows
    \draw[thick] (0.2ex,1.15ex)--(0.5ex,1.6ex)(-0.2ex,1.15ex)--(-0.5ex,1.6ex);
\end{tikzpicture}$\rangle
$,
respectively.
}

\section{Coordinates}
\marginnote{For proofs and additional information see \S 46 in    \cite{halmos-vs}}
The coordinates of a vector with respect to some basis
represent the coding of that vector in that particular basis.
It is important to realize that, as bases change, so do coordinates.
Indeed, the changes in coordinates have to ``compensate'' for the bases change,
because the same coordinates in a different basis would render an altogether different
vector.
Figure \ref{2011-m-bases} presents some geometrical demonstration of
these thoughts, for your contemplation.

\begin{figure}[ht]
\caption{Coordinazation of vectors:
(a) some primitive vector;
(b)  some primitive vectors, laid out in some space, denoted by dotted lines
(c) vector coordinates $x_1$ and $x_2$ of the vector  ${\bf x} =  (x_1,x_2) = x_1{\bf e}_1 +  x_2{\bf e}_2$ in a standard basis;
(d) vector coordinates $x_1'$ and $x_2'$ of the vector  ${\bf x} = (x_1',x_2') =  x_1'{\bf e}_1' +  x_2'{\bf e}_2'$  in some nonorthogonal basis.
\label{2011-m-bases}}
\begin{center}
\begin{tabular}{cc}

%TeXCAD (http://texcad.sf.net/) Picture. File: [p1.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{4.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(60,60)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
\end{picture}
&
%TeXCAD (http://texcad.sf.net/) Picture. File: [p2.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(79.375,76)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
%\dottedbox(1,1)(78.375,75)
\multiput(.93,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\multiput(.93,.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(.93,75.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(79.305,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
%\end
\end{picture}
\\
(a)&(b)\\
$\;$\\
%TeXCAD (http://texcad.sf.net/) Picture. File: [p3.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(79.375,76)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
%\dottedbox(1,1)(78.375,75)
\multiput(.93,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\multiput(.93,.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(.93,75.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(79.305,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
%\end
\thinlines
{\color{blue}
\put(20,20){\vector(0,1){50}}
\put(20,20){\vector(1,0){50}}
}
%\dashline{1}(60,20)(60,60)
\put(59.93,19.93){\line(0,1){.9756}}
\put(59.93,21.881){\line(0,1){.9756}}
\put(59.93,23.832){\line(0,1){.9756}}
\put(59.93,25.783){\line(0,1){.9756}}
\put(59.93,27.735){\line(0,1){.9756}}
\put(59.93,29.686){\line(0,1){.9756}}
\put(59.93,31.637){\line(0,1){.9756}}
\put(59.93,33.588){\line(0,1){.9756}}
\put(59.93,35.539){\line(0,1){.9756}}
\put(59.93,37.491){\line(0,1){.9756}}
\put(59.93,39.442){\line(0,1){.9756}}
\put(59.93,41.393){\line(0,1){.9756}}
\put(59.93,43.344){\line(0,1){.9756}}
\put(59.93,45.296){\line(0,1){.9756}}
\put(59.93,47.247){\line(0,1){.9756}}
\put(59.93,49.198){\line(0,1){.9756}}
\put(59.93,51.149){\line(0,1){.9756}}
\put(59.93,53.1){\line(0,1){.9756}}
\put(59.93,55.052){\line(0,1){.9756}}
\put(59.93,57.003){\line(0,1){.9756}}
\put(59.93,58.954){\line(0,1){.9756}}
%\end
%\dashline{1}(60,60)(20,60)
\put(59.93,59.93){\line(-1,0){.9756}}
\put(57.978,59.93){\line(-1,0){.9756}}
\put(56.027,59.93){\line(-1,0){.9756}}
\put(54.076,59.93){\line(-1,0){.9756}}
\put(52.125,59.93){\line(-1,0){.9756}}
\put(50.174,59.93){\line(-1,0){.9756}}
\put(48.222,59.93){\line(-1,0){.9756}}
\put(46.271,59.93){\line(-1,0){.9756}}
\put(44.32,59.93){\line(-1,0){.9756}}
\put(42.369,59.93){\line(-1,0){.9756}}
\put(40.418,59.93){\line(-1,0){.9756}}
\put(38.466,59.93){\line(-1,0){.9756}}
\put(36.515,59.93){\line(-1,0){.9756}}
\put(34.564,59.93){\line(-1,0){.9756}}
\put(32.613,59.93){\line(-1,0){.9756}}
\put(30.661,59.93){\line(-1,0){.9756}}
\put(28.71,59.93){\line(-1,0){.9756}}
\put(26.759,59.93){\line(-1,0){.9756}}
\put(24.808,59.93){\line(-1,0){.9756}}
\put(22.857,59.93){\line(-1,0){.9756}}
\put(20.905,59.93){\line(-1,0){.9756}}
%\end
\put(68.375,12.125){\makebox(0,0)[cc]{${\bf e}_{ 1}$}}
\put(11.375,68.125){\makebox(0,0)[cc]{${\bf e}_{ 2}$}}
\put(38.125,16.625){\makebox(0,0)[cc]{$x_1$}}
\put(13.375,43.375){\makebox(0,0)[cc]{$x_2$}}
\put(15,15){\makebox(0,0)[cc]{$0$}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
\end{picture}
&
%TeXCAD (http://texcad.sf.net/) Picture. File: [p4.pic]. Options on following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(79.375,76)(0,0)
\thicklines
\put(20,20){\color{red}\vector(1,1){40}}
%\dottedbox(1,1)(78.375,75)
\multiput(.93,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
\multiput(.93,.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(.93,75.93)(.992089,0){80}{{\rule{.8pt}{.8pt}}}
\multiput(79.305,75.93)(0,-.986842){77}{{\rule{.8pt}{.8pt}}}
%\end
\thinlines
%\vector(20,20)(74.375,37.5)
{\color{blue}
\put(74.375,37.5){\vector(3,1){.07}}\multiput(20,20)(.1047687861,.0337186898){519}{\line(1,0){.1047687861}}
%\end
%\vector(20,20)(42.75,71.5)
\put(42.75,71.5){\vector(1,2){.07}}\multiput(20,20)(.0337037037,.0762962963){675}{\line(0,1){.0762962963}}
%\end
}
%\dashline{1}(60,60)(44.25,28)
\multiput(59.93,59.93)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(59.078,58.2)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(58.227,56.47)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(57.376,54.741)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(56.524,53.011)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(55.673,51.281)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(54.822,49.551)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(53.97,47.822)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(53.119,46.092)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(52.268,44.362)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(51.416,42.632)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(50.565,40.903)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(49.713,39.173)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(48.862,37.443)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(48.011,35.713)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(47.159,33.984)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(46.308,32.254)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(45.457,30.524)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
\multiput(44.605,28.795)(-.0327443,-.0665281){13}{\line(0,-1){.0665281}}
%\end
%\dashline{1}(60,60)(32.625,48.75)
\multiput(59.93,59.93)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(58.164,59.204)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(56.397,58.478)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(54.631,57.752)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(52.865,57.026)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(51.099,56.301)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(49.333,55.575)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(47.567,54.849)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(45.801,54.123)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(44.035,53.397)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(42.268,52.672)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(40.502,51.946)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(38.736,51.22)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(36.97,50.494)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(35.204,49.768)(-.080279,-.032991){11}{\line(-1,0){.080279}}
\multiput(33.438,49.043)(-.080279,-.032991){11}{\line(-1,0){.080279}}
%\end
\put(36.5,19.875){\makebox(0,0)[cc]{${x_1}'$}}
\put(20.5,39.625){\makebox(0,0)[cc]{${x_2}'$}}
\put(72.375,29.125){\makebox(0,0)[cc]{${\bf e}_{ 1}'$}}
\put(33.72,70){\makebox(0,0)[cc]{${\bf e}_{ 2}'$}}
\put(15,15){\makebox(0,0)[cc]{$0$}}
\put(65,65){\makebox(0,0)[cc]{${\bf x}$}}
\end{picture}
\\
(c)&(d)\\
\end{tabular}
\end{center}
\end{figure}

Elementary high school tutorials often condition students into believing that the components of the vector
``is'' the vector, rather then emphasizing that these components {\em represent} or {\em encode}
the vector with respect to some (mostly implicitly assumed) basis.
A similar situation occurs in many introductions to quantum theory,
where the span
(i.e., the onedimensional linear subspace spanned by that vector)
\index{span}
$\{
{\bf y}
\mid
{\bf y} = \alpha {\bf x}, \alpha \in {\Bbb C}
\}$, or, equivalently,  for orthogonal projections,
the {\em projection} (i.e., the projection operator; see also page \pageref{2011-m-projec})
\index{projection}
$\textsf{\textbf{E}}_{\bf x} = {\bf x}^T \otimes {\bf x}$
corresponding to a unit (of length $1$) vector ${\bf x}$
often is identified with that vector.
In many instances, this is a great help and,
if administered properly, is consistent and fine (at least for all practical purposes).

The standard (Cartesian) basis in $n$-dimensional complex space ${\Bbb C}^n$
is the set of (usually ``straight'')
vectors $x_i, i=1, \ldots , n$, represented by $n$-tuples,
defined by the condition that the $i$'th coordinate of the $j$'th basis vector
${\bf e}_j$ is given by $\delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta function
\index{Kronecker delta function}
\begin{equation}
\delta_{ij} =\begin{cases}
0  &\text{ for }i\neq j , \\
1  &\text{ for }i = j.
\end{cases}
\end{equation}
Thus,
\begin{equation}
\begin{split}
{\bf e}_1=(1,0,\ldots,0),\\
{\bf e}_2=(0,1,\ldots,0),\\
\vdots\\
{\bf e}_n=(0,0,\ldots,1).
\end{split}
\end{equation}


In terms of these standard base vectors, every vector ${\bf x}$
can be written as a linear combination
\begin{equation}
{\bf x} = \sum_{i=1}^n x_i{\bf e}_i = (x_1,x_2, \ldots , x_n),
\end{equation}
or, in ``dot product notation,''
that is,
``column times row''
and
``row times column;'' the dot is usually omitted (the superscript ``$T$'' stands for transposition),
\begin{equation}
{\bf x} = ({\bf e}_1,{\bf e}_2, \ldots , {\bf e}_n)
\cdot
(x_1,x_2, \ldots , x_n)^T
=
({\bf e}_1,{\bf e}_2, \ldots , {\bf e}_n)
\begin{pmatrix}
x_1\\x_2\\ \vdots \\ x_n
\end{pmatrix}
,
\end{equation}
of the product of the coordinates $x_i$  with respect to that standard basis.
Here the equality sign ``$=$'' really means ``coded with respect to that standard basis.''

In what follows, we shall often identify the column vector
$$
\begin{pmatrix}
x_1\\x_2\\ \vdots \\ x_n
\end{pmatrix}
$$
containing the coordinates of the vector ${\bf x}$
with the vector ${\bf x}$, but we always need to keep in mind that
the tuples of coordinates are defined only with respect to a particular basis
$\{ {\bf e}_1,{\bf e}_2, \ldots , {\bf e}_n \}$; otherwise these numbers lack any meaning whatsoever.

Indeed, with respect to some arbitrary  basis ${\frak B}=\{
{\bf f}_1, \ldots , {\bf f}_n\}$ of some $n$-dimensional vector space ${\frak V}$
with the base vectors ${\bf f}_i$, $1\le i\le n$, every vector ${\bf x}$ in  ${\frak V}$
can be written as a unique linear combination
\begin{equation}
{\bf x} = \sum_{i=1}^n x_i{\bf f}_i = (x_1,x_2, \ldots , x_n)
\end{equation}
of the product of the coordinates $x_i$ with respect to the basis  ${\frak B}$.

{\color{OliveGreen}
\bproof
The uniqueness of the coordinates is proven indirectly by {\em reductio ad absurdum:}
Suppose there is another decomposition
${\bf x} = \sum_{i=1}^n y_i{\bf f}_i = (y_1,y_2, \ldots , y_n) $;
then by subtraction, $0 = \sum_{i=1}^n (x_i-y_i) {\bf f}_i = (0,0, \ldots , 0)$.
Since the basis vectors ${\bf f}_i$ are linearly independent,
this can only be valid if all coefficients in the summation  vanish;
thus $x_i-y_i=0$ for all $1\le i\le n$; hence finally  $x_i=y_i$ for all $1\le i\le n$.
This is in contradiction with our assumption that the coordinates $x_i$ and $y_i$
(or at least some of them) are different.
Hence the only consistent alternative is the assumption that, with respect to a given basis, the coordinates are uniquely determined.
\eproof
}

A  set    $\frak B = \{
{\bf a}_1, \ldots , {\bf a}_n\}$
of  vectors   of the inner product space $\frak V$
is {\em orthonormal}
\index{orthonormal}
if, for all
 ${\bf a}_i\in\frak B$ and
 ${\bf a}_j\in\frak B$,
it follows that
\begin{equation}
\langle {\bf a}_i \mid {\bf a}_j \rangle =\delta_{ij}.
\label{2013-m-ch-fdvs-orthonorm}
\end{equation}
Any such set is called {\em complete}
\index{completeness}
if it is not a subset of any larger orthonormal set of vectors of $\frak V$.
Any complete set is a basis.
If, instead of Eq.~(\ref{2013-m-ch-fdvs-orthonorm}),
$\langle {\bf a}_i \mid {\bf a}_j \rangle = \alpha_i \delta_{ij}$
with nontero factors $\alpha_i$, the set is called {\em orthogonal}.


\section{Finding orthogonal bases from nonorthogonal ones}

A {\em Gram-Schmidt process} is a systematic method for orthonormalising a set of vectors
\index{Gram-Schmidt process}
\index{scalar product}
\index{inner product}
in a space equipped with a {\em scalar product,}
or by a synonym preferred in mathematics, {\em inner product.}
\marginnote{The scalar or inner product
$\langle {\bf x}\vert {\bf y} \rangle$ of two vectors
${\bf x}$ and ${\bf y}$ is defined on page \pageref{2011-m-scalarproduct}.
In Euclidean  space such as ${\Bbb R}^n$,
one often identifies the ``dot product''
${\bf x}\cdot {\bf y} =x_1y_1+ \cdots +x_ny_n$
of two vectors ${\bf x}$ and $ {\bf y}$ with their scalar or inner product.}
The Gram-Schmidt process takes a finite, linearly independent set
of base vectors
and generates an orthonormal basis that spans the same (sub)space as the original set.

The general method is to start out with the original basis,
say,  \\
$\{
{\bf x}_1,
{\bf x}_2,
{\bf x}_3,
\ldots ,
{\bf x}_n
\}$,
and generate a new orthogonal basis \\
$\{
{\bf y}_1,
{\bf y}_2,
{\bf y}_3,
\ldots ,
{\bf y}_n
\}$
by
\begin{equation}
\begin{split}
{\bf y}_1={\bf x}_1,\\
{\bf y}_2={\bf x}_2 - P_{{\bf y}_1}({\bf x}_2),\\
{\bf y}_3={\bf x}_3 - P_{{\bf y}_1}({\bf x}_3)- P_{{\bf y}_2}({\bf x}_3),\\
 \vdots \\
{\bf y}_n={\bf x}_n -\sum_{i=1}^{n-1} P_{{\bf y}_i}({\bf x}_n),
\end{split}
\end{equation}
where
\begin{equation}
P_{{\bf y}}({\bf x}) =
\frac{\langle {\bf x}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
{\bf y}
,\textrm{ and }
P_{{\bf y}}^\perp ({\bf x}) = {\bf x} -
\frac{\langle {\bf x}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
{\bf y}
\end{equation}
are the orthogonal projections of ${\bf x}$ onto ${\bf y}$ and ${\bf y}^\perp$, respectively
(the latter is mentioned for the sake of completeness and is not required here).
\label{2011-m-gsp}
Note that these orthogonal projections are idempotent
\index{idempotence}
%(i.e., $p^2=p(p)=p$ and $(p^\perp)^2=p^\perp (p^\perp)=p^\perp$)
and mutually orthogonal; that is,
\begin{equation}
\begin{split}
P_{{\bf y}}^2({\bf x})  = P_{{\bf y}}(P_{{\bf y}}({\bf x}) ) =
\frac{\langle {\bf x}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
\frac{\langle {\bf y}\vert {\bf y} \rangle }
{\langle {\bf y}\vert {\bf y} \rangle }
{\bf y} =P_{{\bf y}}({\bf x}),  \\
%
(P_{{\bf y}}^\perp)^2({\bf x})  = P_{{\bf y}}^\perp(P_{{\bf y}}^\perp({\bf x}) ) =
{\bf x}- \frac{\langle {\bf x}\vert {\bf y} \rangle }{\langle {\bf y}\vert {\bf y} \rangle }{\bf y}
-\left(
\frac{\langle {\bf x}\vert {\bf y} \rangle }{\langle {\bf y}\vert {\bf y} \rangle }
-
\frac{\langle {\bf x}\vert {\bf y} \rangle \langle {\bf y}\vert {\bf y}
\rangle}{\langle {\bf y}\vert {\bf y} \rangle^2 }
\right)
{\bf y},
=P_{{\bf y}}^\perp({\bf x}),  \\
P_{{\bf y}}(P_{{\bf y}}^\perp({\bf x}) ) =  P_{{\bf y}}^\perp(P_{{\bf y}}({\bf x}) ) =
\frac{\langle {\bf x}\vert {\bf y} \rangle }{\langle {\bf y}\vert {\bf y} \rangle }{\bf y}
-
\frac{\langle {\bf x}\vert {\bf y} \rangle \langle {\bf y}\vert {\bf y} \rangle}{\langle {\bf y}\vert {\bf y} \rangle^2 }
{\bf y}
=0.
\end{split}
\end{equation}
For a more general discussion of projections, see also page \pageref{2011-m-projec}.

Subsequently, in order to obtain an orthonormal basis,
one can divide every basis vector by its length.

{\color{OliveGreen}
\bproof
The idea of the proof is as follows (see also Greub \cite{Greub75}, section 7.9).
In order to generate an orthogonal basis from a nonorthogonal one,
the first vector of the old basis is identified with the first vector of the new basis;
that is ${\bf y}_1={\bf x}_1$.
Then, as depicted in Fig.~\ref{2012-m-fdvs-ideaofGS}, the second vector of the new basis is obtained by
taking the second vector of the old basis and
subtracting its projection on the first vector of the new basis.
\begin{marginfigure}%
{\color{black}
% This is a LaTeX picture output by TeXCAD.
% File name: [1.pic].
% Version of TeXCAD: 4.3
% Reference / build: 30-Jun-2012 (rev. 105)
% For new versions, check: http://texcad.sf.net/
% Options on the following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.4mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(120,65)(10,0)
\put(10,10){\color{blue}\vector(1,0){110}}
\put(10,10){\color{blue}\vector(2,1){60}}
\put(10,10){\color{orange}\vector(0,1){30}}
%\dottedline(70,40)(70,10)
\multiput(69.93,39.93)(0,-.96774){32}{{\rule{.4pt}{.4pt}}}
%\end
%\dottedline(70,40)(10,40)
\multiput(69.93,39.93)(-.983607,0){62}{{\rule{.4pt}{.4pt}}}
%\end
%\vector{dash}{1}(10,10)(70,10)
\put(70,10){\vector(1,0){.07}}\put(9.93,9.93){\line(1,0){.9836}}
\put(11.897,9.93){\line(1,0){.9836}}
\put(13.864,9.93){\line(1,0){.9836}}
\put(15.831,9.93){\line(1,0){.9836}}
\put(17.799,9.93){\line(1,0){.9836}}
\put(19.766,9.93){\line(1,0){.9836}}
\put(21.733,9.93){\line(1,0){.9836}}
\put(23.7,9.93){\line(1,0){.9836}}
\put(25.667,9.93){\line(1,0){.9836}}
\put(27.635,9.93){\line(1,0){.9836}}
\put(29.602,9.93){\line(1,0){.9836}}
\put(31.569,9.93){\line(1,0){.9836}}
\put(33.536,9.93){\line(1,0){.9836}}
\put(35.503,9.93){\line(1,0){.9836}}
\put(37.471,9.93){\line(1,0){.9836}}
\put(39.438,9.93){\line(1,0){.9836}}
\put(41.405,9.93){\line(1,0){.9836}}
\put(43.372,9.93){\line(1,0){.9836}}
\put(45.34,9.93){\line(1,0){.9836}}
\put(47.307,9.93){\line(1,0){.9836}}
\put(49.274,9.93){\line(1,0){.9836}}
\put(51.241,9.93){\line(1,0){.9836}}
\put(53.208,9.93){\line(1,0){.9836}}
\put(55.176,9.93){\line(1,0){.9836}}
\put(57.143,9.93){\line(1,0){.9836}}
\put(59.11,9.93){\line(1,0){.9836}}
\put(61.077,9.93){\line(1,0){.9836}}
\put(63.044,9.93){\line(1,0){.9836}}
\put(65.012,9.93){\line(1,0){.9836}}
\put(66.979,9.93){\line(1,0){.9836}}
\put(68.946,9.93){\line(1,0){.9836}}
%\end
\put(115,5){\makebox(0,0)[cc]{\color{blue}${\bf x}_1 ={\bf y}_1$}}
\put(70,5){\makebox(0,0)[cc]{$P_{{\bf y}_1}({\bf x}_2)$}}
\put(7,45){\makebox(0,0)[lc]{\color{orange}${\bf y}_2= {\bf x}_2 - P_{{\bf y}_1}({\bf x}_2)$}}
\put(70,45){\makebox(0,0)[cc]{\color{blue}${\bf x}_2$}}
\end{picture}
}
\caption{Gram-Schmidt construction for two nonorthogonal vectors ${\bf x}_1$ and ${\bf x}_2$,
yielding two  orthogonal vectors ${\bf y}_1$ and ${\bf y}_2$.}
  \label{2012-m-fdvs-ideaofGS}
\end{marginfigure}
More precisely, take the Ansatz
\begin{equation}
{\bf y}_2={\bf x}_2 + \lambda  {\bf y}_1,
\end{equation}
thereby determining the arbitrary scalar $\lambda$ such that
${\bf y}_1$
and
${\bf y}_2$
are orthogonal; that is,
$\langle {\bf y}_1\vert {\bf y}_2 \rangle =0$.
This yields
\begin{equation}
\langle {\bf y}_2\vert {\bf y}_1  \rangle
=\langle {\bf x}_2\vert {\bf y}_1  \rangle
+ \lambda
\langle {\bf y}_1\vert {\bf y}_1 \rangle =0,
\end{equation}
and thus, since ${\bf y}_1 \neq 0$,
\begin{equation}
\lambda =
-
\frac{\langle {\bf x}_2\vert {\bf y}_1  \rangle}
{\langle {\bf y}_1\vert {\bf y}_1 \rangle} .
\end{equation}
To obtain the third vector ${\bf y}_3$ of the new basis,
take the Ansatz
\begin{equation}
{\bf y}_3={\bf x}_3 + \mu  {\bf y}_1  + \nu  {\bf y}_2,
\label{2012-m-ch-gs1}
\end{equation}
and require that it is orthogonal to the two previous orthogonal basis vectors
${\bf y}_1$
and
${\bf y}_2$;
that is
$\langle {\bf y}_1\vert {\bf y}_3 \rangle =\langle {\bf y}_2\vert {\bf y}_3 \rangle =0$.
We already know that $\langle {\bf y}_1\vert {\bf y}_2 \rangle = 0$.
Consider the scalar products of ${\bf y}_1$
and ${\bf y}_2$
with the {\it Ansatz} for ${\bf y}_3$ in Eq.~(\ref{2012-m-ch-gs1}); that is,
\begin{equation}
\begin{split}
\langle {\bf y}_3\vert {\bf y}_1\rangle
=
\langle {\bf y}_3\vert {\bf x}_1 \rangle + \mu  \langle {\bf y}_1\vert {\bf y}_1 \rangle  + \nu  \langle {\bf y}_2\vert {\bf y}_1\rangle
\\
0
=
\langle {\bf y}_3\vert {\bf x}_1 \rangle + \mu  \langle {\bf y}_1\vert {\bf y}_1 \rangle  + \nu  \cdot 0 ,
\end{split}
\end{equation}
and
\begin{equation}
\begin{split}
\langle {\bf y}_3\vert {\bf y}_2\rangle =\langle {\bf y}_3\vert {\bf x}_2 \rangle + \mu  \langle {\bf y}_1\vert {\bf y}_2 \rangle  + \nu  \langle {\bf y}_2\vert {\bf y}_2\rangle
\\
0 =\langle {\bf y}_3\vert {\bf x}_2 \rangle + \mu   \cdot 0  + \nu  \langle {\bf y}_2\vert {\bf y}_2 \rangle  .
\end{split}
\end{equation}
As a result,
\begin{equation}
\mu = -  \frac{\langle {\bf x}_3\vert {\bf y}_1  \rangle}
{\langle {\bf y}_1\vert {\bf y}_1 \rangle},\quad
\nu =- \frac{\langle {\bf x}_3\vert {\bf y}_2  \rangle}
{\langle {\bf y}_2\vert {\bf y}_2 \rangle}.
\end{equation}
A generalization of this construction
for all the other new base vectors
${\bf y}_3, \ldots ,  {\bf y}_n$, and thus a proof by complete induction,
proceeds by a generalized construction.
\eproof
}

{\color{blue}
\bexample
Consider, as an example, the standard Euclidean scalar product denoted by ``$\cdot$''
and the basis
$\{(0,1),(1,1)\}$.
Then two orthogonal bases are obtained obtained by taking
\begin{itemize}
\item[(i)]
either the basis vector
$(0,1)$ and
$$
(1,1) -
\frac{(1,1)\cdot (0,1)}{(0,1)\cdot (0,1)} (0,1) = (1,0),
$$
\item[(ii)]
or the basis vector
$(1,1)$ and
$$
(0,1) -
\frac{(0,1)\cdot (1,1)}{(1,1)\cdot (1,1)} (1,1) = \frac{1}{2}(-1,1). \textrm{\eexample}
$$
\end{itemize}
}





\section{Dual space}
\label{2011-m-dvs}
\marginnote{For proofs and additional information see \S 13--15 in   \cite{halmos-vs}}

Every vector space ${\frak V}$
has a corresponding {\em dual vector space}
\index{dual vector space}
\index{dual space}
(or just {\em dual space})
consisting of all linear functionals on ${\frak V}$.

A {\em linear functional}
\index{linear functional}
on a vector space ${\frak V}$ is a scalar-valued linear function ${\bf y}$
defined for every vector   ${\bf x} \in {\frak V}$, with the linear property that
\begin{equation}
{\bf y} (\alpha_1 {\bf x}_1 +\alpha_2 {\bf x}_2)
=
\alpha_1 {\bf y} ({\bf x}_1) +\alpha_2 {\bf y} ({\bf x}_2) .
\end{equation}

{\color{blue}
\bexample
For example,
let ${\bf x} = (x_1,\ldots , x_n)$, and
take
${\bf y} ({\bf x}) =x_1$.

For another example,
let again ${\bf x} = (x_1,\ldots , x_n)$, and
let $\alpha_1,\ldots , \alpha_n \in {\Bbb C}$ be scalars; and
take
${\bf y} ({\bf x}) =\alpha_1 x_1 + \cdots +\alpha_n x_n$.
\eexample
}


We adopt a square bracket notation ``$[\cdot , \cdot ]$''
for the functional
\begin{equation}
{\bf y} ({\bf x})
=
[{\bf x},{\bf y}].
\end{equation}

Note that the usual arithmetic operations of addition and multiplication,
that is,
\begin{equation}
(a {\bf y} + b {\bf z}) ({\bf x})
=
a {\bf y} ({\bf x}) + b {\bf z} ({\bf x}) ,
\end{equation}
together with the ``zero functional''
(mapping every argument to zero)
induce a kind of linear vector space, the ``vectors''
being identified with the linear functionals.
This vector space will be called {\em dual space} ${\frak V}^*$.
\index{dual space}


As a result, this ``bracket'' functional is
{\em bilinear} in its two arguments; that is,
\begin{equation}
[ \alpha_1 {\bf x}_1 +\alpha_2 {\bf x}_2, {\bf y}]
=
\alpha_1 [{\bf x}_1 ,{\bf y}]  +\alpha_2  [{\bf x}_2,{\bf y}],
\end{equation}
and
\begin{equation}
[
{\bf x}, \alpha_1 {\bf y}_1 +\alpha_2 {\bf y}_2
]
=
\alpha_1
[{\bf x},{\bf y}_1 ]
+
\alpha_2
[{\bf x},{\bf y}_2].
\end{equation}
\marginnote{The square bracket can be identified with the scalar dot product
$[ {\bf x},{\bf y} ] = \langle {\bf x}\mid {\bf y}\rangle$
only for Euclidean
vector spaces ${\Bbb R}^n$, since for complex spaces this would no longer be positive definite.
That is, for Euclidean
vector spaces ${\Bbb R}^n$ the inner or scalar product is bilinear.
}


Because of linearity, we can completely characterize an arbitrary linear functional
${\bf y} \in {\frak V}^*$ by its values of the vectors of some basis of ${\frak V}$:
If we know the functional value on the basis vectors in ${\frak B}$, we know the functional
on all elements of the vector space ${\frak V}$.
If ${\frak V}$ is an $n$-dimensional vector space, and if ${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is a basis of  ${\frak V}$, and if
$\{\alpha_1, \ldots ,\alpha_n\}$  is any set of $n$ scalars, then there is
a unique linear functional ${\bf y}$  on  ${\frak V}$ such that
$ [ {\bf f}_i, {\bf y}] = \alpha_i $ for all $0\le i \le n$.

{\color{OliveGreen}
\bproof
A constructive proof  of this theorem can be given as follows:
Because every ${\bf x}\in {\frak V}$
can be written as a linear combination $ {\bf x} = x_1 {\bf f}_1 +\cdots + x_n {\bf f}_n$
of the basis vectors of ${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
in one and only one (unique) way, we obtain for any arbitrary linear functional ${\bf y} \in {\frak V}^*$  a unique decomposition
in terms of the basis vectors of  ${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$; that is,
\begin{equation}
[{\bf x},{\bf y}]
=
x_1 [{\bf f}_1 ,{\bf y}] +\cdots + x_n [{\bf f}_n ,{\bf y}] .
\end{equation}
By identifying  $[{\bf f}_i ,{\bf y}]=\alpha_i$ we obtain
\begin{equation}
[{\bf x},{\bf y}]
=
x_1 \alpha_1 +\cdots + x_n \alpha_n .
\end{equation}
\eproof
}

Conversely, if we {\em define} ${\bf y}$ by $[{\bf x},{\bf y}] = x_1\alpha_1 + \cdots + x_n\alpha_n$, then ${\bf y}$
can be interpreted as a linear functional in ${\frak V}^*$ with $[{\bf f}_i,{\bf y}] = \alpha_i$.

If we introduce a {\em dual basis}
by requiring that $[{\bf f}_i,  {\bf f}_j^*]=\delta_{ij}$ (cf. Eq.~\ref{2011-m-Dualbasis-e1} below),
then the coefficients $[{\bf f}_i ,{\bf y}] = \alpha_i$,
$1\le i \le n$, can be interpreted
as the {\em coordinates} of the linear functional ${\bf y}$ with respect to the dual
basis ${\frak B}^\ast$, such that ${\bf y}=(\alpha_1,\alpha_2,\ldots , \alpha_n)^T$.

Likewise, as will be shown in (\ref{2014-m-ch-fdlvs-kju}),
$
x_i =
 [{\bf x},{\bf f}_i^*]
$; that is, the vector coordinates can be represented by the functionals of the elements of the dual basis.


{\color{blue}
\bexample
Let us explicitly construct an example of a linear functional $\varphi ({\bf x})\equiv [{\bf x},\varphi]$ that is defined
on all vectors ${\bf x}=
\alpha {\bf e}_1
+
\beta {\bf e}_2
$
of a two-dimensional vector space with the basis $\{{\bf e}_1, {\bf e}_2 \}$
by enumerating its ``performannce on the basis vectors''  ${\bf e}_1=(1,0)$ and ${\bf e}_2=(0,1)$;
more explicitly,  say, for an example's sake,
$\varphi ({\bf e}_1 ) \equiv [ {\bf e}_1,\varphi ] = 2$ and
$\varphi ({\bf e}_2 ) \equiv [ {\bf e}_2,\varphi ] = 3$.
Therefore, for example,
$\varphi ((5,7) ) \equiv [(5,7), \varphi ] = 5 [{\bf e}_1, \varphi ] + 7 [ {\bf e}_2,\varphi ]
=10+21=31$.
\eexample
}

\subsection{Dual basis}
\label{2011-m-Dualbasis}

We now can define a {\em dual basis}, or, used synonymuously, a {\em reciprocal basis}.
\index{dual basis}
\index{reciprocal basis}
If ${\frak V}$ is an $n$-dimensional vector space, and if
${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
is a basis of  ${\frak V}$,
then there is a unique {\em dual basis}
${\frak B}^*
=\{{\bf f}_1^*,\ldots , {\bf f}_n^*\}$ in the dual vector space ${\frak V}^*$
defined by
\begin{equation}
[{\bf f}_i,  {\bf f}_j^*]=\delta_{ij},
\label{2011-m-Dualbasis-e1}
\end{equation}
where  $\delta_{ij}$
is the Kronecker delta function.
The dual space  ${\frak V}^*$ spanned by the dual basis ${\frak B}^*$ is $n$-dimensional.

More generally, if $g$ is the {\em metric tensor},
\index{metric tensor}
\index{metric}
the dual basis is defined by
\begin{equation}
g( {\bf f}_i,{\bf f}_j^*)=\delta_{ij}.
\label{2011-m-Dualbasis-e2}
\end{equation}
or, in a different notation in which ${\bf f}_j^* = {\bf f}^j$,
\begin{equation}
g( {\bf f}_i,{\bf f}^j)=\delta_{i}^j.
\label{2011-m-Dualbasis-e3}
\end{equation}
In terms of the inner product, the representation
of the metric $g$ as outlined and characterized on page \pageref{2011-m-metrict} with respect to a particular basis.
${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
may be defined by $g_{ij}=g({\bf f}_i,{\bf f}_j)=\langle {\bf f}_i\mid {\bf f}_j\rangle$.
Note however, that the coordinates $g_{ij}$ of
the metric $g$ need not necessarily be positive definite.
For example,  special relativity uses the ``pseudo-Euclidean'' metric
 $g={\rm diag}(+1,+1,+1,-1)$ (or just $g={\rm diag}(+,+,+,-)$), where ``${\rm diag}$''
stands for the {\em diagonal matrix}
\index{diagonal matrix}
with the arguments in the diagonal.
\marginnote{The metric tensor $g_{ij}$ represents a {\em bilinear functional}
$g({\bf x},{\bf y}) =x^iy^j g_{ij}$ that is {\em symmetric}; that is,
$g({\bf x},{\bf y}) = g({\bf x},{\bf y})$
and {\em nondegenerate}; that is, for any nonzero vector ${\bf x}\in {\frak V}$,   ${\bf x}\neq 0$,
there is some  vector  ${\bf y}\in {\frak V}$, so that  $g({\bf x},{\bf y}) \neq 0$.
$g$ also satisfies the triangle
inequality
$\vert\vert {\bf x} -{\bf z} \vert\vert  \le \vert\vert {\bf x} - {\bf y} \vert\vert  + \vert\vert  {\bf y} - {\bf z} \vert\vert $.
}




In a real Euclidean vector space ${\Bbb R}^n$
with the dot product as the scalar product,
the dual basis of an orthogonal basis  is also orthogonal, and contains vectors with the same directions,
although with {\em reciprocal length} (thereby exlaining the wording ``reciprocal basis'').
Moreover, for an orthonormal basis, the basis vectors are uniquely identifiable by
${\bf e_i} \longrightarrow {\bf e_i}^* = {\bf e_i}^T$.
This identification can only be made for orthonomal bases; it is {\em not} true for nonorthonormal bases.

{\color{OliveGreen}
\bproof
%For a proof ask your audience or a wizard \frownie


A ``reverse construction'' of the elements ${\bf f}_j^*$ of the dual basis ${\frak B}^*$
-- thereby using the definition ``$[{\bf f}_i,{\bf y}] = \alpha_i$
for all $1 \le i \le n$''
for any element ${\bf y}$ in ${\frak V}^*$ introduced earlier
--
can be given as follows:
for every $1\le j \le n$,
we can {\em define} a vector ${\bf f}_j^*$ in the dual basis ${\frak B}^*$
by the {\em requirement}   $[{\bf f}_i,{\bf f}_j^*] = \delta_{ij}$.
That is, in words:
the dual basis element, when applied to the elements of the original $n$-dimensional basis,
yields one if and only if  it corresponds to the respective equally indexed basis element;
for all the other $n-1$ basis elements it yields zero.

What remains to be proven is the conjecture that
${\frak B}^* = \{{\bf f}_1^*,\ldots , {\bf f}_n^*\}$
is a basis of ${\frak V}^*$; that is, that the vectors in ${\frak B}^*$ are linear independent,
and that they span ${\frak V}^*$.

First observe that ${\frak B}^*$ is a set of linear independent vectors,
for if
$ \alpha_1 {\bf f}_1^* + \cdots + \alpha_n {\bf f}_n^*=0$, then also
\begin{equation}
 [{\bf x},\alpha_1 {\bf f}_1^* + \cdots + \alpha_n {\bf f}_n^*]=
 \alpha_1 [{\bf x},{\bf f}_1^*] + \cdots + \alpha_n [{\bf x},{\bf f}_n^*]=
0
\end{equation}
for arbitrary ${\bf x}\in {\frak V}$.
In particular, by identifying ${\bf x}$ with ${\bf f}_i \in {\frak B}$, for $1 \le i \le n$,
\begin{equation}
 \alpha_1 [{\bf f}_i,{\bf f}_1^*] + \cdots + \alpha_n [{\bf f}_i,{\bf f}_n^*]= \alpha_j [{\bf f}_i,{\bf f}_j^*] = \alpha_j \delta_{ij} = \alpha_i=
0.
\end{equation}

Second,  every ${\bf y} \in {\frak V}^*$ is a linear combination of elements in
${\frak B}^* = \{{\bf f}_1^*,\ldots , {\bf f}_n^*\}$, because by
starting from
$[{\bf f}_i,  {\bf y}]=\alpha_i$,
with
$ {\bf x} = x_1 {\bf f}_1 +\cdots + x_n {\bf f}_n$
we obtain
\begin{equation}
 [{\bf x},{\bf y}]
= x_1 [{\bf f}_1,{\bf y}] +\cdots + x_n [{\bf f}_n,{\bf y}]
= x_1 \alpha_1 +\cdots + x_n \alpha_n.
\label{2014-m-fdvs-ba}
\end{equation}
Note that , for arbitrary  ${\bf x}\in {\frak V}$,
\begin{equation}
 [{\bf x},{\bf f}_i^*]
= x_1 [{\bf f}_1,{\bf f}_i^*] +\cdots + x_n [{\bf f}_n,{\bf f}_i^*]=  x_j [{\bf f}_j,{\bf f}_i^*]=  x_j \delta_{ji}=  x_i,
\label{2014-m-ch-fdlvs-kju}
\end{equation}
and by substituting $[{\bf x},{\bf f}_i]$ for $x_i$ in Eq.~(\ref{2014-m-fdvs-ba}) we obtain
\begin{equation}
\begin{split}
 [{\bf x},{\bf y}] =
 x_1\alpha_1 +\cdots + x_n \alpha_n \\
= [{\bf x},{\bf f}_1] \alpha_1 +\cdots + [{\bf x},{\bf f}_n] \alpha_n  \\
= [{\bf x},\alpha_1 {\bf f}_1 +\cdots + \alpha_n{\bf f}_n] ,
\label{2014-m-fdvs-ba1}
\end{split}
\end{equation}
and therefore ${\bf y} =\alpha_1 {\bf f}_1 +\cdots + \alpha_n{\bf f}_n=\alpha_i{\bf f}_i$.

\eproof
}

How can one determine the dual basis from a given,
not necessarily orthogonal, basis?
Suppose for the rest of this section that the metric is identical to the Euclidean metric ${\rm det}(+,+,+)$ representable as the usual ``dot product.''
The tuples of {\em row vectors} of the basis ${\frak B} = \{{\bf f}_1,\ldots , {\bf f}_n\}$
can be arranged into a matrix
\begin{equation}
\textsf{\textbf{B}}
=
\begin{pmatrix}
{\bf f}_{1}\\
{\bf f}_{2}\\
\vdots  \\
{\bf f}_{n}
\end{pmatrix}
=
\begin{pmatrix}
{\bf f}_{1,1}&\cdots & {\bf f}_{1,n}\\
{\bf f}_{2,1}&\cdots & {\bf f}_{2,n}\\
\vdots&\vdots & \vdots \\
{\bf f}_{n,1}&\cdots & {\bf f}_{n,n}
\end{pmatrix}
.
\end{equation}
Then take the
{\em inverse matrix}
$\textsf{\textbf{B}}^{-1}$,
and interpret the
{\em column vectors} of
\begin{equation}
\begin{split}
\textsf{\textbf{B}}^{\ast}=\textsf{\textbf{B}}^{-1}
\\
\quad
=\left({\bf f}_{1}^\ast ,\cdots , {\bf f}_{n}^\ast\right)
\\
\quad
=
\begin{pmatrix}
{\bf f}_{1,1}^\ast&\cdots & {\bf f}_{n,1}^\ast\\
{\bf f}_{1,2}^\ast&\cdots & {\bf f}_{n,2}^\ast\\
\vdots&\vdots & \vdots \\
{\bf f}_{1,n}^\ast&\cdots & {\bf f}_{n,n}^\ast
\end{pmatrix}
\end{split}
\end{equation}
as the tuples of elements of the dual basis of ${\frak B}^*$.

For orthogonal but not orthonormal bases, the term {\em reciprocal} basis
can be easily explained from the fact that the norm (or length) of each vector in the {\em reciprocal basis}
is just the {\em inverse} of the length of the original vector.

{\color{OliveGreen}
\bproof
For a direct proof, consider $\textsf{\textbf{B}}\cdot \textsf{\textbf{B}}^{-1} ={\Bbb I}_n$.
\eproof
}


{\color{blue}
\bexample
\begin{itemize}
\item[(i)]
For example,
if
$$
{\frak B}=\{{\bf e}_1, {\bf e}_2,\ldots ,{\bf e}_n\}
=
\{
(1,0,\ldots,0),
(0,1,\ldots,0),
\ldots,
(0,0,\ldots,1)
\}
$$
is the standard basis in $n$-dimensional vector space containing unit vectors of norm (or length) one,
then  (the superscript ``$T$'' indicates transposition)
\begin{equation}
\begin{split}
{\frak B}^*
=
\{{\bf e}_1^* ,{\bf e}_2^*, \ldots ,{\bf e}_n^* \}
\\
=
\left\{
(1,0,\ldots,0)^T,
(0,1,\ldots,0)^T,
\ldots,
(0,0,\ldots,1)^T
\right\}
\\
=
\left\{
\begin{pmatrix}
1\\
0\\
\vdots \\
0
\end{pmatrix}
,
\begin{pmatrix}
0\\
1\\
\vdots \\
0
\end{pmatrix}
,
\ldots ,
\begin{pmatrix}
0\\
0\\
\vdots \\
1
\end{pmatrix}
\right
\}
\end{split}
\end{equation}
has elements with identical components,
but those tuples are the transposed tuples.

\item[(ii)]
If $$
{\frak X}
=
\{\alpha_1 {\bf e}_1, \alpha_2 {\bf e}_2,\ldots ,\alpha_n {\bf e}_n\}
=
\{
(\alpha_1 ,0,\ldots,0),
(0,\alpha_2 ,\ldots,0),
\ldots,
(0,0,\ldots,\alpha_n )
\},
$$  $\alpha_1,\alpha_2,\ldots ,\alpha_n \in {\Bbb R}$,
is a ``dilated'' basis in $n$-dimensional vector space containing vectors of norm (or length) $\alpha_i$,
then
\begin{equation}
\begin{split}
{\frak X}^*
=\{\frac{1}{\alpha_1 }{\bf e}_1^* ,\frac{1}{\alpha_2 }{\bf e}_2^*, \ldots ,\frac{1}{\alpha_n }{\bf e}_n^* \}\\
=\left\{
(\frac{1}{\alpha_1 },0,\ldots,0)^T,
(0,\frac{1}{\alpha_2 },\ldots,0)^T,
\ldots,
(0,0,\ldots,\frac{1}{\alpha_n })^T\right\}  \\
=  \left\{
\frac{1}{\alpha_1 }
\begin{pmatrix}
1\\
0\\
\vdots \\
0
\end{pmatrix}
,
\frac{1}{\alpha_2 }
\begin{pmatrix}
0\\
1\\
\vdots \\
0
\end{pmatrix}
,
\ldots ,
\frac{1}{\alpha_n }
\begin{pmatrix}
0\\
0\\
\vdots \\
1
\end{pmatrix}
\right\}
\end{split}
\end{equation}
has elements with identical components of inverse length $\frac{1}{\alpha_i }$,
and again those tuples are the transposed tuples.

\item[(iii)]
Consider the nonorthogonal basis
${\frak B} =
\{(1, 2), (3, 4)\}$.
The associated row matrix is
$$
\textsf{\textbf{B}}
=
\begin{pmatrix}
1&2\\
3&4
\end{pmatrix}
.
$$
The inverse matrix is
$$
\textsf{\textbf{B}}^{-1}
=
\begin{pmatrix}
-2&1\\
\frac{3}{2}&-\frac{1}{2}
\end{pmatrix}
,
$$
and the associated dual basis is obtained from the columns of $\textsf{\textbf{B}}^{-1} $ by
\begin{equation}
{\frak B}^* =
\left\{
\begin{pmatrix}
-2\\
\frac{3}{2}
\end{pmatrix},
\begin{pmatrix}
1\\
-\frac{1}{2}
\end{pmatrix}
\right\} =
\left\{\frac{1}{2}
\begin{pmatrix}
-4\\
3
\end{pmatrix},
\frac{1}{2}
\begin{pmatrix}
2\\
-1
\end{pmatrix}
\right\}
.
\label{2011-m-cenobdb}
\end{equation}
\eexample
\end{itemize}
}


\subsection{Dual coordinates}

With respect to a given basis,
the components of a vector are often written as tuples of ordered
(``$x_i$ is written before $x_{i+1}$'' -- not ``$x_i < x_{i+1}$'')
scalars  as {\em column vectors}
\begin{equation}
{\bf x}=
\begin{pmatrix}
x_1\\x_2\\
\vdots \\
x_n
\end{pmatrix}
,
\end{equation}
whereas the components of vectors in dual spaces are often written in terms of
 tuples of ordered
scalars  as {\em row vectors}
\begin{equation}
{\bf x}^*= (x_1^*,x_2^*,\ldots , x_n^*)
.
\end{equation}
The coordinates  $(x_1,x_2,\ldots , x_n)^T
=
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
$
are called
{\em covariant},
\index{covariant coordinates}
whereas the coordinates  $(x_1^*,x_2^*,\ldots , x_n^*)$
are called
{\em contravariant},
\index{contravariant coordinates}.
Alternatively, one can denote
covariant coordinates by subscripts,
and contravariant coordinates by superscripts; that is
(see also
Havlicek \cite{havlicek-laftm}, Section 11.4),
\begin{equation}
x_i =
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
\textrm{ and }
 x^i =
(x_1^*,x_2^*,\ldots , x_n^* ).
\end{equation}
Note again that the covariant and contravariant components
$x_i$ and $x^i$ are not absolute, but always defined {\em with respect to}
a particular (dual) basis.

Note that the {\em Einstein summation convention}
\index{Einstein summation convention}
requires that, when an index variable appears twice in a single term, one has to
sum over all of the possible index values.
This saves us from drawing the sum sign ``$\sum_i$'' for the index $i$;
for instance $x_iy_i =\sum_{i}x_iy_i$.

In the particular context of covariant and contravariant components
--
made necessary by nonorthogonal bases whose associated dual bases are {\em not} identical
--
the summation always is between some superscript and some subscript;
e.g., $x_iy^i$.

Note again that for orthonormal basis,
$x^i=x_i$.


\subsection{Representation of a functional by inner product}
\label{2011-m-corr-bil-ip}
\marginnote{For proofs and additional information see \S 67 in   \cite{halmos-vs}}
The following representation theorem,
often called
{\em Riesz representation theorem}
\index{Riesz representation theorem}
\index{Fr\'echet-Riesz representation theorem}
(sometimes also called the {\em Fr\'echet-Riesz theorem}),
is about the connection between any functional
in a vector space and its inner product; it is stated without proof:
To any linear functional ${\bf z}$
on a finite-dimensional inner product space ${\frak V}$
there corresponds a unique vector   ${\bf y}\in {\frak V}$,
such that
\begin{equation}
{\bf z} ({\bf x}) =[{\bf x}, {\bf z}]= \langle {\bf x}\mid {\bf y}\rangle
\end{equation}
for all ${\bf x}\in {\frak V}$.

Note that in  real or complex vector space ${\Bbb R}^n$ or ${\Bbb C}^n$, and with the dot product,
 ${\bf y}^\dagger = {\bf z}$.

Note also that every inner product
$\langle {\bf x}\mid {\bf y}\rangle = \phi_y(x)$ defines a linear
functional $\phi_y(x)$ for all ${\bf x}\in {\frak V}$.



{\color{Purple}
In quantum mechanics,
this representation of a functional by the inner product suggests
the (unique) existence of
the bra vector $\langle \psi \vert \in {\frak V}^*$
associated with every ket vector $\vert \psi \rangle \in {\frak V}$.

It also suggests a ``natural'' duality between
propositions and states
--
that is, between (i)
dichotomic (yes/no, or 1/0) observables
represented by projections $\textsf{\textbf{E}}_{\bf x}= \vert {\bf x} \rangle \langle {\bf x} \vert$
and their associated linear subspaces  spanned by unit vectors  $\vert {\bf x} \rangle $
on the one hand,
and (ii) pure states, which are also represented by projections $\boldsymbol{\rho}_{\psi}= \vert \psi \rangle \langle \psi \vert$
and their associated subspaces spanned by unit vectors  $ \vert {\psi} \rangle$
on the other hand
--
{\em via} the scalar product ``$\langle \cdot \vert \cdot \rangle$.''
In particular \cite{hamhalter-book},
\begin{equation}
{{\psi}} ({\bf x}) \equiv [{\bf x},  \psi ]= \langle {\bf x} \mid \psi \rangle
\end{equation}
represents the {\em probability amplitude.}
By the {\em Born rule}
\index{Born rule}
for pure states,
the absolute square $\vert \langle {\bf x} \mid \psi \rangle \vert^2$
of this probability amplitude is identified with the probability of the occurrence of the proposition
$\textsf{\textbf{E}}_{\bf x}$,
given the state  $ \vert {\psi} \rangle$.

More general,  due to linearity and the spectral theorem
(cf. Section \ref{2012-m-ch-Spectraltheorem} on page \pageref{2012-m-ch-Spectraltheorem}),
the statistical expectation for a Hermitean (normal) operator $\textsf{\textbf{A}}=
\sum_{i=0}^k   \lambda_i \textsf{\textbf{E}}_i$
and a quantized system prepared in pure state
\index{pure state}
(cf. Sec.~\ref{2011-m-projec})
$\boldsymbol{\rho}_\psi = \vert {\psi}\rangle \langle \psi \vert$ for some unit vector $\vert {\psi}\rangle$
is given by the {\em Born rule}
\index{Born rule}
\begin{equation}
\begin{split}
\langle \textsf{\textbf{A}}\rangle_{\psi} = \text{Tr} (\boldsymbol{\rho}_\psi \textsf{\textbf{A}})\\
=
\text{Tr}  \left[\boldsymbol{\rho}_\psi  \left(\sum_{i=0}^k   \lambda_i  \textsf{\textbf{E}}_i  \right)\right] =
\text{Tr}  \left(\sum_{i=0}^k   \lambda_i \boldsymbol{\rho}_\psi  \textsf{\textbf{E}}_i  \right)\\
=
 \text{Tr} \left(\sum_{i=0}^k   \lambda_i (\vert \psi \rangle \langle \psi \vert )( \vert {\bf x}_i \rangle \langle {\bf x}_i \vert )\right)\\
=
 \text{Tr} \left(\sum_{i=0}^k   \lambda_i  \vert \psi \rangle \langle \psi \vert   {\bf x}_i \rangle \langle {\bf x}_i \vert  \right)\\
=
\sum_{j=0}^k \langle {\bf x}_j \vert
\left(  \sum_{i=0}^k   \lambda_i  \vert \psi \rangle  \langle \psi   \vert {\bf x}_i \rangle   \langle {\bf x}_i \vert \right)   \vert{\bf x}_j \rangle    \\
=
\sum_{j=0}^k
   \sum_{i=0}^k   \lambda_i  \langle {\bf x}_j \vert \psi \rangle   \langle \psi   \vert {\bf x}_i \rangle
\underbrace{ \langle {\bf x}_i \vert    {\bf x}_j \rangle }_{\delta_{ij}}  \\
=
\sum_{i=0}^k   \lambda_i  \langle {\bf x}_i \vert \psi \rangle \langle \psi   \vert {\bf x}_i \rangle
\\
=
\sum_{i=0}^k   \lambda_i \vert \langle {\bf x}_i \vert \psi \rangle \vert^2,
\end{split}
\end{equation}
where $\text{Tr}$ stands for the trace (cf. Section \ref{2013-ch-fdvs-trace} on page \pageref{2013-ch-fdvs-trace}),
and we have used the spectral decomposition $\textsf{\textbf{A}}= \sum_{i=0}^k   \lambda_i \textsf{\textbf{E}}_i$
(cf. Section \ref{2012-m-ch-Spectraltheorem} on page \pageref{2012-m-ch-Spectraltheorem}).
}


\subsection{Double dual space}
\index{double dual space}
\label{2012-m-dds}

In the following, we strictly limit the discussion to finite dimensional vector spaces.

Because to every vector  space ${\frak V}$
there exists a dual vector  space ${\frak V}^\ast$ ``spanned'' by all linear functionals on ${\frak V}$,
there exists also a dual vector space $({\frak V}^{\ast})^{\ast}={\frak V}^{\ast \ast}$ to the dual  vector  space ${\frak V}^\ast$
 ``spanned'' by all linear functionals on ${\frak V}^\ast$.
We state without proof
\marginnote{{\tiny https://www.dpmms.cam.ac.uk/~wtg10/meta.doubledual.html}}
that ${\frak V}^{\ast \ast}$ is closely related to, and can be {\em canonically identified} with ${\frak V}$
{\em via} the {\em canonical bijection}
\begin{equation}
\begin{split}
{\frak V}  \rightarrow {\frak V}^{\ast \ast }: {\bf x}  \mapsto \langle \cdot  \vert {\bf x}\rangle ,
\textrm{ with} \\
\langle \cdot  \vert {\bf x}\rangle : {\frak V}^\ast \rightarrow {\Bbb R}
\textrm{ or }
{\Bbb C} : {\bf a}^\ast  \mapsto \langle {\bf a}^\ast   \vert {\bf x}\rangle
;
\end{split}
\end{equation}
\index{canonical identification}
indeed, more generally,
\begin{equation}
\begin{split}
{\frak V}\equiv {\frak V}^{\ast \ast},\\
{\frak V}^\ast \equiv {\frak V}^{\ast \ast \ast},\\
{\frak V}^{\ast \ast} \equiv {\frak V}^{\ast \ast \ast \ast}\equiv {\frak V},\\
{\frak V}^{\ast  \ast \ast} \equiv {\frak V}^{\ast \ast \ast  \ast \ast}\equiv {\frak V}^\ast ,\\
\qquad \vdots
\end{split}
\end{equation}



\section{Direct sum}
\index{direct sum}
\marginnote{For proofs and additional information see \S 18 in   \cite{halmos-vs}}

Let
${\frak U}$
and
${\frak V}$
be vector spaces (over the same field, say ${\Bbb C}$).
Their {\em direct sum} is a vector space
${\frak W}={\frak U}\oplus{\frak V}$
consisting of all ordered pairs
$({\bf x},{\bf y})$, with
${\bf x}\in {\frak U}$ in
${\bf y}\in {\frak V}$,
and with the linear operations defined by
\begin{equation}
(
\alpha {\bf x}_1 +\beta {\bf x}_2
,
\alpha {\bf y}_1 +\beta {\bf y}_2
)
=
\alpha  ({\bf x}_1,{\bf y}_1)
+
\beta   ({\bf x}_2,{\bf y}_2).
\end{equation}

We state without proof
\marginnote{For proofs and additional information see \S 19 in   \cite{halmos-vs}}
that the dimension of the direct sum is the sum of the dimensions of its summands.

We also state without proof
\marginnote{For proofs and additional information see \S 18 in   \cite{halmos-vs}}
that,
if
${\frak U}$
and
${\frak V}$
are subspaces of a vector space
${\frak W}$,
then the following three conditions are equivalent:
\begin{itemize}
\item[(i)]
${\frak W}={\frak U}\oplus{\frak V}$;
\item[(ii)]
${\frak U}\bigcap{\frak V}={\frak 0}$
and
${\frak U}+{\frak V}={\frak W}$, that is, ${\frak W}$ is spanned by ${\frak U}$ and ${\frak V}$
(i.e., ${\frak U}$
and
${\frak V}$
are complements of each other);
\item[(iii)]
every vector ${\bf z}\in {\frak W}$ can be written as
${\bf z}={\bf x}+{\bf y}$, with
${\bf x}\in {\frak U}$  and
${\bf y}\in {\frak V}$, in one and only one way.
\end{itemize}

Very often the direct sum will be used to ``compose'' a vector space by the direct sum of its subspaces.
Note that there is no ``natural'' way of composition.
A different way of putting two vector spaces together is by the {\em tensor product}.


\section{Tensor product}
\label{2011-m-tensorp}
\marginnote{For proofs and additional information see \S 24 in   \cite{halmos-vs}}


\subsection{Sloppy definition}

For the moment, suffice it to say that
the {\em tensor product}
\index{tensor product}
 ${\frak V} \otimes {\frak U}$
of two linear vector spaces  ${\frak V}$ and  ${\frak U}$
should be such that,
to every
${\bf x} \in  {\frak V}$
and every
${\bf y} \in  {\frak U}$
there corresponds a tensor product ${\bf z} = {\bf x} \otimes {\bf y}
\in {\frak V} \otimes {\frak U}$
which is bilinear in both factors.



A generalization to more than one factors is straightforward.

\subsection{Definition}
A more rigorous definition is as follows:
The {\em tensor product} ${\frak V} \otimes {\frak U}$ of two vector spaces ${\frak V}$
and
${\frak U}$ (over the same field, say ${\Bbb C}$)
is the dual vector space of all bilinear forms on ${\frak V} \oplus {\frak U}$.

For each pair of vectors ${\bf x} \in  {\frak V}$
and
${\bf y} \in  {\frak U}$   the tensor product ${\bf z} = {\bf x} \otimes {\bf y}$
is the element of ${\frak V} \otimes {\frak U}$
such that ${\bf z}({\bf w})={\bf w}({\bf x},{\bf y})$ for every bilinear form
${\bf w}$ on ${\frak V} \oplus {\frak U}$.

Alternatively we could define the {\em tensor product}  as the linear superpositions of products ${\bf e}_i \otimes {\bf f}_j$
of all basis vectors
${\bf e}_i \in  {\frak V}$, with $1\le i \le n$,
and
${\bf f}_j \in  {\frak U}$, with $1\le i \le m$ as follows.
First we note without proof that if ${\frak A} =\{{\bf e}_1,\ldots , {\bf e}_n\}$ and
${\frak B} =\{{\bf f}_1,\ldots , {\bf f}_m\}$
are bases of  $n$- and $m$-
dimensional vector spaces ${\frak V}$ and  ${\frak U}$, respectively,
then the set
of vectors ${\bf e}_i \otimes {\bf f}_j$
with $i=1,\ldots n$ and $j=1,\ldots m$
 is a basis of the { tensor product}
 ${\frak V} \otimes {\frak U}$.
Then an arbitrary tensor product can be written as the linear superposition of all
its basis vectors ${\bf e}_i \otimes {\bf f}_j$
with
${\bf e}_i \in  {\frak V}$, with $1\le i \le n$,
and
${\bf f}_j \in  {\frak U}$, with $1\le i \le m$; that is,
\begin{equation}
{\bf z}=\sum_{i,j} c_{ij} \; {\bf e}_i \otimes {\bf f}_j.
\label{2014-m-ch-fdvs-lsqv}
\end{equation}

We state without proof that the dimension
of ${\frak V} \otimes {\frak U}$ of an $n$-dimensional vector space ${\frak V}$
and  an  an $m$-dimensional vector space
${\frak U}$
is multiplicative,
that is, the dimension of  ${\frak V} \otimes {\frak U}$ is $nm$.
Informally, this is evident from the number of basis pairs ${\bf e}_i \otimes {\bf f}_j$.

\subsection{Representation}

A product of vectors ${\bf z} = {\bf x} \times {\bf y}$
has three equivalent notations or representations:
\begin{itemize}
\item[(i)]
as the scalar coordinates $x_iy_j$ with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
\item[(ii)]
as the quasi-matrix $z_{ij}  =x_iy_j$, whose components $z_{ij}$ are  defined with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded;
\item[(iii)]
as a quasi-vector or ``flattened matrix'' defined by the Kronecker product
${\bf z} = ({ x}_1  {\bf y}, { x}_2  {\bf y}, \ldots , { x}_n  {\bf y})=
({ x}_1  { y}_1, { x}_1  { y}_2, \ldots , { x}_n  { y}_n)
$. Again, the scalar coordinates $x_iy_j$ are defined
with respect to the basis in which the vectors ${\bf  x}$ and ${\bf y}$ have been defined and coded.
\index{Kronecker product}
\end{itemize}
In all three cases, the pairs $x_i y_j$  are properly represented by distinct mathematical entities.

{\color{blue}
\bexample
Take, for example,
${\bf x}=(2,3)$
and
${\bf y}=(5,7,11)$.
Then ${\bf z} = {\bf x} \otimes {\bf y}$  can be representated by
(i) the four scalars
$x_1y_1=10$,
$x_1y_2=14$,
$x_1y_3=22$,
$x_2y_1=15$,
$x_2y_2=21$,
$x_2y_3=33$,
or by
(ii) a $2 \times 3$ matrix
$
\begin{pmatrix}
10&14&22\\
15&21&33
\end{pmatrix}
$,
or by
(iii) a $4$-touple
$
(10,14,22,15,21,33)
$.
\eexample
}

Note, however, that this kind of quasi-matrix or quasi-vector representation of vector products
can be misleading insofar as
it (wrongly) suggests that all vectors in the tensor product space are accessible (representable) as quasi-vectors
-- they are, however, accessible by {\em linear superpositions} (\ref{2014-m-ch-fdvs-lsqv})
\index{linear superposition}
of such quasi-vectors. \label{2012-m-c-fdvs-entanglement}
\marginnote{In quantum mechanics this amounts to the fact that not all pure two-particle states can be
written in terms of (tensor) products of single-particle states; see also Section 1.5 of \cite{mermin-07}
}
For instance, take the arbitrary form of a (quasi-)vector in ${\Bbb C}^4$, which can be parameterized by
\begin{equation}
(\alpha_1,\alpha_2,\alpha_3,\alpha_4), \textrm{ with } \alpha_1,\alpha_3,\alpha_3,\alpha_4 \in {\Bbb C},
\label{2012-m-ch-fdvs-dectp-gf}
\end{equation}
and compare (\ref{2012-m-ch-fdvs-dectp-gf}) with the general form of a tensor product of two quasi-vectors in  ${\Bbb C}^2$
\begin{equation}
(a_1,a_2)\otimes (b_1,b_2)\equiv (a_1b_1, a_1 b_2,a_2b_1,a_2b_2), \textrm{ with } a_1,a_2,b_1,b_2\in {\Bbb C}.
\label{2012-m-ch-fdvs-dectp-gftp}
\end{equation}
A comparison of the coordinates in
(\ref{2012-m-ch-fdvs-dectp-gf})
and
(\ref{2012-m-ch-fdvs-dectp-gftp})
yields
\begin{equation}
\begin{split}
\alpha_1=a_1b_1,\\
\alpha_2=a_1b_2,\\
\alpha_3=a_2b_1,\\
\alpha_4=a_2b_2.
\end{split}
\label{2012-m-ch-fdvs-dectp-gftp-a}
\end{equation}
By taking the quotient of the two top and the two bottom equations and equating these quotients, one obtains
\begin{equation}
\begin{split}
\frac{\alpha_1}{\alpha_2}=\frac{b_1}{b_2}
=\frac{\alpha_3}{\alpha_4},\textrm{ and thus }\\
{\alpha_1}{\alpha_4}={\alpha_2}{\alpha_3},
\end{split}
\label{2012-m-ch-fdvs-dectp-gftp-fr}
\end{equation}
which amounts to a condition for the four coordinates  $\alpha_1,\alpha_2,\alpha_3,\alpha_4$
in order for this four-dimensional vector to be decomposable into a tensor product of two two-dimensional quasi-vectors.
In quantum mechanics, pure states which are not decomposable into a single tensor product
are called {\em entangled}.
\index{entanglement}


{\color{blue}
\bexample
\label{bellstate1}
A typical example of an entangled state is the
{\em Bell state}, \index{Bell state}  $\vert \Psi^- \rangle$
or, more generally, states in the Bell basis  \index{Bell basis}
\begin{equation}
\begin{split}
\vert \Psi^- \rangle = \frac{1}{\sqrt{2}}\left(\vert 0 \rangle \vert 1 \rangle - \vert 1 \rangle \vert 0 \rangle  \right),\\
\vert \Psi^+ \rangle = \frac{1}{\sqrt{2}}\left(\vert 0 \rangle \vert 1 \rangle + \vert 1 \rangle \vert 0 \rangle  \right),\\
\vert \Phi^- \rangle = \frac{1}{\sqrt{2}}\left(\vert 0 \rangle \vert 0 \rangle - \vert 1 \rangle \vert 1 \rangle  \right),\\
\vert \Phi^+ \rangle = \frac{1}{\sqrt{2}}\left(\vert 0 \rangle \vert 0 \rangle + \vert 1 \rangle \vert 1 \rangle  \right),
\end{split}
\label{2014-m-ch-fdvs-bellbasis}
\end{equation}
or just
\begin{equation}
\begin{split}
\vert \Psi^- \rangle = \frac{1}{\sqrt{2}}\left(\vert 0   1 \rangle - \vert 1   0 \rangle  \right),\\
\vert \Psi^+ \rangle = \frac{1}{\sqrt{2}}\left(\vert 0   1 \rangle + \vert 1   0 \rangle  \right),\\
\vert \Phi^- \rangle = \frac{1}{\sqrt{2}}\left(\vert 0   0 \rangle - \vert 1   1 \rangle  \right),\\
\vert \Phi^+ \rangle = \frac{1}{\sqrt{2}}\left(\vert 0   0 \rangle + \vert 1   1 \rangle  \right).
\end{split}
\label{2014-m-ch-fdvs-bellbasis2}
\end{equation}
For instance, in the case of $\vert \Psi^- \rangle$ a comparison of coefficient yields
\begin{equation}
\begin{split}
\alpha_1=a_1b_1=0,\\
\alpha_2=a_1b_2=\frac{1}{\sqrt{2}},\\
\alpha_3=a_2b_1-\frac{1}{\sqrt{2}},\\
\alpha_4=a_2b_2=0;
\end{split}
\label{2012-m-ch-fdvs-BellSCC}
\end{equation}
and thus  $${\alpha_1}{\alpha_4}=0 \neq {\alpha_2}{\alpha_3}=\frac{1}{2}.$$
This shows that  $\vert \Psi^- \rangle$ cannot be considered as a two particle state product.
Indeed, the state can only be characterized by considering the {\em joint properties}
of the two particles --
in the case of  $\vert \Psi^- \rangle$ they are associated with the statements~\cite{zeil-99}:
``the quantum numbers (in this case ``$0$'' and ``$1$'') of the two particles are always different.''

\eexample
}



\section{Linear transformation}
\marginnote{For proofs and additional information see \S 32-34 in   \cite{halmos-vs}}

\subsection{Definition}
A {\em linear transformation}, or, used synonymuosly, a {\em linear operator},
\index{linear transformation}
\index{linear operator}
$\textsf{\textbf{A}} $ on a vector space ${\frak V}$ is a correspondence that assigns every vector
${\bf x}\in {\frak V}$ a vector $\textsf{\textbf{A}} {\bf x}\in {\frak V}$,
in a linear way; such  that
\begin{equation}
\textsf{\textbf{A}}  (\alpha {\bf x}+ \beta {\bf y}) = \alpha \textsf{\textbf{A}}({\bf x})
+  \beta \textsf{\textbf{A}} ({\bf y}) = \alpha \textsf{\textbf{A}}{\bf x}
+  \beta \textsf{\textbf{A}} {\bf y},
\end{equation}
identically for all vectors ${\bf x},{\bf y}\in {\frak V}$ and all scalars $\alpha , \beta$.


\subsection{Operations}
The {\em sum}
\index{sum of transformations}
$\textsf{\textbf{S}} =\textsf{\textbf{A}} +\textsf{\textbf{B}} $
of two linear transformations $\textsf{\textbf{A}}$ and $\textsf{\textbf{B}} $
is defined by
$\textsf{\textbf{S}} {\bf x}=\textsf{\textbf{A}}{\bf x} +\textsf{\textbf{B}} {\bf x}$
for every ${\bf x}\in {\frak V}$.

The {\em product}
\index{product of transformations}
$\textsf{\textbf{P}} =\textsf{\textbf{A}} \textsf{\textbf{B}} $
of two linear transformations $\textsf{\textbf{A}}$ and $\textsf{\textbf{B}} $
is defined by
$\textsf{\textbf{P}} {\bf x}=\textsf{\textbf{A}}(\textsf{\textbf{B}} {\bf x})$
for every ${\bf x}\in {\frak V}$.

The notation
$\textsf{\textbf{A}}^n\textsf{\textbf{A}}^m=\textsf{\textbf{A}}^{n+m}$
and $(\textsf{\textbf{A}}^n)^m= \textsf{\textbf{A}}^{nm}$,
with $\textsf{\textbf{A}}^1=\textsf{\textbf{A}}$ and
$\textsf{\textbf{A}}^0 =\textsf{\textbf{1}}$ turns out to be useful.

With the exception of commutativity, all formal algebraic properties
of numerical addition and multiplication,
are valid for transformations; that is
$
\textsf{\textbf{A}}\textsf{\textbf{0}}=
\textsf{\textbf{0}}\textsf{\textbf{A}} =\textsf{\textbf{0}}
$,
$
\textsf{\textbf{A}}\textsf{\textbf{1}}=
\textsf{\textbf{1}}\textsf{\textbf{A}} =\textsf{\textbf{A}}
$,
$
\textsf{\textbf{A}} (\textsf{\textbf{B}}+\textsf{\textbf{C}})=
\textsf{\textbf{A}} \textsf{\textbf{B}}
+
\textsf{\textbf{A}} \textsf{\textbf{C}}
$,
$
(\textsf{\textbf{A}}+ \textsf{\textbf{B}})\textsf{\textbf{C}}=
\textsf{\textbf{A}} \textsf{\textbf{C}}
+
\textsf{\textbf{B}} \textsf{\textbf{C}}
$,  and
$
\textsf{\textbf{A}} (\textsf{\textbf{B}}\textsf{\textbf{C}})=
(\textsf{\textbf{A}} \textsf{\textbf{B}})
 \textsf{\textbf{C}}
$.
In {\em matrix notation},  $\textsf{\textbf{1}} ={\Bbb{1}}$, and the entries of $\textsf{\textbf{0}}$
are $0$ everywhere.

The {\em inverse operator}
\index{inverse operator}
$\textsf{\textbf{A}}^{-1}$
of $\textsf{\textbf{A}}$
is defined by
$\textsf{\textbf{A}}\textsf{\textbf{A}}^{-1}=\textsf{\textbf{A}}^{-1}\textsf{\textbf{A}}=
{\Bbb I}$.


The {\em commutator}
\index{commutator}
of two matrices $\textsf{\textbf{A}}$  and $\textsf{\textbf{B}}$ is defined by
\begin{equation}
[\textsf{\textbf{A}}, \textsf{\textbf{B}} ]
=
\textsf{\textbf{A}} \textsf{\textbf{B}}
-
 \textsf{\textbf{B}}      \textsf{\textbf{A}}.
\end{equation}
\marginnote{The commutator should not be confused with the bilinear funtional
introduced for dual spaces.}

The {\em polynomial}
\index{polynomial}
can be directly adopted from ordinary arithmetic; that is,
any finite polynomial $p$ of degree $n$
of an operator (transformation) $\textsf{\textbf{A}}$ can be written as
\begin{equation}
p(\textsf{\textbf{A}})= \alpha_0   \textsf{\textbf{1}}
+ \alpha_1   \textsf{\textbf{A}}^1
+ \alpha_2   \textsf{\textbf{A}}^2+
\cdots
+
\alpha_n   \textsf{\textbf{A}}^n
=\sum_{i=0}^n \alpha_i \textsf{\textbf{A}}^i
.
\end{equation}

The Baker-Hausdorff formula
 \begin{equation}
 e^{i\textsf{\textbf{A}}}\textsf{\textbf{B}}e^{-i\textsf{\textbf{A}}}=
B+i[\textsf{\textbf{A}},\textsf{\textbf{B}}]+
{i^2\over 2!}[\textsf{\textbf{A}},[\textsf{\textbf{A}},\textsf{\textbf{B}}]]+\cdots
 \end{equation}
for two arbitrary noncommutative linear operators $\textsf{\textbf{A}}$ and
$\textsf{\textbf{B}}$ is mentioned without proof
(cf.  Messiah, {\sl Quantum Mechanics, Vol. 1} \cite{messiah-61}).

If $[\textsf{\textbf{A}},\textsf{\textbf{B}}]$ commutes with $\textsf{\textbf{A}}$ and
$\textsf{\textbf{B}}$, then
 \begin{equation}
 e^\textsf{\textbf{A}}e^\textsf{\textbf{B}}=
e^{\textsf{\textbf{A}}+\textsf{\textbf{B}}+{1\over 2}\left[\textsf{\textbf{A}},\textsf{\textbf{B}}\right]}.
 \end{equation}

If  $\textsf{\textbf{A}}$ commutes with $\textsf{\textbf{B}}$, then
 \begin{equation}
 e^\textsf{\textbf{A}}e^\textsf{\textbf{B}}=
e^{\textsf{\textbf{A}}+\textsf{\textbf{B}}}.
 \end{equation}

\subsection{Linear transformations as matrices}

\index{matrix}
\index{transformation matrix}


Let ${\frak V}$ be an $n$-dimensional vector space;
let
${\frak B}=\{{\bf f}_1,{\bf f}_2,\ldots ,{\bf f}_n\}$ be any basis of ${\frak V}$,
and let  $\textsf{\textbf{A}}$ be a linear transformation on ${\frak V}$.

Because every vector is a linear combination of the basis vectors
${\bf f}_i$,
every linear transformation can be defined by
``its performance on the basis vectors;'' that is,
by the particular mapping of
all $n$ basis vectors into the transformed vectors, which in turn can be represented as linear combination of the $n$ basis vectors.

Therefore it is possible to define some $n \times n$ matrix with $n^2$ coefficients or coordinates
$\alpha_{ij}$ such that
\begin{equation}
\textsf{\textbf{A}} {\bf f}_j = \sum_i \alpha_{ij}{\bf f}_i
\end{equation}
for all $j=1,\ldots ,n$.
Again, note that this definition of a {\em transformation matrix}
is ``tied to'' a basis.

For {\em orthonormal bases}
there is an even closer connection -- representable as scalar product -- between a matrix
defined by an $n$-by-$n$ square array and the representation in terms of the elements of the bases; that is,
\begin{equation}
\begin{split}
\textsf{\textbf{A}}
\equiv \langle i\vert A\vert j\rangle =
 \langle {\bf f}_i \vert A \vert {\bf f}_j\rangle
= \langle {\bf f}_i \vert A   {\bf f}_j\rangle \\
= \langle {\bf f}_i \vert \sum_l \alpha_{lj}{\bf f}_l \rangle
=  \sum_l \alpha_{lj} \langle {\bf f}_i \vert {\bf f}_l \rangle
=  \sum_l \alpha_{lj} \delta_{il}  = \alpha_{ij} \\
\equiv
\begin{pmatrix}
\alpha_{11}&
\alpha_{12}&
\cdots    &
\alpha_{1n}\\
\alpha_{21}&
\alpha_{22}&
\cdots    &
\alpha_{2n}\\
\vdots&
\vdots&
\cdots    &
\vdots\\
\alpha_{n1}&
\alpha_{n2}&
\cdots   &
\alpha_{nn}
\end{pmatrix}
.
\end{split}
\end{equation}





{\color{blue}
\bexample
In terms of this matrix notation, it is quite easy to present an example
for which the commutator
$
[\textsf{\textbf{A}}, \textsf{\textbf{B}} ]
$
does not vanish; that is
$\textsf{\textbf{A}}$  and $\textsf{\textbf{B}}$
do not commute.

Take, for the sake of an example, the
{\em Pauli spin matrices}
\index{Pauli spin matrices}
which are proportional to the angular momentum operators along the $x,y,z$-axis
\cite{schiff-55}:
\begin{equation}
\begin{split}
\sigma_1=\sigma_x
=
\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}
,   \\
\sigma_2=\sigma_y
=
\begin{pmatrix}
0&-i\\
i&0
\end{pmatrix}
,   \\
\sigma_3=\sigma_z
=
\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
.
\end{split}
\end{equation}
Together with unity, i.e., ${\Bbb I}_2=\textrm{diag}(1,1)$,
they form a complete basis of all $(4\times 4)$ matrices.
Now take, for instance, the commutator
\begin{equation}
\begin{split}
[\sigma_1,\sigma_3]= \sigma_1\sigma_3-\sigma_3\sigma_1\\
\qquad
=
\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}
\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
-
\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}
\\
\qquad
=  2
\begin{pmatrix}
0&-1\\
1&0
\end{pmatrix}
\neq
\begin{pmatrix}
0&0\\
0&0
\end{pmatrix}
.    \textrm{\eexample}
\end{split}
\end{equation}
%\eexample
}

\section{Projection or projection operator}
\label{2011-m-projec}
\marginnote{For proofs and additional information see \S 41 in    \cite{halmos-vs}}
\index{projection}
\index{projection operator}

The more I learned about quantum mechanics the more
I realized the importance of projection operators for its conceptualization \cite{v-neumann-49,v-neumann-55,birkhoff-36}:
\begin{itemize}
\item[(i)]
Projections represent (pure) states.
\index{pure state}
\item[(ii)]
Mixed states, should they ontologically exist, can be composed from projections by summing over projectors.
\item[(iii)]
Projectors serve as the most elementary obsevables -- they correspond to yes-no propositions.
\item[(iv)]
In Section~\ref{2012-m-ch-Spectraltheorem} we will learn
that every observable can be (de-)composed into weighted (spectral) sums of projections.
\item[(v)]
Furthermore, from dimension three onwards, Gleason's theorem (cf. Section~\ref{Gleasontheorem}) allows
quantum probability theory to be based upon maximal (in terms of co-measurability) ``quasi-classical''
blocks of projectors.
\item[(vi)]
Such maximal blocks of projectors can be bundled together to show (cf. Section~\ref{2011-m-KST})
that the corresponding algebraic
structure has no two-valued measure (interpretable as truth assignment), and
therefore cannot be ``embedded'' into a ``larger'' classical (Boolean) algebra.
\end{itemize}



\subsection{Definition}
If ${\frak V}$ is the direct sum of some subspaces
${\frak M}$
and
${\frak N}$
so that every ${\bf z} \in {\frak V}$ can be uniquely written in the form
$
{\bf z}
=
{\bf x}
+
{\bf y}
$, with
${\bf x} \in {\frak M}$
and with
${\bf y} \in {\frak N}$,
then
the {\em projection}, or, used synonymuosly,
{\em projection}
on ${\frak M}$
along ${\frak N}$ is the transformation $\textsf{\textbf{E}}$
defined by $\textsf{\textbf{E}}{\bf z}={\bf x}$.
Conversely,
 $\textsf{\textbf{F}}{\bf z}={\bf y}$  is the projection
on ${\frak N}$
along ${\frak M}$.

A (nonzero) linear transformation
$\textsf{\textbf{E}}$ is a projection if and only if
it is idempotent; that is,
\index{idempotence}
$\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}\neq 0$.

{\color{OliveGreen}
\bproof
For a proof note that, if $\textsf{\textbf{E}}$  is the projection
on ${\frak M}$
along ${\frak N}$,
and if
$
{\bf z}
=
{\bf x}
+
{\bf y}
$, with
${\bf x} \in {\frak M}$
and with
${\bf y} \in {\frak N}$,
the decomposition of ${\bf x}$ yields
${\bf x}+0$, so that
$\textsf{\textbf{E}}^2{\bf z}=\textsf{\textbf{E}}\textsf{\textbf{E}}{\bf z}=\textsf{\textbf{E}}{\bf x}
={\bf x}=\textsf{\textbf{E}}{\bf z}$.
The converse --
idempotence
``$\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}$''
implies that $\textsf{\textbf{E}}$ is a projection -- is more difficult to prove.
For this proof we refer to the literature; e.g., Halmos \cite{halmos-vs}.
\eproof
}

We also mention without proof that a linear transformation
$\textsf{\textbf{E}}$ is a projection if and only if
$\textsf{\textbf{1}}-\textsf{\textbf{E}}$ is a projection.
Note that $(\textsf{\textbf{1}}-\textsf{\textbf{E}})^2
=\textsf{\textbf{1}}-\textsf{\textbf{E}}-\textsf{\textbf{E}}+ \textsf{\textbf{E}}^2
=\textsf{\textbf{1}}-\textsf{\textbf{E}}$;
furthermore,
$
\textsf{\textbf{E}}(\textsf{\textbf{1}}-\textsf{\textbf{E}})=
(\textsf{\textbf{1}}-\textsf{\textbf{E}})\textsf{\textbf{E}}=
\textsf{\textbf{E}}- \textsf{\textbf{E}}^2=0
$.


Furthermore, if $\textsf{\textbf{E}}$  is the projection
on ${\frak M}$
along ${\frak N}$,
then
 $\textsf{\textbf{1}}-\textsf{\textbf{E}}$ is the projection
on ${\frak N}$
along ${\frak M}$.




\subsection{Construction of projections from unit vectors}

How can we construct projections from unit vectors, or systems of orthogonal projections from some vector in some orthonormal basis
with the standard dot product?

Let ${\bf x}$ be the coordinates of a unit vector;
that is $\|{\bf x} \| =1$.
Transposition is indicated by the superscript ``$T$''
in real vector space.
In complex vector space the transposition has to be substituted
 for the {\em conjugate transpose} (also denoted as
{\em Hermitian conjugate} or {\em Hermitian adjoint}),
\index{conjugate transpose}
\index{Hermitian conjugate}
\index{Hermitian adjoint}
``$\dagger$,'' standing for transposition and complex conjugation of the coordinates.
More explicitly,
\begin{equation}(x_1,\ldots, x_n)^T=
\begin{pmatrix}
x_1\\ \vdots\\ x_n
\end{pmatrix}
,
\end{equation}
\begin{equation}
\begin{pmatrix}
x_1\\ \vdots\\ x_n
\end{pmatrix}^T
=
(x_1,\ldots, x_n)
,\end{equation}
and
\begin{equation} (x_1,\ldots, x_n)^\dagger =
%\left(
\begin{pmatrix}
\overline{x_1}\\ \vdots\\ \overline{x_n}
\end{pmatrix}
,
\end{equation}
\begin{equation}
%\left(
\begin{pmatrix}
x_1\\ \vdots\\ x_n
\end{pmatrix}^\dagger
= (\overline{x_1},\ldots, \overline{x_n})
.
\end{equation}
Note that, just as
\begin{equation}\left({\bf x}^T\right)^T={\bf x},
\end{equation}
 so is
\begin{equation}\left({\bf x}^\dagger \right)^\dagger={\bf x}.
\end{equation}

In real vector space, the {\em dyadic product}, or {\em tensor product}, or {\em outer product}
\index{outer product}
\index{dyadic product}
\index{tensor product}
(also in Dirac's bra and ket notation),
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_{\bf x} = {\bf x} \otimes {\bf x}^T = \vert{\bf x}\rangle \langle {\bf x}\vert
\\
\qquad
=
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
(x_1,x_2,\ldots ,x_n)\\
\qquad
=
%\left(
\begin{pmatrix}
x_1(x_1,x_2,\ldots ,x_n)\\
x_2(x_1,x_2,\ldots ,x_n)\\
\vdots  \\
x_n(x_1,x_2,\ldots ,x_n)
\end{pmatrix}
\\
\qquad
=
%\left(
\begin{pmatrix}
x_1x_1&x_1x_2& \cdots&x_1x_n\\
x_2x_1&x_2x_2& \cdots&x_2x_n\\
\vdots & \vdots & \vdots &\vdots \\
x_nx_1&x_nx_2& \cdots&x_nx_n
\end{pmatrix}
\end{split}
\end{equation}
is the projection
associated with ${\bf x}$.

If the vector ${\bf x}$ is not normalized,
then the associated projection is
\begin{equation}
\textsf{\textbf{E}}_{\bf x} = \frac{{\bf x} \otimes {\bf x}^T}{\langle {\bf x}\mid {\bf x}\rangle}
= \frac{\vert{\bf x}\rangle \langle {\bf x}\vert}{\langle {\bf x}\mid {\bf x}\rangle}
\end{equation}
This construction is related to
$P_{\bf x}$ on page \pageref{2011-m-gsp}
by $P_{\bf x}({\bf y})=\textsf{\textbf{E}}_{\bf x}{\bf y}$.

{\color{OliveGreen}
\bproof
For a proof, consider only normalized vectors ${\bf x}$, and
let $\textsf{\textbf{E}}_{\bf x} = {\bf x}\otimes {\bf x}^T $,
then
$$
\textsf{\textbf{E}}_{\bf x}\textsf{\textbf{E}}_{\bf x}
=
(\vert{\bf x}\rangle \langle {\bf x}\vert)
(\vert{\bf x}\rangle \langle {\bf x}\vert)
=
\vert{\bf x}\rangle \langle {\bf x}\vert {\bf x}\rangle \langle {\bf x}\vert
=\vert{\bf x}\rangle \cdot 1 \cdot \rangle \langle {\bf x}\vert
=  \textsf{\textbf{E}}_{\bf x}.
$$
More explicitly, by writing out the coordinate tuples, the equivalent proof is
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_{\bf x}\textsf{\textbf{E}}_{\bf x}
= ({\bf x}\otimes {\bf x}^T ) \cdot ({\bf x}\otimes {\bf x}^T )
\\
\qquad
=
\left(
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
(x_1,x_2,\ldots ,x_n)
\right)
\left(
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
(x_1,x_2,\ldots ,x_n)
 \right)
\\
\qquad
=
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
(x_1,x_2,\ldots ,x_n)
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
(x_1,x_2,\ldots ,x_n)
\\
\qquad
=
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
\cdot 1 \cdot
(x_1,x_2,\ldots ,x_n)
=
\begin{pmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{pmatrix}
(x_1,x_2,\ldots ,x_n)
=\textsf{\textbf{E}}_{\bf x}. \textrm{\eproof }
\end{split}
\end{equation}
}

{
\color{blue}
\bexample
Fo two examples, let
${\bf x}=(1,0)^T$
and
${\bf y}=(1,-1)^T$;
then
$$
\textsf{\textbf{E}}_{\bf x}
=
\begin{pmatrix}
1\\
0
\end{pmatrix}
(1,0)
=
\begin{pmatrix}
1(1,0)\\
0(1,0)
\end{pmatrix}
=
\begin{pmatrix}
1&0\\
0&0
\end{pmatrix}
,
$$
and
$$
\textsf{\textbf{E}}_{\bf y}
= \frac{1}{2}
\begin{pmatrix}
1\\
-1
\end{pmatrix}
(1,-1)
= \frac{1}{2}
\begin{pmatrix}
1(1,-1)\\
-1(1,-1)
\end{pmatrix}
= \frac{1}{2}
\begin{pmatrix}
1&-1\\
-1&1
\end{pmatrix}
.
\textrm{\eexample}
$$
%\eexample
}

Note also that
\begin{equation}
\textsf{\textbf{E}}_{\bf x} {\bf y}= \langle {\bf x}\vert  {\bf y} \rangle {\bf x},
\end{equation}
which can be directly proven by insertion.


%\subsection{Combination of projections}



\section{Change of basis}
\label{2012-m-ch-fdlvs-changeofbasis}
\index{change of basis}
\marginnote{For proofs and additional information see \S 46 in    \cite{halmos-vs}}
\index{basis change}
\index{change of basis}

Let ${\frak V}$ be an $n$-dimensional vector space and let
${\frak X}
=
\{
{\bf e}_1,
\ldots ,
{\bf e}_n
\}$
and
${\frak Y}
=  \{
{\bf f}_1,
\ldots ,
{\bf f}_n
\}$ be two bases of ${\frak V}$.

Take an arbitrary vector ${\bf z}\in {\frak V}$.
In terms of the two bases
${\frak X}$ and
${\frak Y}$,
${\bf z}$ can be written as
\begin{equation}
{\bf z}=
\sum_{i=1}^n x^i{\bf e}_i
=
\sum_{i=1}^n  y^i{\bf f}_i,
\label{2011-m-btbexy}
\end{equation}
where $x^i$ and $y^i$ stand for the coordinates of the vector  ${\bf z}$
with respect to the bases ${\frak X}$ and
${\frak Y}$,
respectively.

The following questions arise:
\begin{itemize}
\item[(i)]
What is the relation between the ``corresponding'' basis vectors ${\bf e}_i$ and ${\bf f}_j$?
\item[(ii)]
What is the relation between the coordinates $x^i$ (with respect to the basis  ${\frak X}$) and $y^j$ (with respect to the basis  ${\frak Y}$)  of the vector ${\bf z}$ in Eq.~(\ref{2011-m-btbexy})?
\item[(iii)]
Suppose one fixes the coordinates, say, $\{
{v}_1,
\ldots ,
{v}_n
\}$, what is the relation beween the vectors
${\bf v}=
\sum_{i=1}^n v^i{\bf e}_i
$
and
${\bf w}=
\sum_{i=1}^n v^i{\bf f}_i
$?
\end{itemize}

As an {\it Ansatz} for answering question (i), recall that, just like any other vector in ${\frak V}$,
the new basis vectors ${\bf f}_i$ contained in the new basis ${\frak Y}$
can be (uniquely) written as a {\em linear combination}
(in quantum physics called {\em linear superposition})
\index{linear combination}
\index{linear superposition}
of the basis vectors
${\bf e}_i$ contained in the old  basis ${\frak X}$.
This can be defined {\it via}
a linear transformation $\textsf{\textbf{A}}$ between the corresponding vectors of the bases
 ${\frak X}$ and
${\frak Y}$ by
\begin{equation}
{\bf f}_i= (\textsf{\textbf{A}}{\bf e})_i
,
\label{2011-m-btbe}
\end{equation}
for all $i=1, \ldots , n$.
%
More specifically, let ${a_i}^j$ be the matrix of the linear transformation $\textsf{\textbf{A}}$
in the basis
${\frak X}
=
\{
{\bf e}_1,
\ldots ,
{\bf e}_n
\}$,
and let us rewrite (\ref{2011-m-btbe}) as a matrix equation
\begin{equation}
{\bf f}_i= \sum_{j=1}^n {a_i}^j {\bf e}_j
.
\label{2011-m-btbe-r}
\end{equation}
If $\textsf{\textbf{A}}$ stands for the matrix whose components (with respect to ${\frak X}$) are  ${a_i}^j$,
and $\textsf{\textbf{A}}^T$
stands for the transpose of $\textsf{\textbf{A}}$
whose components (with respect to ${\frak X}$) are  ${a^j}_i$,
then
\begin{equation}
\begin{pmatrix}
{\bf f}_1\\
{\bf f}_2\\
\vdots\\
{\bf f}_n
\end{pmatrix}
= \textsf{\textbf{A}}
\begin{pmatrix}
{\bf e}_1\\
{\bf e}_2\\
\vdots\\
{\bf e}_n
\end{pmatrix}
.
\label{2011-m-btbe-r1}
\end{equation}

%\marginpar{Please have a look  at {\footnotesize {\tt
%http://ocw.mit.edu/courses/mathematics/}\\
%{\tt 18-06-linear-algebra-spring-2010/}\\
%{\tt video-lectures/}\\
%{\tt lecture-31-change-of-basis-}\\
%{\tt image-compression/} \\
%{ }
%}
%}
That is, very explicitly,
\begin{equation}
\begin{split}
{\bf f}_1=
(\textsf{\textbf{A}}{\bf e})_1
=
{a_1}^1 {\bf e}_1 +{a_1}^2 {\bf e}_2+\cdots +{a^n}_{1} {\bf e}_n    = \sum_{i=1}^n {a^i}_{1}{\bf e}_i,\\
{\bf f}_2=
(\textsf{\textbf{A}}{\bf e})_2
=
{a_2}^1 {\bf e}_1 +{a_2}^2 {\bf e}_2+\cdots +{a^n}_{2} {\bf e}_n    = \sum_{i=1}^n {a^i}_{2}{\bf e}_i,\\
 \vdots \\
{\bf f}_n=
(\textsf{\textbf{A}}{\bf e})_n
= {a^1}_{n} {\bf e}_1 +{a^2}_{n} {\bf e}_2+\cdots +{a^n}_{n} {\bf e}_n    = \sum_{i=1}^n {a^i}_{n}{\bf e}_i.
\end{split}
\label{2011-m-btbe-eigen}
\end{equation}

This implies
\begin{equation}
\sum_{i=1}^n v^i {\bf f}_i= \sum_{i=1}^n v^i (\textsf{\textbf{A}}{\bf e})_i
= \textsf{\textbf{A}} \left(\sum_{i=1}^n v^i{\bf e}_i\right)
.
\label{2011-m-btbe-2-implied}
\end{equation}

\begin{itemize}
\item
Note   that the $n$ equalities (\ref {2011-m-btbe-eigen})
really represent $n^2$ linear equations for the $n^2$
unknowns ${a_i}^j$, $1\le i,j\le n$, since every pair of basis vectors
$\{{\bf f}_i,{\bf e}_i\}$, $1\le i\le n$ has $n$ components or coefficients.

\item
If one knows how the basis vectors
$
\{
{\bf e}_1,
\ldots ,
{\bf e}_n
\}$ of ${\frak X}$    transform, then one knows (by linearity) how
all other vectors
${\bf v}=
\sum_{i=1}^n v^i{\bf e}_i
$
(represented in this basis) transform; namely
$\textsf{\textbf{A}}({\bf v})=
\sum_{i=1}^n v^i (\textsf{\textbf{A}}{\bf e})_i
$.

\item
Finally note that, if  ${\frak X}$ is an orthonormal basis,
then the basis transformation has a diagonal form
\begin{equation}
\textsf{\textbf{A}} =   \sum_{i=1}^n  {\bf f}_i^\dagger  {\bf e}_i
=
\sum_{i=1}^n \vert {\bf f}_i \rangle \langle {\bf e}_i \vert
\label{2013-m-ch-fdvs-dftm}
\end{equation}
because all the off-diagonal components $a_{ij}$, $i\neq j$ of $\textsf{\textbf{A}}$
explicitely written down in Eqs.(\ref{2011-m-btbe-eigen}) vanish.
This can be easily checked by applying $\textsf{\textbf{A}}$ to the elements ${\bf e}_i $ of the basis ${\frak X}$.
See also Section
\ref{2012-m-ch-citoob} on page \pageref{2012-m-ch-citoob}
for a representation of unitary transformations in terms of basis changes.
In quantum mechanics, the temporal evolution is represented by nothing but a change of orthonormal bases in Hilbert space.
\end{itemize}

Having settled question (i) by the {\it Ansatz}
(\ref{2011-m-btbe}),
we turn to question (ii) next.
Since
\begin{equation}
{\bf z} =
 \sum_{j=1}^n y^j {\bf f}_j=
 \sum_{j=1}^n  y^j (\textsf{\textbf{A}}{\bf e})_j=
 \sum_{j=1}^n  y^j  \sum_{i=1}^n {a_j}^i {\bf e}_i=
  \sum_{i=1}^n \left(\sum_{j=1}^n  {a_j}^i y^j \right)   {\bf e}_i;
\end{equation}
we obtain by comparison of the coefficients in Eq. (\ref{2011-m-btbexy}),
\begin{equation}
x^i= \sum_{j=1}^n {a_j}^i y^j.
\label{2012-m-ch-e-tl1}
\end{equation}
That is, in terms of the ``old'' coordinates $x^i$,
the ``new'' coordinates are
\begin{equation}
\sum_{i=1}^n {(a^{-1})_i}^l x^i= \sum_{i=1}^n {(a^{-1})_i}^l  \sum_{j=1}^n  {a_j}^i y^j
=  \sum_{i=1}^n \sum_{j=1}^n {(a^{-1})_i}^l  {a_j}^i y^j
=  \sum_{i=1}^n \sum_{j=1}^n {a_j}^i {(a^{-1})_i}^l  y^j
=   \sum_{j=1}^n \delta^l_j y^j
=  y^{l}
.
\label{2012-m-ch-e-tl2}
\end{equation}

If we prefer to represent the vector coordinates of
${\bf x}$ and ${\bf y}$ as $n$-tuples,
then Eqs.~(\ref{2012-m-ch-e-tl1})  and (\ref{2012-m-ch-e-tl2})
have an interpretation as matrix multiplication; that is,
\begin{equation}
{\bf x} =  \textsf{\textbf{A}}^T{\bf y}, {\text{ and }}
{\bf y} =  (\textsf{\textbf{A}}^{-1})^T{\bf x}
.
\label{2012-m-ch-e-tl3}
\end{equation}

Finally, let us answer question (iii)
by substituting the {\it Ansatz} ${\bf f}_i  =\textsf{\textbf{A}}{\bf e}_i$ defined in  Eq.~(\ref{2011-m-btbe}),
while considering
\begin{equation}
{\bf w}= \sum_{i=1}^n v^i {\bf f}_i  = \sum_{i=1}^n v^i \textsf{\textbf{A}}{\bf e}_i= \textsf{\textbf{A}} \sum_{i=1}^n v^i {\bf e}_i
= \textsf{\textbf{A}} \left( \sum_{i=1}^n v^i {\bf e}_i \right)
=   \textsf{\textbf{A}}{\bf v}
.
\end{equation}

%In matrix notation,
%\begin{equation}
% \textsf{\textbf{A}}\left(\sum_{i=1}^n x^i{\bf e}_i\right)  =   \sum_{i=1}^n x^i{\bf f}_i
%.
%\end{equation}



{
\color{blue}
\bexample
For the sake of an example,
\begin{enumerate}

\item
\begin{marginfigure}%
% This is a LaTeX picture output by TeXCAD.
% File name: [1.pic].
% Version of TeXCAD: 4.3
% Reference / build: 30-Jun-2012 (rev. 105)
% For new versions, check: http://texcad.sf.net/
% Options on the following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{6.7272}
\unitlength 0.3mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(200,108.5)(30,0)
\put(100,0){\vector(1,0){100}}
\put(100,0){\vector(0,1){100}}
\put(100,0){\color{orange}\vector(1,1){70.25}}
\put(100,0){\color{orange}\vector(-1,1){70.25}}
\put(199.25,6.5){\makebox(0,0)[cc]{${\bf x}_1 =(1,0)^T$}}
\put(100,108.5){\makebox(0,0)[cc]{${\bf x}_2 =(0,1)^T$}}
\put(50,82.5){\makebox(0,0)[cc]{\color{orange}${\bf y}_2 =\frac{1}{\sqrt{2}}(-1,1)^T$}}
\put(160,82.5){\makebox(0,0)[cc]{\color{orange}${\bf y}_1 =\frac{1}{\sqrt{2}}(1,1)^T$}}
{\color{orange}
\qbezier(139.25,1.25)(139.25,20.875)(132.25,29)
\qbezier(99,46)(81.25,45)(70.5,33)
%\vector(70.609,33.149)(69.569,31.96)
\put(69.569,31.96){\vector(-1,-1){.07}}\multiput(70.609,33.149)(-.0335663,-.0383615){31}{\line(0,-1){.0383615}}
%\end
%\vector(132.597,28.69)(131.407,30.027)
\put(131.407,30.027){\vector(-1,1){.07}}\multiput(132.597,28.69)(-.03303353,.03716272){36}{\line(0,1){.03716272}}
%\end
\put(145.232,16.798){\makebox(0,0)[lc]{$\varphi = \frac{\pi}{4}$}}
\put(79.082,55){\makebox(0,0)[cc]{$\varphi = \frac{\pi}{4}$}}
}
\end{picture}
\caption{Basis change by rotation of $\varphi = \frac{\pi}{4}$ around the origin.}
  \label{2012-m-basischange}
\end{marginfigure}
consider
a change of basis in the plane ${\Bbb R}^2$ by rotation of an angle $\varphi = \frac{\pi}{4}$ around the origin,
depicted in Fig.~\ref{2012-m-basischange}.
According to Eq.~(\ref{2011-m-btbe}),
we have
\begin{equation}
\begin{split}
{\bf f}_1=   {a_1}^1{\bf e}_1 +  {a_2}^1{\bf e}_2,\\
{\bf f}_2=   {a_1}^2{\bf e}_1 +  {a_2}^2{\bf e}_2
,
\end{split}
\end{equation}
which amounts to four linear equations in the four unknowns ${a_1}^1$, ${a_1}^2$,
${a_2}^1$, and ${a_2}^2$.
By
inserting the basis vectors
$ {\bf x}_1$, ${\bf x}_2$, ${\bf y}_1$, and ${\bf y}_2$
one obtains for the rotation matrix with respect to the basis ${\frak X}$
\begin{equation}
\begin{split}
\frac{1}{\sqrt{2}}
%\left(
\begin{pmatrix}
1,1\\
-1,1
\end{pmatrix}
=
%\left(
\begin{pmatrix}
{a_1}^1&{a_2}^1\\
{a_1}^2&{a_2}^2
\end{pmatrix}
%\left(
\begin{pmatrix}
1,0 \\
0,1
\end{pmatrix}
,
\end{split}
\end{equation}
the first pair of equations yielding
${a_1}^1={a_2}^1=\frac{1}{\sqrt{2}}$,
the second pair of equations yielding
${a_1}^2=-\frac{1}{\sqrt{2}}$ and ${a_2}^2=\frac{1}{\sqrt{2}}$.
Thus,
\begin{equation}
 \textsf{\textbf{A}}=
%\left(
\begin{pmatrix}
{a_1}^1&{a_1}^2\\
{a_2}^1&{a_2}^2
\end{pmatrix}
=
\frac{1}{\sqrt{2}}
%\left(
\begin{pmatrix}
1&-1\\
1&1
\end{pmatrix}
.
\end{equation}

As both coordinate systems ${\frak X}
=
\{
{\bf e}_1,
{\bf e}_2
\}$
and
${\frak Y}
=  \{
{\bf f}_1,
{\bf f}_2
\}$ are orthogonal, we might have just computed
the diagonal form (\ref{2013-m-ch-fdvs-dftm})
\begin{equation}
\begin{split}
 \textsf{\textbf{A}}=
\frac{1}{\sqrt{2}}
\left[
\begin{pmatrix}
1\\
1
\end{pmatrix}
\begin{pmatrix}
1,0
\end{pmatrix}
+
\begin{pmatrix}
-1\\
1
\end{pmatrix}
\begin{pmatrix}
0,1
\end{pmatrix}
\right] \\
=
\frac{1}{\sqrt{2}}
\left[
\begin{pmatrix}
1(1,0)\\
1 (1,0)
\end{pmatrix}
+
\begin{pmatrix}
-1(0,1)\\
1(0,1)
\end{pmatrix}
\right] \\
=
\frac{1}{\sqrt{2}}
\left[
\begin{pmatrix}
1&0\\
1&0
\end{pmatrix}
+
\begin{pmatrix}
0&-1\\
0&1
\end{pmatrix}
\right]
=
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1&-1\\
1&1
\end{pmatrix}
.
\end{split}
\end{equation}


Likewise, the rotation matrix with respect to the basis ${\frak Y}$  is
\begin{equation}
\begin{split}
 \textsf{\textbf{A}}'=
\frac{1}{\sqrt{2}}
\left[
\begin{pmatrix}
1\\
0
\end{pmatrix}
\begin{pmatrix}
1,1
\end{pmatrix}
+
\begin{pmatrix}
0\\
1
\end{pmatrix}
\begin{pmatrix}
-1,1
\end{pmatrix}
\right]
=
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1&1\\
-1&1
\end{pmatrix}
.
\end{split}
\end{equation}



\item
By a similar calculation, taking into account the definition for the sine and cosine functions,
one obtains the transformation matrix $\textsf{\textbf{A}}(\varphi )$
associated with an arbitrary angle $\varphi$,
\begin{equation}
 \textsf{\textbf{A}}
=
%\left(
\begin{pmatrix}
\cos \varphi &-\sin \varphi\\
\sin \varphi &\cos \varphi
\end{pmatrix}
.
\label{2012-m-ch-fdvs-otd2}
\end{equation}
The coordinates transform as
\begin{equation}
 \textsf{\textbf{A}}^{-1}
=
%\left(
\begin{pmatrix}
\cos \varphi &\sin \varphi\\
-\sin \varphi &\cos \varphi
\end{pmatrix}
.
\end{equation}

\item
Consider the more general rotation depicted in Fig.~\ref{2012-m-basischange1}.
Again, by inserting the basis vectors
$ {\bf e}_1,{\bf e}_2, {\bf f}_1$, and ${\bf f}_2$,
one obtains
\begin{equation}
\frac{1}{{2}}
%\left(
\begin{pmatrix}
\sqrt{3},1\\
1,\sqrt{3}
\end{pmatrix}
=
%\left(
\begin{pmatrix}
{a_1}^1&{a_2}^1\\
{a_1}^2&{a_2}^2
\end{pmatrix}
%\left(
\begin{pmatrix}
1,0\\
0,1
\end{pmatrix}
,
\end{equation}
yielding
${a_1}^1={a_2}^2=\frac{\sqrt{3}}{2}$,
the second pair of equations yielding
${a_1}^2= {a_2}^1=\frac{1}{{2}}$.
\begin{marginfigure}%
% This is a LaTeX picture output by TeXCAD.
% File name: [2.pic].
% Version of TeXCAD: 4.3
% Reference / build: 30-Jun-2012 (rev. 105)
% For new versions, check: http://texcad.sf.net/
% Options on the following lines.
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\pvinsert{% Your \input, \def, etc. here}
%\quality{8.000}
%\graddiff{0.005}
%\snapasp{1}
%\zoom{8.0000}
\unitlength 0.3mm % = 2.845pt
\linethickness{0.4pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(110,108.5)(0,0)
\put(10,0){\vector(1,0){100}}
\put(10,0){\vector(0,1){100}}
\put(109.25,6.5){\makebox(0,0)[cc]{${\bf x}_1 =(1,0)^T$}}
\put(10,108.5){\makebox(0,0)[cc]{${\bf x}_2 =(0,1)^T$}}
\put(61,95){\color{orange}\makebox(0,0)[cc]{${\bf y}_2 =\frac{1}{2}(1,\sqrt{3})^T$}}
\put(96.75,56){\color{orange}\makebox(0,0)[cc]{${\bf y}_1 =\frac{1}{2}(\sqrt{3},1)^T$}}
\put(25,55){\color{orange}\makebox(0,0)[cc]{$\varphi = \frac{\pi}{6}$}}
\put(65,11){\color{orange}\makebox(0,0)[cc]{$\varphi = \frac{\pi}{6}$}}
{\color{orange}
%\vector(10,0)(96.75,48.375)
\put(96.75,48.375){\vector(2,1){.07}}\multiput(10,0)(.06049486066,.03373416581){1434}{\line(1,0){.06049486066}}
%\end
%\vector(10,0)(58.375,86.75)
\put(58.375,86.75){\vector(1,2){.07}}\multiput(10,0)(.03373416581,.06049486066){1434}{\line(0,1){.06049486066}}
%\end
%\qbezvec(49.375,.375)(49.375,15.875)(46.875,20.375)
\put(46.875,20.375){\vector(-1,2){.07}}\qbezier(49.375,.375)(49.375,15.875)(46.875,20.375)
%\end
%\qbezvec(10.5,46.125)(20.562,45.5)(31.375,39.375)
\put(31.375,39.375){\vector(2,-1){.07}}\qbezier(10.5,46.125)(20.562,45.5)(31.375,39.375)
%\end
}
\end{picture}
\caption{More general basis change by rotation.}
  \label{2012-m-basischange1}
\end{marginfigure}
Thus,
\begin{equation}
 \textsf{\textbf{A}}=
%\left(
\begin{pmatrix}
a&b\\
b&a
\end{pmatrix}
=
\frac{1}{{2}}
%\left(
\begin{pmatrix}
\sqrt{3}&1\\
1&\sqrt{3}
\end{pmatrix}
.
\end{equation}
The coordinates transform according to the inverse transformation, which in this case can be represented by
\begin{equation}
 \textsf{\textbf{A}}^{-1}=
\frac{1}{{a^2-b^2}}
%\left(
\begin{pmatrix}
a &-b\\
-b&a
\end{pmatrix}
=
%\left(
\begin{pmatrix}
\sqrt{3}&-1\\
-1&\sqrt{3}
\end{pmatrix}
.
\end{equation}

\end{enumerate}


\eexample
}


\section{Mutually unbiased bases}
\index{mutually unbiased bases}

Two  orthonormal bases
${\frak B} =\{
{\bf e}_1,
\ldots ,
{\bf e}_n
\}$
and
${\frak B}'=\{
{\bf f}_1,
\ldots ,
{\bf f}_n
\}$
are said to be {\em mutually unbiased}
if
their scalar or inner products are
\begin{equation}
\vert \langle {\bf e}_i\vert {\bf f}_j  \rangle \vert^2
=
\frac{1}{n}
\end{equation}
for all $1\le i,j\le n$.
Note without proof -- that is, you do not have to be concerned
that you need to understand  this from what has been said so far --
that ``the elements of two or more mutually unbiased bases are mutually maximally apart.''

{\color{Purple}
In physics, one seeks maximal sets of orthogonal bases who
are maximally apart \cite{WooFie,durt} .
Such maximal sets of bases are used in quantum information theory
to assure maximal performance of certain protocols
used in quantum cryptography, or for the production of
quantum random sequences by beam splitters.
They are essential for the practical exploitations of quantum complementary properties
and resources.
}



Schwinger presented an algorithm (see \cite{Schwinger.60} for a proof)
to construct a new mutually unbiased basis ${\frak B}$   from an existing orthogonal one.
The proof idea
is to create a new basis ``inbetween'' the old basis vectors.
by the following construction steps:
\begin{itemize}
\item[(i)]
take the existing orthogonal basis and permute all of its elements by ``shift-permuting'' its elements; that is, by
changing
the basis vectors according to their enumeration $i \rightarrow i+1$ for $i=1,\ldots , n-1$, and $n \rightarrow 1$;
or any other nontrivial (i.e., do not consider identity for any basis element) permutation;
\item[(ii)]
consider the {\em (unitary) transformation} (cf. Sections \ref{2012-m-ch-fdlvs-changeofbasis} and \ref{2012-m-ch-citoob})
corresponding to the basis change from the old basis to the new, ``permutated'' basis;
\item[(iii)]
finally, consider the (orthonormal) {\em eigenvectors} \index{eigenvector}
of this (unitary; cf. page
\pageref{2014-m-ch-fdvs-unitary}) transformation associated with the basis change.
These eigenvectors are the elements of a new bases  ${\frak B}'$.
Together with ${\frak B}$ these two bases
-- that is, ${\frak B}$ and ${\frak B}'$ --  are mutually unbiased.
\end{itemize}

{\color{blue}
\bexample
Consider, for example,
\marginnote{For a {\em Mathematica(R)} program,
see
{\tt http://tph.tuwien.ac.at/}\\
$\sim${\tt svozil/publ/2012-schwinger.m}}
the real plane ${\Bbb R}^2$,
and the basis
$${\frak B}=\{ {\bf e}_1 , {\bf e}_2\} \equiv \{(1,0),(0,1)\}.$$
The shift-permutation [step (i)] brings ${\frak B}$ to a new, ``shift-permuted'' basis  ${\frak S}$; that is,
$$\{ {\bf e}_1 , {\bf e}_2\} \mapsto {\frak S}= \{ {\bf f}_1={\bf e}_2,{\bf f}_1={\bf e}_1 \} \equiv \{(0,1),(1,0)\}.$$
The (unitary) basis transformation [step (ii)] between $ {\frak B} $ and
$  {\frak S}$ can be constructed by a diagonal sum
\begin{equation}
\begin{split}
\textsf{\textbf{U}} ={\bf f}_1^\dagger  {\bf e}_1+ {\bf f}_2^\dagger  {\bf e}_2 = {\bf e}_2^\dagger   {\bf e}_1+ {\bf e}_1^\dagger  {\bf e}_2   \\
\qquad  =\vert {\bf f}_1\rangle \langle {\bf  e}_1\vert + \vert {\bf f}_2\rangle \langle   {\bf e}_2 \vert = \vert {\bf e}_2\rangle \langle    {\bf e}_1\vert + \vert {\bf e}_1\rangle \langle   {\bf e}_2    \vert         \\
\qquad \equiv
%\left(
\begin{pmatrix}
0 \\
1
\end{pmatrix}
(1,0)+
%\left(
\begin{pmatrix}
1 \\
0
\end{pmatrix}
(0,1)  \\
\qquad \equiv
%\left(
\begin{pmatrix}
0 (1,0)\\
1 (1,0)
\end{pmatrix}
+
%\left(
\begin{pmatrix}
1 (0,1)\\
0 (0,1)
\end{pmatrix}
   \\
\qquad \equiv
%\left(
\begin{pmatrix}
0&0\\1&0
\end{pmatrix}
+
%\left(
\begin{pmatrix}
0&1\\0&0
\end{pmatrix}
=
%\left(
\begin{pmatrix}
0&1\\1&0
\end{pmatrix}.
\end{split}
\end{equation}
The set of eigenvectors [step (iii)] of this  (unitary) basis transformation $\textsf{\textbf{U}}$ forms a new basis
\begin{equation}
\begin{split}
{\frak B}' =
\{ \frac{1}{\sqrt{2}} ({\bf f}_1 - {\bf e}_1),
 \frac{1}{\sqrt{2}}( {\bf f}_2 + {\bf e}_2 )\}
\\
 \qquad =
\{ \frac{1}{\sqrt{2}} (\vert {\bf f}_1\rangle  - \vert {\bf e}_1\rangle ),
 \frac{1}{\sqrt{2}}(\vert  {\bf f}_2 \rangle + \vert {\bf e}_2\rangle  )\}
\\
 \qquad  =
\{ \frac{1}{\sqrt{2}} (\vert {\bf e}_2\rangle  - \vert {\bf e}_1\rangle ),
 \frac{1}{\sqrt{2}}(\vert  {\bf e}_1 \rangle + \vert {\bf e}_2\rangle  )\}
\\
 \qquad \equiv \left\{ \frac{1}{\sqrt{2}}(-1,1),  \frac{1}{\sqrt{2}} (1,1)\right\}.
\end{split}
\end{equation}
For a proof of mutually unbiasedness, just form the four inner products of one vector in ${\frak B}$ times one vector in ${\frak B}'$,
respectively.

In three-dimensional complex vector space ${\Bbb C}^3$, a similar construction
from the Cartesian standard basis
${\frak B}=\{ {\bf e}_1 , {\bf e}_2, {\bf e}_3\} \equiv \{(1,0,0),(0,1,0),(0,0,1)\}$
yields
\begin{equation}
\begin{split}
 {\frak B}' \equiv   \frac{1}{\sqrt{3}}  \left\{
 (1,1,1),\right. \\
\qquad
 \left(\frac{1}{2} \left[{\sqrt{3}} i-1\right] , \frac{1}{2} \left[-{\sqrt{3}} i-1\right] ,
  1\right), \\
\qquad  \left.
 \left(
 \frac{1}{ 2} \left[-{\sqrt{3}} i-1\right], \frac{1}{ 2} \left[{\sqrt{3}} i-1\right],1  \right)
   \right\} .
\end{split}
\end{equation}
\eexample
}

Nobody knows how to systematically derive and construct a {\em complete} or {\em maximal}
set of mutually unbiased bases; nor is it clear in general, that is, for arbitrary dimensions,
{\em how many} bases there are in such sets.


\section{Representation of unity in terms of base vectors}

Unity ${\Bbb I}_n$ in an $n$-dimensional vector space ${\frak V}$ can be represented in terms of the sum
over all outer (by another naming tensor or dyadic) products of all vectors of an arbitrary orthonormal basis
\index{outer product}
\index{dyadic product}
\index{tensor product}
${\frak B} =\{
{\bf e}_1,
\ldots ,
{\bf e}_n
\}
\equiv
\{
\vert {\bf e}_1 \rangle ,
\ldots ,
\vert{\bf e}_n \rangle
\}
$; that is,
\begin{equation}
 {\Bbb I}_n = \sum_{i=1}^n \vert {\bf e}_i \rangle \langle {\bf e}_i \vert
\equiv  \sum_{i=1}^n {\bf e}_i  {\bf e}_i^\dagger  .
\end{equation}

{\color{OliveGreen}
\bproof

For a proof, consider an arvbitrary vector $\vert {\bf x} \rangle  \in {\frak V}$.
Then,
\begin{equation}
 {\Bbb I}_n \vert {\bf x} \rangle
 =
\left(\sum_{i=1}^n \vert {\bf e}_i \rangle \langle {\bf e}_i \vert \right)
\vert {\bf x} \rangle
=
\sum_{i=1}^n \vert {\bf e}_i \rangle \langle {\bf e}_i \vert {\bf x} \rangle
=
\sum_{i=1}^n \vert {\bf e}_i \rangle  x_i
=  \vert {\bf x} \rangle
.
\end{equation}
\eproof
}


{\color{blue}
\bexample
Consider, for example, the basis
${\frak B}=\{ \vert {\bf e}_1 \rangle , \vert {\bf e}_2 \rangle \} \equiv \{(1,0)^T,(0,1)^T\}$.
Then the two-dimensional unity ${\Bbb I}_2$
can be written as
\begin{equation}
\begin{split}
{\Bbb I}_2 =   \vert {\bf e}_1 \rangle \langle  {\bf e}_1 \vert   +     \vert {\bf e}_2 \rangle  \langle  {\bf e}_2 \vert \\
=   (1,0)^T (1,0) +   (0,1)^T (0,1)
=
\begin{pmatrix}
1 (1,0) \\  0 (1,0)
\end{pmatrix}
 +
\begin{pmatrix}
0 (0,1)\\
1 (0,1)
\end{pmatrix} \\
 =
\begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix}
+
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
.
\end{split}
\end{equation}

Consider, for another example, the basis
${\frak B}' \equiv \{\frac{1}{\sqrt{2}}(-1,1)^T,\frac{1}{\sqrt{2}}(1,1)^T\}$.
Then the two-dimensional unity ${\Bbb I}_2$
can be written as
\begin{equation}
\begin{split}
{\Bbb I}_2 =  \frac{1}{\sqrt{2}}   (-1,1)^T  \frac{1}{\sqrt{2}}(-1,1) +   \frac{1}{\sqrt{2}}(1,1)^T \frac{1}{\sqrt{2}}(1,1)
=
\frac{1}{{2}}
\begin{pmatrix}
-1 (-1,1) \\  1 (-1,1)
\end{pmatrix}
 +
\frac{1}{{2}}
\begin{pmatrix}
1 (1,1)\\
1 (1,1)
\end{pmatrix} \\
 =
\frac{1}{{2}}
\begin{pmatrix}
1 & -1 \\
-1 & 1
\end{pmatrix}
+
\frac{1}{{2}}
\begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
.
\end{split}
\end{equation}


\eexample
}


\section{Rank}
\label{2014-m-fdvs-rank}
\index{rank of matrix}
\index{matrix rank}
\index{rank}

The (column or row) {\em rank}, $\rho (  \textsf{\textbf{A}}  )$,
\index{column rank of matrix}
\index{row rank of matrix}
or $\textrm{rk} ( \textsf{\textbf{A}} )$,
of a linear transformation $ \textsf{\textbf{A}} $
in an $n$-dimensional vector space ${\frak V}$
is the maximum number of linearly independent (column or, equivalently,
row) vectors of the associated
$n$-by-$n$ square matrix $ A $, represented by its entries  $a_{ij}$.

This definition can be generalized to arbitrary
$m$-by-$n$ matrices $A$, represented by its entries  $a_{ij}$.
Then, the row and column ranks of $A$ are identical; that is,
\begin{equation}
\textrm{row rk} (A) =
\textrm{column rk} (A)  =
\textrm{rk} (A).
\end{equation}


{\color{OliveGreen}
\bproof

For a proof, consider   Mackiw's   argument  \cite{Mackiw-1995}.
First we show that $\textrm{row rk} (A)\le \textrm{column rk} (A)$ for any real
(a generalization to complex vector space requires some adjustments)
$m$-by-$n$ matrix $A$. Let the vectors
$\{{\bf e}_1,{\bf e}_2, \ldots ,{\bf e}_r\}$ with ${\bf e}_i \in {\Bbb R}^n$, $1\le i\le r$,
be a basis spanning the
{\em row space} of
\index{row space} $A$; that is, all vectors
that can be obtained by a linear combination of the $m$ row vectors
$$
%\begin{equation}
%\left(
\begin{pmatrix}
(a_{11},a_{12},\ldots ,a_{1n})\\
(a_{21},a_{22},\ldots ,a_{2n})\\
\vdots                    \\
(a_{m1},a_{n2},\ldots ,a_{mn})
\end{pmatrix}
%\end{equation}
$$
of $A$ can also be obtained as a linear combination of ${\bf e}_1,{\bf e}_2, \ldots ,{\bf e}_r$.
Note that $r\le m$.

Now form the {\em column vectors} $A{\bf e}_i^T$ for $1\le i\le r$, that is,
$A{\bf e}_1^T,A{\bf e}_2^T, \ldots ,A{\bf e}_r^T$ {\em via} the usual rules of matrix multiplication.
Let us prove that these resulting column vectors $A{\bf e}_i^T$ are linearly independent.

Suppose they were not (proof by contradiction).
Then, for some scalars
$c_1,c_2, \ldots ,c_r \in {\Bbb R}$,
$$
c_1 A{\bf e}_1^T+ c_2 A{\bf e}_2^T+ \ldots + c_r A{\bf e}_r^T
=
 A\left(c_1{\bf e}_1^T+ c_2  {\bf e}_2^T+ \ldots + c_r  {\bf e}_r^T \right)=0
$$
without all $c_i$'s vanishing.

That is,
$
{\bf v} =
c_1{\bf e}_1^T+ c_2  {\bf e}_2^T+ \ldots + c_r  {\bf e}_r^T
$, must be in the {\em null space}
\index{null space}
of $A$ defined by all vectors ${\bf x}$ with $A{\bf x}={\bf 0}$,
and $A({\bf v})={\bf 0}$ .
(In this case the inner (Euclidean) product of ${\bf x}$ with all the rows of $A$ must vanish.)
But since the ${\bf e}_i$'s form also a basis of the row vectors,
$
{\bf v}^T
$
is also some vector in the row space of $A$.
The linear independence of the basis elements
${\bf e}_1,{\bf e}_2, \ldots ,{\bf e}_r$   of the row  space of $A$
guarantees that
all the coefficients $c_i$ have to vanish; that is,
$c_1=c_2= \cdots =c_r =0$.

At the same time,
as
for every vector ${\bf x}\in {\Bbb R}^n$,
$A{\bf x}$ is a linear combination of the column vectors
$$
%\begin{equation}
%\left(
\begin{pmatrix}
%\left(
\begin{pmatrix}
a_{11}\\
a_{21}\\
\vdots \\
a_{m1}
\end{pmatrix},
&
%\left(
\begin{pmatrix}
a_{12}\\
a_{22}\\
\vdots \\
a_{m2}
\end{pmatrix},
&
\cdots  ,
&
%\left(
\begin{pmatrix}
a_{1n}\\
a_{2n}\\
\vdots \\
a_{mn}
\end{pmatrix}
\end{pmatrix}
,
%\end{equation}
$$
the $r$ linear independent vectors
$A{\bf e}_1^T,A{\bf e}_2^T, \ldots ,A{\bf e}_r^T$
are all linear combinations of the column vectors of $A$.
Thus, they are in the column space of $A$.
\index{column space}
Hence, $r\le \textrm{column rk}(A)$.
And, as $r= \textrm{row rk}(A)$,
we obtain
$ \textrm{row rk}(A)\le \textrm{column rk}(A)$.

By considering the transposed matrix $A^T$, and by an analogous argument   we obtain
that
$ \textrm{row rk}(A^T)\le \textrm{column rk}(A^T)$.
But
$ \textrm{row rk}(A^T)= \textrm{column rk}(A)$
and
$ \textrm{column rk}(A^T)= \textrm{row rk}(A)$,
and thus
$ \textrm{row rk}(A^T)= \textrm{column rk}(A)\le \textrm{column rk}(A^T)= \textrm{row rk}(A)$.
Finally, by considering both estimates
$ \textrm{row rk}(A)\le \textrm{column rk}(A)$
as well as
$\textrm{column rk}(A)\le \textrm{row rk}(A)$,
we obtain that
$ \textrm{row rk}(A) = \textrm{column rk}(A)$.
\eproof
}

\section{Determinant}
\index{determinant}

\subsection{Definition}

In what follows, the {\em determinant} of a matrix $A$ will be denoted by $\textrm{det} A$ or,
equivalently, by $\vert A \vert$.

Suppose $A=a_{ij}$ is the  $n$-by-$n$ square matrix representation of
a linear transformation $\textsf{\textbf{A}}$
in an $n$-dimensional vector space ${\frak V}$.
We shall define its {\em determinant}
in two equivalent ways.


The
{\em Leibniz formula}
\index{Leibniz formula} defines the determinant of the $n$-by-$n$ square matrix  $A=a_{ij}$ by
\begin{equation}
\textrm{det}A
=\sum_{\sigma \in S_n} \textrm{sgn}(\sigma) \prod_{i=1}^n a_{\sigma(i),j} ,
\end{equation}
where ``sgn'' represents the {\em sign function}
\index{sign function}
of permutations in the permutation group $S_n$
on $n$ elements $\{1,2, \ldots , n\}$,
which returns $-1$ and $+1$ for odd and even permutations,
respectively.

An equivalent definition
\begin{equation}
\textrm{det}A
=\varepsilon_{i_1 i_2\cdots i_n} a_{1i_1}a_{2i_2} \cdots a_{ni_n},
\end{equation}
makes use of the  totally antisymmetric Levi-Civita symbol  (\ref{2014-m-ch-lcs}) on page \pageref{2014-m-ch-lcs},
\index{Levi-Civita symbol}
\index{antisymmetric tensor} and makes use of the
 Einstein summation convention.
\index{Einstein summation convention}


The second,
{\em Laplace formula}
\index{Laplace formula}
definition of the determinant
is recursive and expands the determinant in cofactors.
It is also called
{\em Laplace expansion},
\index{Laplace expansion}
or
{\em cofactor expansion}
\index{cofactor expansion}.
First,
a {\em minor}
\index{minor}
$M_{ij}$ of an  $n$-by-$n$ square matrix  $A$ is
defined to be the determinant of the
$(n-1)\times (n-1)$ submatrix
that remains after the entire $i$th row and $j$th column have been deleted from $A$.

A {\em cofactor}
\index{cofactor}
$A_{ij}$
of an $n$-by-$n$ square matrix  $A$
is defined in terms of its associated minor by
\begin{equation}
A_{ij}=(-1)^{i+j}M_{ij}.
\end{equation}

The {\em determinant} of a square matrix $A$, denoted by
$\textrm{det} A$ or $\vert A\vert$, is a scalar rekursively defined by
\begin{equation}
\textrm{det}A
=\sum_{j=1}^n a_{ij}A_{ij}
=\sum_{i=1}^n a_{ij}A_{ij}
\end{equation}
for any $i$ (row expansion) or $j$ (column expansion), with $i,j=1,\ldots ,n$.
For $1\times 1$ matrices (i.e., scalars), $\textrm{det}A =a_{11}$.

\subsection{Properties}

The following properties of determinants are mentioned (almost) without proof:

\begin{itemize}
\item[(i)]
If $A$ and $B$ are square matrices of the same order, then
$\textrm{det}AB = (\textrm{det}A)  (\textrm{det}B)$.

\item[(ii)]
If either two rows or two columns are exchanged, then the determinant is multiplied
by a factor ``$-1$.''


\item[(iii)]
The determinant of the transposed matrix is equal to the determinant of the original matrix; that is,
$\textrm{det}(A^T) = \textrm{det}A $ .

\item[(iv)]
The determinant $\textrm{det}A $ of a matrix $A$ is nonzero if and only if $A$ is invertible.
In particular, if $A$ is not invertible, $\textrm{det}A =0$.
If $A$ has an inverse matrix $A^{-1}$, then $\textrm{det}(A^{-1}) = (\textrm{det}A)^{-1} $.

This is a very important property which we shall use in Eq.~(\ref{2014-m-eve-ce}) on page \pageref{2014-m-eve-ce}
for the determination of nontrivial
eigenvalues $\lambda$ (including the associated eigenvectors)
\index{eigenvalue}
\index{eigenvector}
\index{eigensystrem}
\index{characteristic equation}
\index{secular determinant}
\index{secular equation}
of a matrix $A$ by solving the secular equation $\textrm{det} (A-\lambda {\Bbb I})=0 $.



\item[(v)]
Multiplication of any row or column with a factor $\alpha$  results in a determinant
which is $\alpha$ times the original determinant.
Consequently,
multiplication of an $n \times n$ matrix with a scalar $\alpha$ results
in a   determinant
which is $\alpha^n$ times the original determinant.

\item[(vi)]
The determinant of a unit matrix is one; that is,
$\textrm{det} \, {\Bbb I}_n =1$.
Likewise, the determinat of a diagonal matrix is just the product of the diagonal entries;
that is,
$\textrm{det} [\textrm{diag}(\lambda_1,\ldots, \lambda_n)] = \lambda_1 \cdots \lambda_n$.

\item[(vii)]
The determinant is not changed if a multiple of an existing row is added to another row.

{\color{OliveGreen}
\bproof
This can be easily demonstrated by considering the Leibniz formula: suppose a multiple $\alpha$
of the $j$'th column is added to the $k$'th column since
\begin{equation}
\begin{split}
\varepsilon_{i_1 i_2\cdots i_j \cdots i_k \cdots i_n} a_{1i_1}a_{2i_2} \cdots a_{j i_j} \cdots (a_{k i_k} + \alpha a_{j i_k}) \cdots a_{ni_n}\\
=
\varepsilon_{i_1 i_2\cdots i_j \cdots i_k \cdots i_n} a_{1i_1}a_{2i_2} \cdots a_{j i_j} \cdots  a_{k i_k} \cdots a_{ni_n} +\\
\alpha \varepsilon_{i_1 i_2\cdots i_j \cdots i_k \cdots i_n} a_{1i_1}a_{2i_2} \cdots a_{j i_j} \cdots   a_{j i_k} \cdots a_{ni_n}.
\end{split}
\end{equation}
The second summation term vanishes, since
$
a_{j i_j} a_{j i_k}
=
a_{j i_k} a_{j i_j}
$
is totally symmetric in the indices $i_j$ and $i_k$,
and the Levi-Civita symbol $\varepsilon_{i_1 i_2\cdots i_j \cdots i_k \cdots i_n}$.
\eproof
}

\item[(viii)]
The absolute value of the determinant of a square matrix $A= \left({\bf e}_1, \ldots {\bf e}_n \right)$ formed by (not necessarily orthogonal)
row (or column) vectors of a basis
$\frak B= \{ {\bf e}_1, \ldots {\bf e}_n\}$
is equal to the {\em volume} of the parallelepiped
\index{volume}
$
\left\{ {\bf x} \mid {\bf x} =\sum_{i=1}^n t_i {\bf e}_i, \; 0 \le t_i \le 1, \; 0\le i \le n \right\}
$
formed by those vectors.

{\color{OliveGreen}
\bproof
This can be demonstrated \marginnote{see, for instance, Section~4.3 of Strang's account \cite{Strang:2009:ILA}}
by supposing that
the square matrix $A$ consists of all the $n$ row (column) vectors of an orthogonal basis of dimension $n$.
Then
$AA^T=A^TA$ is a  diagonal matrix  which just contains the square of the
length of all the basis vectors forming a perpendicular parallelepiped which
is just an $n$ dimensional box.
Therefore the volume is just the positive square root of
$\textrm{det} ( AA^T ) =
(\textrm{det} A) (\textrm{det} A^T ) = (\textrm{det} A) (\textrm{det} A^T )=(\textrm{det} A)^2$.

For any nonorthogonal basis all we need to employ is a Gram-Schmidt process
to obtain a (perpendicular) box of equal volume to the original parallelepiped
formed by the nonorthogonal basis vectors --
any volume that is cut is compensated by adding the same amount to the new volume.
Note that the Gram-Schmidt process operates by adding (subtracting) the projections
of already existing orthogonalized vectors
from the old basis vectors (to render these sums orthogonal to the existing vectors of the new orthogonal basis);
a process which does not change the determinant.
\index{Gram-Schmidt process}
\eproof
}

This result can be used for changing the differential volume element in integrals {\it via} the Jacobian matrix J
\index{Jacobian matrix}
(\ref{2013-m-t-jm}), as
\begin{equation}
dx_1'\, dx_2' \cdots dx_n'
= \vert det J \vert dx_1\, dx_2 \cdots dx_n
= \sqrt{\left[\textrm{det}\left(\frac{dx_i'}{dx_j}\right)\right]^2} dx_1\, dx_2 \cdots dx_n
.
\end{equation}

\item[(ix)]
The {\em sign} of a  determinant of a matrix formed by the row (column)
 vectors of a basis indicates the {orientation}
of that basis.
\index{sign}
\index{orientation}

\end{itemize}


\section{Trace}
\label{2013-ch-fdvs-trace}
\index{trace}
\index{Spur}

\subsection{Definition}
\marginnote{The German word for trace is {\em Spur}.}
The {\em trace} of an $n$-by-$n$ square matrix $A=a_{ij}$, denoted by
$\textrm{Tr} A$,  is a scalar
defined to be the sum of the elements on the main diagonal
 (the diagonal from the upper left to the lower right) of A; that is  (also in Dirac's bra and ket notation),
\begin{equation}
\textrm{Tr}\,A
= a_{11} +a_{22}+ \cdots +a_{nn}
=\sum_{i=1}^n a_{ii}=\sum_{i=1}^n \langle i \vert A\vert i \rangle.
\end{equation}

In quantum mechanics, traces can be realized {\it via} an orthonormal basis ${\frak B} =\{
{\bf e}_1,
\ldots ,
{\bf e}_n
\}$
by ``sandwiching'' an operator $\textsf{\textbf{A}}$ between all basis elements -- thereby effectively taking the diagonal components
of    $\textsf{\textbf{A}}$ with respect to the basis ${\frak B}$ --
and summing over all these scalar compontents; that is,
\begin{equation}
\textrm{Tr}\;\textsf{\textbf{A}}
=\sum_{i=1}^n   \langle {\bf e}_i \vert \textsf{\textbf{A}} \vert {\bf e}_i \rangle
=\sum_{i=1}^n   \langle {\bf e}_i \vert \textsf{\textbf{A}}  {\bf e}_i \rangle
.
\end{equation}

\subsection{Properties}

The following properties of traces are mentioned without proof:

\begin{itemize}
\item[(i)]
$\textrm{Tr}(A+B)=\textrm{Tr}A+\textrm{Tr}B$;
\item[(ii)]
$\textrm{Tr}(\alpha A)= \alpha \textrm{Tr}A$, with $\alpha \in {\Bbb C}$;
\item[(iii)]
$\textrm{Tr}(AB) = \textrm{Tr}(BA)$, hence the trace of the  commutator vanishes; that
is, $\textrm{Tr}([A,B])=0$;
\item[(iv)]
$\textrm{Tr}A = \textrm{Tr}A^T$;
\item[(v)]
$\textrm{Tr}(A\otimes B)= (\textrm{Tr}A) (\textrm{Tr}B)$;
\item[(vi)]
the trace is the sum of the eigenvalues of a {\em normal operator} (cf. page \pageref{2014-m-fdvs-normality});
\index{normal operator}
\index{normal transformation}
\item[(vii)]
$ \textrm{det}(e^A)=e^{\textrm{Tr}A} $;
\item[(viii)]
 the trace is the derivative of the determinant at the identity;
\item[(ix)]
the complex conjugate of the trace of an operator is equal to the trace of its adjoint
(cf. page~\pageref{2014-m-fdvs-adjoint}); that is
$\overline{(  \textrm{Tr} A)}=\textrm{Tr} (A^\dagger)$;
\item[(x)]
the trace is invariant under rotations of the basis as well as
under cyclic permutations.
\item[(xi)]
the trace of an $n \times n$ matrix $A$ for which $AA=\alpha A$ for some $\alpha \in {\Bbb R}$ is
$ \textrm{Tr} A =
\alpha \textrm{rank}(A)$,
where  $\textrm{rank}$ is the rank of $A$ defined on page~\pageref{2014-m-fdvs-rank}.
Consequently, the trace of an idempotent (with $\alpha=1$) operator -- that is, a projection --
\index{idempotence}
is equal to its rank;
and, in particular, the trace of a one-dimensional projection is one.
\index{rank}
\end{itemize}


A {\em trace class} operator is a compact operator for which a trace is finite and independent of the choice of basis.
\index{trace class}

\subsection{Partial trace}
\index{partial trace}
\label{2015-partialtrace}

The quantum mechanics of multi-particle (multipartite) systems allows for configurations -- actually rather processes --
that can be informally described as ``beam dump experiments;'' in which we start out with entangled states
(such as the Bell states on page~\pageref{2014-m-ch-fdvs-bellbasis})  which carry information
about {\em joint properties of the constituent quanta}
and {\em choose to disregard} one quantum state entirely; that is, we pretend
not to care of the (possible) outcomes of a measurement on this particle.
In this case, we have to {\em trace out} that particle; and as a result we obtain a {\em reduced state} without this particle we do
not care about.


{\color{blue}
\bexample
\label{bellstate}
For an example's sake, consider the Bell state  \index{Bell state}       $\vert \Psi^- \rangle$
defined in Eq.~(\ref{2014-m-ch-fdvs-bellbasis}).
\marginnote{The same is true for all elements of the Bell basis.}
Suppose we do not care about the state of the first particle, then we may ask what kind of reduced state results from this
pretension.\marginnote{Be careful here to make the experiment in such a way that in no way you could know the state of the first particle.
You may actually think about this as a measurement of the state of the first particle
by a degenerate observable with only a single, nondiscriminating measurement outcome.}
Then the partial trace is just the trace over the first particle; that is, with subscripts referring to the particle number,
\begin{equation}
\begin{split}
\textrm{Tr}_1\, \vert \Psi^- \rangle \langle  \Psi^-   \vert  \\
=\sum_{i_1=0}^1 \langle i_1 \vert \Psi^- \rangle \langle  \Psi^-  \vert i_1 \rangle \\
=\langle 0_1 \vert \Psi^- \rangle \langle  \Psi^-  \vert 0_1 \rangle
+
\langle 1_1  \vert \Psi^- \rangle \langle  \Psi^-  \vert 1_1 \rangle  \\
=\langle 0_1 \vert  \frac{1}{\sqrt{2}}\left(\vert 0_1   1_2 \rangle - \vert 1_1   0_2 \rangle  \right)  \frac{1}{\sqrt{2}}\left(\langle 0_1   1_2 \vert  - \langle 1_1   0_2 \vert   \right)  \vert 0_1 \rangle\\
\qquad
+
\langle 1_1 \vert  \frac{1}{\sqrt{2}}\left(\vert 0_1   1_2 \rangle - \vert 1_1   0_2 \rangle  \right)  \frac{1}{\sqrt{2}}\left(\langle 0_1   1_2 \vert  - \langle 1_1   0_2 \vert   \right)  \vert 1_1 \rangle  \\
= \frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
.
\end{split}
\end{equation}

The resulting state is a
{\em mixed state}
\index{mixed state}
defined by the property that its trace is equal to one,
but the trace of its square is smaller than one; in this case the trace is $\frac{1}{2}$, because
\begin{equation}
\begin{split}
\textrm{Tr}_2\,
\frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right) \\
= \frac{1}{2}  \langle 0_2 \vert
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\vert 0_2 \rangle
+  \frac{1}{2}
\langle 1_2  \vert
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\vert 1_2 \rangle  \\
=  \frac{1}{2} + \frac{1}{2} =1;
\end{split}
\end{equation}
but
\begin{equation}
\begin{split}
\textrm{Tr}_2\,
\left[
\frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\frac{1}{2}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
\right]
\\ =
\textrm{Tr}_2\,
\frac{1}{4}
\left(
\vert 1_2 \rangle \langle   1_2 \vert
+
\vert 0_2 \rangle \langle   0_2 \vert
\right)
= \frac{1}{2}.
\end{split}
\end{equation}
This {\em mixed state} is a 50:50 mixture of the pure particle states  $\vert 0_2 \rangle$ and $\vert 1_2 \rangle$, respectively.
Note that this is different from
a coherent superposition
\index{coherent superposition}
$\vert 0_2 \rangle + \vert 1_2 \rangle$
 of the pure particle states  $\vert 0_2 \rangle$ and $\vert 1_2 \rangle$, respectively --
also formalizing a 50:50 mixture with respect to measurements of property $0$ {\it versus} $1$, respectively.

\eexample
}

\section{Adjoint}
\label{2014-m-fdvs-adjoint}
\index{adjoints}
\index{adjoint operator}

\subsection{Definition}

Let ${\frak V}$ be a vector space and let ${\bf y}$
be any element of its dual space ${\frak V}^\ast$.
For any linear transformation $\textsf{\textbf{A}}$, consider
the bilinear functional
\marginnote{Here $[\cdot ,\cdot ]$ is the bilinear functional, not the commutator.}
${\bf y}' ({\bf x}) =[{\bf x} ,{\bf y}'] =[\textsf{\textbf{A}}{\bf x},{\bf y}]$
Let the {\em adjoint} transformation $\textsf{\textbf{A}}^\dagger$ be defined by
\begin{equation}
[{\bf x},\textsf{\textbf{A}}^\ast{\bf y}]=
[\textsf{\textbf{A}}{\bf x},{\bf y}].
\end{equation}
In real inner product spaces,
\begin{equation}
[{\bf x},\textsf{\textbf{A}}^T{\bf y}]=
[\textsf{\textbf{A}}{\bf x},{\bf y}].
\end{equation}
In complex inner product spaces,
\begin{equation}
[{\bf x},\textsf{\textbf{A}}^\dagger{\bf y}]=
[\textsf{\textbf{A}}{\bf x},{\bf y}].
\end{equation}


\subsection{Properties}
We mention without proof that the adjoint operator is a linear operator.
Furthermore,
$\textsf{\textbf{0}}^\dagger = \textsf{\textbf{0}}$,
$\textsf{\textbf{1}}^\dagger = \textsf{\textbf{1}}$,
$(\textsf{\textbf{A}}+\textsf{\textbf{B}})^\dagger = \textsf{\textbf{A}}^\dagger+\textsf{\textbf{B}}^\dagger$,
$(\alpha \textsf{\textbf{A}})^\dagger = \alpha \textsf{\textbf{A}}^\dagger$,
$( \textsf{\textbf{A}}\textsf{\textbf{B}})^\dagger =   \textsf{\textbf{B}}^\dagger
 \textsf{\textbf{A}}^\dagger$,
and
$( \textsf{\textbf{A}}^{-1})^\dagger
=
( \textsf{\textbf{A}}^\dagger )^{-1}
$;
as well as  (in finite dimensional spaces)
\begin{equation}
\textsf{\textbf{A}}^{\dagger \dagger}=
\textsf{\textbf{A}}.
\end{equation}

\subsection{Adjoint matrix notation}

In matrix notation and in complex vector space with the dot product,
note that there is a correspondence with the inner product
(cf. page \pageref{2011-m-corr-bil-ip})
so that, for all ${\bf z}\in {\frak V}$ and for all ${\bf x}\in {\frak V}$,
 there exist a unique ${\bf y}\in {\frak V}$ with
\begin{equation}
\begin{split}
[\textsf{\textbf{A}}{\bf x}, {\bf z}] =\langle \textsf{\textbf{A}} {\bf x}\mid {\bf y}\rangle
=\overline{ \langle{\bf y}\mid \textsf{\textbf{A}} {\bf x}\rangle }   \\
 = \overline{ \left( \overline{ y_i} \right) } \overline{ A}_{ij} \overline{ x_j}   = y_i\overline{ A}_{ij} \overline{ x_j}
 = y_i\overline{ A}_{ji}^T \overline{ x_j }    = \overline{ x }\overline{ A}^T y,
\end{split}
\end{equation}
and another unique vector ${\bf y}'$ obtained from ${\bf y}$ by
some linear operator $\textsf{\textbf{A}}^\dagger$
such that ${\bf y}'=\textsf{\textbf{A}}^\dagger {\bf y}$ with
\begin{equation}
\begin{split}
[{\bf x}, \textsf{\textbf{A}}^\dagger {\bf z}] =
\langle {\bf x}\mid {\bf y}'\rangle         =
\langle {\bf x}\mid \textsf{\textbf{A}}^\dagger {\bf y}\rangle        \\
 = \overline{ x_i } A_{ij}^\dagger y_j      =   \overline{ x }A ^\dagger y;
\end{split}
\end{equation}
and therefore
\begin{equation}
A ^\dagger =(\overline{ A})^T =\overline{ A^T}, \textrm{ or } A^\dagger_{ij}=\overline{A}_{ji} .
\end{equation}
In words: in matrix notation, the adjoint transformation is just the
transpose of the complex conjugate of the original matrix.

\section{Self-adjoint transformation}
\index{self-adjoint transformation}



The following definition yields some analogy to real numbers as compared to complex numbers
(``a complex number $z$ is real if $\overline{z}=z$''),
expressed in terms of operators on a complex vector space.


An operator    $\textsf{\textbf{A}}$   on a linear vector space   ${\frak V}$
is called {\em self-adjoint}, if
\begin{equation}
\textsf{\textbf{A}}^{\ast}=
\textsf{\textbf{A}}
\end{equation}
and if the domains of $\textsf{\textbf{A}}$ and $\textsf{\textbf{A}}^{\ast}$
-- that is, the set of vectors on which they are well defined -- coincide.

In finite dimensional {\em real} inner product spaces,
self-adoint operators are called {\em symmetric,}
since they are symmetric with respect to transpositions; that is,
\index{symmetric operator}
\begin{equation}
\textsf{\textbf{A}}^{\ast}= \textsf{\textbf{A}}^{T}=
\textsf{\textbf{A}}.
\end{equation}

In finite dimensional\marginnote{For infinite dimensions,
a distinction must be made between self-adjoint operators and Hermitian ones; see, for instance,
\cite{grau,Gieres-2000,2001-Bonneau}}
{\em complex} inner product spaces,
self-adoint operators are called {\em Hermitian,}
since they are identical with respect to Hermitian conjugation (transposition of the matrix and complex conjugation of its
entries); that is,
\index{Hermitian operator}
\begin{equation}
\textsf{\textbf{A}}^{\ast}= \textsf{\textbf{A}}^{\dagger}=
\textsf{\textbf{A}}.
\end{equation}

In what follows, we shall consider only the latter case and identify self-adjoint operators with Hermitian ones.
In terms of matrices, a matrix $A$ corresponding to an operator $\textsf{\textbf{A}}$ in
some fixed basis is self-adjoint
if
\begin{equation}
A^{\dagger}\equiv (\overline{A_{ij}})^T=  \overline{A_{ji}} =A_{ij} \equiv A.
\end{equation}
That is, suppose $A_{ij}$ is the matrix representation
corresponding to a linear transformation $\textsf{\textbf{A}}$  in some basis ${\frak B}$,
then the {\em Hermitian} matrix $\textsf{\textbf{A}}^\ast = \textsf{\textbf{A}}^\dagger$
to the dual basis
${\frak B}^*$
is
$\overline{(A_{ij}})^T$.



{\color{blue}
\bexample
For the sake of an example, consider again the
{\em Pauli spin matrices}
\index{Pauli spin matrices}
\begin{equation}
\begin{split}
\sigma_1=\sigma_x=
\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}
,   \\
\sigma_2=\sigma_y=
\begin{pmatrix}
0&-i\\
i&0
\end{pmatrix}
,   \\
\sigma_1=\sigma_z=
\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}
.
\end{split}
\end{equation}
which, together with unity, i.e., ${\Bbb I}_2=\textrm{diag}(1,1)$,  are all self-adjoint.

The following operators are not self-adjoint:
\begin{equation}
\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}
 ,
\begin{pmatrix}
1&1\\
0&0
\end{pmatrix}
,
\begin{pmatrix}
1&0\\
i&0
\end{pmatrix}
,
\begin{pmatrix}
0&i\\
i&0
\end{pmatrix}
.{\textrm{\eexample}}
\end{equation}
%
}

Note that the coherent real-valued superposition
of a self-adjoint transformations
(such as the sum or difference of correlations in
the Clauser-Horne-Shimony-Holt expression~\cite{filipp-svo-04-qpoly-prl})
is a self-adjoint transformation.

{\color{OliveGreen}\bproof
For a direct proof,
suppose that $\alpha_i \in {\Bbb R}$ for all $1\le i \le n$ are $n$ real-valued coefficients and
$\textsf{\textbf{A}}_1, \ldots \textsf{\textbf{A}}_n$ are $n$ self-adjoint operators.
Then
$\textsf{\textbf{B}} = \sum_{i=1}^n \alpha_i \textsf{\textbf{A}}_i$
is self-adjoint, since
\begin{equation}
\textsf{\textbf{B}}^* = \sum_{i=1}^n \overline{\alpha_i} \textsf{\textbf{A}}_i^* = \sum_{i=1}^n  \alpha_i  \textsf{\textbf{A}}_i
=\textsf{\textbf{B}}
.
\end{equation}
\eproof
}

\section{Positive transformation}
\index{positive transformation}

A linear transformation  $\textsf{\textbf{A}}$ on an inner product space ${\frak V}$ is {\em positive},
that is in symbols $\textsf{\textbf{A}}\ge 0$, if it is self-adjoint,
and if $\langle \textsf{\textbf{A}}{\bf x}\mid {\bf x}\rangle  \ge 0$ for all ${\bf x}\in {\frak V}$.
If  $\langle \textsf{\textbf{A}}{\bf x}\mid {\bf x}\rangle = 0$ implies
${\bf x}=0$, $\textsf{\textbf{A}}$ is called {\em strictly positive}.


\section{Permutation}
\index{permutation}

Permutation (matrices) are the ``classical analogues'' \cite{mermin-04,mermin-07}
of unitary transformations (matrices) which will be introduced later on page~\pageref{2014-m-ch-fdvs-unitary}.
The permutation matrices are defined by the requirement that they only contain a single nonvanishing entry ``$1$'' per row and column;
all the other row and column entries vanish ``$0$.''
For example, the matrices ${\Bbb I}_n=\textrm{diag}(\underbrace{1,\ldots ,1}_{n \textrm{ times}})$,
or
$$
\sigma_1=
%\left(
\begin{pmatrix}
0&1\\
1&0
\end{pmatrix}
\textrm{, or }\;
%\left(
\begin{pmatrix}
0&1&0\\
1&0&0\\
0&0&1
\end{pmatrix}
$$
are permutation matrices.

Note that from the definition and from matrix multiplication follows that,
if $P$ is a permutation matrix, then $PP^T=P^T P={\Bbb I}_n$.
That is, $P^T$ represents the inverse element of $P$.
As $P$ is real-valued, it is a {\em normal operator} (cf. page \pageref{2014-m-fdvs-normality}).
\index{normal operator}
\index{normal transformation}


Note further that any permuation matrix can be interpreted in terms of row and column vectors.
The set of all these row and column vectors constitute the Cartesian standard basis of $n$-dimensional vector space,
with permuted elements.

Note also that, if $P$ and $Q$ are permutation matrices, so is $PQ$ and $QP$.
The set of all $n!$
permutation $(n\times n)-$matrices correponding to permutations of $n$ elements of $\{ 1,2,\ldots ,n\}$ form the
{\em symmetric group $S_n$}, with ${\Bbb I}_n$ being the identity element.
\index{symmetric group}


\section{Orthonormal (orthogonal) transformations}
\index{orthonormal transformation}
\index{orthogonal transformation}

An {\em orthonormal} or {\em orthogonal transformation} $\textsf{\textbf{R}}$ is a linear transformation
whose corresponding square matrix $R$ has real-valued entries
and mutually ortogonal, normalized row (or, equivalently, column) vectors.
As a consequence,
\begin{equation}
RR^T= R^TR= {\Bbb I}, \textrm{ or } R^{-1}=R^T .
\end{equation}
If $\textrm{det} R=1$, $\textsf{\textbf{R}}$ corresponds to a {\em rotation.}
\index{rotation}
If $\textrm{det} R=-1$, $\textsf{\textbf{R}}$ corresponds to a rotation and a {\em reflection.}
\index{reflection}
A reflection is an isometry (a distance preserving map) with a hyperplane as set of fixed points.

Orthonomal transformations $\textsf{\textbf{R}}$ are ``real valued cases'' of the more general unitary transformations discussed next.
They preserve a symmetric inner product; that is,
$\langle \textsf{\textbf{R}}{\bf x}\mid \textsf{\textbf{R}}{\bf y} \rangle
=
\langle {\bf x}\mid {\bf y} \rangle$ for all ${\bf x} ,{\bf y} \in {\frak V}$


{\color{blue}
\bexample
As a  two-dimensional  example of rotations in the plane ${\Bbb R}^2$,
take the rotation matrix in Eq.~(\ref{2012-m-ch-fdvs-otd2})
representing a rotation of the basis by an angle $\varphi$.

Permutation matrices represent orthonormal transformations.
\eexample
}

\section{Unitary transformations and isometries}
\index{unitary transformation}
\label{2014-m-ch-fdvs-unitary}
\marginnote{For proofs and additional information see \S 73 in    \cite{halmos-vs}}
\index{isometry}

\subsection {Definition}
Note that a complex number $z$ has absolute value one if $\overline{z}=1/z$, or $z\overline{z}=1$.
In analogy to this ``modulus one'' behavior,
consider {\em unitary transformations}, or, used synonymuously, {\em (one-to-one) isometries}
$\textsf{\textbf{U}}$ for which
\begin{equation}
\textsf{\textbf{U}}^*= \textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^{-1},
\textrm{ or } \textsf{\textbf{U}}\textsf{\textbf{U}}^\dagger =\textsf{\textbf{U}}^\dagger \textsf{\textbf{U}}={\Bbb I}.
\end{equation}
Alternatively, we mention without proof that the following conditions are equivalent:
\begin{itemize}
\item[(i)]
$\langle \textsf{\textbf{U}}{\bf x}\mid \textsf{\textbf{U}}{\bf y} \rangle
=
\langle {\bf x}\mid {\bf y} \rangle$ for all ${\bf x} ,{\bf y} \in {\frak V}$;
\item[(ii)]
$\| \textsf{\textbf{U}}{\bf x}\|
=
\|{\bf x}\|$ for all ${\bf x}  \in {\frak V}$;
\end{itemize}

Unitary transformations can also be defined via {\em permutations preserving the scalar product.}
That is, functions such as
$f: x \mapsto x' =\alpha x$ with $\alpha \neq e^{i\varphi}$, $\varphi \in {\Bbb R}$,
do not correspond to a  unitary transformation in a one-dimensional Hilbert space, as
the scalar product $f:
\langle x \vert y \rangle
\mapsto
\langle x'\vert y'\rangle = \vert \alpha \vert^2 \langle x\vert y\rangle$
is not preserved; whereas if $\alpha$ is a modulus of one; that is,
with $\alpha = e^{i\varphi}$, $\varphi \in {\Bbb R}$,
$\vert \alpha \vert^2=1$, and the scalar product is preseved.
Thus, $u: x \mapsto x' =e^{i\varphi} x$, $\varphi \in {\Bbb R}$,
represents a unitary transformation.

\subsection {Characterization of change of orthonormal basis}

Let ${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
be an orthonormal basis of an $n$-dimensional inner product space ${\frak V}$.
If
$\textsf{\textbf{U}}$ is an isometry, then
 $\textsf{\textbf{U}}{\frak B}=\{\textsf{\textbf{U}}{\bf f}_1, \textsf{\textbf{U}} {\bf f}_2,
\ldots ,\textsf{\textbf{U}} {\bf f}_n\}$
is also an orthonormal basis of  ${\frak V}$.
(The converse is also true.)

\subsection {Characterization in terms of orthonormal basis}
\label{2012-m-ch-citoob}


A complex matrix $\textsf{\textbf{U}}$ is unitary if and only if its row (or column) vectors form
an orthonormal basis.

This can be readily verified \cite{Schwinger.60} by writing $\textsf{\textbf{U}}$
in terms of two orthonormal bases
${\frak B}=\{{\bf e}_1,  {\bf e}_2, \ldots , {\bf e}_n\}$
${\frak B}'=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$ as
\begin{equation}
\textsf{\textbf{U}}_{ef}= \sum_{i=1}^n  {\bf e}_i^\dagger {\bf f}_i
=  \sum_{i=1}^n  \vert {\bf e}_i\rangle \langle {\bf f}_i \vert
.
\end{equation}
Together with $\textsf{\textbf{U}}_{fe}= \sum_{i=1}^n  {\bf f}_i^\dagger {\bf e}_i=  \sum_{i=1}^n  \vert {\bf f}_i\rangle \langle {\bf e}_i \vert $
we form
\begin{equation}
\begin{split}
{\bf e}_k \textsf{\textbf{U}}_{ef}\\
\quad = {\bf e}_k\sum_{i=1}^n  {\bf e}_i^\dagger {\bf f}_i \\
\quad
= \sum_{i=1}^n  ({\bf e}_k{\bf e}_i^\dagger) {\bf f}_i \\
\quad
= \sum_{i=1}^n  \delta_{ki} {\bf f}_i \\
\quad  = {\bf f}_k
.
\end{split}
\end{equation}
In a similar way we find that
\begin{equation}
\begin{split}
\textsf{\textbf{U}}_{ef} {\bf f}_k^\dagger = {\bf e}_k^\dagger,\\
{\bf f}_k\textsf{\textbf{U}}_{fe}   = {\bf e}_k,\\
\textsf{\textbf{U}}_{fe} {\bf e}_k^\dagger = {\bf f}_k^\dagger.
\end{split}
\end{equation}
Moreover,
\begin{equation}
\begin{split}
\textsf{\textbf{U}}_{ef}\textsf{\textbf{U}}_{fe}\\
\quad
=
 \sum_{i=1}^n  \sum_{j=1}^n
(\vert {\bf e}_i\rangle \langle {\bf f}_i \vert )
(\vert {\bf f}_j\rangle \langle {\bf e}_j \vert )\\
\quad
=
 \sum_{i=1}^n  \sum_{j=1}^n
\vert {\bf e}_i\rangle \delta_{ij} \langle {\bf e}_j \vert \\
\quad
=
 \sum_{i=1}^n
\vert {\bf e}_i\rangle   \langle {\bf e}_i \vert \\
\quad
=
{\Bbb I}
.
\end{split}
\end{equation}
In a similar way we obtain
$\textsf{\textbf{U}}_{fe}\textsf{\textbf{U}}_{ef}=
{\Bbb I}$.
Since
\begin{equation}
\textsf{\textbf{U}}_{ef}^\dagger = \sum_{i=1}^n  {\bf f}_i^\dagger ( {\bf e}_i^\dagger)^\dagger
= \sum_{i=1}^n  {\bf f}_i^\dagger {\bf e}_i
= \textsf{\textbf{U}}_{fe},
\end{equation}
we obtain that $\textsf{\textbf{U}}_{ef}^\dagger = (\textsf{\textbf{U}}_{ef})^{-1}$
and $\textsf{\textbf{U}}_{fe}^\dagger = (\textsf{\textbf{U}}_{fe})^{-1}$.

Note also that the {\em composition} holds; that is, $\textsf{\textbf{U}}_{ef} \textsf{\textbf{U}}_{fg}=  \textsf{\textbf{U}}_{eg}$.



If we
identify one of the bases  ${\frak B}$ and ${\frak B}'$ by the Cartesian standard basis,
it becomes clear that, for instance,
every unitary operator  $\textsf{\textbf{U}}$  can be written in terms of an orthonormal basis
${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
by ``stacking'' the vectors of that orthonormal basis ``on top of each other\marginnote{For a quantum mechanical application, see \cite{rzbb}};''
that is
\marginnote{For proofs and additional information see
 \S 5.11.3, Theorem 5.1.5 and subsequent Corollary in   \cite{Joglekar-I}}
\begin{equation}
\textsf{\textbf{U}}=
\begin{pmatrix}
{\bf f}_1\\
{\bf f}_2\\
\vdots\\
{\bf f}_n
\end{pmatrix}
.
\end{equation}
Thereby the vectors of the orthonormal basis  ${\frak B}$ serve as the
rows of $\textsf{\textbf{U}}$.

Also, every unitary operator  $\textsf{\textbf{U}}$  can be written in terms of an orthonormal basis
${\frak B}=\{{\bf f}_1,  {\bf f}_2, \ldots , {\bf f}_n\}$
by ``pasting'' the (transposed) vectors of that orthonormal basis ``one after another;''
that is
\begin{equation}
\textsf{\textbf{U}}= \left(
{\bf f}_1^T,
{\bf f}_2^T,
\cdots,
{\bf f}_n^T
\right).
\end{equation}
Thereby the (transposed) vectors of the orthonormal basis  ${\frak B}$ serve as the
columns of $\textsf{\textbf{U}}$.

Note also that any permutation of vectors in ${\frak B}$ would also yield unitary matrices.







\section{Perpendicular projections}
\index{orthogonal projection}
\index{perpendicular projection}
\marginnote{For proofs and additional information see \S 42, \S 75 \& \S 76 in    \cite{halmos-vs}}

{\em Perpendicular projections}
are associated with a {\em direct sum decomposition} of the vector space ${\frak V}$;
that is,
\begin{equation}
 {\frak M}\oplus {\frak M}^\perp ={\frak V}.
\label{2012-m-ch-fdvs-perp}
\end{equation}
Let $\textsf{\textbf{E}}=P_{\frak M}$ denote the projection on ${\frak M}$
along ${\frak M}^\perp$.  The following propositions are stated without proof.



A  linear transformation $\textsf{\textbf{E}}$ is a perpendicular projection
if and only if
$\textsf{\textbf{E}} = \textsf{\textbf{E}}^2=\textsf{\textbf{E}}^*$.

Perpendicular projections are {\em positive} linear transformations,
with
$\left\| \textsf{\textbf{E}}{\bf x} \right\| \le \| {\bf x} \|$
for all
${\bf x} \in {\frak V}$.
Conversely,
if a linear transformation $\textsf{\textbf{E}}$
is idempotent; that is,
$\textsf{\textbf{E}}^2=\textsf{\textbf{E}}$,
and  $\left\| \textsf{\textbf{E}}{\bf x} \right\| \le \| {\bf x} \|$
for all
${\bf x} \in {\frak V}$,
then  is self-adjoint; that is,
$\textsf{\textbf{E}}=\textsf{\textbf{E}}^\ast$.

Recall that
for {\em real} inner product spaces, the self-adjoint operator can be identified with a {\em symmetric} operator
$\textsf{\textbf{E}}=\textsf{\textbf{E}}^T$,
\index{symmetric operator}
whereas
for {\em complex} inner product spaces, the self-adjoint operator can be identified with a {\em Hermitean} operator
$\textsf{\textbf{E}}=\textsf{\textbf{E}}^\dagger$.
\index{Hermitian operator}


If $\textsf{\textbf{E}}_1,\textsf{\textbf{E}}_2, \ldots , \textsf{\textbf{E}}_n$ are (perpendicular)
projections,
then a necessary and sufficient condition that
$\textsf{\textbf{E}} =\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2+\cdots +\textsf{\textbf{E}}_n$
be a (perpendicular) projection is that
 $\textsf{\textbf{E}}_i \textsf{\textbf{E}}_j =\delta_{ij}\textsf{\textbf{E}}_i =\delta_{ij}\textsf{\textbf{E}}_j$;
and, in particular,
$\textsf{\textbf{E}}_i \textsf{\textbf{E}}_j =0$
whenever $i\neq j$; that is, that all $E_i$ are pairwise orthogonal.

{\color{OliveGreen}\bproof
For a start, consider just two projections
 $\textsf{\textbf{E}}_1$ and $\textsf{\textbf{E}}_2$.
Then we can assert that   $\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2$ is a projection if and only if
 $\textsf{\textbf{E}}_1 \textsf{\textbf{E}}_2=\textsf{\textbf{E}}_2 \textsf{\textbf{E}}_1=0$.

Because, for
 $\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2$ to be a projection, it must be idempotent; that is,
\index{idempotence}
 \begin{equation}
(\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2)^2 =
(\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2)(\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2)  =
\textsf{\textbf{E}}_1^2 +   \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 + \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2^2
=
 \textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2 .
\label{2012-m-ch-fdvs-pr3}
\end{equation}
As a consequence, the cross-product terms in (\ref{2012-m-ch-fdvs-pr3}) must vanish; that is,
\begin{equation}
\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 + \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 =0.
\label{2012-m-ch-fdvs-pr4}
\end{equation}
Multiplication of (\ref{2012-m-ch-fdvs-pr4}) with $\textsf{\textbf{E}}_1$ from the left and from the right yields
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_1\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 + \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 =0, \\
 \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 + \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 =0;\textrm{ and} \\
\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1\textsf{\textbf{E}}_1 =0, \\
\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 + \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1  =0.
\end{split}
\label{2012-m-ch-fdvs-pr5}
\end{equation}
Subtraction of the resulting pair of equations yields
\begin{equation}
 \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 - \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1  =
\left[ \textsf{\textbf{E}}_1,\textsf{\textbf{E}}_2 \right]
=0,
\label{2012-m-ch-fdvs-pr6}
\end{equation}
or
\begin{equation}
 \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 = \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 .
\label{2012-m-ch-fdvs-pr67}
\end{equation}
Hence, in order for the cross-product terms in Eqs. (\ref{2012-m-ch-fdvs-pr3} ) and (\ref{2012-m-ch-fdvs-pr4})
to vanish, we must have
\begin{equation}
 \textsf{\textbf{E}}_1\textsf{\textbf{E}}_2 = \textsf{\textbf{E}}_2\textsf{\textbf{E}}_1 =0.
\label{2012-m-ch-fdvs-pr8}
\end{equation}

Proving the reverse statement is straightforward, since (\ref{2012-m-ch-fdvs-pr8}) implies  (\ref{2012-m-ch-fdvs-pr3}).

A generalisation by induction to more than two projections is straightforward,
since, for instance,
$\left(\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2\right)\textsf{\textbf{E}}_3=0$
implies
$ \textsf{\textbf{E}}_1\textsf{\textbf{E}}_3+\textsf{\textbf{E}}_2\textsf{\textbf{E}}_3=0$.
Multiplication with $\textsf{\textbf{E}}_1$ from the left yields
$
\textsf{\textbf{E}}_1\textsf{\textbf{E}}_1\textsf{\textbf{E}}_3+\textsf{\textbf{E}}_1\textsf{\textbf{E}}_2\textsf{\textbf{E}}_3=
\textsf{\textbf{E}}_1 \textsf{\textbf{E}}_3=
0$.
 \eproof }


\section{Proper value or eigenvalue}
\index{proper value}
\marginnote{For proofs and additional information see \S 54 in    \cite{halmos-vs}}
\index{proper vector}
\index{eigenvalue}
\index{eigenvector}
\index{eigensystem}

\subsection{Definition}

A scalar $\lambda$ is a {\em proper value} or {\em eigenvalue},
and a nonzero vector ${\bf x}$ is a {\em proper vector} or {\em eigenvector}
of a linear transformation $\textsf{\textbf{A}}$
if
\begin{equation}
\textsf{\textbf{A}}{\bf x}=   \lambda {\bf x} =   \lambda {\Bbb I} {\bf x}.
\end{equation}
In an $n$-dimensional
vector space $\frak V$
The set of the set of eigenvalues and the set of the associated eigenvectors
$\{\{\lambda_1,\ldots ,\lambda_k\},\{{\bf x}_1,\ldots ,{\bf x}_n\}\}$
of a linear transformation $\textsf{\textbf{A}}$ form an {\em eigensystem} of $\textsf{\textbf{A}}$.

\subsection{Determination}
\index{characteristic equation}
\index{secular determinant}
\index{secular equation}


% http://vergil.chemistry.gatech.edu/notes/linear_algebra/node5.html

Since the eigenvalues and eigenvectors are those scalars $\lambda$  vectors ${\bf x}$ for which $\textsf{\textbf{A}}{\bf x}=   \lambda {\bf x}$,
this equation can be rewritten with a zero vector on the right side of the equation; that is (${\Bbb I}=\textrm{diag}(1,\ldots ,1)$ stands for the identity matrix),
\begin{equation}
(\textsf{\textbf{A}} - \lambda {\Bbb I}){\bf x}= {\bf 0}.
\label{2011-m-eve}
\end{equation}
Suppose that $\textsf{\textbf{A}} - \lambda {\Bbb I}$ is invertible. Then we could formally write
${\bf x} = (\textsf{\textbf{A}} - \lambda {\Bbb I})^{-1}{\bf 0}$; hence ${\bf x}$ must be the zero vector.

We are not interested in this trivial solution of Eq. (\ref{2011-m-eve}).
Therefore, suppose that, contrary to the previous assumption,
$\textsf{\textbf{A}} - \lambda {\Bbb I}$ is {\em not} invertible.
We have mentioned earlier (without proof) that this implies that its determinant vanishes; that is,
\begin{equation}
\textrm{det} (\textsf{\textbf{A}} - \lambda {\Bbb I}) = \vert \textsf{\textbf{A}} - \lambda {\Bbb I}\vert =0.
\label{2014-m-eve-ce}
\end{equation}
This determinant is often called the {\em secular determinant};
\index{secular determinant}
\index{secular equation}
and the corresponding equation after expansion of the determinant is called the
{\em secular equation}
or {\em characteristic equation}.
Once the eigenvalues, that is, the roots (i.e., the solutions) of this equation are determined,
the eigenvectors can be obtained one-by-one by inserting these eigenvalues one-by-one into Eq. (\ref{2011-m-eve}).


{\color{blue}
\bexample
For the sake of an example, consider  the
{matrix}
\begin{equation}
A=
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}.
\end{equation}

The secular equation is
\index{secular equation}
$$
\left|
\begin{matrix}
1-\lambda &0&1\\
0&1-\lambda &0\\
1&0&1-\lambda
\end{matrix}
\right| = 0,
$$
yielding the characteristic equation
$
(1-\lambda )^3 -(1-\lambda ) =(1-\lambda )[(1-\lambda )^2 - 1]=(1-\lambda )[\lambda ^2 - 2\lambda ]= - \lambda (1-\lambda )(2-\lambda ) =0$,
and therefore three  eigenvalues
$\lambda_1=0$,
$\lambda_2=1$, and
$\lambda_3=2$ which are the roots of $\lambda (1-\lambda )(2-\lambda ) =0$.

Next let us determine the eigenvectors of $A$, based on the eigenvalues.
Insertion  $\lambda_1=0$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}  -
\begin{pmatrix}
0&0&0\\
0&0&0\\
0&0&0
\end{pmatrix}
\right]
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
;
\end{equation}
therefore $x_1+x_3=0$ and $x_2=0$.
We are free to choose any (nonzero) $x_1=-x_3$,
but if we are interested in normalized eigenvectors, we obtain
${\bf x}_1 =(1/\sqrt{2})(1,0,-1)^T$.

Insertion  $\lambda_2=1$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}  -
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{pmatrix}
\right]
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
0&0&1\\
0&0&0\\
1&0&0
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
;
\end{equation}
therefore $x_1=x_3=0$ and $x_2$ is arbitrary.
We are again free to choose any (nonzero) $x_2$,
but if we are interested in normalized eigenvectors, we obtain
${\bf x}_2 = (0,1,0)^T$.


Insertion  $\lambda_3=2$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}  -
\begin{pmatrix}
2&0&0\\
0&2&0\\
0&0&2
\end{pmatrix}
\right]
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
-1&0&1\\
0&-1&0\\
1&0&-1
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
;
\end{equation}
therefore $-x_1+x_3=0$ and $x_2=0$.
We are free to choose any (nonzero) $x_1=x_3$,
but if we are once more interested in normalized eigenvectors, we obtain
${\bf x}_3 =(1/\sqrt{2})(1,0,1)^T$.

Note that the eigenvectors are mutually orthogonal.
We can construct the corresponding orthogonal projections by the outer (dyadic or tensor) product
\index{outer product}
\index{dyadic product}
\index{tensor product}
of the eigenvectors; that is,
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_1 =
{\bf x}_1 \otimes {\bf x}_1^T =
\frac{1}{2} (1,0,-1)^T(1,0,-1) =
\frac{1}{2}
\begin{pmatrix}
1(1,0,-1)\\
0(1,0,-1)\\
-1(1,0,-1)
\end{pmatrix} =
\frac{1}{2}
\begin{pmatrix}
1&0&-1\\
0&0&0\\
-1&0&1
\end{pmatrix}
\\
\textsf{\textbf{E}}_{2} =
{\bf x}_{2} \otimes {\bf x}_{2}^T =
 (0,1,0)^T(0,1,0) =
\begin{pmatrix}
0(0,1,0)\\
1(0,1,0)\\
0(0,1,0)
\end{pmatrix} =
\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}
\\
\textsf{\textbf{E}}_{3} =
{\bf x}_{3} \otimes {\bf x}_{3}^T =
\frac{1}{2} (1,0,1)^T(1,0,1) =
\frac{1}{2}
\begin{pmatrix}
1(1,0,1)\\
0(1,0,1)\\
1(1,0,1)
\end{pmatrix} =
\frac{1}{2}
\begin{pmatrix}
1&0&1\\
0&0&0\\
1&0&1
\end{pmatrix}
\end{split}
\end{equation}
Note also that $A$ can be written as the sum of the products of the
eigenvalues with the associated projections; that is (here, $\textsf{\textbf{E}}$
stands for the corresponding matrix),
$A= 0  \textsf{\textbf{E}}_1 + 1  \textsf{\textbf{E}}_{2} +2\textsf{\textbf{E}}_{3} $.
Also, the projections are mutually orthogonal
-- that is,
$\textsf{\textbf{E}}_1 \textsf{\textbf{E}}_2 = \textsf{\textbf{E}}_1\textsf{\textbf{E}}_3=\textsf{\textbf{E}}_2\textsf{\textbf{E}}_3=0$
--
and add up to unity; that is,
$\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2+\textsf{\textbf{E}}_3={\Bbb I}$.
{\textrm{\eexample}}
}

If the eigenvalues obtained are not distinct und thus some eigenvalues are {\em degenerate},
\index{degenerate eigenvalues}
the associated eigenvectors traditionally -- that is, by convention and not necessity -- are chosen to be
{\em mutually orthogonal.}
A more formal motivation will come from the spectral theorem below.


{\color{blue}
\bexample
For the sake of an example, consider  the
{matrix}
\begin{equation}
B=
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}.
\end{equation}

The secular equation yields
\index{secular equation}
$$
\left|
\begin{matrix}
1-\lambda &0&1\\
0&2-\lambda &0\\
1&0&1-\lambda
\end{matrix}
\right| = 0,
$$
which yields the characteristic equation
$
(2-\lambda )(1-\lambda )^2 +[-(2-\lambda )]=
(2-\lambda )[(1-\lambda )^2 -1]=
-\lambda (2-\lambda )^2 =0$,
and therefore just two  eigenvalues
$\lambda_1=0$,  and
$\lambda_2=2$ which are the roots of $\lambda (2-\lambda )^2 =0$.

Let us now determine the eigenvectors of $B$, based on the eigenvalues.
Insertion  $\lambda_1=0$ into Eq. (\ref{2011-m-eve})  yields
\begin{equation}
\left[
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}  -
\begin{pmatrix}
0&0&0\\
0&0&0\\
0&0&0
\end{pmatrix}
\right]
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
;
\end{equation}
therefore $x_1+x_3=0$ and $x_2=0$.
Again we are free to choose any (nonzero) $x_1=-x_3$,
but if we are interested in normalized eigenvectors, we obtain
${\bf x}_1 =(1/\sqrt{2})(1,0,-1)^T$.

Insertion  $\lambda_2=2$ into Eq. (\ref{2011-m-eve}) yields
\begin{equation}
\left[
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}  -
\begin{pmatrix}
2&0&0\\
0&2&0\\
0&0&2
\end{pmatrix}
\right]
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
-1&0&1\\
0&0&0\\
1&0&-1
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix}
;
\end{equation}
therefore $x_1=x_3$; $x_2$ is arbitrary.
We are again free to choose any values of $x_1$, $x_3$ and $x_2$ as long
 $x_1=x_3$ as well as $x_2$ are satisfied.
Take, for the sake of choice, the orthogonal
normalized eigenvectors
${\bf x}_{2,1} = (0,1,0)^T$ and
${\bf x}_{2,2} = (1/\sqrt{2})(1,0,1)^T$,
which are also orthogonal to ${\bf x}_1 =(1/\sqrt{2})(1,0,-1)^T$.

Note again that we can find the corresponding orthogonal projections by the outer (dyadic or tensor) product
\index{outer product}
\index{dyadic product}
\index{tensor product}
of the eigenvectors; that is,  by
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_1 =
{\bf x}_1 \otimes {\bf x}_1^T =
\frac{1}{2} (1,0,-1)^T(1,0,-1) =
\frac{1}{2}
\begin{pmatrix}
1(1,0,-1)\\
0(1,0,-1)\\
-1(1,0,-1)
\end{pmatrix} =
\frac{1}{2}
\begin{pmatrix}
1&0&-1\\
0&0&0\\
-1&0&1
\end{pmatrix}
\\
\textsf{\textbf{E}}_{2,1} =
{\bf x}_{2,1} \otimes {\bf x}_{2,1}^T =
(0,1,0)^T(0,1,0) =
\begin{pmatrix}
0(0,1,0)\\
1(0,1,0)\\
0(0,1,0)
\end{pmatrix} =
\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}
\\
\textsf{\textbf{E}}_{2,2} =
{\bf x}_{2,2} \otimes {\bf x}_{2,2}^T =
\frac{1}{2} (1,0,1)^T(1,0,1) =
\frac{1}{2}
\begin{pmatrix}
1(1,0,1)\\
0(1,0,1)\\
1(1,0,1)
\end{pmatrix} =
\frac{1}{2}
\begin{pmatrix}
1&0&1\\
0&0&0\\
1&0&1
\end{pmatrix}
\end{split}
\end{equation}
Note also that $B$ can be written as the sum of the products of the
eigenvalues with the associated projections; that is (here, $\textsf{\textbf{E}}$
stands for the corresponding matrix),
$B= 0  \textsf{\textbf{E}}_1 + 2 (\textsf{\textbf{E}}_{2,1} + \textsf{\textbf{E}}_{2,2} )$.
Again, the projections are mutually orthogonal
-- that is,
$\textsf{\textbf{E}}_1 \textsf{\textbf{E}}_{2,1} = \textsf{\textbf{E}}_1\textsf{\textbf{E}}_{2,2}=
\textsf{\textbf{E}}_{2,1}\textsf{\textbf{E}}_{2,2}=0$
--
and add up to unity; that is,
$\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_{2,1}+\textsf{\textbf{E}}_{2,2}={\Bbb I}$.
This leads us to the much more general spectral theorem.

Another, extreme, example would be the unit matrix in $n$ dimensions; that is,
${\Bbb I}_n=\textrm{diag}(\underbrace{1,\ldots ,1}_{n \textrm{ times}})$,
which has an $n$-fold degenerate eigenvalue $1$ corresponding to a solution to
$(1-\lambda )^n=0$.
The corresponding projection operator is ${\Bbb I}_n$.  [Note that $({\Bbb I}_n)^2 ={\Bbb I}_n$
and thus ${\Bbb I}_n$ is a projection.]
If one (somehow arbitrarily but conveniently) chooses a decomposition of unity ${\Bbb I}_n$
into projections corresponding to the standard basis (any other orthonormal basis would do as well),
then
\begin{equation}
\begin{split}
{\Bbb I}_n = \textrm{diag}( 1,0,0,\ldots ,0 )
+   \textrm{diag}( 0,1,0,\ldots ,0 )
+ \cdots
+   \textrm{diag}( 0,0,0,\ldots ,1 )\\
%\left(
\begin{pmatrix}
 1&0&0&\cdots &0\\
 0&1&0&\cdots &0\\
 0&0&1&\cdots &0\\
&&&\vdots&\\
 0&0&0&\cdots &1
\end{pmatrix}  =
%\left(
\begin{pmatrix}
 1&0&0&\cdots &0\\
 0&0&0&\cdots &0\\
 0&0&0&\cdots &0\\
&&&\vdots&\\
 0&0&0&\cdots &0
\end{pmatrix}  +
\\
\quad
+%\left(
\begin{pmatrix}
 0&0&0&\cdots &0\\
 0&1&0&\cdots &0\\
 0&0&0&\cdots &0\\
&&&\vdots&\\
 0&0&0&\cdots &0
\end{pmatrix} +\cdots
+%\left(
\begin{pmatrix}
 0&0&0&\cdots &0\\
 0&0&0&\cdots &0\\
 0&0&0&\cdots &0\\
&&&\vdots&\\
 0&0&0&\cdots &1
\end{pmatrix}
,
\end{split}
\end{equation}
where all the matrices in the sum carrying one nonvanishing entry ``$1$''
in their diagonal are  projections.
Note that
\begin{equation}
\begin{split}
{\bf e}_i=\vert {\bf e}_i \rangle  \\
\quad \equiv ( \underbrace{0,\ldots , 0}_{i-1 \textrm{ times}}, 1, \underbrace{0,\ldots , 0}_{n-i \textrm{ times}} )   \\
\quad \equiv  \textrm{diag}(  \underbrace{0,\ldots , 0}_{i-1 \textrm{ times}}, 1, \underbrace{0,\ldots , 0}_{n-i \textrm{ times}}  )
  \\
\quad \equiv  \textsf{\textbf{E}}_{i}.
\end{split}
\end{equation}
{\textrm{\eexample}}
}


The following theorems are enumerated without proofs.

If $\textsf{\textbf{A}}$
is a self-adjoint transformation on an inner product space, then every proper value (eigenvalue)  of $\textsf{\textbf{A}}$
is real.
If $\textsf{\textbf{A}}$ is positive, or stricly positive,
then every proper value of  $\textsf{\textbf{A}}$ is positive, or stricly positive, respectively

Due to their idempotence $\textsf{\textbf{E}}\textsf{\textbf{E}}=\textsf{\textbf{E}}$,
projections have eigenvalues $0$ or $1$.

Every eigenvalue of an isometry has absolute value one.

If  $\textsf{\textbf{A}}$  is either a self-adjoint transformation or an isometry,
then proper vectors of $ \textsf{\textbf{A}}$
belonging to distinct proper values are orthogonal.


\section{Normal transformation}
\index{normal transformation}
\index{normal operator}
\label{2014-m-fdvs-normality}

A transformation $\textsf{\textbf{A}}$ is called {\em normal}
if it commutes with its adjoint; that is,
\begin{equation}
[\textsf{\textbf{A}},\textsf{\textbf{A}}^*]= \textsf{\textbf{A}}\textsf{\textbf{A}}^* -
\textsf{\textbf{A}}^* \textsf{\textbf{A}} =0.
\end{equation}


It follows from their definition that Hermitian and unitary transformations are normal. That is,
$\textsf{\textbf{A}}^\ast =\textsf{\textbf{A}}^\dagger$,
and for Hermitian operators,
$\textsf{\textbf{A}}=\textsf{\textbf{A}}^\dagger$,
and thus
$[\textsf{\textbf{A}},\textsf{\textbf{A}}^\dagger]= \textsf{\textbf{A}}\textsf{\textbf{A}} -
\textsf{\textbf{A}} \textsf{\textbf{A}} =(\textsf{\textbf{A}})^2 -(\textsf{\textbf{A}})^2=0$.
For unitary operators,
$\textsf{\textbf{A}}^\dagger =\textsf{\textbf{A}}^{-1}$,
and thus
$[\textsf{\textbf{A}},\textsf{\textbf{A}}^\dagger]= \textsf{\textbf{A}}\textsf{\textbf{A}}^{-1} -
\textsf{\textbf{A}}^{-1} \textsf{\textbf{A}} ={\Bbb I} -{\Bbb I} =0$.


We mention without proof that
a normal transformation on a finite-dimensional unitary space is
(i) Hermitian,
(ii) positive,
(iii) strictly positive,
(iv) unitary,
(v) invertible,
(vi) idempotent
\index{idempotence}
if and only if all its proper values are
(i) real,
(ii) positive,
(iii) strictly positive,
(iv) of absolute value one,
(v) different from zero,
(vi) equal to zero or one.

\section{Spectrum}
\index{spectrum}
\marginnote{For proofs and additional information see \S 78 in   \cite{halmos-vs}}

\subsection{Spectral theorem}
\index{Spectral theorem}
\label{2012-m-ch-Spectraltheorem}

Let $\frak V$ be an $n$-dimensional linear vector space.
The {\em spectral theorem} states
\index{spectral theorem}
that to every self-adjoint (more general, normal) transformation $ \textsf{\textbf{A}}$
on an $n$-dimensional inner product space there correspond real numbers, the {\em spectrum}
$
\lambda_1,
\lambda_2, \ldots ,
\lambda_k
$
of all the eigenvalues of   $ \textsf{\textbf{A}}$,
and their associated  orthogonal projections
$
\textsf{\textbf{E}}_1,
\textsf{\textbf{E}}_2, \ldots ,
\textsf{\textbf{E}}_k
$
where $0<k\le n$ is a strictly positive integer so that
\begin{itemize}
\item[(i)]
the $\lambda_i$ are pairwise distinct,
\item[(ii)]
the $\textsf{\textbf{E}}_i$ are pairwise orthogonal and different from $\textsf{\textbf{0}}$,
\item[(iii)]
$\sum_{i=1}^k \textsf{\textbf{E}}_i={\Bbb I}_n$, and
\item[(iv)]
$
\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i
$
is the {\em spectral form} of $\textsf{\textbf{A}}$.
\index{spectral form}
\end{itemize}

{\color{OliveGreen}
\bproof
Rather than proving the spectral theorem in its full generality,
we suppose that the spectrum of a Hermitian (self-adjoint) operator $ \textsf{\textbf{A}}$ is {\em nondegenerate};
that is, all $n$ eigenvalues of $\textsf{\textbf{A}}$ are pairwise distinct.
That is, we are assuming a strong form of (i).

This distinctness of the eigenvalues then translates into mutual orthogonality of all the eigenvectors of $ \textsf{\textbf{A}}$.
Thereby, the set of $n$ eigenvectors form an orthogonal basis of the $n$-dimensional linear vector space $\frak V$.
The respective normalized eigenvectors can then be represented by perpendicular projections which can be summed up to yield the identity
(iii).

More explicitly, suppose, for the sake of a proof by contradiction of the pairwise orthogonality of the eigenvectors (ii),
that two different eigenvalues
$\lambda_1$
and
$\lambda_2$
belong to two respective eigenvectors
${\bf x}_1$
and
${\bf x}_2$
which are not orthogonal.
But then, because $\textsf{\textbf{A}}$ is self-adjoint with real eigenvalues,
\begin{equation}
\begin{split}
\lambda_1  \langle {\bf x}_1 \vert {\bf x}_2 \rangle =
\langle \lambda_1  {\bf x}_1 \vert {\bf x}_2 \rangle =
  \langle \textsf{\textbf{A}}{\bf x}_1 \vert {\bf x}_2 \rangle \\=
  \langle {\bf x}_1 \vert \textsf{\textbf{A}}^* {\bf x}_2 \rangle =
  \langle {\bf x}_1 \vert \textsf{\textbf{A}}{\bf x}_2 \rangle =
  \langle {\bf x}_1 \vert \lambda_2 {\bf x}_2 \rangle=
  \lambda_2 \langle {\bf x}_1 \vert {\bf x}_2 \rangle,
\end{split}
\end{equation}
which implies that either   $\lambda_1  = \lambda_2$ -- which is in contradiction to our assumption of the distinctness of  $\lambda_1$ and $\lambda_2$;
or that  $\langle {\bf x}_1 \vert {\bf x}_2 \rangle =0$  (thus allowing $\lambda_1  \neq \lambda_2$) --
which is in contradiction to our assumption that ${\bf x}_1$ and $ {\bf x}_2$ are {\em not} orthogonal.
Hence, for distinct   $\lambda_1$ and $\lambda_2$, the associated eigenvectors must be orthogonal,
thereby assuring (ii).

Since by our assumption there are $n$ distinct eigenvalues, this implies that there are $n$ orthogonal eigenvectors.
These $n$ mutually orthogonal eigenvectors
span the entire $n$-dimensional linear vector space $\frak V$;
and hence their union $\{ \vert {\bf x}_i \rangle \vert i\le i \le n\}$ forms an orthogonal basis.
Consequently, the sum of the associated perpendicular projections $\textsf{\textbf{E}}_i = \frac{\vert {\bf x}_i \rangle \langle {\bf x}_i \vert}{\langle {\bf x}_i \vert {\bf x}_i  \rangle}$
is a decomposition of unity ${\Bbb I}_n$, thereby justifying (iii).

In the last step, let us define the $i$'th projection of an arbitrary vector ${\bf x}\in \frak V$
by ${\bf x}_i =  \textsf{\textbf{E}}_i  {\bf x}$, thereby keeping in mind that the resulting vector ${\bf x}_i$
is an eigenvector of $\textsf{\textbf{A}}$  with the associated eigenvalue $\lambda_i$; that is, $\textsf{\textbf{A}}{\bf x}_i = \lambda {\bf x}_i$.
Then,
\begin{equation}
\begin{split}
\textsf{\textbf{A}} {\bf x} =
\textsf{\textbf{A}} {\Bbb I}_n {\bf x} =
\textsf{\textbf{A}} \left(\sum_{i=1}^n \textsf{\textbf{E}}_i\right) {\bf x} =
\textsf{\textbf{A}} \left(\sum_{i=1}^n \textsf{\textbf{E}}_i {\bf x}\right) =
\textsf{\textbf{A}} \left(\sum_{i=1}^n {\bf x}_i\right)  \\
=
 \sum_{i=1}^n \textsf{\textbf{A}} {\bf x}_i  =
 \sum_{i=1}^n \lambda_i {\bf x}_i   =
 \sum_{i=1}^n \lambda_i \textsf{\textbf{E}}_i   {\bf x}=
\left( \sum_{i=1}^n \lambda_i \textsf{\textbf{E}}_i \right) {\bf x},
\end{split}
\end{equation}
which is the spectral form of $\textsf{\textbf{A}}$.
\eproof
}

\subsection{Composition of the spectral form}

If the spectrum of a  Hermitian (or, more general, normal) operator $\textsf{\textbf{A}}$ is nondegenerate, that is, $k=n$, then the
$i$th projection
can be written as the outer (dyadic or tensor) product
\index{outer product}
\index{dyadic product}
\index{tensor product}
$
\textsf{\textbf{E}}_i={\bf x}_i \otimes {\bf x}_i^T$
of the $i$th normalized eigenvector ${\bf x}_i $ of $\textsf{\textbf{A}}$.
In this case, the set of all normalized eigenvectors $\{{\bf x}_1, \ldots ,{\bf x}_n\}$ is an orthonormal basis of the vector space $\frak V$.
If the spectrum of $\textsf{\textbf{A}}$ is degenerate, then the projection can be chosen to be the orthogonal sum of projections
corresponding to orthogonal eigenvectors, associated with the same  eigenvalues.

Furthermore, for a  Hermitian (or, more general, normal) operator $\textsf{\textbf{A}}$,
if $1\le i \le k$,
then there exist polynomials with real coefficients, such as,  for instance,
\begin{equation}
p_i  (t)
=
\prod_{
\begin{array}{c}
1\le j\le k\\
j\neq i
\end{array}
}
\frac{t-\lambda_j}{\lambda_i -\lambda_j}
\label{2011-m-epsf}
\end{equation}
so that
$p_i(\lambda_j) =\delta_{ij}$;
moreover, for every such polynomial,
$p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$.

{\color{OliveGreen}\bproof

For a proof, it is not too difficult
to show that
$p_i  (\lambda_i)=1$, since in this case in the product of fractions all numerators are equal to denominators,
and
$p_i  (\lambda_j)=0$ for $j\neq i $, since some numerator in the product of fractions vanishes.

Now, substituting for $t$ the spectral form $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
of $\textsf{\textbf{A}}$, as well as
decomposing unity in terms of the projections $\textsf{\textbf{E}}_i$ in the spectral form of
$\textsf{\textbf{A}}$; that is, ${\Bbb I}_n=\sum_{i=1}^k \textsf{\textbf{E}}_i$,
yields
\begin{equation}
p_i  (\textsf{\textbf{A}})
=
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\textsf{\textbf{A}} - \lambda_j{\Bbb I}_n}{\lambda_i -\lambda_j}
=
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\sum_{l=1}^k \lambda_l\textsf{\textbf{E}}_l - \lambda_j\sum_{l=1}^k \textsf{\textbf{E}}_l}{\lambda_i -\lambda_j},
\end{equation}
and, because of the idempotence and pairwise orthogonality of the projections  $\textsf{\textbf{E}}_l$,
\index{idempotence}
\begin{equation}
\begin{split}
=
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\sum_{l=1}^k \textsf{\textbf{E}}_l(\lambda_l- \lambda_j)}{\lambda_i -\lambda_j}  \\
= \sum_{l=1}^k \textsf{\textbf{E}}_l
\prod_{
1\le j\le k,\;
j\neq i
}\frac{\lambda_l- \lambda_j}{\lambda_i -\lambda_j}
= \sum_{l=1}^k \textsf{\textbf{E}}_l
\delta_{li} = \textsf{\textbf{E}}_i.
\label{2012-m-ch-fdvs-cc}
\end{split}
\end{equation}
\eproof
}

With the help of the polynomial $p_i(t)$ defined in Eq. (\ref{2011-m-epsf}),
which requires knowledge of the eigenvalues,
the spectral form of a Hermitian (or, more general, normal) operator  $\textsf{\textbf{A}}$ can thus be rewritten as
\begin{equation}
\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i p_i(\textsf{\textbf{A}})=  \sum_{i=1}^k \lambda_i \prod_{
1\le j\le k,\;
j\neq i
}\frac{\textsf{\textbf{A}} - \lambda_j{\Bbb I}_n}{\lambda_i -\lambda_j}.
\end{equation}
That is, knowledge of all the eigenvalues entails construction
of all the projections in the spectral decomposition
of a normal transformation.


{\color{blue}
\bexample
For the sake of an example, consider again the
{matrix}
\begin{equation}
A=
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}
\end{equation}
and the associated Eigensystem
\begin{equation}
\begin{split}
\left\{
\left\{  \lambda_1,\lambda_2,\lambda_3 \right\},\left\{ \textsf{\textbf{E}}_1,\textsf{\textbf{E}}_2,\textsf{\textbf{E}}_3\right\} \right\}\\
=
\left\{
\left\{  0,1,2 \right\},
\left\{
\frac{1}{2}
\begin{pmatrix}
1&0&-1\\
0&0&0\\
-1&0&1
\end{pmatrix}
,
\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}
,
\frac{1}{2}
\begin{pmatrix}
1&0&1\\
0&0&0\\
1&0&1
\end{pmatrix}
\right\}
\right\} .
\end{split}
\end{equation}

The projections associated with the eigenvalues, and, in particular, $\textsf{\textbf{E}}_1$,
can be obtained from  the set of eigenvalues $\left\{  0,1,2 \right\}$ by
\begin{equation}
\begin{split}
p_1(A)=
\left( \frac{A-\lambda_2{\Bbb I}}{\lambda_1-\lambda_2} \right)
\left( \frac{A-\lambda_3{\Bbb I}}{\lambda_1-\lambda_3} \right) \\
=
\frac{
\left(
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}
-
1\cdot
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{pmatrix}
\right)}
{(0-1)}
\cdot
\frac{
\left(
\begin{pmatrix}
1&0&1\\
0&1&0\\
1&0&1
\end{pmatrix}
-
2\cdot
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{pmatrix}
\right)}
{(0-2)}
\\
=
\frac{1}{2}
\begin{pmatrix}
0&0&1\\
0&0&0\\
1&0&0
\end{pmatrix}
\begin{pmatrix}
-1&0&1\\
0&-1&0\\
1&0&-1
\end{pmatrix}
=
\frac{1}{2}
\begin{pmatrix}
1&0&-1\\
0&0&0\\
-1&0&1
\end{pmatrix}
=\textsf{\textbf{E}}_1.
\end{split}
\end{equation}

For the sake of another, degenerate, example consider again the
{matrix}
\begin{equation}
B=
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}
\end{equation}



Again, the projections $\textsf{\textbf{E}}_1, \textsf{\textbf{E}}_2$
can be obtained from  the set of eigenvalues $\left\{  0,2 \right\}$ by
\begin{equation}
\begin{split}
p_1(A)=   \frac{A-\lambda_2{\Bbb I}}{\lambda_1-\lambda_2}
=
\frac{
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}
-
2\cdot
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{pmatrix}
}
{(0-2)}
=
\frac{1}{2}
\begin{pmatrix}
1&0&-1\\
0&0&0\\
-1&0&1
\end{pmatrix}
=\textsf{\textbf{E}}_1
,\\
p_2(A)=   \frac{A-\lambda_1{\Bbb I}}{\lambda_2-\lambda_1}
=
\frac{
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}
-
0\cdot
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{pmatrix}
}
{(2-0)}
=
\frac{1}{2}
\begin{pmatrix}
1&0&1\\
0&2&0\\
1&0&1
\end{pmatrix}
=\textsf{\textbf{E}}_2.
\end{split}
\end{equation}
Note that, in accordance with the spectral theorem,
$\textsf{\textbf{E}}_1 \textsf{\textbf{E}}_2=0 $,
$\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2={\Bbb I}$
and
$0\cdot \textsf{\textbf{E}}_1+2\cdot \textsf{\textbf{E}}_2=B$.

\eexample
}







\section{Functions of normal transformations}
\index{Functions of normal transformation}

Suppose $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i  \textsf{\textbf{E}}_i $ is a normal transformation
in its spectral form.
If $f$ is an arbitrary complex-valued function defined at least at the eigenvalues of $\textsf{\textbf{A}}$,
then a linear transformation  $f(\textsf{\textbf{A}})$ can be defined by
\begin{equation}
f(\textsf{\textbf{A}})=
f\left(\sum_{i=1}^k \lambda_i \textsf{\textbf{E}}_i\right)
=\sum_{i=1}^k f(\lambda_i)  \textsf{\textbf{E}}_i
 .
\end{equation}
Note that, if $f$ has a polynomial expansion such as analytic functions, then orthogonality and idempotence
\index{idempotence}
of the projections $\textsf{\textbf{E}}_i $ in the spectral form guarantees this kind of ``linearization.''

{
\color{blue}
\bexample
For the definition of the ``square root''
for every positve operator $\textsf{\textbf{A}})$, consider
\begin{equation}
\sqrt{\textsf{\textbf{A}}}=\sum_{i=1}^k \sqrt{\lambda_i}  \textsf{\textbf{E}}_i.
\end{equation}
With this definition,
$\left(\sqrt{\textsf{\textbf{A}}}\right)^2=
\sqrt{\textsf{\textbf{A}}}\sqrt{\textsf{\textbf{A}}}= {\textsf{\textbf{A}}}$.

Consider, for instance, the ``square root''  of the $\textsf{\textbf{not}}$ operator
\index{not operator}
\index{square root of not operator}
\marginpar{The denomination ``not'' for  $\textsf{\textbf{not}}$
can be motivated by enumerating its performance at the two ``classical bit states''
$\vert 0 \rangle \equiv (1,0)^T$
and
$\vert 1 \rangle \equiv (0,1)^T$:
$\textsf{\textbf{not}}\vert 0 \rangle = \vert 1 \rangle $
and
$\textsf{\textbf{not}}\vert 1 \rangle = \vert 0 \rangle $.}
\begin{equation}
\textsf{\textbf{not}}
=
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}.
\end{equation}
To enumerate $\sqrt{\textsf{\textbf{not}}}$  we need to find the {\em spectral form} of $\textsf{\textbf{not}}$ first.
\index{spectral form}
The eigenvalues of  $\textsf{\textbf{not}}$ can be obtained by solving the
secular equation
\index{secular equation}
\begin{equation}
\text{det}
\left(
\textsf{\textbf{not}} - \lambda {\Bbb I}_2
\right)
=
\text{det}
\left(
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}
-
\lambda
\begin{pmatrix}
1&0\\  0&1
\end{pmatrix}
\right)
=
\text{det}
\begin{pmatrix}
 -\lambda&1\\
1&-\lambda
\end{pmatrix}
   =\lambda^2-1=0.
\end{equation}
$\lambda^2=1$ yields the two eigenvalues
$\lambda_1=1$
and
$\lambda_1=-1$.
The associated eigenvectors
${\bf x}_1$
and
${\bf x}_2$
can be derived from either the equations
$\textsf{\textbf{not}}\,{\bf x}_1={\bf x}_1$
and
$\textsf{\textbf{not}}\,{\bf x}_2=-{\bf x}_2$,
or inserting the eigenvalues into the polynomial~(\ref{2011-m-epsf}).

We choose the former method.
Thus, for $\lambda_1=1$,
\begin{equation}
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}
\begin{pmatrix}
x_{1,2}\\x_{1,2}
\end{pmatrix}
=\begin{pmatrix}
x_{1,2}\\x_{1,2}
\end{pmatrix}
,
\end{equation}
which yields  $x_{1,2}=x_{1,2}$, and thus, by normalizing the eigenvector,
${\bf x}_1=(1/\sqrt{2})(1,1)^T$.
The associated projection is
\begin{equation}
\textsf{\textbf{E}}_1={\bf x}_1{\bf x}_1^T=\frac{1}{2}
\begin{pmatrix}
 1&1\\  1&1
\end{pmatrix}
.
\end{equation}


Likewise, for $\lambda_2=-1$,
\begin{equation}
\begin{pmatrix}
 0&1\\  1&0
\end{pmatrix}
\begin{pmatrix}
x_{2,2}\\x_{2,2}
\end{pmatrix}
=-\begin{pmatrix}
x_{2,2}\\x_{2,2}
\end{pmatrix}
,
\end{equation}
which yields  $x_{2,2}=-x_{2,2}$, and thus, by normalizing the eigenvector,
${\bf x}_2=(1/\sqrt{2})(1,-1)^T$.
The associated projection is
\begin{equation}
\textsf{\textbf{E}}_2={\bf x}_2{\bf x}_2^T=\frac{1}{2}
\begin{pmatrix}
 1&-1\\  -1&1
\end{pmatrix}
.
\end{equation}

Thus we are finally able to calculate
$\sqrt{\textsf{\textbf{not}}}$
through its spectral form
\begin{equation}
\begin{split}
\sqrt{\textsf{\textbf{not}}}=
\sqrt{\lambda_1}\textsf{\textbf{E}}_1 +
\sqrt{\lambda_2}\textsf{\textbf{E}}_2
\\=  \sqrt{1}
\frac{1}{2} \begin{pmatrix}
 1&1\\  1&1
\end{pmatrix}
+  \sqrt{-1}
\frac{1}{2} \begin{pmatrix}
 1&-1\\  -1&1
\end{pmatrix}
=
\frac{1}{2}
\begin{pmatrix}
 1+i&1-i\\  1-i&1+i
\end{pmatrix}
.
\end{split}
\end{equation}
It can be readily verified that  $\sqrt{\textsf{\textbf{not}}}\sqrt{\textsf{\textbf{not}}}=\textsf{\textbf{not}}$.

\eexample
}

\section{Decomposition of operators}
\index{decomposition}
\marginnote{For proofs and additional information see \S 83 in   \cite{halmos-vs}}

\subsection{Standard decomposition}

In analogy to the decomposition of every imaginary number $z= \Re z +i \Im z$ with $\Re z,\Im z\in {\Bbb R}$,
every arbitrary transformation $\textsf{\textbf{A}}$ on a finite-dimensional vector space can be decomposed into two Hermitian operators
$\textsf{\textbf{B}}$
and
$\textsf{\textbf{C}}$
such that
\begin{equation}
\begin{split}
\textsf{\textbf{A}}=\textsf{\textbf{B}} + i \textsf{\textbf{C}}; \textrm{ with }  \\
\textsf{\textbf{B}}=\frac{1}{2}(\textsf{\textbf{A}} +   \textsf{\textbf{A}}^\dagger ), \\
\textsf{\textbf{C}}=\frac{1}{2i}(\textsf{\textbf{A}} -   \textsf{\textbf{A}}^\dagger ).
\end{split}
\end{equation}

{\color{OliveGreen}
\bproof
Proof by insertion; that is,
\begin{equation}
\begin{split}
\textsf{\textbf{A}}=\textsf{\textbf{B}} + i \textsf{\textbf{C}}\\
\quad =
\frac{1}{2}(\textsf{\textbf{A}} +   \textsf{\textbf{A}}^\dagger ) + i \left[\frac{1}{2i}(\textsf{\textbf{A}} -   \textsf{\textbf{A}}^\dagger )\right],
\\
\textsf{\textbf{B}}^\dagger=   \left[\frac{1}{2}(\textsf{\textbf{A}} +   \textsf{\textbf{A}}^\dagger )\right]^\dagger
  =    \frac{1}{2}\left[\textsf{\textbf{A}}^\dagger +  ( \textsf{\textbf{A}}^\dagger )^\dagger \right]\\
 =    \frac{1}{2}\left[\textsf{\textbf{A}}^\dagger +    \textsf{\textbf{A}} \right]
 =  \textsf{\textbf{B}} , \\
\textsf{\textbf{C}}^\dagger=   \left[\frac{1}{2i}(\textsf{\textbf{A}} -   \textsf{\textbf{A}}^\dagger )\right]^\dagger
 =   -\frac{1}{2i}\left[\textsf{\textbf{A}}^\dagger -  ( \textsf{\textbf{A}}^\dagger )^\dagger \right]\\
  =   -\frac{1}{2i}\left[\textsf{\textbf{A}}^\dagger -    \textsf{\textbf{A}}\right]
 =  \textsf{\textbf{C}} .
\end{split}
\end{equation}
\eproof
}


\subsection{Polar representation}

In analogy to the polar representation of every imaginary number $z= R e^{i\varphi}$ with $R,\varphi \in {\Bbb R}$, $R>0$,
$0\le \varphi < 2\pi$,
every arbitrary transformation $\textsf{\textbf{A}}$ on a finite-dimensional inner product space can be decomposed into
a unique positive transform
$\textsf{\textbf{P}}$ and an isometry
$\textsf{\textbf{U}}$, such that $\textsf{\textbf{A}}= \textsf{\textbf{U}} \textsf{\textbf{P}}$.
If $\textsf{\textbf{A}}$ is invertible, then $\textsf{\textbf{U}}$  is uniquely determined by
$\textsf{\textbf{A}}$.
A necessary and sufficient condition that $\textsf{\textbf{A}}$ is normal is that
$\textsf{\textbf{U}} \textsf{\textbf{P}}=\textsf{\textbf{P}} \textsf{\textbf{U}} $.

\subsection{Decomposition of isometries}

Any unitary or orthogonal transformation   in finite-dimensional inner product space
\index{decomposition}
can be composed from a succession of two-parameter unitary transformations in
two-dimensional subspaces,
and a multiplication of a single diagonal matrix with elements of modulus one
in an algorithmic, constructive and tractable manner.
The method is similar to Gaussian elimination and facilitates the parameterization of elements
of the unitary group  in arbitrary dimensions (e.g., Ref. \cite{murnaghan}, Chapter 2).

{\color{Purple}
It has been suggested to implement
these group theoretic results by realizing interferometric analogues
of any discrete unitary and Hermitian operator
in a unified and experimentally feasible way by ``generalized beam splitters''   \cite{rzbb,reck-94}.}


\subsection{Singular value decomposition}

The {\em singular value decomposition}
\index{singular value decomposition}
(SVD)
of an ($m\times n$)  matrix $\textsf{\textbf{A}}$ is a factorization of the form
\begin{equation}
\textsf{\textbf{A}} = \textsf{\textbf{U}} \Sigma \textsf{\textbf{V}} ,
\end{equation}
where
$\textsf{\textbf{U}}$ is a unitary ($m\times m$)  matrix (i.e. an isometry),
$\textsf{\textbf{V}}$ is a unitary ($n\times n$)  matrix,
and
$\Sigma$ is a unique ($m\times n$)   diagonal matrix with nonnegative real numbers on the diagonal;
that is,
\begin{equation}
\Sigma =
\begin{pmatrix}
\sigma_1&&&{|}&&\vdots& \\
  &\ddots &&{|}&\cdots &0&\cdots \\
&&\sigma_r&{|}&&\vdots& \\
-&-&-&&-&-&- \\
&\vdots&&{|}&&\vdots& \\
\cdots &0&\cdots &{|}&\cdots &0&\cdots \\
&\vdots&&{|}&&\vdots& \\
\end{pmatrix}.
\end{equation}
The entries $\sigma_1\ge \sigma_2 \cdots \ge \sigma_r$>0 of $\Sigma$ are called {\em singular values}
of $\textsf{\textbf{A}}$.  No proof is presented here.
\index{singular values}

\subsection{Schmidt decomposition of the tensor product of two vectors}
\index{Schmidt decomposition}
\label{2011-m-Schmidtdecomposition}

Let  ${\frak U}$  and   ${\frak V}$ be
two linear vector spaces
of dimension $n\ge m$ and $m$, respectively.
Then, for any vector
${\bf z} \in {\frak U}\otimes {\frak V}$
in the tensor product space,
there exist
orthonormal basis sets of vectors
$\{ {\bf u}_1, \ldots ,{\bf u}_n \}  \subset  {\frak U}$
and
$\{ {\bf v}_1, \ldots ,{\bf v}_m \}  \subset  {\frak V}$
such that
${\bf z}=\sum_{i=1}^m
\sigma_i  {\bf u}_i \otimes  {\bf v}_i$,
where the $\sigma_i$s are nonnegative scalars and the set of scalars is uniquely determined by
${\bf z}$.

Equivalently \cite{nielsen-book}, suppose that
$\vert {\bf z}\rangle $
 is some tensor product  contained in   the set of all tensor products of vectors
$ {\frak U}\otimes {\frak V}$ of     two linear vector spaces
 ${\frak U}$  and   ${\frak V}$.
Then there exist orthonormal vectors
$ \vert {\bf u}_i  \rangle \in  {\frak U}$
and
$ \vert {\bf v}_j  \rangle \in  {\frak V}$
so that
\begin{equation}
  \vert {\bf z}\rangle = \sum_i \sigma_i   \vert {\bf u}_i  \rangle  \vert {\bf v}_i  \rangle ,
\label{2011-e-sd}
\end{equation}
where the  $\sigma_i$s are nonnegative scalars; if $  \vert {\bf z}\rangle$
is normalized, then the  $\sigma_i$s are  satisfying
$\sum_i \sigma_i^2=1$;
they are called the
{\em Schmidt coefficients}.
\index{Schmidt coefficients}

{\color{OliveGreen}
\bproof
For a proof by reduction to the singular value decomposition,
let
$\vert i\rangle$
and
$\vert j\rangle$
be any two fixed orthonormal bases of $ {\frak U}$ and $ {\frak V}$, respectively.
Then,
$\vert {\bf z}\rangle $
can be expanded as
$\vert {\bf z}\rangle  = \sum_{ij}a_{ij} \vert i\rangle \vert j\rangle$,
where the $a_{ij}$s can be interpreted as the components of a matrix
$\textsf{\textbf{A}}$.
$\textsf{\textbf{A}}$ can then be subjected to a
singular value decomposition
$\textsf{\textbf{A}} = \textsf{\textbf{U}} \Sigma \textsf{\textbf{V}}$,
or, written in index form [note that $\Sigma=\textrm{diag}(\sigma_1, \ldots, \sigma_n)$ is a diagonal matrix],
$a_{ij}= \sum_l u_{il}\sigma_l v_{lj}$;
and hence  $\vert {\bf z}\rangle  = \sum_{ijl} u_{il}\sigma_l v_{lj}\vert i\rangle \vert j\rangle$.
Finally, by identifying
$\vert {\bf u}_l  \rangle = \sum_i u_{il} \vert i\rangle$
as well as
$\vert {\bf v}_l  \rangle = \sum_l v_{lj} \vert j\rangle$
one obtains the Schmidt decompsition (\ref{2011-e-sd}).
Since $u_{il}$ and $v_{ lj}$ represent unitary martices,
and because
 $\vert i\rangle$ as well as
 $\vert j\rangle$
are orthonormal,
the newly formed vectors
$\vert {\bf u}_l \rangle$
as well as
$\vert {\bf v}_l  \rangle$
form orthonormal bases as well.
The sum of squares of the $\sigma_i$'s is one if  $\vert {\bf z}\rangle $ is a unit vector,
because  (note that $\sigma_i$s are real-valued)
 $\langle {\bf z}\vert {\bf z}\rangle =1
=   \sum_{lm} \sigma_l \sigma_m   \langle {\bf u}_l  \vert  {\bf u}_m  \rangle   \langle  {\bf v}_l  \vert  {\bf v}_m  \rangle
=   \sum_{lm} \sigma_l \sigma_m  \delta_{lm}
=   \sum_{l} \sigma_l^2
$.
\eproof
}

Note that the Schmidt decomposition cannot, in general, be extended for more factors than two.
Note also that the Schmidt decomposition needs not be unique \cite{ekert:415};
in particular if some of the Schmidt coefficients $\sigma_i$ are equal.
For the sake of an example of nonuniqueness of the Schmidt decomposition,
take, for instance, the representation of the {\em Bell state} \index{Bell state}
with the two bases
\begin{equation}
\begin{split}
\left\{
\vert {\bf e}_1\rangle \equiv (1,0),
\vert {\bf e}_2\rangle \equiv (0,1)
\right\}
\textrm{ and }\\
\left\{
\vert {\bf f}_1\rangle \equiv \frac{1}{\sqrt{2}}(1,1),
\vert {\bf f}_2\rangle \equiv \frac{1}{\sqrt{2}}(-1,1)
\right\}.
\end{split}
\end{equation}
as follows:
\begin{equation}
\begin{split}
\vert {\Psi^-} \rangle =
\frac{1}{\sqrt{2}}
\left(
\vert {\bf e}_1\rangle
\vert {\bf e}_2\rangle
-
\vert {\bf e}_2\rangle
\vert {\bf e}_1\rangle
\right)\\
\quad \equiv
\frac{1}{\sqrt{2}}
\left[
(1 (0,1),0(0,1))- (0 (1,0),1(1,0))\right] = \frac{1}{\sqrt{2}} (0,1,-1,0); \\
\vert {\Psi^-} \rangle =
\frac{1}{\sqrt{2}}
\left(
\vert {\bf f}_1\rangle
\vert {\bf f}_2\rangle
-
\vert {\bf f}_2\rangle
\vert {\bf f}_1\rangle
\right) \\
\quad \equiv
\frac{1}{2\sqrt{2}}
\left[
(1 (-1,1),1(-1,1))- ( -1 (1,1),1(1,1))\right]  \\
\quad \equiv
\frac{1}{2\sqrt{2}}
\left[
( -1  , 1 ,-1 ,1 ) - ( -1,-1 ,1 ,1 )\right]
 = \frac{1}{\sqrt{2}} (0,1,-1,0)
.
\end{split}
\end{equation}



\section{Purification}
\index{purification}

\marginnote{For additional information see page~110, Sect.~2.5 in    \cite{nielsen-book10}}

In general, quantum states ${\boldsymbol{\rho}}$ satisfy three criteria \cite{ba-89}:
(i)
$\textrm{Tr}({\boldsymbol{\rho}}) =1$,
(ii) ${\boldsymbol{\rho}}^\dagger ={\boldsymbol{\rho}}$, and
(iii)
$
\langle {\bf x} \vert {\boldsymbol{\rho}} \vert {\bf x} \rangle
=\langle {\bf x} \vert {\boldsymbol{\rho}}  {\bf x} \rangle \ge  0
$ for all vectors ${\bf x}$ of some  Hilbert space.

With dimension $n$ it follows immediately from (ii) that $\boldsymbol{\rho}$ is normal and thus has a spectral decomposition
\begin{equation}
\boldsymbol{\rho} =\sum_{i=1}^n \rho_i \vert \psi_i\rangle \langle \psi_i \vert
\label{2015-sfgs}
\end{equation}
into projections $\vert \psi_i\rangle \langle \psi_i \vert$,
with
(i) yielding $\sum_{i=1}^n\rho_i =1$
(hint: take a trace with the orthonormal basis corresponding to all the $\vert \psi_i\rangle$);
(ii) yielding  $\overline{\rho_i}=\rho_i$;
and (iii) implying $\rho_i \ge 0$, and hence [with (i)] $0 \le \rho_i \le 1$
for all $1\le i \le n$.


Quantum mechanics differentiates between ``two sorts of states,'' namely
pure states and one mixed ones:
\begin{itemize}
\item[(i)]
Pure states ${\boldsymbol{\rho}}_p$  are
represented by projections.
They can be written as ${\boldsymbol{\rho}}_p =  \vert \psi \rangle \langle \psi  \vert$ for some unit vector $\vert \psi \rangle$
(discussed in Sec.~\ref{2011-m-projec}), and
satisfy $({\boldsymbol{\rho}}_p)^2={\boldsymbol{\rho}}_p$.
\item[(ii)]
General, mixed states ${\boldsymbol{\rho}}_m$, are ones that are no projections and therefore
satisfy $({\boldsymbol{\rho}}_m)^2 \neq {\boldsymbol{\rho}}_m$.
They can be composed from projections by their spectral form (\ref{2015-sfgs}).
\end{itemize}

The question arises: is it possible to ``purify'' any mixed state by (maybe somewhat superficially) ``enlarging'' its
Hilbert space, such that the resulting state ``living in a larger Hilbert space'' is pure?
This can indeed be achieved by a rather simple procedure:
By considering the spectral form (\ref{2015-sfgs}) of a general mixed state ${\boldsymbol{\rho}}$,
define a new, ``enlarged,'' pure state  $\vert \Psi\rangle \langle \Psi \vert$, with
\begin{equation}
\vert \Psi\rangle = \sum_{i=1}^n \sqrt{\rho_i}  \vert \psi_i\rangle  \vert \psi_i\rangle
.
\label{2015-puran}
\end{equation}

{\color{OliveGreen}\bproof
That $\vert \Psi\rangle \langle \Psi \vert$ is pure can be tediously verified by proving that it is idempotent:
\index{idempotence}
\begin{equation}
\begin{split}
(\vert \Psi\rangle \langle \Psi \vert )^2
\\=
\left\{
\left[\sum_{i=1}^n \sqrt{\rho_i}  \vert \psi_i\rangle  \vert \psi_i\rangle \right]
\left[\sum_{j=1}^n \sqrt{\rho_j}  \langle  \psi_j\vert \langle \psi_j\vert \right]
\right\}^2
\\=
\left[\sum_{i_1=1}^n \sqrt{\rho_{i_1}}  \vert \psi_{i_1}\rangle  \vert \psi_{i_1}\rangle \right]
\left[\sum_{j_1=1}^n \sqrt{\rho_{j_1}}  \langle  \psi_{j_1}\vert \langle \psi_{j_1}\vert \right]\times \\
\left[\sum_{i_2=1}^n \sqrt{\rho_{i_2}}  \vert \psi_{i_2}\rangle  \vert \psi_{i_2}\rangle \right]
\left[\sum_{j_2=1}^n \sqrt{\rho_{j_2}}  \langle  \psi_{j_2}\vert \langle \psi_{j_2}\vert \right]
\qquad
\\=
\left[\sum_{i_1=1}^n \sqrt{\rho_{i_1}}  \vert \psi_{i_1}\rangle  \vert \psi_{i_1}\rangle \right]
\left[\sum_{j_1=1}^n \sum_{i_2=1}^n \sqrt{\rho_{j_1}}\sqrt{\rho_{i_2}}  (\delta_{i_2 j_1})^2 \right]
\left[\sum_{j_2=1}^n \sqrt{\rho_{j_2}}  \langle  \psi_{j_2}\vert \langle \psi_{j_2}\vert \right]
\\=
\left[\sum_{i_1=1}^n \sqrt{\rho_{i_1}}  \vert \psi_{i_1}\rangle  \vert \psi_{i_1}\rangle \right]
\left[\sum_{j_2=1}^n \sqrt{\rho_{j_2}}  \langle  \psi_{j_2}\vert \langle \psi_{j_2}\vert \right]
\\=  \vert \Psi\rangle \langle \Psi \vert
.
\label{2015-puranproof}
\end{split}
\end{equation}
}

Note that this construction is not unique -- any auxiliary component representing some orthonormal basis would suffice.

The original mixed state ${\boldsymbol{\rho}}$ is obtained from the pure state
corresponding to the unit vector $\vert \Psi\rangle = \vert \psi \rangle \vert \psi^a \rangle  = \vert \psi \psi^a \rangle$
-- we might say that ``the superscript $a$ stands for auxiliary'' --
by a partial trace (cf. Sec.~\ref{2015-partialtrace}) over one of its components, say  $\vert \psi^a\rangle$.
\index{partial trace}

{\color{OliveGreen}\bproof
For the sake of a proof let us ``trace out of the auxiliary components $\vert \psi^a\rangle$,'' that is,
take the trace
\begin{equation}
\textrm{Tr}_{a} (\vert \Psi\rangle \langle \Psi \vert )
=
\sum_{k=1}^n  \langle \psi^a_{k} \vert  (\vert \Psi\rangle \langle \Psi \vert ) \vert \psi^a_{k}\rangle
\end{equation}
of
$\vert \Psi\rangle \langle \Psi \vert$
with respect to one of its components $\vert \psi^a\rangle$:
\begin{equation}
\begin{split}
\textrm{Tr}_{a}\left(
\vert \Psi\rangle \langle \Psi \vert
\right)
\\=
\textrm{Tr}_{a}\left(
\left[\sum_{i=1}^n \sqrt{\rho_{i}}  \vert \psi_{i}\rangle  \vert \psi^a_{i}\rangle \right]
\left[\sum_{j=1}^n \sqrt{\rho_{j}}  \langle  \psi^a_{j}\vert \langle \psi_{j}\vert \right]
\right)
\\=
\sum_{k=1}^n  \left\langle \psi^a_{k} \left\vert  \left(
\left[\sum_{i=1}^n \sqrt{\rho_{i}}  \vert \psi_{i}\rangle  \vert \psi^a_{i}\rangle \right]
\left[\sum_{j=1}^n \sqrt{\rho_{j}}  \langle  \psi^a_{j}\vert \langle \psi_{j}\vert \right]
\right)
\right\vert \psi^a_{k}\right\rangle
\\=
\sum_{k=1}^n \sum_{i=1}^n \sum_{j=1}^n  \delta_{ki} \delta_{kj}
\sqrt{\rho_{i}} \sqrt{\rho_{j}}
\vert \psi_{i}\rangle
\langle \psi_{j}\vert
\\=
\sum_{k=1}^n
\rho_{l}
\vert \psi_{k}\rangle
\langle \psi_{k}\vert
= {\boldsymbol{\rho}}
.
\label{2015-puranproof1}
\end{split}
\end{equation}
}






\section{Commutativity}
\index{commutativity}
\marginnote{For proofs and additional information see \S 79 \& \S 84 in    \cite{halmos-vs}}

If $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
is the spectral form of a self-adjoint transformation  $\textsf{\textbf{A}}$
on a finite-dimensional inner product space,
then a necessary and sufficient condition (``if and only if $=$ iff'')
that a linear transformation
 $\textsf{\textbf{B}}$ commutes with
 $\textsf{\textbf{A}}$
is that it commutes with each
$\textsf{\textbf{E}}_i$, $1\le i\le k$.

{\color{OliveGreen}\bproof
Sufficiency is derived easily: whenever   $\textsf{\textbf{B}}$
commutes with all the procectors $\textsf{\textbf{E}}_i$, $1\le i\le k$
in the spectral composition of   $\textsf{\textbf{A}}$,
then, by linearity, it commutes with $\textsf{\textbf{A}}$.

Necessity follows from the fact that, if  $\textsf{\textbf{B}}$
commutes with  $\textsf{\textbf{A}}$
then it also commutes with every polynomial of  $\textsf{\textbf{A}}$;
and hence also with $p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$,
as shown in (\ref{2012-m-ch-fdvs-cc}).
\eproof
}

If $\textsf{\textbf{A}}=\sum_{i=1}^k \lambda_i\textsf{\textbf{E}}_i$
and
$\textsf{\textbf{B}}=\sum_{j=1}^l \mu_i\textsf{\textbf{F}}_j$
are the spectral forms of a self-adjoint transformations
$\textsf{\textbf{A}}$ and $\textsf{\textbf{B}}$
on a finite-dimensional inner product space,
then a necessary and sufficient condition (``if and only if $=$ iff'')
that  $\textsf{\textbf{A}}$ and
 $\textsf{\textbf{B}}$ commute
is that the projections
$\textsf{\textbf{E}}_i$, $1\le i\le k$
and
$\textsf{\textbf{F}}_j$, $1\le j\le l$
commute with each other; i.e.,
$\left[\textsf{\textbf{E}}_i,\textsf{\textbf{F}}_j\right] =
\textsf{\textbf{E}}_i \textsf{\textbf{F}}_j-
\textsf{\textbf{F}}_j\textsf{\textbf{E}}_i =0 $.

{\color{OliveGreen}\bproof
Again, sufficiency is derived easily: if $\textsf{\textbf{F}}_j$, $1\le j\le l$
occurring in the spectral decomposition of $\textsf{\textbf{B}}$
commutes with all the procectors $\textsf{\textbf{E}}_i$, $1\le i\le k$
in the spectral composition of   $\textsf{\textbf{A}}$,
then, by linearity, $\textsf{\textbf{B}}$ commutes with $\textsf{\textbf{A}}$.

Necessity follows from the fact that, if $\textsf{\textbf{F}}_j$, $1\le j\le l$
commutes with  $\textsf{\textbf{A}}$
then it also commutes with every polynomial of  $\textsf{\textbf{A}}$;
and
hence also with $p_i(\textsf{\textbf{A}})=\textsf{\textbf{E}}_i$,
as shown in (\ref{2012-m-ch-fdvs-cc}).
Conversely,
if $\textsf{\textbf{E}}_i$, $1\le i\le k$
commutes with  $\textsf{\textbf{B}}$
then it also commutes with every polynomial of  $\textsf{\textbf{B}}$;
and
hence also with the associated polynomial
$q_j(\textsf{\textbf{A}})=\textsf{\textbf{E}}_j$,
as shown in (\ref{2012-m-ch-fdvs-cc}).
\eproof
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If
$\textsf{\textbf{E}}_{\bf x} = \vert {\bf x} \rangle \langle {\bf x} \vert$
and
$\textsf{\textbf{E}}_{\bf y} = \vert {\bf y} \rangle \langle {\bf y} \vert$
are two commuting projections (into one-dimensional subspaces of ${\frak V}$)
corresponding to the normalized vectors ${\bf x}$  and ${\bf y}$,
respectively; that is, if
$\left[
\textsf{\textbf{E}}_{\bf x}
,
\textsf{\textbf{E}}_{\bf y}
\right]=
\textsf{\textbf{E}}_{\bf x}
\textsf{\textbf{E}}_{\bf y}
-
\textsf{\textbf{E}}_{\bf y}
\textsf{\textbf{E}}_{\bf x}
=0$,
then they are either identical (the vectors are collinear) or orthogonal (the vectors ${\bf x}$ is orthogonal to ${\bf y}$).


{\color{OliveGreen}\bproof
For a proof,
note that if $\textsf{\textbf{E}}_{\bf x}$ and $\textsf{\textbf{E}}_{\bf y}$ commute, then
$\textsf{\textbf{E}}_{\bf x}\textsf{\textbf{E}}_{\bf y} =  \textsf{\textbf{E}}_{\bf y}    \textsf{\textbf{E}}_{\bf x}$; and
hence $
\vert {\bf x} \rangle \langle {\bf x} \vert {\bf y} \rangle \langle {\bf y} \vert
=
\vert {\bf y} \rangle \langle {\bf y} \vert {\bf x} \rangle \langle {\bf x} \vert $. Thus,
$
(
\langle {\bf x} \vert {\bf y} \rangle )\vert {\bf x} \rangle  \langle {\bf y}  \vert
=
(\overline{\langle {\bf x} \vert {\bf y} \rangle})
\vert {\bf y} \rangle  \langle {\bf x} \vert  ,
$
which, applied to arbitrary vectors $ \vert   v\rangle \in {\frak V}$, is only true if either ${\bf x} = \pm {\bf y}$,
or if ${\bf x} \perp {\bf y}$ (and thus $\langle {\bf x} \vert {\bf y} \rangle =0$).
\eproof
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A set $\textsf{\textbf{M}}
=
\{
\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\}
$
of  self-adjoint transformations on a finite-dimensional inner product space
are mutually commuting if and only if there exists
a self-adjoint transformation  $\textsf{\textbf{R}}$ and
a set of real-valued functions
$F
=
\{
f_1,
f_2,
\ldots ,
f_k
\}
$ of a real variable so that
$
\textsf{\textbf{A}}_1=f_1 (\textsf{\textbf{R}})
$,
$
\textsf{\textbf{A}}_2=f_2 (\textsf{\textbf{R}})
$,
$\ldots $,
$\textsf{\textbf{A}}_k=f_k (\textsf{\textbf{R}})$.
If such a {\em maximal operator} $\textsf{\textbf{R}}$ exists, then
it can be written as a function of all transformations in the set $\textsf{\textbf{M}}$; that is,
$\textsf{\textbf{R}}=G(\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k)$,
where $G$ is a suitable real-valued function of $n$ variables
(cf. Ref. \cite{v-neumann-31}, Satz 8).
\index{maximal operator}

The  maximal operator $\textsf{\textbf{R}}$ can be interpreted as
encoding or containing all the information of a collection of commuting operators at once;
stated pointedly, rather than consider all the  operators in $\textsf{\textbf{M}}$
separately,
the  maximal operator  $\textsf{\textbf{R}}$  represents  $\textsf{\textbf{M}}$;
in a sense, the operators  $\textsf{\textbf{A}}_i \in \textsf{\textbf{M}}$
are all just incomplete {\em aspects}  of,
or individual ``lossy'' (i.e., one-to-many) functional views on, the  maximal operator $\textsf{\textbf{R}}$.

\newcommand{\comment}[1]{}
\comment{


a = {{0, 1, 0}, {1, 0, 0}, {0, 0, 0}};
b = {{2, 3, 0}, {3, 2, 0}, {0, 0, 0}};
c = {{5, 7, 0}, {7, 5, 0}, {0, 0, 11}};

Commutator[x_,y_]=x.y-y.x;

MatrixForm[Commutator[a, b]]
MatrixForm[Commutator[a, c]]
MatrixForm[Commutator[b, c]]


Eigensystem[a]
Eigensystem[b]
Eigensystem[c]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};

Solve[a.{{x11, x12,x13},
{x21, x22,x23},
{x31, x32,x33}}-{{x11, x12,x13},
{x21, x22,x23},
{x31, x32,x33}}.a ==  {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}},{x11, x12,x13,x21, x22,x23,x31, x32,x33}]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};
b = {{0, 0, 2}, {0, 0, 0}, {0, 0, 0}};

Solve[{a.{{x11, x12, x13}, {x21, x22, x23}, {x31, x32, x33}} - {{x11,
       x12, x13}, {x21, x22, x23}, {x31, x32, x33}}.a == {{0, 0,
     0}, {0, 0, 0}, {0, 0, 0}},
  b.{{x11, x12, x13}, {x21, x22, x23}, {x31, x32, x33}} - {{x11, x12,
       x13}, {x21, x22, x23}, {x31, x32, x33}}.b == {{0, 0, 0}, {0, 0,
      0}, {0, 0, 0}}}, {x11, x12, x13, x21, x22, x23, x31, x32, x33}]

a = {{0, 1, 0}, {0, 0, 0}, {0, 0, 0}};
b = {{0, 0, 2}, {0, 0, 0}, {0, 0, 0}};
c = {{3, 5, 7}, {0, 3, 0}, {0, 0, 3}};

Commutator[x_,y_]=x.y-y.x;

MatrixForm[Commutator[a, b]]
MatrixForm[Commutator[a, c]]
MatrixForm[Commutator[b, c]]
}


{\color{blue}
\bexample
Let us demonstrate the machinery developed so far by an example.
Consider the normal matrices
$$
\textsf{\textbf{A}} = %\left(
\begin{pmatrix}
0& 1& 0\\ 1& 0& 0\\ 0& 0& 0
\end{pmatrix},\;
\textsf{\textbf{B}} = %\left(
\begin{pmatrix}
2& 3& 0\\ 3& 2& 0\\ 0& 0& 0
\end{pmatrix},\;
\textsf{\textbf{C}} = %\left(
\begin{pmatrix}
5& 7& 0\\ 7& 5& 0\\ 0& 0& 11
\end{pmatrix},
$$
which are mutually commutative; that is,
$
[\textsf{\textbf{A}}, \textsf{\textbf{B}}]=
\textsf{\textbf{A}} \textsf{\textbf{B}}-\textsf{\textbf{B}}\textsf{\textbf{A}}=
[\textsf{\textbf{A}}, \textsf{\textbf{C}}]=
\textsf{\textbf{A}} \textsf{\textbf{C}}-\textsf{\textbf{B}}\textsf{\textbf{C}}=
[\textsf{\textbf{B}}, \textsf{\textbf{C}}]=
\textsf{\textbf{B}} \textsf{\textbf{C}}-\textsf{\textbf{C}}\textsf{\textbf{B}}=0$.

The eigensystems -- that is, the set of the set of eigenvalues and the set of the associated eigenvectors -- of $\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$
and
$\textsf{\textbf{C}}$
are
\begin{equation}
\begin{split}
\{\{1,-1,  0\}, \{(1, 1, 0)^T, (-1, 1, 0)^T, (0, 0, 1)^T\}\} ,\\
\{\{5, -1, 0\},  \{(1, 1, 0)^T, (-1, 1, 0)^T, (0, 0, 1)^T\}\},\\
\{\{12, -2, 11\},  \{(1, 1, 0)^T, (-1, 1, 0)^T, (0, 0, 1)^T\}\}.
\end{split}
\end{equation}
They share a common orthonormal set of eigenvectors
$$
\left\{
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1\\ 1\\ 0
\end{pmatrix},
\frac{1}{\sqrt{2}}
\begin{pmatrix}
-1\\ 1\\ 0
\end{pmatrix},
\begin{pmatrix}
0\\ 0\\ 1\end{pmatrix}
\right\}
$$
which form an orthonormal basis of ${\Bbb R}^3$ or ${\Bbb C}^3$.
The associated projections are obtained by the outer (dyadic or tensor) products
\index{outer product}
\index{dyadic product}
\index{tensor product}
of these vectors; that is,
\begin{equation}
\begin{split}
\textsf{\textbf{E}}_1= \frac{1}{2}
\begin{pmatrix}
1& 1& 0\\
1& 1& 0\\
0& 0& 0
\end{pmatrix},\\
\textsf{\textbf{E}}_2= \frac{1}{2}
\begin{pmatrix}
1& -1& 0\\
-1& 1& 0\\
0& 0& 0
\end{pmatrix},\\
\textsf{\textbf{E}}_3=
\begin{pmatrix}
0& 0& 0\\
0& 0& 0\\
0& 0& 1
\end{pmatrix}.
\end{split}
\end{equation}
Thus the spectral decompositions of
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$  and
$\textsf{\textbf{C}}$ are
\begin{equation}
\begin{split}
\textsf{\textbf{A}}= \textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0  \textsf{\textbf{E}}_3,\\
\textsf{\textbf{B}}= 5\textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0 \textsf{\textbf{E}}_3,\\
\textsf{\textbf{C}}= 12\textsf{\textbf{E}}_1  -2 \textsf{\textbf{E}}_2  + 11\textsf{\textbf{E}}_3,
\end{split}
\label{2011-m-empc}
\end{equation}
respectively.


One way to define the  maximal operator  $\textsf{\textbf{R}}$ for this problem
would be
$$
\textsf{\textbf{R}} = \alpha \textsf{\textbf{E}}_1  + \beta \textsf{\textbf{E}}_2  + \gamma  \textsf{\textbf{E}}_3,
$$
with
$\alpha ,  \beta ,   \gamma \in {\Bbb R}-0$ and
$\alpha  \neq \beta  \neq   \gamma \neq \alpha  $.
The functional coordinates
$f_i(\alpha )$, $f_i(\beta)$, and $f_i(\gamma)$,
$i\in \{\textsf{\textbf{A}},\textsf{\textbf{B}},\textsf{\textbf{C}}\}$,  of the three functions
$ f_\textsf{\textbf{A}}(\textsf{\textbf{R}})$,
$ f_\textsf{\textbf{B}}(\textsf{\textbf{R}})$, and
$ f_\textsf{\textbf{C}}(\textsf{\textbf{R}})$
chosen to match the projection coefficients obtained in Eq. (\ref{2011-m-empc});
that is,
\begin{equation}
\begin{split}
\textsf{\textbf{A}}= f_\textsf{\textbf{A}}(\textsf{\textbf{R}})=  \textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0  \textsf{\textbf{E}}_3,\\
\textsf{\textbf{B}}=  f_\textsf{\textbf{B}}(\textsf{\textbf{R}})= 5\textsf{\textbf{E}}_1  - \textsf{\textbf{E}}_2  + 0 \textsf{\textbf{E}}_3,\\
\textsf{\textbf{C}}=  f_\textsf{\textbf{C}}(\textsf{\textbf{R}})= 12\textsf{\textbf{E}}_1  -2 \textsf{\textbf{E}}_2  + 11\textsf{\textbf{E}}_3.
\end{split}
\end{equation}
As a consequence, the functions
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$,
$\textsf{\textbf{C}}$ need to satisfy the relations
\begin{equation}
\begin{split}
f_\textsf{\textbf{A}}(\alpha ) =1,\; f_\textsf{\textbf{A}}(\beta ) =-1,\; f_\textsf{\textbf{A}}(\gamma ) =0,\\
f_\textsf{\textbf{B}}(\alpha ) =5,\; f_\textsf{\textbf{B}}(\beta ) =-1,\; f_\textsf{\textbf{B}}(\gamma ) =0,\\
f_\textsf{\textbf{C}}(\alpha ) =12,\; f_\textsf{\textbf{C}}(\beta ) =-2,\; f_\textsf{\textbf{C}}(\gamma ) =11.
\end{split}
\label{2011-m-empc-fsr}
\end{equation}

\eexample
}

It is no coincidence that the projections in the spectral forms of
$\textsf{\textbf{A}}$,
$\textsf{\textbf{B}}$  and
$\textsf{\textbf{C}}$ are identical.
Indeed it can be shown that mutually commuting {\em normal operators} always share the same eigenvectors; and thus also the same projections.
\index{normal operator}
\index{normal transformation}

Let the set $\textsf{\textbf{M}}
=
\{
\textsf{\textbf{A}}_1,
\textsf{\textbf{A}}_2,
\ldots ,
\textsf{\textbf{A}}_k
\}
$
be mutually commuting  normal (or Hermitian, or self-adjoint) transformations on an $n$-dimensional inner product space.
Then there exists an orthonormal basis
${\frak B}= \{
{\bf f}_1,
\ldots ,
{\bf f}_n\}$
such that every ${\bf f}_j \in {\frak B}$  is an eigenvector  of each of the $\textsf{\textbf{A}}_i \in  \textsf{\textbf{M}}$.
Equivalently, there exist $n$ orthogonal projections  (let the vectors ${\bf f}_j$ be represented by the coordinates which are column vectors)
$\textsf{\textbf{E}}_j= {\bf f}_j\otimes {\bf f}_j^T$
such that every $\textsf{\textbf{E}}_j$, $1\le j\le n$ occurs in the spectral form of each of the $\textsf{\textbf{A}}_i \in  \textsf{\textbf{M}}$.


Informally speaking,
a ``generic'' maximal operator $\textsf{\textbf{R}}$ on an $n$-dimensional Hilbert space ${\frak V}$
can be interpreted as some orthonormal basis
$\{{\bf f}_1,{\bf f}_2,\ldots ,{\bf f}_n\}$ of ${\frak V}$
-- indeed, the $n$ elements of that basis would have to correspond to the projections occurring
in the spectral decomposition of the self-adjoint operators
generated by $\textsf{\textbf{R}}$.

{\color{Purple}
Likewise, the ``maximal knowledge'' about a quantized physical system -- in terms of empirical operational quantities --
would correspond to such a single maximal operator;
or to the orthonormal basis corresponding to the spectral decomposition of it.
Thus it might not be unreasonable to speculate that a particular (pure) physical state is best characterized by a particular orthonomal basis.
}


\section{Measures on closed subspaces}

In what follows we shall assume that all {\em (probability) measures}
or {\em states} $w$
\index{probability measures}
\index{measures}
\index{states}
behave quasi-classically on sets of mutually commuting self-adjoint operators,
and in particular on orthogonal projections.

Suppose
 $\textsf{\textbf{E}}
=
\{
\textsf{\textbf{E}}_1,
\textsf{\textbf{E}}_2,
\ldots ,
\textsf{\textbf{E}}_n
\}
$
is a set of mutually commuting orthogonal projections
on a finite-dimensional inner product space   ${\frak V}$.
Then, the probability measure $w$ should be {\em additive}; that is,
\begin{equation}
w(\textsf{\textbf{E}}_1+\textsf{\textbf{E}}_2\cdots +\textsf{\textbf{E}}_n)=
w(\textsf{\textbf{E}}_1)+
w(\textsf{\textbf{E}}_2)+
\cdots +
w(\textsf{\textbf{E}}_n).
\end{equation}

Stated differently, we shall assume that,
for any two orthogonal projections $ \textsf{\textbf{E}},\textsf{\textbf{F}}$
\index{orthogonal projection}
so that
 $ \textsf{\textbf{E}}\textsf{\textbf{F}}= \textsf{\textbf{F}}\textsf{\textbf{E}}=0$,
their sum
 $  \textsf{\textbf{G}} =\textsf{\textbf{E}}+\textsf{\textbf{F}}$
has expectation value
\begin{equation}
\langle \textsf{\textbf{G}}\rangle =
\langle \textsf{\textbf{E}} \rangle +
\langle \textsf{\textbf{F}} \rangle.
\end{equation}

We shall consider only vector spaces of dimension three or greater, since only in these cases two  orthonormal bases can be interlinked by a common vector -- in two dimensions,
distinct orthonormal bases contain distinct basis vectors.

\subsection{Gleason's theorem}
\index{Gleason's theorem}
\label{Gleasontheorem}
Suppose again the additivity of probabilites of mutually commuting (co-measurable) perpendicular projections.
Then, for a Hilbert space of dimension three or greater,
the only possible form of the  expectation value
of an self-adjoint operator  $\textsf{\textbf{A}}$
has the form
\cite{Gleason,r:dvur-93,pitowsky:218,rich-bridge,peres,hamhalter-book}
\begin{equation}
\langle
\textsf{\textbf{A}}
\rangle
=
\textrm{Tr}({  \rho} \textsf{\textbf{A}}),
\end{equation}
the trace of the operator product of the  density matrix (which is a positive operator of the trace class)
${  \rho}$
for the system with the matrix representation of $\textsf{\textbf{A}}$.

In particular, if $\textsf{\textbf{A}}$ is a projection $\textsf{\textbf{E}}$ corresponding to an elementary yes-no proposition
{\it ``the system has property Q,''} then $\langle \textsf{\textbf{E}}\rangle = \textrm{Tr}({  \rho}  \textsf{\textbf{E}})$ corresponds
to the probability of that property $Q$ if the system is in state ${  \rho}$.




\subsection{Kochen-Specker theorem}
\index{Kochen-Specker theorem}
\label{2011-m-KST}

For a Hilbert space of dimension three or greater,
there does not exist any
two-valued probability measures interpretable as consistent, overall truth assignment
\cite{specker-60,kochen1}.
As a result of the nonexistence of two-valued states, the classical strategy
to construct probabilities by a convex combination of all two-valued states fails entirely.

In  {\em Greechie diagram} \cite{greechie:71},
points  represent basis vectors.
\index{Greechie diagram}
If they belong to the same basis, they are connected by  smooth curves.

\begin{center}
%TeXCAD Picture [1.pic]. Options:
%\grade{\on}
%\emlines{\off}
%\epic{\off}
%\beziermacro{\on}
%\reduce{\on}
%\snapping{\off}
%\quality{8.000}
%\graddiff{0.010}
%\snapasp{1}
%\zoom{5.6569}
\unitlength .6mm % = 1.423pt
%\allinethickness{2pt}
 \thicklines %\linethickness{0.8pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi % GNUPLOT compatibility
\begin{picture}(134.09,125.99)(0,0)

%\emline(86.39,101.96)(111.39,58.46)
\multiput(86.39,101.96)(.119617225,-.208133971){209}{{\color{green}\line(0,-1){.208133971}}}
%\end
%\emline(86.39,14.96)(111.39,58.46)
\multiput(86.39,14.96)(.119617225,.208133971){209}{{\color{red}\line(0,1){.208133971}}}
%\end
%\emline(36.47,101.96)(11.47,58.46)
\multiput(36.47,101.96)(-.119617225,-.208133971){209}{{\color{yellow}\line(0,-1){.208133971}}}
%\end
%\emline(36.47,14.96)(11.47,58.46)
\multiput(36.47,14.96)(-.119617225,.208133971){209}{{\color{magenta}\line(0,1){.208133971}}}
%\end
\color{blue}\put(86.39,15.21){\color{blue}\line(-1,0){50}}
\put(86.39,101.71){\color{violet}\line(-1,0){50}}
%
\put(36.34,15.16){\color{magenta}\circle{6}}
\put(36.34,15.16){\color{blue}\circle{4}}
\put(52.99,15.16){\color{blue}\circle{4}}
\put(52.99,15.16){\color{cyan}\circle{6}}
\put(69.68,15.16){\color{blue}\circle{4}}
\put(69.68,15.16){\color{orange}\circle{6}}
\put(86.28,15.16){\color{blue}\circle{4}}
\put(86.28,15.16){\color{red}\circle{6}}
%
\put(93.53,27.71){\color{red}\circle{4}}
\put(93.53,27.71){\color{orange}\circle{6}}
\put(102.37,43.44){\color{red}\circle{4}}
\put(102.37,43.44){\color{olive}\circle{6}}
\put(111.21,58.45){\color{red}\circle{4}}
\color{green}\put(111.21,58.45){\circle{6}}
%
\put(102.37,73.47){\color{green}\circle{4}}
\put(102.37,73.47){\color{olive}\circle{6}}
\put(93.53,89.21){\color{green}\circle{4}}
\put(93.53,89.21){\color{cyan}\circle{6}}
\put(86.28,101.76){\color{green}\circle{4}}
\put(86.28,101.76){\color{violet}\circle{6}}
%
\put(69.68,101.76){\color{violet}\circle{4}}
\put(69.68,101.76){\color{cyan}\circle{6}}
\put(52.99,101.76){\color{violet}\circle{4}}
\put(52.99,101.76){\color{orange}\circle{6}}
\put(36.34,101.76){\color{violet}\circle{4}}
\put(36.34,101.76){\color{yellow}\circle{6}}
%
\put(29.24,89.21){\color{yellow}\circle{4}}
\put(29.24,89.21){\color{orange}\circle{6}}
\put(20.4,73.47){\color{yellow}\circle{4}}
\put(20.4,73.47){\color{olive}\circle{6}}
\put(11.56,58.45){\color{yellow}\circle{4}}
\put(11.56,58.45){\color{magenta}\circle{6}}

\put(20.4,43.44){\color{magenta}\circle{4}}
\put(20.4,43.44){\color{olive}\circle{6}}
\put(29.24,27.71){\color{magenta}\circle{4}}
\put(29.24,27.71){\color{cyan}\circle{6}}

\color{cyan}
\qbezier(29.2,27.73)(23.55,-5.86)(52.99,15.24)
\qbezier(29.2,27.88)(36.93,75)(69.63,101.91)
\qbezier(52.69,15.24)(87.47,40.96)(93.72,89.27)
\qbezier(93.72,89.27)(98.4,125.99)(69.49,102.06)
\color{orange}
\qbezier(93.57,27.73)(99.22,-5.86)(69.78,15.24)
\qbezier(93.57,27.88)(85.84,75)(53.13,101.91)
\qbezier(70.08,15.24)(35.3,40.96)(29.05,89.27)
\qbezier(29.05,89.27)(24.37,125.99)(53.28,102.06)
\color{olive}
\qbezier(20.15,73.72)(-11.67,58.52)(20.15,43.31)
\qbezier(20.33,73.72)(61.34,93.16)(102.36,73.72)
\qbezier(102.36,73.72)(134.09,58.52)(102.53,43.31)
\qbezier(102.53,43.31)(60.99,23.43)(20.15,43.49)
{\color{black}
\put(30.41,114.02){\makebox(0,0)[cc]{$M$}}
\put(30.41,2.65){\makebox(0,0)[cc]{$A$}}
\put(52.68,114.38){\makebox(0,0)[cc]{$L$}}
\put(52.68,2.3){\makebox(0,0)[cc]{$B$}}
\put(91.93,114.2){\makebox(0,0)[cc]{$J$}}
\put(91.93,2.48){\makebox(0,0)[cc]{$D$}}
\put(69.65,114.38){\makebox(0,0)[cc]{$K$}}
\put(73.65,2.3){\makebox(0,0)[cc]{$C$}}
\put(103.24,94.22){\makebox(0,0)[cc]{$I$}}
\put(17.45,94.22){\makebox(0,0)[cc]{$ N$}}
\put(106.24,22.45){\makebox(0,0)[cc]{$E$}}
\put(17.45,22.45){\makebox(0,0)[cc]{$ R$}}
\put(115.13,77.96){\makebox(0,0)[cc]{$H$}}
\put(8.55,77.96){\makebox(0,0)[cc]{$ O$}}
\put(115.13,38.72){\makebox(0,0)[cc]{$F$}}
\put(10.55,38.72){\makebox(0,0)[cc]{$ Q$}}
\put(120.92,57.98){\makebox(0,0)[l]{$ G$}}
\put(1.77,57.98){\makebox(0,0)[rc]{$  P$}}
}
\put(61.341,9.192){\color{blue}\makebox(0,0)[cc]{$a$}}
\put(102.883,35.355){\color{red}\makebox(0,0)[cc]{$b$}}
\put(102.53,84.322){\color{green}\makebox(0,0)[cc]{$c$}}
\put(60.457,108.01){\color{violet}\makebox(0,0)[cc]{$d$}}
\put(18.031,84.145){\color{yellow}\makebox(0,0)[cc]{$e$}}
\put(18.561,33.057){\color{magenta}\makebox(0,0)[cc]{$f$}}
\put(61.341,39.774){\color{olive}\makebox(0,0)[cc]{$g$}}
\put(72.124,67.882){\color{orange}\makebox(0,0)[cc]{$h$}}
\put(48.79,67.705){\color{cyan}\makebox(0,0)[cc]{$i$}}
\end{picture}
\end{center}
The most compact way of deriving the Kochen-Specker theorem in four dimensions has been given by Cabello \cite{cabello-96,cabello-99}.
For the sake of demonstration, consider a Greechie (orthogonality) diagram of a finite subset of the continuum of blocks or contexts embeddable in
four-dimensional real Hilbert space without a two-valued probability measure
The proof of the Kochen-Specker theorem  uses  nine tightly interconnected contexts
$\color{blue}a=\{A,B,C,D\}$,
$\color{red}b=\{D,E,F,G\}$,
$\color{green}c=\{G,H,I,J\}$,
$\color{violet}d=\{J,K,L,M\}$,
$\color{yellow}e=\{M,N,O,P\}$,
$\color{magenta}f=\{P,Q,R,A\}$,
$\color{orange}g=\{B,I,K,R\}$,
$\color{olive}h=\{C,E,L,N\}$,
$\color{cyan}i=\{F,H,O,Q\}$
consisting of the 18 projections associated with the one dimensional subspaces spanned by  the vectors from the origin $(0,0,0,0)$ to
$ A=(0,0,1,-1)    $,
$ B=(1,-1,0,0)    $,
$ C=(1,1,-1,-1)   $,
$ D=(1,1,1,1)     $,
$  E=(1,-1,1,-1)  $,
$  F=(1,0,-1,0)   $,
$  G=(0,1,0,-1)   $,
$  H=(1,0,1,0)    $,
$  I=(1,1,-1,1)   $,
$ J=(-1,1,1,1)    $,
$ K=(1,1,1,-1)    $,
$ L=(1,0,0,1)     $,
$ M=(0,1,-1,0)    $,
$  N=(0,1,1,0)    $,
$  O=(0,0,0,1)    $,
$  P=(1,0,0,0)    $,
$  Q=(0,1,0,0)    $,
$  R=(0,0,1,1)    $, respectively.
%
Greechie diagrams represent atoms by points, and  contexts by maximal smooth, unbroken curves.

{\color{OliveGreen}\bproof
In a proof by contradiction,note that, on the one hand, every observable proposition occurs in exactly {\em two} contexts.
Thus, in an enumeration of the four observable propositions of each of the nine contexts,
there appears to be an {\em even} number of true propositions,
provided that the value of an observable does not depend on the context (i.e. the assignment is {\em noncontextual}).
Yet, on the other hand, as there is an {\em odd} number (actually nine) of contexts,
there should be an {\em odd} number (actually nine) of true propositions.
\bproof
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{center}
{\color{olive}   \Huge
%\decofourright
 %\decofourright \decofourleft
%\aldine X \decoone c
%\floweroneright
% \aldineleft ] \decosix g \leafleft
% \aldineright Y \decothreeleft f \leafNE
% \aldinesmall Z \decothreeright h \leafright
% \decofourleft a \decotwo d \starredbullet
%\decofourright
 \floweroneleft
}
\end{center}


